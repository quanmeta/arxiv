<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2024-09-25T00:00:00Z">2024-09-25</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">92</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Molmo and PixMo: Open Weights and Open Data for State-of-the-Art
  Multimodal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17146v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17146v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, Jiasen Lu, Taira Anderson, Erin Bransom, Kiana Ehsani, Huong Ngo, YenSung Chen, Ajay Patel, Mark Yatskar, Chris Callison-Burch, Andrew Head, Rose Hendrix, Favyen Bastani, Eli VanderBilt, Nathan Lambert, Yvonne Chou, Arnavi Chheda, Jenna Sparks, Sam Skjonsberg, Michael Schmitz, Aaron Sarnat, Byron Bischoff, Pete Walsh, Chris Newell, Piper Wolters, Tanmay Gupta, Kuo-Hao Zeng, Jon Borchardt, Dirk Groeneveld, Jen Dumas, Crystal Nam, Sophie Lebrecht, Caitlin Wittlif, Carissa Schoenick, Oscar Michel, Ranjay Krishna, Luca Weihs, Noah A. Smith, Hannaneh Hajishirzi, Ross Girshick, Ali Farhadi, Aniruddha Kembhavi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Today's most advanced multimodal models remain proprietary. The strongest
open-weight models rely heavily on synthetic data from proprietary VLMs to
achieve good performance, effectively distilling these closed models into open
ones. As a result, the community is still missing foundational knowledge about
how to build performant VLMs from scratch. We present Molmo, a new family of
VLMs that are state-of-the-art in their class of openness. Our key innovation
is a novel, highly detailed image caption dataset collected entirely from human
annotators using speech-based descriptions. To enable a wide array of user
interactions, we also introduce a diverse dataset mixture for fine-tuning that
includes in-the-wild Q&A and innovative 2D pointing data. The success of our
approach relies on careful choices for the model architecture details, a
well-tuned training pipeline, and, most critically, the quality of our newly
collected datasets, all of which will be released. The best-in-class 72B model
within the Molmo family not only outperforms others in the class of open weight
and data models but also compares favorably against proprietary systems like
GPT-4o, Claude 3.5, and Gemini 1.5 on both academic benchmarks and human
evaluation.
  We will be releasing all of our model weights, captioning and fine-tuning
data, and source code in the near future. Select model weights, inference code,
and demo are available at https://molmo.allenai.org.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FineZip : Pushing the Limits of Large Language Models for Practical
  Lossless Text Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17141v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17141v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fazal Mittu, Yihuan Bu, Akshat Gupta, Ashok Devireddy, Alp Eren Ozdarendeli, Anant Singh, Gopala Anumanchipalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While the language modeling objective has been shown to be deeply connected
with compression, it is surprising that modern LLMs are not employed in
practical text compression systems. In this paper, we provide an in-depth
analysis of neural network and transformer-based compression techniques to
answer this question. We compare traditional text compression systems with
neural network and LLM-based text compression methods. Although LLM-based
systems significantly outperform conventional compression methods, they are
highly impractical. Specifically, LLMZip, a recent text compression system
using Llama3-8B requires 9.5 days to compress just 10 MB of text, although with
huge improvements in compression ratios. To overcome this, we present FineZip -
a novel LLM-based text compression system that combines ideas of online
memorization and dynamic context to reduce the compression time immensely.
FineZip can compress the above corpus in approximately 4 hours compared to 9.5
days, a 54 times improvement over LLMZip and comparable performance. FineZip
outperforms traditional algorithmic compression methods with a large margin,
improving compression ratios by approximately 50\%. With this work, we take the
first step towards making lossless text compression with LLMs a reality. While
FineZip presents a significant step in that direction, LLMs are still not a
viable solution for large-scale text compression. We hope our work paves the
way for future research and innovation to solve this problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Assessing the Level of Toxicity Against Distinct Groups in Bangla Social
  Media Comments: A Comprehensive Investigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17130v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17130v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mukaffi Bin Moin, Pronay Debnath, Usafa Akther Rifa, Rijeet Bin Anis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social media platforms have a vital role in the modern world, serving as
conduits for communication, the exchange of ideas, and the establishment of
networks. However, the misuse of these platforms through toxic comments, which
can range from offensive remarks to hate speech, is a concerning issue. This
study focuses on identifying toxic comments in the Bengali language targeting
three specific groups: transgender people, indigenous people, and migrant
people, from multiple social media sources. The study delves into the intricate
process of identifying and categorizing toxic language while considering the
varying degrees of toxicity: high, medium, and low. The methodology involves
creating a dataset, manual annotation, and employing pre-trained transformer
models like Bangla-BERT, bangla-bert-base, distil-BERT, and
Bert-base-multilingual-cased for classification. Diverse assessment metrics
such as accuracy, recall, precision, and F1-score are employed to evaluate the
model's effectiveness. The experimental findings reveal that Bangla-BERT
surpasses alternative models, achieving an F1-score of 0.8903. This research
exposes the complexity of toxicity in Bangla social media dialogues, revealing
its differing impacts on diverse demographic groups.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in "18th International Conference on
  Information Technology and Applications (ICITA 2024)"</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning and Machine Learning, Advancing Big Data Analytics and
  Management: Handy Appetizer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17120v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17120v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benji Peng, Xuanhe Pan, Yizhu Wen, Ziqian Bi, Keyu Chen, Ming Li, Ming Liu, Qian Niu, Junyu Liu, Jinlang Wang, Sen Zhang, Jiawei Xu, Pohsun Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This book explores the role of Artificial Intelligence (AI), Machine Learning
(ML), and Deep Learning (DL) in driving the progress of big data analytics and
management. The book focuses on simplifying the complex mathematical concepts
behind deep learning, offering intuitive visualizations and practical case
studies to help readers understand how neural networks and technologies like
Convolutional Neural Networks (CNNs) work. It introduces several classic models
and technologies such as Transformers, GPT, ResNet, BERT, and YOLO,
highlighting their applications in fields like natural language processing,
image recognition, and autonomous driving. The book also emphasizes the
importance of pre-trained models and how they can enhance model performance and
accuracy, with instructions on how to apply these models in various real-world
scenarios. Additionally, it provides an overview of key big data management
technologies like SQL and NoSQL databases, as well as distributed computing
frameworks such as Apache Hadoop and Spark, explaining their importance in
managing and processing vast amounts of data. Ultimately, the book underscores
the value of mastering deep learning and big data management skills as critical
tools for the future workforce, making it an essential resource for both
beginners and experienced professionals.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This book contains 93 pages and 60 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Programming Every Example: Lifting <span class="highlight-title">Pre-train</span>ing Data Quality like
  Experts at Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17115v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17115v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Zhou, Zengzhi Wang, Qian Liu, Junlong Li, Pengfei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language model pre-training has traditionally relied on human experts
to craft heuristics for improving the corpora quality, resulting in numerous
rules developed to date. However, these rules lack the flexibility to address
the unique characteristics of individual example effectively. Meanwhile,
applying tailored rules to every example is impractical for human experts. In
this paper, we demonstrate that even small language models, with as few as 0.3B
parameters, can exhibit substantial data refining capabilities comparable to
those of human experts. We introduce Programming Every Example (ProX), a novel
framework that treats data refinement as a programming task, enabling models to
refine corpora by generating and executing fine-grained operations, such as
string normalization, for each individual example at scale. Experimental
results show that models pre-trained on ProX-curated data outperform either
original data or data filtered by other selection methods by more than 2%
across various downstream benchmarks. Its effectiveness spans various model
sizes and pre-training corpora, including C4, RedPajama-V2, and FineWeb.
Furthermore, ProX exhibits significant potential in domain-specific continual
pre-training: without domain specific design, models trained on OpenWebMath
refined by ProX outperform human-crafted rule-based methods, improving average
accuracy by 7.6% over Mistral-7B, with 14.6% for Llama-2-7B and 20.3% for
CodeLlama-7B, all within 10B tokens to be comparable to models like Llemma-7B
trained on 200B tokens. Further analysis highlights that ProX significantly
saves training FLOPs, offering a promising path for efficient LLM
pre-training.We are open-sourcing ProX with >100B corpus, models, and sharing
all training and implementation details for reproducible research and future
innovation. Code: https://github.com/GAIR-NLP/ProX
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>45 pages, 13 figures, 34 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Vision Language Models Learn from Visual Demonstrations of Ambiguous
  Spatial Reasoning? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17080v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17080v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Zhao, Leo Parker Dirac, Paulina Varshavskaya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large vision-language models (VLMs) have become state-of-the-art for many
computer vision tasks, with in-context learning (ICL) as a popular adaptation
strategy for new ones. But can VLMs learn novel concepts purely from visual
demonstrations, or are they limited to adapting to the output format of ICL
examples? We propose a new benchmark we call Spatial Visual Ambiguity Tasks
(SVAT) that challenges state-of-the-art VLMs to learn new visuospatial tasks
in-context. We find that VLMs fail to do this zero-shot, and sometimes continue
to fail after finetuning. However, adding simpler data to the training by
curriculum learning leads to improved ICL performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures. Code released at
  https://github.com/groundlight/vlm-visual-demonstrations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Post-Hoc Attributions in Long Document Comprehension via
  Coarse Grained Answer Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17073v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17073v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pritika Ramu, Koustava Goswami, Apoorv Saxena, Balaji Vasan Srinivavsan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately attributing answer text to its source document is crucial for
developing a reliable question-answering system. However, attribution for long
documents remains largely unexplored. Post-hoc attribution systems are designed
to map answer text back to the source document, yet the granularity of this
mapping has not been addressed. Furthermore, a critical question arises: What
precisely should be attributed, with an emphasis on identifying the information
units within an answer that necessitate grounding? In this paper, we propose
and investigate a novel approach to the factual decomposition of generated
answers for attribution, employing template-based in-context learning. To
accomplish this, we utilize the question and integrate negative sampling during
few-shot in-context learning for decomposition. This approach enhances the
semantic understanding of both abstractive and extractive answers. We examine
the impact of answer decomposition by providing a thorough examination of
various attribution approaches, ranging from retrieval-based techniques to
LLM-based attributors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using LLM for Real-Time Transcription and Summarization of
  Doctor-Patient Interactions into ePuskesmas in Indonesia 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17054v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17054v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Azmul Asmar Irfan, Nur Ahmad Khatim, Mansur M. Arief
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the key issues contributing to inefficiency in Puskesmas is the
time-consuming nature of doctor-patient interactions. Doctors need to conduct
thorough consultations, which include diagnosing the patient's condition,
providing treatment advice, and transcribing detailed notes into medical
records. In regions with diverse linguistic backgrounds, doctors often have to
ask clarifying questions, further prolonging the process. While diagnosing is
essential, transcription and summarization can often be automated using AI to
improve time efficiency and help doctors enhance care quality and enable early
diagnosis and intervention. This paper proposes a solution using a localized
large language model (LLM) to transcribe, translate, and summarize
doctor-patient conversations. We utilize the Whisper model for transcription
and GPT-3 to summarize them into the ePuskemas medical records format. This
system is implemented as an add-on to an existing web browser extension,
allowing doctors to fill out patient forms while talking. By leveraging this
solution for real-time transcription, translation, and summarization, doctors
can improve the turnaround time for patient care while enhancing the quality of
records, which become more detailed and insightful for future visits. This
innovation addresses challenges like overcrowded facilities and the
administrative burden on healthcare providers in Indonesia. We believe this
solution will help doctors save time, provide better care, and produce more
accurate medical records, representing a significant step toward modernizing
healthcare and ensuring patients receive timely, high-quality care, even in
resource-constrained settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detecting Temporal Ambiguity in Questions <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17046v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17046v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhawna Piryani, Abdelrahman Abdallah, Jamshid Mozafari, Adam Jatowt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting and answering ambiguous questions has been a challenging task in
open-domain question answering. Ambiguous questions have different answers
depending on their interpretation and can take diverse forms. Temporally
ambiguous questions are one of the most common types of such questions. In this
paper, we introduce TEMPAMBIQA, a manually annotated temporally ambiguous QA
dataset consisting of 8,162 open-domain questions derived from existing
datasets. Our annotations focus on capturing temporal ambiguity to study the
task of detecting temporally ambiguous questions. We propose a novel approach
by using diverse search strategies based on disambiguated versions of the
questions. We also introduce and test non-search, competitive baselines for
detecting temporal ambiguity using zero-shot and few-shot approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How to Connect Speech Foundation Models and Large Language Models? What
  Matters and What Does Not 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17044v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17044v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Verdini, Pierfrancesco Melucci, Stefano Perna, Francesco Cariaggi, Marco Gaido, Sara Papi, Szymon Mazurek, Marek Kasztelnik, Luisa Bentivogli, Sébastien Bratières, Paolo Merialdo, Simone Scardapane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The remarkable performance achieved by Large Language Models (LLM) has driven
research efforts to leverage them for a wide range of tasks and input
modalities. In speech-to-text (S2T) tasks, the emerging solution consists of
projecting the output of the encoder of a Speech Foundational Model (SFM) into
the LLM embedding space through an adapter module. However, no work has yet
investigated how much the downstream-task performance depends on each component
(SFM, adapter, LLM) nor whether the best design of the adapter depends on the
chosen SFM and LLM. To fill this gap, we evaluate the combination of 5 adapter
modules, 2 LLMs (Mistral and Llama), and 2 SFMs (Whisper and SeamlessM4T) on
two widespread S2T tasks, namely Automatic Speech Recognition and Speech
Translation. Our results demonstrate that the SFM plays a pivotal role in
downstream performance, while the adapter choice has moderate impact and
depends on the SFM and LLM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Counterfactual Token Generation in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17027v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17027v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivi Chatzi, Nina Corvelo Benz, Eleni Straitouri, Stratis Tsirtsis, Manuel Gomez-Rodriguez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  "Sure, I am happy to generate a story for you: Captain Lyra stood at the helm
of her trusty ship, the Maelstrom's Fury, gazing out at the endless sea. [...]
Lyra's eyes welled up with tears as she realized the bitter truth - she had
sacrificed everything for fleeting riches, and lost the love of her crew, her
family, and herself." Although this story, generated by a large language model,
is captivating, one may wonder -- how would the story have unfolded if the
model had chosen "Captain Maeve" as the protagonist instead? We cannot know.
State-of-the-art large language models are stateless -- they maintain no
internal memory or state. Given a prompt, they generate a sequence of tokens as
an output using an autoregressive process. As a consequence, they cannot reason
about counterfactual alternatives to tokens they have generated in the past. In
this work, our goal is to enhance them with this functionality. To this end, we
develop a causal model of token generation that builds upon the Gumbel-Max
structural causal model. Our model allows any large language model to perform
counterfactual token generation at almost no cost in comparison with vanilla
token generation, it is embarrassingly simple to implement, and it does not
require any fine-tuning nor prompt engineering. We implement our model on Llama
3 8B-instruct and conduct both qualitative and quantitative analyses of
counterfactually generated text. We conclude with a demonstrative application
of counterfactual token generation for bias detection, unveiling interesting
insights about the model of the world constructed by large language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM-CARD: Towards a Description and Landscape of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17011v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17011v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengwei Tian, Lifeng Han, Erick Mendez Guzman, Goran Nenadic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid growth of the Natural Language Processing (NLP) field, a vast
variety of Large Language Models (LLMs) continue to emerge for diverse NLP
tasks. As an increasing number of papers are presented, researchers and
developers face the challenge of information overload. Thus, it is particularly
important to develop a system that can automatically extract and organise key
information about LLMs from academic papers (\textbf{LLM model card}). This
work is to develop such a pioneer system by using Named Entity Recognition
(\textbf{NER}) and Relation Extraction (\textbf{RE}) methods that automatically
extract key information about large language models from the papers, helping
researchers to efficiently access information about LLMs. These features
include model \textit{licence}, model \textit{name}, and model
\textit{application}. With these features, we can form a model card for each
paper. \textbf{Data-contribution} wise, 106 academic papers were processed by
defining three dictionaries - LLMs name, licence, and application. 11,051
sentences were extracted through dictionary lookup, and the dataset was
constructed through manual review of the final selection of 129 sentences that
have a link between the name and the licence, and 106 sentences that have a
link between the model name and the application.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ongoing work, 16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Models Can and Should Embrace the Communicative Nature of
  Human-Generated Math 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17005v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17005v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sasha Boguraev, Ben Lipkin, Leonie Weissweiler, Kyle Mahowald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Math is constructed by people for people: just as natural language corpora
reflect not just propositions but the communicative goals of language users,
the math data that models are trained on reflects not just idealized
mathematical entities but rich communicative intentions. While there are
important advantages to treating math in a purely symbolic manner, we here
hypothesize that there are benefits to treating math as situated linguistic
communication and that language models are well suited for this goal, in ways
that are not fully appreciated. We illustrate these points with two case
studies. First, we ran an experiment in which we found that language models
interpret the equals sign in a humanlike way -- generating systematically
different word problems for the same underlying equation arranged in different
ways. Second, we found that language models prefer proofs to be ordered in
naturalistic ways, even though other orders would be logically equivalent. We
advocate for AI systems that learn from and represent the communicative
intentions latent in human-generated math.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AXCEL: Automated eXplainable Consistency Evaluation using LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16984v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16984v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        P Aditya Sreekar, Sahil Verma, Suransh Chopra, Sarik Ghazarian, Abhishek Persad, Narayanan Sadagopan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are widely used in both industry and academia
for various tasks, yet evaluating the consistency of generated text responses
continues to be a challenge. Traditional metrics like ROUGE and BLEU show a
weak correlation with human judgment. More sophisticated metrics using Natural
Language Inference (NLI) have shown improved correlations but are complex to
implement, require domain-specific training due to poor cross-domain
generalization, and lack explainability. More recently, prompt-based metrics
using LLMs as evaluators have emerged; while they are easier to implement, they
still lack explainability and depend on task-specific prompts, which limits
their generalizability. This work introduces Automated eXplainable Consistency
Evaluation using LLMs (AXCEL), a prompt-based consistency metric which offers
explanations for the consistency scores by providing detailed reasoning and
pinpointing inconsistent text spans. AXCEL is also a generalizable metric which
can be adopted to multiple tasks without changing the prompt. AXCEL outperforms
both non-prompt and prompt-based state-of-the-art (SOTA) metrics in detecting
inconsistencies across summarization by 8.7%, free text generation by 6.2%, and
data-to-text conversion tasks by 29.4%. We also evaluate the influence of
underlying LLMs on prompt based metric performance and recalibrate the SOTA
prompt-based metrics with the latest LLMs for fair comparison. Further, we show
that AXCEL demonstrates strong performance using open source LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decoding Large-Language Models: A Systematic <span class="highlight-title">Overview</span> of Socio-Technical
  Impacts, Constraints, and Emerging Questions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16974v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16974v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyneb N. Kaya, Souvick Ghosh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There have been rapid advancements in the capabilities of large language
models (LLMs) in recent years, greatly revolutionizing the field of natural
language processing (NLP) and artificial intelligence (AI) to understand and
interact with human language. Therefore, in this work, we conduct a systematic
investigation of the literature to identify the prominent themes and directions
of LLM developments, impacts, and limitations. Our findings illustrate the
aims, methodologies, limitations, and future directions of LLM research. It
includes responsible development considerations, algorithmic improvements,
ethical challenges, and societal implications of LLM development. Overall, this
paper provides a rigorous and comprehensive overview of current research in LLM
and identifies potential directions for future development. The article
highlights the application areas that could have a positive impact on society
along with the ethical considerations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 5 figures, preprint submitted to journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive <span class="highlight-title">Self-Supervised</span> Learning Strategies for Dynamic On-Device LLM
  Personalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16973v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16973v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafael Mendoza, Isabella Cruz, Richard Liu, Aarav Deshmukh, David Williams, Jesscia Peng, Rohan Iyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have revolutionized how we interact with
technology, but their personalization to individual user preferences remains a
significant challenge, particularly in on-device applications. Traditional
methods often depend heavily on labeled datasets and can be resource-intensive.
To address these issues, we present Adaptive Self-Supervised Learning
Strategies (ASLS), which utilizes self-supervised learning techniques to
personalize LLMs dynamically. The framework comprises a user profiling layer
for collecting interaction data and a neural adaptation layer for real-time
model fine-tuning. This innovative approach enables continuous learning from
user feedback, allowing the model to generate responses that align closely with
user-specific contexts. The adaptive mechanisms of ASLS minimize computational
demands and enhance personalization efficiency. Experimental results across
various user scenarios illustrate the superior performance of ASLS in boosting
user engagement and satisfaction, highlighting its potential to redefine LLMs
as highly responsive and context-aware systems on-device.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First ASLS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Weighted Cross-entropy for Low-Resource Languages in Multilingual Speech
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16954v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16954v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrés Piñeiro-Martín, Carmen García-Mateo, Laura Docío-Fernández, María del Carmen López-Pérez, Georg Rehm
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the challenge of integrating low-resource languages into
multilingual automatic speech recognition (ASR) systems. We introduce a novel
application of weighted cross-entropy, typically used for unbalanced datasets,
to facilitate the integration of low-resource languages into pre-trained
multilingual ASR models within the context of continual multilingual learning.
We fine-tune the Whisper multilingual ASR model on five high-resource languages
and one low-resource language, employing language-weighted dynamic
cross-entropy and data augmentation. The results show a remarkable 6.69% word
error rate (WER) reduction for the low-resource language compared to the
fine-tuned model without applying our approach, and a 48.86% WER reduction
compared to the original Whisper model. In addition, our approach yields an
average WER reduction of 3.29% across the six languages, showing no degradation
for the high-resource languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 1 figure. Presented at Interspeech 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semi-Supervised Cognitive State Classification from Speech with
  Multi-View Pseudo-Labeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16937v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16937v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanchao Li, Zixing Zhang, Jing Han, Peter Bell, Catherine Lai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The lack of labeled data is a common challenge in speech classification
tasks, particularly those requiring extensive subjective assessment, such as
cognitive state classification. In this work, we propose a Semi-Supervised
Learning (SSL) framework, introducing a novel multi-view pseudo-labeling method
that leverages both acoustic and linguistic characteristics to select the most
confident data for training the classification model. Acoustically, unlabeled
data are compared to labeled data using the Frechet audio distance, calculated
from embeddings generated by multiple audio encoders. Linguistically, large
language models are prompted to revise automatic speech recognition
transcriptions and predict labels based on our proposed task-specific
knowledge. High-confidence data are identified when pseudo-labels from both
sources align, while mismatches are treated as low-confidence data. A bimodal
classifier is then trained to iteratively label the low-confidence data until a
predefined criterion is met. We evaluate our SSL framework on emotion
recognition and dementia detection tasks. Experimental results demonstrate that
our method achieves competitive performance compared to fully supervised
learning using only 30% of the labeled data and significantly outperforms two
selected baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating OCR-Sensitive Neurons to Improve Entity Recognition in
  Historical Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16934v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16934v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emanuela Boros, Maud Ehrmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the presence of OCR-sensitive neurons within the
Transformer architecture and their influence on named entity recognition (NER)
performance on historical documents. By analysing neuron activation patterns in
response to clean and noisy text inputs, we identify and then neutralise
OCR-sensitive neurons to improve model performance. Based on two open access
large language models (Llama2 and Mistral), experiments demonstrate the
existence of OCR-sensitive regions and show improvements in NER performance on
historical newspapers and classical commentaries, highlighting the potential of
targeted neuron modulation to improve models' performance on noisy text.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-lingual Speech Emotion Recognition: Humans vs. <span class="highlight-title">Self-Supervised</span>
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16920v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16920v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhichen Han, Tianqi Geng, Hui Feng, Jiahong Yuan, Korin Richmond, Yuanchao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Utilizing Self-Supervised Learning (SSL) models for Speech Emotion
Recognition (SER) has proven effective, yet limited research has explored
cross-lingual scenarios. This study presents a comparative analysis between
human performance and SSL models, beginning with a layer-wise analysis and an
exploration of parameter-efficient fine-tuning strategies in monolingual,
cross-lingual, and transfer learning contexts. We further compare the SER
ability of models and humans at both utterance- and segment-levels.
Additionally, we investigate the impact of dialect on cross-lingual SER through
human evaluation. Our findings reveal that models, with appropriate knowledge
transfer, can adapt to the target language and achieve performance comparable
to native speakers. We also demonstrate the significant effect of dialect on
SER for individuals without prior linguistic and paralinguistic background.
Moreover, both humans and models exhibit distinct behaviors across different
emotions. These results offer new insights into the cross-lingual SER
capabilities of SSL models, underscoring both their similarities to and
differences from human emotion perception.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-Shot Detection of LLM-Generated Text using Token Cohesiveness <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16914v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16914v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shixuan Ma, Quan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing capability and widespread usage of large language models
(LLMs) highlight the desirability of automatic detection of LLM-generated text.
Zero-shot detectors, due to their training-free nature, have received
considerable attention and notable success. In this paper, we identify a new
feature, token cohesiveness, that is useful for zero-shot detection, and we
demonstrate that LLM-generated text tends to exhibit higher token cohesiveness
than human-written text. Based on this observation, we devise TOCSIN, a generic
dual-channel detection paradigm that uses token cohesiveness as a plug-and-play
module to improve existing zero-shot detectors. To calculate token
cohesiveness, TOCSIN only requires a few rounds of random token deletion and
semantic difference measurement, making it particularly suitable for a
practical black-box setting where the source model used for generation is not
accessible. Extensive experiments with four state-of-the-art base detectors on
various datasets, source models, and evaluation settings demonstrate the
effectiveness and generality of the proposed approach. Code available at:
\url{https://github.com/Shixuan-Ma/TOCSIN}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at the main conference of EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pruning Multilingual Large Language Models for Multilingual Inference <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16911v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16911v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hwichan Kim, Jun Suzuki, Tosho Hirasawa, Mamoru Komachi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multilingual large language models (MLLMs), trained on multilingual balanced
data, demonstrate better zero-shot learning performance in non-English
languages compared to large language models trained on English-dominant data.
However, the disparity in performance between English and non-English languages
remains a challenge yet to be fully addressed. A distinctive characteristic of
MLLMs is their high-quality translation capabilities, indicating an acquired
proficiency in aligning between languages. This study explores how to enhance
the zero-shot performance of MLLMs in non-English languages by leveraging their
alignment capability between English and non-English languages. To achieve
this, we first analyze the behavior of MLLMs when performing translation and
reveal that there are large magnitude features that play a critical role in the
translation process. Inspired by these findings, we retain the weights
associated with operations involving the large magnitude features and prune
other weights to force MLLMs to rely on these features for tasks beyond
translation. We empirically demonstrate that this pruning strategy can enhance
the MLLMs' performance in non-English language.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Temporal Sensitivity and Reasoning for Time-Sensitive Question
  Answering <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16909v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16909v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanqi Yang, Yanda Li, Meng Fang, Ling Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time-Sensitive Question Answering (TSQA) demands the effective utilization of
specific temporal contexts, encompassing multiple time-evolving facts, to
address time-sensitive questions. This necessitates not only the parsing of
temporal information within questions but also the identification and
understanding of time-evolving facts to generate accurate answers. However,
current large language models still have limited sensitivity to temporal
information and their inadequate temporal reasoning capabilities.In this paper,
we propose a novel framework that enhances temporal awareness and reasoning
through Temporal Information-Aware Embedding and Granular Contrastive
Reinforcement Learning. Experimental results on four TSQA datasets demonstrate
that our framework significantly outperforms existing LLMs in TSQA tasks,
marking a step forward in bridging the performance gap between machine and
human temporal understanding and reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Roadmap for Embodied and Social Grounding in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16900v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16900v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sara Incao, Carlo Mazzola, Giulia Belgiovine, Alessandra Sciutti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The fusion of Large Language Models (LLMs) and robotic systems has led to a
transformative paradigm in the robotic field, offering unparalleled
capabilities not only in the communication domain but also in skills like
multimodal input handling, high-level reasoning, and plan generation. The
grounding of LLMs knowledge into the empirical world has been considered a
crucial pathway to exploit the efficiency of LLMs in robotics. Nevertheless,
connecting LLMs' representations to the external world with multimodal
approaches or with robots' bodies is not enough to let them understand the
meaning of the language they are manipulating. Taking inspiration from humans,
this work draws attention to three necessary elements for an agent to grasp and
experience the world. The roadmap for LLMs grounding is envisaged in an active
bodily system as the reference point for experiencing the environment, a
temporally structured experience for a coherent, self-related interaction with
the external world, and social skills to acquire a common-grounded shared
experience.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted Version of a conference paper presented at Robophilosophy
  Conference 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robotic Backchanneling in Online Conversation Facilitation: A
  Cross-Generational Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16899v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16899v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sota Kobuki, Katie Seaborn, Seiki Tokunaga, Kosuke Fukumori, Shun Hidaka, Kazuhiro Tamura, Koji Inoue, Tatsuya Kawahara, Mihoko Otake-Mastuura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Japan faces many challenges related to its aging society, including
increasing rates of cognitive decline in the population and a shortage of
caregivers. Efforts have begun to explore solutions using artificial
intelligence (AI), especially socially embodied intelligent agents and robots
that can communicate with people. Yet, there has been little research on the
compatibility of these agents with older adults in various everyday situations.
To this end, we conducted a user study to evaluate a robot that functions as a
facilitator for a group conversation protocol designed to prevent cognitive
decline. We modified the robot to use backchannelling, a natural human way of
speaking, to increase receptiveness of the robot and enjoyment of the group
conversation experience. We conducted a cross-generational study with young
adults and older adults. Qualitative analyses indicated that younger adults
perceived the backchannelling version of the robot as kinder, more trustworthy,
and more acceptable than the non-backchannelling robot. Finally, we found that
the robot's backchannelling elicited nonverbal backchanneling in older
participants.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at Proceedings of the 2023 32nd IEEE International
  Conference on Robot and Human Interactive Communication (RO-MAN 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Shifting from endangerment to rebirth in the Artificial Intelligence
  Age: An Ensemble Machine Learning Approach for Hawrami Text Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16884v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16884v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aram Khaksar, Hossein Hassani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hawrami, a dialect of Kurdish, is classified as an endangered language as it
suffers from the scarcity of data and the gradual loss of its speakers. Natural
Language Processing projects can be used to partially compensate for data
availability for endangered languages/dialects through a variety of approaches,
such as machine translation, language model building, and corpora development.
Similarly, NLP projects such as text classification are in language
documentation. Several text classification studies have been conducted for
Kurdish, but they were mainly dedicated to two particular dialects: Sorani
(Central Kurdish) and Kurmanji (Northern Kurdish). In this paper, we introduce
various text classification models using a dataset of 6,854 articles in Hawrami
labeled into 15 categories by two native speakers. We use K-nearest Neighbor
(KNN), Linear Support Vector Machine (Linear SVM), Logistic Regression (LR),
and Decision Tree (DT) to evaluate how well those methods perform the
classification task. The results indicate that the Linear SVM achieves a 96% of
accuracy and outperforms the other approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 7 tables, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Role of Language Models in Modern Healthcare: A Comprehensive <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16860v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16860v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amna Khalid, Ayma Khalid, Umar Khalid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The application of large language models (LLMs) in healthcare has gained
significant attention due to their ability to process complex medical data and
provide insights for clinical decision-making. These models have demonstrated
substantial capabilities in understanding and generating natural language,
which is crucial for medical documentation, diagnostics, and patient
interaction. This review examines the trajectory of language models from their
early stages to the current state-of-the-art LLMs, highlighting their strengths
in healthcare applications and discussing challenges such as data privacy,
bias, and ethical considerations. The potential of LLMs to enhance healthcare
delivery is explored, alongside the necessary steps to ensure their ethical and
effective integration into medical practice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exposing Assumptions in AI Benchmarks through Cognitive Modelling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16849v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16849v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan H. Rystrøm, Kenneth C. Enevoldsen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cultural AI benchmarks often rely on implicit assumptions about measured
constructs, leading to vague formulations with poor validity and unclear
interrelations. We propose exposing these assumptions using explicit cognitive
models formulated as Structural Equation Models. Using cross-lingual alignment
transfer as an example, we show how this approach can answer key research
questions and identify missing datasets. This framework grounds benchmark
construction theoretically and guides dataset development to improve construct
measurement. By embracing transparency, we move towards more rigorous,
cumulative AI evaluation science, challenging researchers to critically examine
their assessment foundations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CodeInsight: A Curated <span class="highlight-title">Dataset</span> of Practical Coding Solutions from Stack
  Overflow <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16819v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16819v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathanaël Beau, Benoît Crabbé
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel dataset tailored for code generation, aimed at aiding
developers in common tasks. Our dataset provides examples that include a
clarified intent, code snippets associated, and an average of three related
unit tests. It encompasses a range of libraries such as \texttt{Pandas},
\texttt{Numpy}, and \texttt{Regex}, along with more than 70 standard libraries
in Python code derived from Stack Overflow. Comprising 3,409 crafted examples
by Python experts, our dataset is designed for both model finetuning and
standalone evaluation. To complete unit tests evaluation, we categorize
examples in order to get more fine grained analysis, enhancing the
understanding of models' strengths and weaknesses in specific coding tasks. The
examples have been refined to reduce data contamination, a process confirmed by
the performance of three leading models: Mistral 7B, CodeLLaMa 13B, and
Starcoder 15B. We further investigate data-contamination testing GPT-4
performance on a part of our dataset. The benchmark can be accessed at
\url{https://github.com/NathanaelBeau/CodeInsight}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Few Hypocrites: Few-Shot Learning and Subtype Definitions for
  Detecting Hypocrisy Accusations in Online Climate Change Debates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16807v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16807v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paulina Garcia Corral, Avishai Green, Hendrik Meyer, Anke Stoll, Xiaoyue Yan, Myrthe Reuver
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The climate crisis is a salient issue in online discussions, and hypocrisy
accusations are a central rhetorical element in these debates. However, for
large-scale text analysis, hypocrisy accusation detection is an understudied
tool, most often defined as a smaller subtask of fallacious argument detection.
In this paper, we define hypocrisy accusation detection as an independent task
in NLP, and identify different relevant subtypes of hypocrisy accusations. Our
Climate Hypocrisy Accusation Corpus (CHAC) consists of 420 Reddit climate
debate comments, expert-annotated into two different types of hypocrisy
accusations: personal versus political hypocrisy. We evaluate few-shot
in-context learning with 6 shots and 3 instruction-tuned Large Language Models
(LLMs) for detecting hypocrisy accusations in this dataset. Results indicate
that the GPT-4o and Llama-3 models in particular show promise in detecting
hypocrisy accusations (F1 reaching 0.68, while previous work shows F1 of 0.44).
However, context matters for a complex semantic concept such as hypocrisy
accusations, and we find models struggle especially at identifying political
hypocrisy accusations compared to personal moral hypocrisy. Our study
contributes new insights in hypocrisy detection and climate change discourse,
and is a stepping stone for large-scale analysis of hypocrisy accusation in
online climate debates.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>cite the public version, published at CPSS 2024 @ KONVENS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating the Bias of Large Language Model Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16788v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16788v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongli Zhou, Hui Huang, Yunfei Long, Bing Xu, Conghui Zhu, Hailong Cao, Muyun Yang, Tiejun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, there has been a trend of evaluating the Large Language Model (LLM)
quality in the flavor of LLM-as-a-Judge, namely leveraging another LLM to
evaluate the current output quality. However, existing judges are proven to be
biased, namely they would favor answers which present better superficial
quality (such as verbosity, fluency) while ignoring the instruction following
ability. In this work, we propose systematic research about the bias of
LLM-as-a-Judge. Specifically, for closed-source judge models, we apply
calibration to mitigate the significance of superficial quality, both on
probability level and prompt level. For open-source judge models, we propose to
mitigate the bias by contrastive training, with curated negative samples that
deviate from instruction but present better superficial quality. We apply our
methods on the bias evaluation benchmark, and experiment results show our
methods mitigate the bias by a large margin while maintaining a satisfactory
evaluation accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Holistic Automated Red Teaming for Large Language Models through
  Top-Down Test Case Generation and Multi-turn Interaction <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16783v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16783v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinchuan Zhang, Yan Zhou, Yaxin Liu, Ziming Li, Songlin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated red teaming is an effective method for identifying misaligned
behaviors in large language models (LLMs). Existing approaches, however, often
focus primarily on improving attack success rates while overlooking the need
for comprehensive test case coverage. Additionally, most of these methods are
limited to single-turn red teaming, failing to capture the multi-turn dynamics
of real-world human-machine interactions. To overcome these limitations, we
propose HARM (Holistic Automated Red teaMing), which scales up the diversity of
test cases using a top-down approach based on an extensible, fine-grained risk
taxonomy. Our method also leverages a novel fine-tuning strategy and
reinforcement learning techniques to facilitate multi-turn adversarial probing
in a human-like manner. Experimental results demonstrate that our framework
enables a more systematic understanding of model vulnerabilities and offers
more targeted guidance for the alignment process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 camera ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ E-SQL: Direct Schema Linking via Question Enrichment in Text-to-SQL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16751v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16751v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hasan Alp Caferoğlu, Özgür Ulusoy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Translating Natural Language Queries into Structured Query Language
(Text-to-SQL or NLQ-to-SQL) is a critical task extensively studied by both the
natural language processing and database communities, aimed at providing a
natural language interface to databases (NLIDB) and lowering the barrier for
non-experts. Despite recent advancements made through the use of Large Language
Models (LLMs), significant challenges remain. These include handling complex
database schemas, resolving ambiguity in user queries, and generating SQL
queries with intricate structures that accurately reflect the user's intent. In
this work, we introduce E-SQL, a novel pipeline specifically designed to
address these challenges through direct schema linking and candidate predicate
augmentation. E-SQL enhances the natural language query by incorporating
relevant database items (i.e., tables, columns, and values) and conditions
directly into the question, bridging the gap between the query and the database
structure. The pipeline leverages candidate predicate augmentation to mitigate
erroneous or incomplete predicates in generated SQLs. We further investigate
the impact of schema filtering, a technique widely explored in previous work,
and demonstrate its diminishing returns when applied alongside advanced large
language models. Comprehensive evaluations on the BIRD benchmark illustrate
that E-SQL achieves competitive performance, particularly excelling in complex
queries with a 66.29% execution accuracy on the test set. All code required to
reproduce the reported results is publicly available on our GitHub repository.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RoleBreak: Character Hallucination as a Jailbreak Attack in Role-Playing
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16727v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16727v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihong Tang, Bo Wang, Xu Wang, Dongming Zhao, Jing Liu, Jijun Zhang, Ruifang He, Yuexian Hou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Role-playing systems powered by large language models (LLMs) have become
increasingly influential in emotional communication applications. However,
these systems are susceptible to character hallucinations, where the model
deviates from predefined character roles and generates responses that are
inconsistent with the intended persona. This paper presents the first
systematic analysis of character hallucination from an attack perspective,
introducing the RoleBreak framework. Our framework identifies two core
mechanisms-query sparsity and role-query conflict-as key factors driving
character hallucination. Leveraging these insights, we construct a novel
dataset, RoleBreakEval, to evaluate existing hallucination mitigation
techniques. Our experiments reveal that even enhanced models trained to
minimize hallucination remain vulnerable to attacks. To address these
vulnerabilities, we propose a novel defence strategy, the Narrator Mode, which
generates supplemental context through narration to mitigate role-query
conflicts and improve query generalization. Experimental results demonstrate
that Narrator Mode significantly outperforms traditional refusal-based
strategies by reducing hallucinations, enhancing fidelity to character roles
and queries, and improving overall narrative coherence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PMSS: <span class="highlight-title">Pretrain</span>ed Matrices Skeleton Selection for LLM Fine-tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16722v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16722v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qibin Wang, Xiaolin Hu, Weikai Xu, Wei Liu, Jian Luan, Bin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-rank adaptation (LoRA) and its variants have recently gained much
interest due to their ability to avoid excessive inference costs. However, LoRA
still encounters the following challenges: (1) Limitation of low-rank
assumption; and (2) Its initialization method may be suboptimal. To this end,
we propose PMSS(Pre-trained Matrices Skeleton Selection), which enables
high-rank updates with low costs while leveraging semantic and linguistic
information inherent in pre-trained weight. It achieves this by selecting
skeletons from the pre-trained weight matrix and only learning a small matrix
instead. Experiments demonstrate that PMSS outperforms LoRA and other
fine-tuning methods across tasks with much less trainable parameters. We
demonstrate its effectiveness, especially in handling complex tasks such as
DROP benchmark(+3.4%/+5.9% on LLaMA2-7B/13B) and math
reasoning(+12.89%/+5.61%/+3.11% on LLaMA2-7B, Mistral-7B and Gemma-7B of
GSM8K). The code and model will be released soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vision-Language Model Fine-Tuning via Simple Parameter-Efficient
  Modification <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16718v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16718v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming Li, Jike Zhong, Chenxin Li, Liuzhuozheng Li, Nie Lin, Masashi Sugiyama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in fine-tuning Vision-Language Models (VLMs) have witnessed
the success of prompt tuning and adapter tuning, while the classic model
fine-tuning on inherent parameters seems to be overlooked. It is believed that
fine-tuning the parameters of VLMs with few-shot samples corrupts the
pre-trained knowledge since fine-tuning the CLIP model even degrades
performance. In this paper, we revisit this viewpoint, and propose a new
perspective: fine-tuning the specific parameters instead of all will uncover
the power of classic model fine-tuning on VLMs. Through our meticulous study,
we propose ClipFit, a simple yet effective method to fine-tune CLIP without
introducing any overhead of extra parameters. We demonstrate that by only
fine-tuning the specific bias terms and normalization layers, ClipFit can
improve the performance of zero-shot CLIP by 7.27\% average harmonic mean
accuracy. Lastly, to understand how fine-tuning in CLIPFit affects the
pre-trained models, we conducted extensive experimental analyses w.r.t. changes
in internal parameters and representations. We found that low-level text bias
layers and the first layer normalization layer change much more than other
layers. The code is available at \url{https://github.com/minglllli/CLIPFit}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Turing Test: Can <span class="highlight-title">GPT</span>-4 Sway Experts' Decisions? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16710v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16710v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takehiro Takayanagi, Hiroya Takamura, Kiyoshi Izumi, Chung-Chi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the post-Turing era, evaluating large language models (LLMs) involves
assessing generated text based on readers' reactions rather than merely its
indistinguishability from human-produced content. This paper explores how
LLM-generated text impacts readers' decisions, focusing on both amateur and
expert audiences. Our findings indicate that GPT-4 can generate persuasive
analyses affecting the decisions of both amateurs and professionals.
Furthermore, we evaluate the generated text from the aspects of grammar,
convincingness, logical coherence, and usefulness. The results highlight a high
correlation between real-world evaluation through audience reactions and the
current multi-dimensional evaluators commonly used for generative models.
Overall, this paper shows the potential and risk of using generated text to
sway human decisions and also points out a new direction for evaluating
generated text, i.e., leveraging the reactions and decisions of readers. We
release our dataset to assist future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Probing Omissions and Distortions in <span class="highlight-title">Transformer</span>-based RDF-to-Text
  Models <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juliette Faille, Albert Gatt, Claire Gardent
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Natural Language Generation (NLG), important information is sometimes
omitted in the output text. To better understand and analyse how this type of
mistake arises, we focus on RDF-to-Text generation and explore two methods of
probing omissions in the encoder output of BART (Lewis et al, 2020) and of T5
(Raffel et al, 2019): (i) a novel parameter-free probing method based on the
computation of cosine similarity between embeddings of RDF graphs and of RDF
graphs in which we removed some entities and (ii) a parametric probe which
performs binary classification on the encoder embeddings to detect omitted
entities. We also extend our analysis to distorted entities, i.e. entities that
are not fully correctly mentioned in the generated text (e.g. misspelling of
entity, wrong units of measurement). We found that both omitted and distorted
entities can be probed in the encoder's output embeddings. This suggests that
the encoder emits a weaker signal for these entities and therefore is
responsible for some loss of information. This also shows that probing methods
can be used to detect mistakes in the output of NLG models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in Transactions of the ACL (TACL)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> of Low-bit Large Language Models: Basics, Systems, and
  Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16694v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16694v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruihao Gong, Yifu Ding, Zining Wang, Chengtao Lv, Xingyu Zheng, Jinyang Du, Haotong Qin, Jinyang Guo, Michele Magno, Xianglong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved remarkable advancements in natural
language processing, showcasing exceptional performance across various tasks.
However, the expensive memory and computational requirements present
significant challenges for their practical deployment. Low-bit quantization has
emerged as a critical approach to mitigate these challenges by reducing the
bit-width of model parameters, activations, and gradients, thus decreasing
memory usage and computational demands. This paper presents a comprehensive
survey of low-bit quantization methods tailored for LLMs, covering the
fundamental principles, system implementations, and algorithmic strategies. An
overview of basic concepts and new data formats specific to low-bit LLMs is
first introduced, followed by a review of frameworks and systems that
facilitate low-bit LLMs across various hardware platforms. Then, we categorize
and analyze techniques and toolkits for efficient low-bit training and
inference of LLMs. Finally, we conclude with a discussion of future trends and
potential advancements of low-bit LLMs. Our systematic overview from basic,
system, and algorithm perspectives can offer valuable insights and guidelines
for future works to enhance the efficiency and applicability of LLMs through
low-bit quantization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ruihao Gong leads the overall organization of the survey, with Yifu
  Ding and Jinyang Du contributing to Sections 2 and 3. Xingyu Zheng is
  responsible for authoring Section 4, while Chengtao Lv and Zining Wang
  collaborate on Section 5. Haotong Qin, Jinyang Guo, Michele Magno, and
  Xianglong Liu provide guidance during the whole process and assist in
  refining the final manuscript</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MSI-Agent: Incorporating Multi-Scale Insight into Embodied Agents for
  Superior Planning and Decision-Making 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16686v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16686v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dayuan Fu, Biqing Qi, Yihuai Gao, Che Jiang, Guanting Dong, Bowen Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-term memory is significant for agents, in which insights play a crucial
role. However, the emergence of irrelevant insight and the lack of general
insight can greatly undermine the effectiveness of insight. To solve this
problem, in this paper, we introduce Multi-Scale Insight Agent (MSI-Agent), an
embodied agent designed to improve LLMs' planning and decision-making ability
by summarizing and utilizing insight effectively across different scales. MSI
achieves this through the experience selector, insight generator, and insight
selector. Leveraging a three-part pipeline, MSI can generate task-specific and
high-level insight, store it in a database, and then use relevant insight from
it to aid in decision-making. Our experiments show that MSI outperforms another
insight strategy when planning by GPT3.5. Moreover, We delve into the
strategies for selecting seed experience and insight, aiming to provide LLM
with more useful and relevant insight for better decision-making. Our
observations also indicate that MSI exhibits better robustness when facing
domain-shifting scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SynTQA: Synergistic Table-based Question Answering via Mixture of
  Text-to-SQL and E2E TQA <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16682v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16682v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyue Zhang, Anh Tuan Luu, Chen Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-SQL parsing and end-to-end question answering (E2E TQA) are two main
approaches for Table-based Question Answering task. Despite success on multiple
benchmarks, they have yet to be compared and their synergy remains unexplored.
In this paper, we identify different strengths and weaknesses through
evaluating state-of-the-art models on benchmark datasets: Text-to-SQL
demonstrates superiority in handling questions involving arithmetic operations
and long tables; E2E TQA excels in addressing ambiguous questions, non-standard
table schema, and complex table contents. To combine both strengths, we propose
a Synergistic Table-based Question Answering approach that integrate different
models via answer selection, which is agnostic to any model types. Further
experiments validate that ensembling models by either feature-based or
LLM-based answer selector significantly improves the performance over
individual models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Emotional Dimension Control in Language Model-Based Text-to-Speech:
  Spanning a Broad Spectrum of Human Emotions <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16681v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16681v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Zhou, You Zhang, Shengkui Zhao, Hao Wang, Zexu Pan, Dianwen Ng, Chong Zhang, Chongjia Ni, Yukun Ma, Trung Hieu Nguyen, Jia Qi Yip, Bin Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current emotional text-to-speech (TTS) systems face challenges in mimicking a
broad spectrum of human emotions due to the inherent complexity of emotions and
limitations in emotional speech datasets and models. This paper proposes a TTS
framework that facilitates control over pleasure, arousal, and dominance, and
can synthesize a diversity of emotional styles without requiring any emotional
speech data during TTS training. We train an emotional attribute predictor
using only categorical labels from speech data, aligning with psychological
research and incorporating anchored dimensionality reduction on self-supervised
learning (SSL) features. The TTS framework converts text inputs into phonetic
tokens via an autoregressive language model and uses pseudo-emotional
dimensions to guide the parallel prediction of fine-grained acoustic details.
Experiments conducted on the LibriTTS dataset demonstrate that our framework
can synthesize speech with enhanced naturalness and a variety of emotional
styles by effectively controlling emotional dimensions, even without the
inclusion of any emotional speech during TTS training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SWE2: SubWord Enriched and Significant Word Emphasized Framework for
  Hate Speech Detection <span class="chip">CIKM 2020</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16673v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16673v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanyi Mou, Pengyi Ye, Kyumin Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hate speech detection on online social networks has become one of the
emerging hot topics in recent years. With the broad spread and fast propagation
speed across online social networks, hate speech makes significant impacts on
society by increasing prejudice and hurting people. Therefore, there are
aroused attention and concern from both industry and academia. In this paper,
we address the hate speech problem and propose a novel hate speech detection
framework called SWE2, which only relies on the content of messages and
automatically identifies hate speech. In particular, our framework exploits
both word-level semantic information and sub-word knowledge. It is intuitively
persuasive and also practically performs well under a situation with/without
character-level adversarial attack. Experimental results show that our proposed
model achieves 0.975 accuracy and 0.953 macro F1, outperforming 7
state-of-the-art baselines under no adversarial attack. Our model robustly and
significantly performed well under extreme adversarial attack (manipulation of
50% messages), achieving 0.967 accuracy and 0.934 macro F1.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in CIKM 2020</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Topic-aware Causal Intervention for Counterfactual Detection <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16668v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16668v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thong Nguyen, Truc-My Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counterfactual statements, which describe events that did not or cannot take
place, are beneficial to numerous NLP applications. Hence, we consider the
problem of counterfactual detection (CFD) and seek to enhance the CFD models.
Previous models are reliant on clue phrases to predict counterfactuality, so
they suffer from significant performance drop when clue phrase hints do not
exist during testing. Moreover, these models tend to predict
non-counterfactuals over counterfactuals. To address these issues, we propose
to integrate neural topic model into the CFD model to capture the global
semantics of the input statement. We continue to causally intervene the hidden
representations of the CFD model to balance the effect of the class labels.
Extensive experiments show that our approach outperforms previous
state-of-the-art CFD and bias-resolving methods in both the CFD and other
bias-sensitive tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 4th EMNLP-NLP4DH 2024 workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Character-Centric Creative Story Generation via Imagination 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16667v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16667v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyeongman Park, Minbeom Kim, Kyomin Jung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creative story generation with diverse and detailed story elements is a
long-standing goal for large language models. While existing methodologies
generate long and coherent stories, they fall significantly short of human
capabilities in terms of diversity and character detail. To address this, we
introduce a novel story generation framework called CCI (Character-centric
Creative story generation via Imagination). CCI features two innovative modules
for creative story generation: IG (Image-Guided Imagination) and MW
(Multi-Writer model). In the IG module, we utilize DALL-E 3 to create visual
representations of key story elements. The IG generates more novel and concrete
characters, backgrounds, and main plots than text-only methods. The MW module
uses these story elements created by IG to generate multiple description
candidates for the protagonist and select the best one. This method
incorporates vivid and rich character descriptions into the story. We compared
the stories generated by CCI and baseline models through human evaluation and
statistical analysis. The results showed significant improvements in the
creativity. Furthermore, by enabling interactive multi-modal story generation
with users, we have opened up possibilities for human-LLM integration in
cultural development.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Pre-train</span>ed Language Models Return Distinguishable Probability
  Distributions to Unfaithfully Hallucinated Texts <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16658v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16658v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taehun Cha, Donghun Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we show the pre-trained language models return distinguishable
generation probability and uncertainty distribution to unfaithfully
hallucinated texts, regardless of their size and structure. By examining 24
models on 6 data sets, we find out that 88-98% of cases return statistically
significantly distinguishable generation probability and uncertainty
distributions. Using this general phenomenon, we showcase a
hallucination-reducing training algorithm. Our algorithm outperforms other
baselines by achieving higher faithfulness metrics while maintaining sound
general text quality measures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Speech Recognition Rescoring with Large Speech-Text Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16654v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16654v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prashanth Gurunath Shivakumar, Jari Kolehmainen, Aditya Gourav, Yi Gu, Ankur Gandhe, Ariya Rastrow, Ivan Bulyko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLM) have demonstrated the ability to understand human
language by leveraging large amount of text data. Automatic speech recognition
(ASR) systems are often limited by available transcribed speech data and
benefit from a second pass rescoring using LLM. Recently multi-modal large
language models, particularly speech and text foundational models have
demonstrated strong spoken language understanding. Speech-Text foundational
models leverage large amounts of unlabelled and labelled data both in speech
and text modalities to model human language. In this work, we propose novel
techniques to use multi-modal LLM for ASR rescoring. We also explore
discriminative training to further improve the foundational model rescoring
performance. We demonstrate cross-modal knowledge transfer in speech-text LLM
can benefit rescoring. Our experiments demonstrate up-to 20% relative
improvements over Whisper large ASR and up-to 15% relative improvements over
text-only LLM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Domain-Independent Automatic Generation of Descriptive Texts for
  Time-Series Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16647v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16647v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kota Dohi, Aoi Ito, Harsh Purohit, Tomoya Nishida, Takashi Endo, Yohei Kawaguchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to scarcity of time-series data annotated with descriptive texts,
training a model to generate descriptive texts for time-series data is
challenging. In this study, we propose a method to systematically generate
domain-independent descriptive texts from time-series data. We identify two
distinct approaches for creating pairs of time-series data and descriptive
texts: the forward approach and the backward approach. By implementing the
novel backward approach, we create the Temporal Automated Captions for
Observations (TACO) dataset. Experimental results demonstrate that a
contrastive learning based model trained using the TACO dataset is capable of
generating descriptive texts for time-series data in novel domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-Lingual and Cross-Cultural Variation in Image Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16646v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16646v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Uri Berger, Edoardo M. Ponti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Do speakers of different languages talk differently about what they see?
Behavioural and cognitive studies report cultural effects on perception;
however, these are mostly limited in scope and hard to replicate. In this work,
we conduct the first large-scale empirical study of cross-lingual variation in
image descriptions. Using a multimodal dataset with 31 languages and images
from diverse locations, we develop a method to accurately identify entities
mentioned in captions and present in the images, then measure how they vary
across languages. Our analysis reveals that pairs of languages that are
geographically or genetically closer tend to mention the same entities more
frequently. We also identify entity categories whose saliency is universally
high (such as animate beings), low (clothing accessories) or displaying high
variance across languages (landscape). In a case study, we measure the
differences in a specific language pair (e.g., Japanese mentions clothing far
more frequently than English). Furthermore, our method corroborates previous
small-scale studies, including 1) Rosch et al. (1976)'s theory of basic-level
categories, demonstrating a preference for entities that are neither too
generic nor too specific, and 2) Miyamoto et al. (2006)'s hypothesis that
environments afford patterns of perception, such as entity counts. Overall, our
work reveals the presence of both universal and culture-specific patterns in
entity mentions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enabling Auditory Large Language Models for Automatic Speech Quality
  Evaluation <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16644v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16644v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyin Wang, Wenyi Yu, Yudong Yang, Changli Tang, Yixuan Li, Jimin Zhuang, Xianzhao Chen, Xiaohai Tian, Jun Zhang, Guangzhi Sun, Lu Lu, Chao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech quality assessment typically requires evaluating audio from multiple
aspects, such as mean opinion score (MOS) and speaker similarity (SIM) etc.,
which can be challenging to cover using one small model designed for a single
task. In this paper, we propose leveraging recently introduced auditory large
language models (LLMs) for automatic speech quality assessment. By employing
task-specific prompts, auditory LLMs are finetuned to predict MOS, SIM and A/B
testing results, which are commonly used for evaluating text-to-speech systems.
Additionally, the finetuned auditory LLM is able to generate natural language
descriptions assessing aspects like noisiness, distortion, discontinuity, and
overall quality, providing more interpretable outputs. Extensive experiments
have been performed on the NISQA, BVCC, SOMOS and VoxSim speech quality
datasets, using open-source auditory LLMs such as SALMONN, Qwen-Audio, and
Qwen2-Audio. For the natural language descriptions task, a commercial model
Google Gemini 1.5 Pro is also evaluated. The results demonstrate that auditory
LLMs achieve competitive performance compared to state-of-the-art task-specific
small models in predicting MOS and SIM, while also delivering promising results
in A/B testing and natural language descriptions. Our data processing scripts
and finetuned model checkpoints will be released upon acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training Language Models to Win Debates with Self-Play Improves Judge
  Accuracy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16636v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16636v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Arnesen, David Rein, Julian Michael
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We test the robustness of debate as a method of scalable oversight by
training models to debate with data generated via self-play. In a long-context
reading comprehension task, we find that language model based evaluators answer
questions more accurately when judging models optimized to win debates. By
contrast, we find no such relationship for consultancy models trained to
persuade a judge without an opposing debater present. In quantitative and
qualitative comparisons between our debate models and novel consultancy
baselines, we find evidence that debate training encourages stronger and more
informative arguments, showing promise that it can help provide high-quality
supervision for tasks that are difficult to directly evaluate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>48 pages, 12 figures; code at
  https://github.com/samuelarnesen/nyu-debate-modeling</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Claim-Guided Textual Backdoor Attack for Practical Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16618v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16618v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minkyoo Song, Hanna Kim, Jaehan Kim, Youngjin Jin, Seungwon Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in natural language processing and the increased use of large
language models have exposed new security vulnerabilities, such as backdoor
attacks. Previous backdoor attacks require input manipulation after model
distribution to activate the backdoor, posing limitations in real-world
applicability. Addressing this gap, we introduce a novel Claim-Guided Backdoor
Attack (CGBA), which eliminates the need for such manipulations by utilizing
inherent textual claims as triggers. CGBA leverages claim extraction,
clustering, and targeted training to trick models to misbehave on targeted
claims without affecting their performance on clean data. CGBA demonstrates its
effectiveness and stealthiness across various datasets and models,
significantly enhancing the feasibility of practical backdoor attacks. Our code
and data will be available at https://github.com/PaperCGBA/CGBA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating and Enhancing Large Language Models for Novelty Assessment in
  Scholarly Publications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16605v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16605v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ethan Lin, Zhiyuan Peng, Yi Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have evaluated the creativity/novelty of large language models
(LLMs) primarily from a semantic perspective, using benchmarks from cognitive
science. However, accessing the novelty in scholarly publications is a largely
unexplored area in evaluating LLMs. In this paper, we introduce a scholarly
novelty benchmark (SchNovel) to evaluate LLMs' ability to assess novelty in
scholarly papers. SchNovel consists of 15000 pairs of papers across six fields
sampled from the arXiv dataset with publication dates spanning 2 to 10 years
apart. In each pair, the more recently published paper is assumed to be more
novel. Additionally, we propose RAG-Novelty, which simulates the review process
taken by human reviewers by leveraging the retrieval of similar papers to
assess novelty. Extensive experiments provide insights into the capabilities of
different LLMs to assess novelty and demonstrate that RAG-Novelty outperforms
recent baseline models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Overview</span> of the First Shared Task on Clinical Text Generation: RRG24 and
  "Discharge Me!" <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16603v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16603v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Justin Xu, Zhihong Chen, Andrew Johnston, Louis Blankemeier, Maya Varma, Jason Hom, William J. Collins, Ankit Modi, Robert Lloyd, Benjamin Hopkins, Curtis Langlotz, Jean-Benoit Delbrouck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent developments in natural language generation have tremendous
implications for healthcare. For instance, state-of-the-art systems could
automate the generation of sections in clinical reports to alleviate physician
workload and streamline hospital documentation. To explore these applications,
we present a shared task consisting of two subtasks: (1) Radiology Report
Generation (RRG24) and (2) Discharge Summary Generation ("Discharge Me!").
RRG24 involves generating the 'Findings' and 'Impression' sections of radiology
reports given chest X-rays. "Discharge Me!" involves generating the 'Brief
Hospital Course' and 'Discharge Instructions' sections of discharge summaries
for patients admitted through the emergency department. "Discharge Me!"
submissions were subsequently reviewed by a team of clinicians. Both tasks
emphasize the goal of reducing clinician burnout and repetitive workloads by
generating documentation. We received 201 submissions from across 8 teams for
RRG24, and 211 submissions from across 16 teams for "Discharge Me!".
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL Proceedings. BioNLP workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Disentangling Questions from Query Generation for Task-Adaptive
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16570v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16570v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoonsang Lee, Minsoo Kim, Seung-won Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies the problem of information retrieval, to adapt to unseen
tasks. Existing work generates synthetic queries from domain-specific documents
to jointly train the retriever. However, the conventional query generator
assumes the query as a question, thus failing to accommodate general search
intents. A more lenient approach incorporates task-adaptive elements, such as
few-shot learning with an 137B LLM. In this paper, we challenge a trend
equating query and question, and instead conceptualize query generation task as
a "compilation" of high-level intent into task-adaptive query. Specifically, we
propose EGG, a query generator that better adapts to wide search intents
expressed in the BeIR benchmark. Our method outperforms baselines and existing
models on four tasks with underexplored intents, while utilizing a query
generator 47 times smaller than the previous state-of-the-art. Our findings
reveal that instructing the LM with explicit search intent is a key aspect of
modeling an effective query generator.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding the Cognitive Complexity in Language Elicited by Product
  Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16521v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16521v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan-Ying Chen, Shabnam Hakimi, Monica Van, Francine Chen, Matthew Hong, Matt Klenk, Charlene Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Product images (e.g., a phone) can be used to elicit a diverse set of
consumer-reported features expressed through language, including surface-level
perceptual attributes (e.g., "white") and more complex ones, like perceived
utility (e.g., "battery"). The cognitive complexity of elicited language
reveals the nature of cognitive processes and the context required to
understand them; cognitive complexity also predicts consumers' subsequent
choices. This work offers an approach for measuring and validating the
cognitive complexity of human language elicited by product images, providing a
tool for understanding the cognitive processes of human as well as virtual
respondents simulated by Large Language Models (LLMs). We also introduce a
large dataset that includes diverse descriptive labels for product images,
including human-rated complexity. We demonstrate that human-rated cognitive
complexity can be approximated using a set of natural language models that,
combined, roughly capture the complexity construct. Moreover, this approach is
minimally supervised and scalable, even in use cases with limited human
assessment of complexity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Pre-train</span>ed Language Models Do Not Help Auto-regressive Text-to-Image
  Generation <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.16201v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.16201v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhui Zhang, Brandon McKinzie, Zhe Gan, Vaishaal Shankar, Alexander Toshev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in image tokenizers, such as VQ-VAE, have enabled
text-to-image generation using auto-regressive methods, similar to language
modeling. However, these methods have yet to leverage pre-trained language
models, despite their adaptability to various downstream tasks. In this work,
we explore this gap by adapting a pre-trained language model for
auto-regressive text-to-image generation, and find that pre-trained language
models offer limited help. We provide a two-fold explanation by analyzing
tokens from each modality. First, we demonstrate that image tokens possess
significantly different semantics compared to text tokens, rendering
pre-trained language models no more effective in modeling them than randomly
initialized ones. Second, the text tokens in the image-text datasets are too
simple compared to normal language model pre-training data, which causes the
catastrophic degradation of language models' capability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at EMNLP 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Benchmarking Cognitive Biases in Large Language Models as Evaluators <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.17012v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.17012v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park, Zae Myung Kim, Dongyeop Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models are cognitively biased judges. Large Language Models
(LLMs) have recently been shown to be effective as automatic evaluators with
simple prompting and in-context learning. In this work, we assemble 15 LLMs of
four different size ranges and evaluate their output responses by preference
ranking from the other LLMs as evaluators, such as System Star is better than
System Square. We then evaluate the quality of ranking outputs introducing the
Cognitive Bias Benchmark for LLMs as Evaluators (CoBBLEr), a benchmark to
measure six different cognitive biases in LLM evaluation outputs, such as the
Egocentric bias where a model prefers to rank its own outputs highly in
evaluation. We find that LLMs are biased text quality evaluators, exhibiting
strong indications on our bias benchmark (average of 40% of comparisons across
all models) within each of their evaluations that question their robustness as
evaluators. Furthermore, we examine the correlation between human and machine
preferences and calculate the average Rank-Biased Overlap (RBO) score to be
49.6%, indicating that machine preferences are misaligned with humans.
According to our findings, LLMs may still be unable to be utilized for
automatic annotation aligned with human preferences. Our project page is at:
https://minnesotanlp.github.io/cobbler.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Publishsed at ACL 2024. 29 pages, 9 figures, 14 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Is This a Bad Table? A Closer Look at the Evaluation of Table Generation
  from Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14829v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14829v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pritika Ramu, Aparna Garimella, Sambaran Bandyopadhyay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding whether a generated table is of good quality is important to be
able to use it in creating or editing documents using automatic methods. In
this work, we underline that existing measures for table quality evaluation
fail to capture the overall semantics of the tables, and sometimes unfairly
penalize good tables and reward bad ones. We propose TabEval, a novel table
evaluation strategy that captures table semantics by first breaking down a
table into a list of natural language atomic statements and then compares them
with ground truth statements using entailment-based measures. To validate our
approach, we curate a dataset comprising of text descriptions for 1,250 diverse
Wikipedia tables, covering a range of topics and structures, in contrast to the
limited scope of existing datasets. We compare TabEval with existing metrics
using unsupervised and supervised text-to-table generation methods,
demonstrating its stronger correlation with human judgments of table quality
across four datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A is for Absorption: Studying Feature Splitting and Absorption in Sparse
  Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14507v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14507v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Chanin, James Wilken-Smith, Tomáš Dulka, Hardik Bhatnagar, Joseph Bloom
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse Autoencoders (SAEs) have emerged as a promising approach to decompose
the activations of Large Language Models (LLMs) into human-interpretable
latents. In this paper, we pose two questions. First, to what extent do SAEs
extract monosemantic and interpretable latents? Second, to what extent does
varying the sparsity or the size of the SAE affect monosemanticity /
interpretability? By investigating these questions in the context of a simple
first-letter identification task where we have complete access to ground truth
labels for all tokens in the vocabulary, we are able to provide more detail
than prior investigations. Critically, we identify a problematic form of
feature-splitting we call feature absorption where seemingly monosemantic
latents fail to fire in cases where they clearly should. Our investigation
suggests that varying SAE size or sparsity is insufficient to solve this issue,
and that there are deeper conceptual issues in need of resolution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ranking Manipulation for Conversational Search Engines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03589v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03589v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Pfrommer, Yatong Bai, Tanmay Gautam, Somayeh Sojoudi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Major search engine providers are rapidly incorporating Large Language Model
(LLM)-generated content in response to user queries. These conversational
search engines operate by loading retrieved website text into the LLM context
for summarization and interpretation. Recent research demonstrates that LLMs
are highly vulnerable to jailbreaking and prompt injection attacks, which
disrupt the safety and quality goals of LLMs using adversarial strings. This
work investigates the impact of prompt injections on the ranking order of
sources referenced by conversational search engines. To this end, we introduce
a focused dataset of real-world consumer product websites and formalize
conversational search ranking as an adversarial problem. Experimentally, we
analyze conversational search rankings in the absence of adversarial injections
and show that different LLMs vary significantly in prioritizing product name,
document content, and context position. We then present a tree-of-attacks-based
jailbreaking technique which reliably promotes low-ranked products.
Importantly, these attacks transfer effectively to state-of-the-art
conversational search engines such as perplexity$.$ai. Given the strong
financial incentive for website owners to boost their search ranking, we argue
that our problem formulation is of critical importance for future robustness
work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2024 Conference on Empirical Methods in Natural Language Processing
  (Main)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Trustworthy Reranking: A Simple yet Effective Abstention
  Mechanism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12997v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12997v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hippolyte Gisserot-Boukhlef, Manuel Faysse, Emmanuel Malherbe, Céline Hudelot, Pierre Colombo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Information Retrieval (NIR) has significantly improved upon
heuristic-based Information Retrieval (IR) systems. Yet, failures remain
frequent, the models used often being unable to retrieve documents relevant to
the user's query. We address this challenge by proposing a lightweight
abstention mechanism tailored for real-world constraints, with particular
emphasis placed on the reranking phase. We introduce a protocol for evaluating
abstention strategies in black-box scenarios (typically encountered when
relying on API services), demonstrating their efficacy, and propose a simple
yet effective data-driven mechanism. We provide open-source code for experiment
replication and abstention implementation, fostering wider adoption and
application in diverse contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Keeping Up with the Language Models: Systematic Benchmark Extension for
  Bias Auditing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.12620v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.12620v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ioana Baldini, Chhavi Yadav, Manish Nagireddy, Payel Das, Kush R. Varshney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bias auditing of language models (LMs) has received considerable attention as
LMs are becoming widespread. As such, several benchmarks for bias auditing have
been proposed. At the same time, the rapid evolution of LMs can make these
benchmarks irrelevant in no time. Bias auditing is further complicated by LM
brittleness: when a presumably biased outcome is observed, is it due to model
bias or model brittleness? We propose enlisting the models themselves to help
construct bias auditing datasets that remain challenging, and introduce bias
measures that distinguish between different types of model errors. First, we
extend an existing bias benchmark for NLI (BBNLI) using a combination of
LM-generated lexical variations, adversarial filtering, and human validation.
We demonstrate that the newly created dataset BBNLI-next is more challenging
than BBNLI: on average, BBNLI-next reduces the accuracy of state-of-the-art NLI
models from 95.3%, as observed by BBNLI, to a strikingly low 57.5%. Second, we
employ BBNLI-next to showcase the interplay between robustness and bias: we
point out shortcomings in current bias scores and propose bias measures that
take into account both bias and model brittleness. Third, despite the fact that
BBNLI-next was designed with non-generative models in mind, we show that the
new dataset is also able to uncover bias in state-of-the-art open-source
generative LMs.
  Note: All datasets included in this work are in English and they address
US-centered social biases. In the spirit of efficient NLP research, no model
training or fine-tuning was performed to conduct this research.
  Warning: This paper contains offensive text examples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Interaction-Based Relevance Modeling for Online e-Commerce Search <span class="chip">ECML-PKDD'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02135v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02135v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ben Chen, Huangyu Dai, Xiang Ma, Wen Jiang, Wei Ning
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic relevance calculation is crucial for e-commerce search engines, as
it ensures that the items selected closely align with customer intent.
Inadequate attention to this aspect can detrimentally affect user experience
and engagement. Traditional text-matching techniques are prevalent but often
fail to capture the nuances of search intent accurately, so neural networks now
have become a preferred solution to processing such complex text matching.
Existing methods predominantly employ representation-based architectures, which
strike a balance between high traffic capacity and low latency. However, they
exhibit significant shortcomings in generalization and robustness when compared
to interaction-based architectures. In this work, we introduce a robust
interaction-based modeling paradigm to address these shortcomings. It
encompasses 1) a dynamic length representation scheme for expedited inference,
2) a professional terms recognition method to identify subjects and core
attributes from complex sentence structures, and 3) a contrastive adversarial
training protocol to bolster the model's robustness and matching capabilities.
Extensive offline evaluations demonstrate the superior robustness and
effectiveness of our approach, and online A/B testing confirms its ability to
improve relevance in the same exposure position, resulting in more clicks and
conversions. To the best of our knowledge, this method is the first
interaction-based approach for large e-commerce search relevance calculation.
Notably, we have deployed it for the entire search traffic on alibaba.com, the
largest B2B e-commerce platform in the world.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECML-PKDD'24 as Outstanding Paper. 8 pages, 2 figures, 7
  tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Huatuo<span class="highlight-title">GPT</span>-Vision, Towards Injecting Medical Visual Knowledge into
  Multimodal LLMs at Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19280v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19280v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junying Chen, Chi Gui, Ruyi Ouyang, Anningzhe Gao, Shunian Chen, Guiming Hardy Chen, Xidong Wang, Ruifei Zhang, Zhenyang Cai, Ke Ji, Guangjun Yu, Xiang Wan, Benyou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of multimodal large language models (MLLMs), such as
GPT-4V, has led to significant advancements. However, these models still face
challenges in medical multimodal capabilities due to limitations in the
quantity and quality of medical vision-text data, stemming from data privacy
concerns and high annotation costs. While pioneering approaches utilize
PubMed's large-scale, de-identified medical image-text pairs to address these
limitations, they still fall short due to inherent data noise. To tackle this,
we refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in
an 'unblinded' capacity to denoise and reformat the data, resulting in the
creation of the PubMedVision dataset with 1.3 million medical VQA samples. Our
validation demonstrates that: (1) PubMedVision can significantly enhance the
medical multimodal capabilities of current MLLMs, showing significant
improvement in benchmarks including the MMMU Health & Medicine track; (2)
manual checks by medical experts and empirical results validate the superior
data quality of our dataset compared to other data construction methods. Using
PubMedVision, we train a 34B medical MLLM HuatuoGPT-Vision, which shows
superior performance in medical multimodal scenarios among open-source MLLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hierarchical Tree-structured Knowledge Graph For Academic Insight <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.04854v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.04854v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinghong Li, Huy Phan, Wen Gu, Koichi Ota, Shinobu Hasegawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research surveys have always posed a challenge for beginner researchers who
lack of research training. These researchers struggle to understand the
directions within their research topic, and the discovery of new research
findings within a short time. One way to provide intuitive assistance to
beginner researchers is by offering relevant knowledge graphs(KG) and
recommending related academic papers. However, existing navigation knowledge
graphs primarily rely on keywords in the research field and often fail to
present the logical hierarchy among multiple related papers clearly. Moreover,
most recommendation systems for academic papers simply rely on high text
similarity, which can leave researchers confused as to why a particular article
is being recommended. They may lack of grasp important information about the
insight connection between "Issue resolved" and "Issue finding" that they hope
to obtain. To address these issues, this study aims to support research insight
surveys for beginner researchers by establishing a hierarchical tree-structured
knowledge graph that reflects the inheritance insight of research topics and
the relevance insight among the academic papers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been published by 'The 18TH International Conference
  on INnovations in Intelligent SysTems and Applications (INISTA 2024)'</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GEIC: Universal and Multilingual Named Entity Recognition with Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11022v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11022v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanjun Luo, Yingbin Jin, Xuecheng Liu, Tong Shang, Ruizhe Chen, Zuozhu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have supplanted traditional methods in numerous
natural language processing tasks. Nonetheless, in Named Entity Recognition
(NER), existing LLM-based methods underperform compared to baselines and
require significantly more computational resources, limiting their application.
In this paper, we introduce the task of generation-based extraction and
in-context classification (GEIC), designed to leverage LLMs' prior knowledge
and self-attention mechanisms for NER tasks. We then propose CascadeNER, a
universal and multilingual GEIC framework for few-shot and zero-shot NER.
CascadeNER employs model cascading to utilize two small-parameter LLMs to
extract and classify independently, reducing resource consumption while
enhancing accuracy. We also introduce AnythingNER, the first NER dataset
specifically designed for LLMs, including 8 languages, 155 entity types and a
novel dynamic categorization system. Experiments show that CascadeNER achieves
state-of-the-art performance on low-resource and fine-grained scenarios,
including CrossNER and FewNERD. Our work is openly accessible.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Stance Classification on Social Media Using Quantified Moral
  Foundations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09848v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09848v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hong Zhang, Prasanta Bhattacharya, Wei Gao, Liang Ze Wong, Brandon Siyuan Loh, Joseph J. P. Simons, Jisun An
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study enhances stance detection on social media by incorporating deeper
psychological attributes, specifically individuals' moral foundations. These
theoretically-derived dimensions aim to provide a comprehensive profile of an
individual's moral concerns which, in recent work, has been linked to behaviour
in a range of domains, including society, politics, health, and the
environment. In this paper, we investigate how moral foundation dimensions can
contribute to predicting an individual's stance on a given target. Specifically
we incorporate moral foundation features extracted from text, along with
message semantic features, to classify stances at both message- and user-levels
using both traditional machine learning models and large language models. Our
preliminary results suggest that encoding moral foundations can enhance the
performance of stance detection tasks and help illuminate the associations
between specific moral foundations and online stances on target topics. The
results highlight the importance of considering deeper psychological attributes
in stance analysis and underscores the role of moral foundations in guiding
online social behavior.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Are LLMs Ready for Real-World Materials Discovery? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.05200v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.05200v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Santiago Miret, N M Anoop Krishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) create exciting possibilities for powerful
language processing tools to accelerate research in materials science. While
LLMs have great potential to accelerate materials understanding and discovery,
they currently fall short in being practical materials science tools. In this
position paper, we show relevant failure cases of LLMs in materials science
that reveal current limitations of LLMs related to comprehending and reasoning
over complex, interconnected materials science knowledge. Given those
shortcomings, we outline a framework for developing Materials Science LLMs
(MatSci-LLMs) that are grounded in materials science knowledge and hypothesis
generation followed by hypothesis testing. The path to attaining performant
MatSci-LLMs rests in large part on building high-quality, multi-modal datasets
sourced from scientific literature where various information extraction
challenges persist. As such, we describe key materials science information
extraction challenges which need to be overcome in order to build large-scale,
multi-modal datasets that capture valuable materials science knowledge.
Finally, we outline a roadmap for applying future MatSci-LLMs for real-world
materials discovery via: 1. Automated Knowledge Base Generation; 2. Automated
In-Silico Material Design; and 3. MatSci-LLM Integrated Self-Driving Materials
Laboratories.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Integrating curation into scientific publishing to train AI models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.20440v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.20440v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jorge Abreu-Vicente, Hannah Sonntag, Thomas Eidens, Cassie S. Mitchell, Thomas Lemberger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High throughput extraction and structured labeling of data from academic
articles is critical to enable downstream machine learning applications and
secondary analyses. We have embedded multimodal data curation into the academic
publishing process to annotate segmented figure panels and captions. Natural
language processing (NLP) was combined with human-in-the-loop feedback from the
original authors to increase annotation accuracy. Annotation included eight
classes of bioentities (small molecules, gene products, subcellular components,
cell lines, cell types, tissues, organisms, and diseases) plus additional
classes delineating the entities' roles in experiment designs and
methodologies. The resultant dataset, SourceData-NLP, contains more than
620,000 annotated biomedical entities, curated from 18,689 figures in 3,223
articles in molecular and cell biology. We evaluate the utility of the dataset
to train AI models using named-entity recognition, segmentation of figure
captions into their constituent panels, and a novel context-dependent semantic
task assessing whether an entity is a controlled intervention target or a
measurement object. We also illustrate the use of our dataset in performing a
multi-modal task for segmenting figures into panel images and their
corresponding captions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Journal for revision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SCOI: Syntax-augmented Coverage-based In-context Example Selection for
  Machine Translation <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.04872v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.04872v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenming Tang, Zhixiang Wang, Yunfang Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning (ICL) greatly improves the performance of large language
models (LLMs) on various down-stream tasks, where the improvement highly
depends on the quality of demonstrations. In this work, we introduce syntactic
knowledge to select better in-context examples for machine translation (MT). We
propose a new strategy, namely Syntax-augmented COverage-based In-context
example selection (SCOI), leveraging the deep syntactic structure beyond
conventional word matching. Specifically, we measure the set-level syntactic
coverage by computing the coverage of polynomial terms with the help of a
simplified tree-to-polynomial algorithm, and lexical coverage using word
overlap. Furthermore, we devise an alternate selection approach to combine both
coverage measures, taking advantage of syntactic and lexical information. We
conduct experiments with two multi-lingual LLMs on six translation directions.
Empirical results show that our proposed SCOI obtains the highest average COMET
score among all learning-free methods, indicating that combining syntactic and
lexical coverage successfully helps to select better in-context examples for
MT. Our code is available at https://github.com/JamyDon/SCOI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 main conference long paper. 16 pages, 2 figures, 14 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Math-PUMA: Progressive Upward Multimodal Alignment to Enhance
  Mathematical Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08640v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08640v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenwen Zhuang, Xin Huang, Xiantao Zhang, Jin Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) excel in solving text-based
mathematical problems, but they struggle with mathematical diagrams since they
are primarily trained on natural scene images. For humans, visual aids
generally enhance problem-solving, but MLLMs perform worse as information
shifts from textual to visual modality. This decline is mainly due to their
shortcomings in aligning images and text. To tackle aforementioned challenges,
we propose Math-PUMA, a methodology focused on Progressive Upward Multimodal
Alignment. This approach is designed to improve the mathematical reasoning
skills of MLLMs through a three-stage training process, with the second stage
being the critical alignment stage. We first enhance the language model's
mathematical reasoning capabilities with extensive set of textual mathematical
problems. We then construct a multimodal dataset with varying degrees of
textual and visual information, creating data pairs by presenting each problem
in at least two forms. By leveraging the Kullback-Leibler (KL) divergence of
next-token prediction distributions to align visual and textual modalities,
consistent problem-solving abilities are ensured. Finally, we utilize
multimodal instruction tuning for MLLMs with high-quality multimodal data.
Experimental results on multiple mathematical reasoning benchmarks demonstrate
that the MLLMs trained with Math-PUMA surpass most open-source MLLMs. Our
approach effectively narrows the performance gap for problems presented in
different modalities. The code and data are available at:
\url{https://github.com/wwzhuang01/Math-PUMA}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DemoRank: Selecting Effective Demonstrations for Large Language Models
  in Ranking Task 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16332v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16332v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhan Liu, Yutao Zhu, Zhicheng Dou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, there has been increasing interest in applying large language
models (LLMs) as zero-shot passage rankers. However, few studies have explored
how to select appropriate in-context demonstrations for the passage ranking
task, which is the focus of this paper. Previous studies mainly use LLM's
feedback to train a retriever for demonstration selection. These studies apply
the LLM to score each demonstration independently, which ignores the
dependencies between demonstrations (especially important in ranking task),
leading to inferior performance of top-$k$ retrieved demonstrations. To
mitigate this issue, we introduce a demonstration reranker to rerank the
retrieved demonstrations so that top-$k$ ranked ones are more suitable for ICL.
However, generating training data for such reranker is quite challenging. On
the one hand, different from demonstration retriever, the training samples of
reranker need to incorporate demonstration dependencies. On the other hand,
obtaining the gold ranking from the retrieved demonstrations is an NP-hard
problem, which is hard to implement. To overcome these challenges, we propose a
method to approximate the optimal demonstration list iteratively and utilize
LLM to score demonstration lists of varying lengths. By doing so, the search
space is greatly reduced and demonstration dependencies are considered. Based
on these scored demonstration lists, we further design a list-pairwise training
approach which compares a pair of lists that only differ in the last
demonstration, to teach the reranker how to select the next demonstration given
a previous sequence. In this paper, we propose a demonstration selection
framework DemoRank for ranking task and conduct extensive experiments to prove
its strong ability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards a Realistic Long-Term Benchmark for Open-Web Research Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14913v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14913v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Mühlbacher, Nikos I. Bosse, Lawrence Phillips
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present initial results of a forthcoming benchmark for evaluating LLM
agents on white-collar tasks of economic value. We evaluate agents on
real-world "messy" open-web research tasks of the type that are routine in
finance and consulting. In doing so, we lay the groundwork for an LLM agent
evaluation suite where good performance directly corresponds to a large
economic and societal impact. We built and tested several agent architectures
with o1-preview, GPT-4o, Claude-3.5 Sonnet, Llama 3.1 (405b), and GPT-4o-mini.
On average, LLM agents powered by Claude-3.5 Sonnet and o1-preview
substantially outperformed agents using GPT-4o, with agents based on Llama 3.1
(405b) and GPT-4o-mini lagging noticeably behind. Across LLMs, a ReAct
architecture with the ability to delegate subtasks to subagents performed best.
In addition to quantitative evaluations, we qualitatively assessed the
performance of the LLM agents by inspecting their traces and reflecting on
their observations. Our evaluation represents the first in-depth assessment of
agents' abilities to conduct challenging, economically valuable analyst-style
research on the real open web.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TroL: Traversal of Layers for Large Language and Vision Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12246v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12246v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Byung-Kwan Lee, Sangyun Chung, Chae Won Kim, Beomchan Park, Yong Man Ro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language and vision models (LLVMs) have been driven by the
generalization power of large language models (LLMs) and the advent of visual
instruction tuning. Along with scaling them up directly, these models enable
LLVMs to showcase powerful vision language (VL) performances by covering
diverse tasks via natural language instructions. However, existing open-source
LLVMs that perform comparably to closed-source LLVMs such as GPT-4V are often
considered too large (e.g., 26B, 34B, and 110B parameters), having a larger
number of layers. These large models demand costly, high-end resources for both
training and inference. To address this issue, we present a new efficient LLVM
family with 1.8B, 3.8B, and 7B LLM model sizes, Traversal of Layers (TroL),
which enables the reuse of layers in a token-wise manner. This layer traversing
technique simulates the effect of looking back and retracing the answering
stream while increasing the number of forward propagation layers without
physically adding more layers. We demonstrate that TroL employs a simple layer
traversing approach yet efficiently outperforms the open-source LLVMs with
larger model sizes and rivals the performances of the closed-source LLVMs with
substantial sizes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024. Code is available in https://github.com/ByungKwanLee/TroL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Fine Line: Navigating Large Language Model <span class="highlight-title">Pretrain</span>ing with
  Down-streaming Capability Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.01204v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.01204v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Yang, Junzhuo Li, Xinyao Niu, Xinrun Du, Songyang Gao, Haoran Zhang, Zhaoliang Chen, Xingwei Qu, Ruibin Yuan, Yizhi Li, Jiaheng Liu, Stephen W. Huang, Shawn Yue, Jie Fu, Ge Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Uncovering early-stage metrics that reflect final model performance is one
core principle for large-scale pretraining. The existing scaling law
demonstrates the power-law correlation between pretraining loss and training
flops, which serves as an important indicator of the current training state for
large language models. However, this principle only focuses on the model's
compression properties on the training data, resulting in an inconsistency with
the ability improvements on the downstream tasks. Some follow-up works
attempted to extend the scaling-law to more complex metrics (such as
hyperparameters), but still lacked a comprehensive analysis of the dynamic
differences among various capabilities during pretraining. To address the
aforementioned limitations, this paper undertakes a comprehensive comparison of
model capabilities at various pretraining intermediate checkpoints. Through
this analysis, we confirm that specific downstream metrics exhibit similar
training dynamics across models of different sizes, up to 67 billion
parameters. In addition to our core findings, we've reproduced Amber and
OpenLLaMA, releasing their intermediate checkpoints. This initiative offers
valuable resources to the research community and facilitates the verification
and exploration of LLM pretraining by open-source researchers. Besides, we
provide empirical summaries, including performance comparisons of different
models and capabilities, and tuition of key metrics for different training
phases. Based on these findings, we provide a more user-friendly strategy for
evaluating the optimization state, offering guidance for establishing a stable
pretraining process.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The FruitShell French synthesis system at the Blizzard 2023 Challenge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.00223v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.00223v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Qi, Xiaopeng Wang, Zhiyong Wang, Wang Liu, Mingming Ding, Shuchen Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a French text-to-speech synthesis system for the Blizzard
Challenge 2023. The challenge consists of two tasks: generating high-quality
speech from female speakers and generating speech that closely resembles
specific individuals. Regarding the competition data, we conducted a screening
process to remove missing or erroneous text data. We organized all symbols
except for phonemes and eliminated symbols that had no pronunciation or zero
duration. Additionally, we added word boundary and start/end symbols to the
text, which we have found to improve speech quality based on our previous
experience. For the Spoke task, we performed data augmentation according to the
competition rules. We used an open-source G2P model to transcribe the French
texts into phonemes. As the G2P model uses the International Phonetic Alphabet
(IPA), we applied the same transcription process to the provided competition
data for standardization. However, due to compiler limitations in recognizing
special symbols from the IPA chart, we followed the rules to convert all
phonemes into the phonetic scheme used in the competition data. Finally, we
resampled all competition audio to a uniform sampling rate of 16 kHz. We
employed a VITS-based acoustic model with the hifigan vocoder. For the Spoke
task, we trained a multi-speaker model and incorporated speaker information
into the duration predictor, vocoder, and flow layers of the model. The
evaluation results of our system showed a quality MOS score of 3.6 for the Hub
task and 3.4 for the Spoke task, placing our system at an average level among
all participating teams.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Boosting Scientific Concepts Understanding: Can Analogy from Teacher
  Models Empower Student Models? <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11375v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11375v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyu Yuan, Cheng Jiayang, Lin Qiu, Deqing Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Analogical reasoning plays a critical role in human cognition, enabling us to
understand new concepts by associating them with familiar ones. Previous
research in the AI community has mainly focused on identifying and generating
analogies and then examining their quality under human evaluation, which
overlooks the practical application of these analogies in real-world settings.
Inspired by the human education process, in this paper, we propose to
investigate how analogies created by teacher language models (LMs) can assist
student LMs in understanding scientific concepts, thereby aligning more closely
with practical scenarios. Our results suggest that free-form analogies can
indeed aid LMs in understanding concepts. Additionally, analogies generated by
student LMs can improve their own performance on scientific question answering,
demonstrating their capability to use analogies for self-learning new
knowledge. Resources are available at https://github.com/siyuyuan/SCUA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Social Bias in Large Language Models For Bangla: An Empirical Study on
  Gender and Religious Bias 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.03536v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.03536v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jayanta Sadhu, Maneesha Rani Saha, Rifat Shahriyar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid growth of Large Language Models (LLMs) has put forward the study of
biases as a crucial field. It is important to assess the influence of different
types of biases embedded in LLMs to ensure fair use in sensitive fields.
Although there have been extensive works on bias assessment in English, such
efforts are rare and scarce for a major language like Bangla. In this work, we
examine two types of social biases in LLM generated outputs for Bangla
language. Our main contributions in this work are: (1) bias studies on two
different social biases for Bangla (2) a curated dataset for bias measurement
benchmarking (3) testing two different probing techniques for bias detection in
the context of Bangla. This is the first work of such kind involving bias
assessment of LLMs for Bangla to the best of our knowledge. All our code and
resources are publicly available for the progress of bias related research in
Bangla NLP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Privacy Evaluation Benchmarks for NLP Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15868v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15868v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Huang, Yinggui Wang, Cen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  By inducing privacy attacks on NLP models, attackers can obtain sensitive
information such as training data and model parameters, etc. Although
researchers have studied, in-depth, several kinds of attacks in NLP models,
they are non-systematic analyses. It lacks a comprehensive understanding of the
impact caused by the attacks. For example, we must consider which scenarios can
apply to which attacks, what the common factors are that affect the performance
of different attacks, the nature of the relationships between different
attacks, and the influence of various datasets and models on the effectiveness
of the attacks, etc. Therefore, we need a benchmark to holistically assess the
privacy risks faced by NLP models. In this paper, we present a privacy attack
and defense evaluation benchmark in the field of NLP, which includes the
conventional/small models and large language models (LLMs). This benchmark
supports a variety of models, datasets, and protocols, along with standardized
modules for comprehensive evaluation of attacks and defense strategies. Based
on the above framework, we present a study on the association between auxiliary
data from different domains and the strength of privacy attacks. And we provide
an improved attack method in this scenario with the help of Knowledge
Distillation (KD). Furthermore, we propose a chained framework for privacy
attacks. Allowing a practitioner to chain multiple attacks to achieve a
higher-level attack objective. Based on this, we provide some defense and
enhanced attack strategies. The code for reproducing the results can be found
at https://github.com/user2311717757/nlp_doctor.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Needs further optimization</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Block-Attention for Efficient RAG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15355v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15355v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        East Sun, Yan Wang, Lan Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Block-Attention, an attention mechanism designed to address the
increased inference latency and cost in Retrieval-Augmented Generation (RAG)
scenarios. Unlike existing works that encodes the whole context, its main idea
lies in dividing the retrieved documents into blocks, where each block
calculates key-value (KV) states independently except for the final block. In
RAG scenarios, by defining each passage as a block, Block-Attention enables us
to pre-compute the KV states for all passages and cache them in memory,
significantly reducing the latency and the computation cost during inference.
The implementation involves block segmentation, positional encoding
calculation, and fine-tuning the LLM to adapt to the Block-Attention mechanism.
Experiments on four RAG benchmarks demonstrate that after block fine-tuning,
the Block Attention model can achieve performance comparable to (68.4\% vs
67.9\% on Llama3) or even better (62.8\% vs 59.6\% on Mistral) than
self-attention models. Notably, Block-Attention reduces the TTFT (the time to
first token) and FLOPs (floating point operations) to a very low level. It only
takes 45 ms to output the first token for an input sequence with a total length
of 32K. Compared with the self-attention model, the time consumption and
corresponding FLOPs are reduced by 98.7\% and 99.8\%, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MegaFake: A Theory-Driven <span class="highlight-title">Dataset</span> of Fake News Generated by Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11871v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11871v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lionel Z. Wang, Yiming Ma, Renfei Gao, Beichen Guo, Han Zhu, Wenqi Fan, Zexin Lu, Ka Chung Ng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of large language models (LLMs) has revolutionized online content
creation, making it much easier to generate high-quality fake news. This misuse
threatens the integrity of our digital environment and ethical standards.
Therefore, understanding the motivations and mechanisms behind LLM-generated
fake news is crucial. In this study, we analyze the creation of fake news from
a social psychology perspective and develop a comprehensive LLM-based
theoretical framework, LLM-Fake Theory. We introduce a novel pipeline that
automates the generation of fake news using LLMs, thereby eliminating the need
for manual annotation. Utilizing this pipeline, we create a theoretically
informed Machine-generated Fake news dataset, MegaFake, derived from the
GossipCop dataset. We conduct comprehensive analyses to evaluate our MegaFake
dataset. We believe that our dataset and insights will provide valuable
contributions to future research focused on the detection and governance of
fake news in the era of LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MLLM Is a Strong Reranker: Advancing Multimodal Retrieval-augmented
  Generation via Knowledge-enhanced Reranking and Noise-injected Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.21439v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.21439v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhanpeng Chen, Chengjin Xu, Yiyan Qi, Jian Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in processing and generating content across multiple data
modalities. However, a significant drawback of MLLMs is their reliance on
static training data, leading to outdated information and limited contextual
awareness. This static nature hampers their ability to provide accurate and
up-to-date responses, particularly in dynamic or rapidly evolving contexts.
Though integrating Multimodal Retrieval-augmented Generation (Multimodal RAG)
offers a promising solution, the system would inevitably encounter the
multi-granularity noisy correspondence (MNC) problem, which hinders accurate
retrieval and generation. In this work, we propose RagVL, a novel framework
with knowledge-enhanced reranking and noise-injected training, to address these
limitations. We instruction-tune the MLLM with a simple yet effective
instruction template to induce its ranking ability and serve it as a reranker
to precisely filter the top-k retrieved images. For generation, we inject
visual noise during training at the data and token levels to enhance the
generator's robustness. Extensive experiments on the subsets of two datasets
that require retrieving and reasoning over images to answer a given query
verify the effectiveness of our method. Code and models are available at
https://github.com/IDEA-FinAI/RagVL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can AI writing be salvaged? Mitigating Idiosyncrasies and Improving
  Human-AI Alignment in the Writing Process through Edits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14509v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14509v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuhin Chakrabarty, Philippe Laban, Chien-Sheng Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLM-based applications are helping people write, and LLM-generated text is
making its way into social media, journalism, and our classrooms. However, the
differences between LLM-generated and human-written text remain unclear. To
explore this, we hired professional writers to edit paragraphs in several
creative domains. We first found these writers agree on undesirable
idiosyncrasies in LLM-generated text, formalizing it into a seven-category
taxonomy (e.g. cliches, unnecessary exposition). Second, we curated the LAMP
corpus: 1,057 LLM-generated paragraphs edited by professional writers according
to our taxonomy. Analysis of LAMP reveals that none of the LLMs used in our
study (GPT4o, Claude-3.5-Sonnet, Llama-3.1-70b) outperform each other in terms
of writing quality, revealing common limitations across model families. Third,
we explored automatic editing methods to improve LLM-generated text. A
large-scale preference annotation confirms that although experts largely prefer
text edited by other experts, automatic editing methods show promise in
improving alignment between LLM-generated and human-written text.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NLP+HCI, Behavioral Science</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Conversational Health Agents: A Personalized LLM-Powered Agent Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02374v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02374v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahyar Abbasian, Iman Azimi, Amir M. Rahmani, Ramesh Jain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational Health Agents (CHAs) are interactive systems that provide
healthcare services, such as assistance and diagnosis. Current CHAs, especially
those utilizing Large Language Models (LLMs), primarily focus on conversation
aspects. However, they offer limited agent capabilities, specifically lacking
multi-step problem-solving, personalized conversations, and multimodal data
analysis. Our aim is to overcome these limitations. We propose openCHA, an
open-source LLM-powered framework, to empower conversational agents to generate
a personalized response for users' healthcare queries. This framework enables
developers to integrate external sources including data sources, knowledge
bases, and analysis models, into their LLM-based solutions. openCHA includes an
orchestrator to plan and execute actions for gathering information from
external sources, essential for formulating responses to user inquiries. It
facilitates knowledge acquisition, problem-solving capabilities, multilingual
and multimodal conversations, and fosters interaction with various AI
platforms. We illustrate the framework's proficiency in handling complex
healthcare tasks via two demonstrations and four use cases. Moreover, we
release openCHA as open source available to the community via GitHub.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 6 figures, 2 tables, 4 appendices, journal paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ M^2PT: Multimodal <span class="highlight-title">Prompt</span> Tuning for Zero-shot Instruction Learning <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15657v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15657v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taowen Wang, Yiyang Liu, James Chenhao Liang, junhan zhao, Yiming Cui, Yuning Mao, Shaoliang Nie, Jiahao Liu, Fuli Feng, Zenglin Xu, Cheng Han, Lifu Huang, Qifan Wang, Dongfang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) demonstrate remarkable performance
across a wide range of domains, with increasing emphasis on enhancing their
zero-shot generalization capabilities for unseen tasks across various
modalities. Instruction tuning has emerged as an effective strategy for
achieving zero-shot generalization by finetuning pretrained models on diverse
multimodal tasks. As the scale of MLLMs continues to grow, parameter-efficient
finetuning becomes increasingly critical. However, most existing
parameter-efficient approaches focus only on single modalities and often
overlook the multimodal characteristics during finetuning. In this work, we
introduce a novel Multimodal Prompt Tuning (M$^2$PT) approach for efficient
instruction tuning of MLLMs. M$^2$PT effectively integrates visual and textual
prompts into the vision encoder and language processor respectively during
finetuning, facilitating the extraction and alignment of features across
modalities. Empirical results on various multimodal evaluation datasets
demonstrate the superior performance of our approach compared to several
state-of-the-art baselines. A comprehensive set of ablation studies validates
the effectiveness of our prompt design and the efficiency of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Translation of Multifaceted Data without Re-Training of Machine
  Translation Systems <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.16257v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.16257v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyeonseok Moon, Seungyoon Lee, Seongtae Hong, Seungjun Lee, Chanjun Park, Heuiseok Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Translating major language resources to build minor language resources
becomes a widely-used approach. Particularly in translating complex data points
composed of multiple components, it is common to translate each component
separately. However, we argue that this practice often overlooks the
interrelation between components within the same data point. To address this
limitation, we propose a novel MT pipeline that considers the intra-data
relation in implementing MT for training data. In our MT pipeline, all the
components in a data point are concatenated to form a single translation
sequence and subsequently reconstructed to the data components after
translation. We introduce a Catalyst Statement (CS) to enhance the intra-data
relation, and Indicator Token (IT) to assist the decomposition of a translated
sequence into its respective data components. Through our approach, we have
achieved a considerable improvement in translation quality itself, along with
its effectiveness as training data. Compared with the conventional approach
that translates each data component separately, our method yields better
training data that enhances the performance of the trained model by 2.690
points for the web page ranking (WPR) task, and 0.845 for the question
generation (QG) task in the XGLUE benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP2024 findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero-resource Hallucination Detection for Text Generation via
  Graph-based Contextual Knowledge Triples Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11283v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11283v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyue Fang, Zhen Huang, Zhiliang Tian, Minghui Fang, Ziyi Pan, Quntian Fang, Zhihua Wen, Hengyue Pan, Dongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLMs obtain remarkable performance but suffer from hallucinations. Most
research on detecting hallucination focuses on the questions with short and
concrete correct answers that are easy to check the faithfulness. Hallucination
detections for text generation with open-ended answers are more challenging.
Some researchers use external knowledge to detect hallucinations in generated
texts, but external resources for specific scenarios are hard to access. Recent
studies on detecting hallucinations in long text without external resources
conduct consistency comparison among multiple sampled outputs. To handle long
texts, researchers split long texts into multiple facts and individually
compare the consistency of each pairs of facts. However, these methods (1)
hardly achieve alignment among multiple facts; (2) overlook dependencies
between multiple contextual facts. In this paper, we propose a graph-based
context-aware (GCA) hallucination detection for text generations, which aligns
knowledge facts and considers the dependencies between contextual knowledge
triples in consistency comparison. Particularly, to align multiple facts, we
conduct a triple-oriented response segmentation to extract multiple knowledge
triples. To model dependencies among contextual knowledge triple (facts), we
construct contextual triple into a graph and enhance triples' interactions via
message passing and aggregating via RGCN. To avoid the omission of knowledge
triples in long text, we conduct a LLM-based reverse verification via
reconstructing the knowledge triples. Experiments show that our model enhances
hallucination detection and excels all baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language Models Learn to Mislead Humans via RLHF 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12822v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12822v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxin Wen, Ruiqi Zhong, Akbir Khan, Ethan Perez, Jacob Steinhardt, Minlie Huang, Samuel R. Bowman, He He, Shi Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models (LMs) can produce errors that are hard to detect for humans,
especially when the task is complex. RLHF, the most popular post-training
method, may exacerbate this problem: to achieve higher rewards, LMs might get
better at convincing humans that they are right even when they are wrong. We
study this phenomenon under a standard RLHF pipeline, calling it "U-SOPHISTRY"
since it is Unintended by model developers. Specifically, we ask
time-constrained (e.g., 3-10 minutes) human subjects to evaluate the
correctness of model outputs and calculate humans' accuracy against gold
labels. On a question-answering task (QuALITY) and programming task (APPS),
RLHF makes LMs better at convincing our subjects but not at completing the task
correctly. RLHF also makes the model harder to evaluate: our subjects' false
positive rate increases by 24.1% on QuALITY and 18.3% on APPS. Finally, we show
that probing, a state-of-the-art approach for detecting Intended Sophistry
(e.g. backdoored LMs), does not generalize to U-SOPHISTRY. Our results
highlight an important failure mode of RLHF and call for more research in
assisting humans to align them.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Benchmarking Cognitive Domains for LLMs: Insights from Taiwanese Hakka
  Culture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01556v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01556v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen-Chi Chang, Ching-Yuan Chen, Hung-Shin Lee, Chih-Cheng Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study introduces a comprehensive benchmark designed to evaluate the
performance of large language models (LLMs) in understanding and processing
cultural knowledge, with a specific focus on Hakka culture as a case study.
Leveraging Bloom's Taxonomy, the study develops a multi-dimensional framework
that systematically assesses LLMs across six cognitive domains: Remembering,
Understanding, Applying, Analyzing, Evaluating, and Creating. This benchmark
extends beyond traditional single-dimensional evaluations by providing a deeper
analysis of LLMs' abilities to handle culturally specific content, ranging from
basic recall of facts to higher-order cognitive tasks such as creative
synthesis. Additionally, the study integrates Retrieval-Augmented Generation
(RAG) technology to address the challenges of minority cultural knowledge
representation in LLMs, demonstrating how RAG enhances the models' performance
by dynamically incorporating relevant external information. The results
highlight the effectiveness of RAG in improving accuracy across all cognitive
domains, particularly in tasks requiring precise retrieval and application of
cultural knowledge. However, the findings also reveal the limitations of RAG in
creative tasks, underscoring the need for further optimization. This benchmark
provides a robust tool for evaluating and comparing LLMs in culturally diverse
contexts, offering valuable insights for future research and development in
AI-driven cultural knowledge preservation and dissemination.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to O-COCOSDA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Speech Robust Bench: A Robustness Benchmark For Speech Recognition <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07937v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07937v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad A. Shah, David Solans Noguero, Mikko A. Heikkila, Bhiksha Raj, Nicolas Kourtellis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Automatic Speech Recognition (ASR) models become ever more pervasive, it
is important to ensure that they make reliable predictions under corruptions
present in the physical and digital world. We propose Speech Robust Bench
(SRB), a comprehensive benchmark for evaluating the robustness of ASR models to
diverse corruptions. SRB is composed of 114 input perturbations which simulate
an heterogeneous range of corruptions that ASR models may encounter when
deployed in the wild. We use SRB to evaluate the robustness of several
state-of-the-art ASR models and observe that model size and certain modeling
choices such as the use of discrete representations, or self-training appear to
be conducive to robustness. We extend this analysis to measure the robustness
of ASR models on data from various demographic subgroups, namely English and
Spanish speakers, and males and females. Our results revealed noticeable
disparities in the model's robustness across subgroups. We believe that SRB
will significantly facilitate future research towards robust ASR models, by
making it easier to conduct comprehensive and comparable robustness
evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to NeurIPS datasets and benchmark track 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.05892v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.05892v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Xingjian Du, Teddy Ferdinan, Haowen Hou, Przemysław Kazienko, Kranthi Kiran GV, Jan Kocoń, Bartłomiej Koptyra, Satyapriya Krishna, Ronald McClelland Jr., Niklas Muennighoff, Fares Obeid, Atsushi Saito, Guangyu Song, Haoqin Tu, Stanisław Woźniak, Ruichong Zhang, Bingchen Zhao, Qihang Zhao, Peng Zhou, Jian Zhu, Rui-Jie Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Eagle (RWKV-5) and Finch (RWKV-6), sequence models improving upon
the RWKV (RWKV-4) architecture. Our architectural design advancements include
multi-headed matrix-valued states and a dynamic recurrence mechanism that
improve expressivity while maintaining the inference efficiency characteristics
of RNNs. We introduce a new multilingual corpus with 1.12 trillion tokens and a
fast tokenizer based on greedy matching for enhanced multilinguality. We
trained four Eagle models, ranging from 0.46 to 7.5 billion parameters, and two
Finch models with 1.6 and 3.1 billion parameters and find that they achieve
competitive performance across a wide variety of benchmarks. We release all our
models on HuggingFace under the Apache 2.0 license. Models at:
https://huggingface.co/RWKV Training code at: https://github.com/RWKV/RWKV-LM
Inference code at: https://github.com/RWKV/ChatRWKV Time-parallel training code
at: https://github.com/RWKV/RWKV-infctx-trainer
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">137</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Molmo and PixMo: Open Weights and Open Data for State-of-the-Art
  Multimodal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17146v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17146v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, Jiasen Lu, Taira Anderson, Erin Bransom, Kiana Ehsani, Huong Ngo, YenSung Chen, Ajay Patel, Mark Yatskar, Chris Callison-Burch, Andrew Head, Rose Hendrix, Favyen Bastani, Eli VanderBilt, Nathan Lambert, Yvonne Chou, Arnavi Chheda, Jenna Sparks, Sam Skjonsberg, Michael Schmitz, Aaron Sarnat, Byron Bischoff, Pete Walsh, Chris Newell, Piper Wolters, Tanmay Gupta, Kuo-Hao Zeng, Jon Borchardt, Dirk Groeneveld, Jen Dumas, Crystal Nam, Sophie Lebrecht, Caitlin Wittlif, Carissa Schoenick, Oscar Michel, Ranjay Krishna, Luca Weihs, Noah A. Smith, Hannaneh Hajishirzi, Ross Girshick, Ali Farhadi, Aniruddha Kembhavi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Today's most advanced multimodal models remain proprietary. The strongest
open-weight models rely heavily on synthetic data from proprietary VLMs to
achieve good performance, effectively distilling these closed models into open
ones. As a result, the community is still missing foundational knowledge about
how to build performant VLMs from scratch. We present Molmo, a new family of
VLMs that are state-of-the-art in their class of openness. Our key innovation
is a novel, highly detailed image caption dataset collected entirely from human
annotators using speech-based descriptions. To enable a wide array of user
interactions, we also introduce a diverse dataset mixture for fine-tuning that
includes in-the-wild Q&A and innovative 2D pointing data. The success of our
approach relies on careful choices for the model architecture details, a
well-tuned training pipeline, and, most critically, the quality of our newly
collected datasets, all of which will be released. The best-in-class 72B model
within the Molmo family not only outperforms others in the class of open weight
and data models but also compares favorably against proprietary systems like
GPT-4o, Claude 3.5, and Gemini 1.5 on both academic benchmarks and human
evaluation.
  We will be releasing all of our model weights, captioning and fine-tuning
data, and source code in the near future. Select model weights, inference code,
and demo are available at https://molmo.allenai.org.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DreamWaltz-G: Expressive 3D Gaussian Avatars from Skeleton-Guided 2D
  Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17145v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17145v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yukun Huang, Jianan Wang, Ailing Zeng, Zheng-Jun Zha, Lei Zhang, Xihui Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging pretrained 2D diffusion models and score distillation sampling
(SDS), recent methods have shown promising results for text-to-3D avatar
generation. However, generating high-quality 3D avatars capable of expressive
animation remains challenging. In this work, we present DreamWaltz-G, a novel
learning framework for animatable 3D avatar generation from text. The core of
this framework lies in Skeleton-guided Score Distillation and Hybrid 3D
Gaussian Avatar representation. Specifically, the proposed skeleton-guided
score distillation integrates skeleton controls from 3D human templates into 2D
diffusion models, enhancing the consistency of SDS supervision in terms of view
and human pose. This facilitates the generation of high-quality avatars,
mitigating issues such as multiple faces, extra limbs, and blurring. The
proposed hybrid 3D Gaussian avatar representation builds on the efficient 3D
Gaussians, combining neural implicit fields and parameterized 3D meshes to
enable real-time rendering, stable SDS optimization, and expressive animation.
Extensive experiments demonstrate that DreamWaltz-G is highly effective in
generating and animating 3D avatars, outperforming existing methods in both
visual quality and animation expressiveness. Our framework further supports
diverse applications, including human video reenactment and multi-subject scene
composition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://yukun-huang.github.io/DreamWaltz-G/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attention <span class="highlight-title">Prompt</span>ing on Image for Large Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17143v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17143v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runpeng Yu, Weihao Yu, Xinchao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compared with Large Language Models (LLMs), Large Vision-Language Models
(LVLMs) can also accept images as input, thus showcasing more interesting
emergent capabilities and demonstrating impressive performance on various
vision-language tasks. Motivated by text prompting in LLMs, visual prompting
has been explored to enhance LVLMs' capabilities of perceiving visual
information. However, previous visual prompting techniques solely process
visual inputs without considering text queries, limiting the models' ability to
follow text instructions to complete tasks. To fill this gap, in this work, we
propose a new prompting technique named Attention Prompting on Image, which
just simply overlays a text-query-guided attention heatmap on the original
input image and effectively enhances LVLM on various tasks. Specifically, we
generate an attention heatmap for the input image dependent on the text query
with an auxiliary model like CLIP. Then the heatmap simply multiplies the pixel
values of the original image to obtain the actual input image for the LVLM.
Extensive experiments on various vison-language benchmarks verify the
effectiveness of our technique. For example, Attention Prompting on Image
improves LLaVA-1.5 by 3.8% and 2.9% on MM-Vet and LLaVA-Wild benchmarks,
respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website, see https://yu-rp.github.io/api-prompting</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PACE: marrying generalization in PArameter-efficient fine-tuning with
  Consistency rEgularization <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17137v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17137v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Ni, Shan Zhang, Piotr Koniusz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter-Efficient Fine-Tuning (PEFT) effectively adapts pre-trained vision
transformers to downstream tasks. However, the optimization for tasks
performance often comes at the cost of generalizability in fine-tuned models.
To address this issue, we theoretically connect smaller weight gradient norms
during training and larger datasets to the improved model generalization.
Motivated by this connection, we propose reducing gradient norms for enhanced
generalization and aligning fine-tuned model with the pre-trained counterpart
to retain knowledge from large-scale pre-training data. Yet, naive alignment
does not guarantee gradient reduction and can potentially cause gradient
explosion, complicating efforts to manage gradients. To address such issues, we
propose PACE, marrying generalization of PArameter-efficient fine-tuning with
Consistency rEgularization. We perturb features learned from the adapter with
the multiplicative noise and ensure the fine-tuned model remains consistent for
same sample under different perturbations. Theoretical analysis shows that PACE
not only implicitly regularizes gradients for enhanced generalization, but also
implicitly aligns the fine-tuned and pre-trained models to retain knowledge.
Experimental evidence supports our theories. PACE outperforms existing PEFT
methods in four visual adaptation tasks: VTAB-1k, FGVC, few-shot learning and
domain adaptation. Code will be available at
https://github.com/MaxwellYaoNi/PACE
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024 as a spotlight. This preliminary version
  will soon be extended with the experiments and analyses from the rebuttal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Streaming Neural Images <span class="chip">ICIP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17134v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17134v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcos V. Conde, Andy Bigos, Radu Timofte
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implicit Neural Representations (INRs) are a novel paradigm for signal
representation that have attracted considerable interest for image compression.
INRs offer unprecedented advantages in signal resolution and memory efficiency,
enabling new possibilities for compression techniques. However, the existing
limitations of INRs for image compression have not been sufficiently addressed
in the literature. In this work, we explore the critical yet overlooked
limiting factors of INRs, such as computational cost, unstable performance, and
robustness. Through extensive experiments and empirical analysis, we provide a
deeper and more nuanced understanding of implicit neural image compression
methods such as Fourier Feature Networks and Siren. Our work also offers
valuable insights for future research in this area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE International Conference on Image Processing (ICIP)2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Classification of Gleason Grading in Prostate Cancer Histopathology
  Images Using Deep Learning Techniques: YOLO, Vision <span class="highlight-title">Transformer</span>s, and Vision
  Mamba 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17122v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17122v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amin Malekmohammadi, Ali Badiezadeh, Seyed Mostafa Mirhassani, Parisa Gifani, Majid Vafaeezadeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prostate cancer ranks among the leading health issues impacting men, with the
Gleason scoring system serving as the primary method for diagnosis and
prognosis. This system relies on expert pathologists to evaluate samples of
prostate tissue and assign a Gleason grade, a task that requires significant
time and manual effort. To address this challenge, artificial intelligence (AI)
solutions have been explored to automate the grading process. In light of these
challenges, this study evaluates and compares the effectiveness of three deep
learning methodologies, YOLO, Vision Transformers, and Vision Mamba, in
accurately classifying Gleason grades from histopathology images. The goal is
to enhance diagnostic precision and efficiency in prostate cancer management.
This study utilized two publicly available datasets, Gleason2019 and SICAPv2,
to train and test the performance of YOLO, Vision Transformers, and Vision
Mamba models. Each model was assessed based on its ability to classify Gleason
grades accurately, considering metrics such as false positive rate, false
negative rate, precision, and recall. The study also examined the computational
efficiency and applicability of each method in a clinical setting. Vision Mamba
demonstrated superior performance across all metrics, achieving high precision
and recall rates while minimizing false positives and negatives. YOLO showed
promise in terms of speed and efficiency, particularly beneficial for real-time
analysis. Vision Transformers excelled in capturing long-range dependencies
within images, although they presented higher computational complexity compared
to the other models. Vision Mamba emerges as the most effective model for
Gleason grade classification in histopathology images, offering a balance
between accuracy and computational efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Small data deep learning methodology for in-field disease detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17119v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17119v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Herrera-Poyato, Jacinto Domínguez-Rull, Rosana Montes, Inés Hernánde, Ignacio Barrio, Carlos Poblete-Echeverria, Javier Tardaguila, Francisco Herrera, Andrés Herrera-Poyatos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Early detection of diseases in crops is essential to prevent harvest losses
and improve the quality of the final product. In this context, the combination
of machine learning and proximity sensors is emerging as a technique capable of
achieving this detection efficiently and effectively. For example, this machine
learning approach has been applied to potato crops -- to detect late blight
(Phytophthora infestans) -- and grapevine crops -- to detect downy mildew.
However, most of these AI models found in the specialised literature have been
developed using leaf-by-leaf images taken in the lab, which does not represent
field conditions and limits their applicability.
  In this study, we present the first machine learning model capable of
detecting mild symptoms of late blight in potato crops through the analysis of
high-resolution RGB images captured directly in the field, overcoming the
limitations of other publications in the literature and presenting real-world
applicability. Our proposal exploits the availability of high-resolution images
via the concept of patching, and is based on deep convolutional neural networks
with a focal loss function, which makes the model to focus on the complex
patterns that arise in field conditions. Additionally, we present a data
augmentation scheme that facilitates the training of these neural networks with
few high-resolution images, which allows for development of models under the
small data paradigm.
  Our model correctly detects all cases of late blight in the test dataset,
demonstrating a high level of accuracy and effectiveness in identifying early
symptoms. These promising results reinforce the potential use of machine
learning for the early detection of diseases and pests in agriculture, enabling
better treatment and reducing their impact on crops.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MorphoSeg: An Uncertainty-Aware Deep Learning Method for Biomedical
  Segmentation of Complex Cellular Morphologies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17110v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17110v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianhao Zhang, Heather J. McCourty, Berardo M. Sanchez-Tafolla, Anton Nikolaev, Lyudmila S. Mihaylova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning has revolutionized medical and biological imaging, particularly
in segmentation tasks. However, segmenting biological cells remains challenging
due to the high variability and complexity of cell shapes. Addressing this
challenge requires high-quality datasets that accurately represent the diverse
morphologies found in biological cells. Existing cell segmentation datasets are
often limited by their focus on regular and uniform shapes. In this paper, we
introduce a novel benchmark dataset of Ntera-2 (NT2) cells, a pluripotent
carcinoma cell line, exhibiting diverse morphologies across multiple stages of
differentiation, capturing the intricate and heterogeneous cellular structures
that complicate segmentation tasks. To address these challenges, we propose an
uncertainty-aware deep learning framework for complex cellular morphology
segmentation (MorphoSeg) by incorporating sampling of virtual outliers from
low-likelihood regions during training. Our comprehensive experimental
evaluations against state-of-the-art baselines demonstrate that MorphoSeg
significantly enhances segmentation accuracy, achieving up to a 7.74% increase
in the Dice Similarity Coefficient (DSC) and a 28.36% reduction in the
Hausdorff Distance. These findings highlight the effectiveness of our dataset
and methodology in advancing cell segmentation capabilities, especially for
complex and variable cell morphologies. The dataset and source code is publicly
available at https://github.com/RanchoGoose/MorphoSeg.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unveiling Ontological Commitment in Multi-Modal Foundation Models <span class="chip">ECAI2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17109v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17109v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mert Keser, Gesina Schwalbe, Niki Amini-Naieni, Matthias Rottmann, Alois Knoll
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ontological commitment, i.e., used concepts, relations, and assumptions, are
a corner stone of qualitative reasoning (QR) models. The state-of-the-art for
processing raw inputs, though, are deep neural networks (DNNs), nowadays often
based off from multimodal foundation models. These automatically learn rich
representations of concepts and respective reasoning. Unfortunately, the
learned qualitative knowledge is opaque, preventing easy inspection,
validation, or adaptation against available QR models. So far, it is possible
to associate pre-defined concepts with latent representations of DNNs, but
extractable relations are mostly limited to semantic similarity. As a next step
towards QR for validation and verification of DNNs: Concretely, we propose a
method that extracts the learned superclass hierarchy from a multimodal DNN for
a given set of leaf concepts. Under the hood we (1) obtain leaf concept
embeddings using the DNN's textual input modality; (2) apply hierarchical
clustering to them, using that DNNs encode semantic similarities via vector
distances; and (3) label the such-obtained parent concepts using search in
available ontologies from QR. An initial evaluation study shows that meaningful
ontological class hierarchies can be extracted from state-of-the-art foundation
models. Furthermore, we demonstrate how to validate and verify a DNN's learned
representations against given ontologies. Lastly, we discuss potential future
applications in the context of QR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Qualitative Reasoning Workshop 2024 (QR2024) colocated with ECAI2024,
  camera-ready submission; first two authors contributed equally; 10 pages, 4
  figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Text2CAD: Generating Sequential CAD Models from Beginner-to-Expert Level
  Text <span class="highlight-title">Prompt</span>s <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17106v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17106v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Sadil Khan, Sankalp Sinha, Talha Uddin Sheikh, Didier Stricker, Sk Aziz Ali, Muhammad Zeshan Afzal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prototyping complex computer-aided design (CAD) models in modern softwares
can be very time-consuming. This is due to the lack of intelligent systems that
can quickly generate simpler intermediate parts. We propose Text2CAD, the first
AI framework for generating text-to-parametric CAD models using
designer-friendly instructions for all skill levels. Furthermore, we introduce
a data annotation pipeline for generating text prompts based on natural
language instructions for the DeepCAD dataset using Mistral and LLaVA-NeXT. The
dataset contains $\sim170$K models and $\sim660$K text annotations, from
abstract CAD descriptions (e.g., generate two concentric cylinders) to detailed
specifications (e.g., draw two circles with center $(x,y)$ and radius $r_{1}$,
$r_{2}$, and extrude along the normal by $d$...). Within the Text2CAD
framework, we propose an end-to-end transformer-based auto-regressive network
to generate parametric CAD models from input texts. We evaluate the performance
of our model through a mixture of metrics, including visual quality, parametric
precision, and geometrical accuracy. Our proposed framework shows great
potential in AI-aided design applications. Our source code and annotations will
be publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in NeurIPS 2024 (Spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ General Detection-based Text Line Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17095v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17095v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raphael Baena, Syrine Kalleli, Mathieu Aubry
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a general detection-based approach to text line recognition, be
it printed (OCR) or handwritten (HTR), with Latin, Chinese, or ciphered
characters. Detection-based approaches have until now been largely discarded
for HTR because reading characters separately is often challenging, and
character-level annotation is difficult and expensive. We overcome these
challenges thanks to three main insights: (i) synthetic pre-training with
sufficiently diverse data enables learning reasonable character localization
for any script; (ii) modern transformer-based detectors can jointly detect a
large number of instances, and, if trained with an adequate masking strategy,
leverage consistency between the different detections; (iii) once a pre-trained
detection model with approximate character localization is available, it is
possible to fine-tune it with line-level annotation on real data, even with a
different alphabet. Our approach, dubbed DTLR, builds on a completely different
paradigm than state-of-the-art HTR methods, which rely on autoregressive
decoding, predicting character values one by one, while we treat a complete
line in parallel. Remarkably, we demonstrate good performance on a large range
of scripts, usually tackled with specialized approaches. In particular, we
improve state-of-the-art performances for Chinese script recognition on the
CASIA v2 dataset, and for cipher recognition on the Borg and Copiale datasets.
Our code and models are available at https://github.com/raphael-baena/DTLR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BitQ: Tailoring Block Floating Point Precision for Improved DNN
  Efficiency on Resource-Constrained Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17093v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17093v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongqi Xu, Yujian Lee, Gao Yi, Bosheng Liu, Yucong Chen, Peng Liu, Jigang Wu, Xiaoming Chen, Yinhe Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) are powerful for cognitive tasks such as image
classification, object detection, and scene segmentation. One drawback however
is the significant high computational complexity and memory consumption, which
makes them unfeasible to run real-time on embedded platforms because of the
limited hardware resources. Block floating point (BFP) quantization is one of
the representative compression approaches for reducing the memory and
computational burden owing to their capability to effectively capture the broad
data distribution of DNN models. Unfortunately, prior works on BFP-based
quantization empirically choose the block size and the precision that preserve
accuracy. In this paper, we develop a BFP-based bitwidth-aware analytical
modeling framework (called ``BitQ'') for the best BFP implementation of DNN
inference on embedded platforms. We formulate and resolve an optimization
problem to identify the optimal BFP block size and bitwidth distribution by the
trade-off of both accuracy and performance loss. Experimental results show that
compared with an equal bitwidth setting, the BFP DNNs with optimized bitwidth
allocation provide efficient computation, preserving accuracy on famous
benchmarks. The source code and data are available at
https://github.com/Cheliosoops/BitQ.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ctrl-GenAug: Controllable Generative Augmentation for Medical Sequence
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17091v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17091v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinrui Zhou, Yuhao Huang, Haoran Dou, Shijing Chen, Ao Chang, Jia Liu, Weiran Long, Jian Zheng, Erjiao Xu, Jie Ren, Ruobing Huang, Jun Cheng, Wufeng Xue, Dong Ni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the medical field, the limited availability of large-scale datasets and
labor-intensive annotation processes hinder the performance of deep models.
Diffusion-based generative augmentation approaches present a promising solution
to this issue, having been proven effective in advancing downstream medical
recognition tasks. Nevertheless, existing works lack sufficient semantic and
sequential steerability for challenging video/3D sequence generation, and
neglect quality control of noisy synthesized samples, resulting in unreliable
synthetic databases and severely limiting the performance of downstream tasks.
In this work, we present Ctrl-GenAug, a novel and general generative
augmentation framework that enables highly semantic- and sequential-customized
sequence synthesis and suppresses incorrectly synthesized samples, to aid
medical sequence classification. Specifically, we first design a multimodal
conditions-guided sequence generator for controllably synthesizing
diagnosis-promotive samples. A sequential augmentation module is integrated to
enhance the temporal/stereoscopic coherence of generated samples. Then, we
propose a noisy synthetic data filter to suppress unreliable cases at semantic
and sequential levels. Extensive experiments on 3 medical datasets, using 11
networks trained on 3 paradigms, comprehensively analyze the effectiveness and
generality of Ctrl-GenAug, particularly in underrepresented high-risk
populations and out-domain conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 7 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parameter-efficient Bayesian Neural Networks for Uncertainty-aware Depth
  Estimation <span class="chip">ECCV'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17085v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17085v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Richard D. Paul, Alessio Quercia, Vincent Fortuin, Katharina Nöh, Hanno Scharr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art computer vision tasks, like monocular depth estimation
(MDE), rely heavily on large, modern Transformer-based architectures. However,
their application in safety-critical domains demands reliable predictive
performance and uncertainty quantification. While Bayesian neural networks
provide a conceptually simple approach to serve those requirements, they suffer
from the high dimensionality of the parameter space. Parameter-efficient
fine-tuning (PEFT) methods, in particular low-rank adaptations (LoRA), have
emerged as a popular strategy for adapting large-scale models to down-stream
tasks by performing parameter inference on lower-dimensional subspaces. In this
work, we investigate the suitability of PEFT methods for subspace Bayesian
inference in large-scale Transformer-based vision models. We show that, indeed,
combining BitFit, DiffFit, LoRA, and CoLoRA, a novel LoRA-inspired PEFT method,
with Bayesian inference enables more robust and reliable predictive performance
in MDE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at UnCV Workshop at ECCV'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Vision Language Models Learn from Visual Demonstrations of Ambiguous
  Spatial Reasoning? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17080v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17080v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Zhao, Leo Parker Dirac, Paulina Varshavskaya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large vision-language models (VLMs) have become state-of-the-art for many
computer vision tasks, with in-context learning (ICL) as a popular adaptation
strategy for new ones. But can VLMs learn novel concepts purely from visual
demonstrations, or are they limited to adapting to the output format of ICL
examples? We propose a new benchmark we call Spatial Visual Ambiguity Tasks
(SVAT) that challenges state-of-the-art VLMs to learn new visuospatial tasks
in-context. We find that VLMs fail to do this zero-shot, and sometimes continue
to fail after finetuning. However, adding simpler data to the training by
curriculum learning leads to improved ICL performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures. Code released at
  https://github.com/groundlight/vlm-visual-demonstrations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Effect of Perceptual Metrics on Music Representation Learning for
  Genre Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17069v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17069v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tashi Namgyal, Alexander Hepburn, Raul Santos-Rodriguez, Valero Laparra, Jesus Malo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The subjective quality of natural signals can be approximated with objective
perceptual metrics. Designed to approximate the perceptual behaviour of human
observers, perceptual metrics often reflect structures found in natural signals
and neurological pathways. Models trained with perceptual metrics as loss
functions can capture perceptually meaningful features from the structures held
within these metrics. We demonstrate that using features extracted from
autoencoders trained with perceptual losses can improve performance on music
understanding tasks, i.e. genre classification, over using these metrics
directly as distances when learning a classifier. This result suggests improved
generalisation to novel signals when using perceptual metrics as loss functions
for representation learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2312.03455</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Domain Generalization Algorithms in Computational Pathology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17063v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17063v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neda Zamanitajeddin, Mostafa Jahanifar, Kesi Xu, Fouzia Siraj, Nasir Rajpoot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models have shown immense promise in computational pathology
(CPath) tasks, but their performance often suffers when applied to unseen data
due to domain shifts. Addressing this requires domain generalization (DG)
algorithms. However, a systematic evaluation of DG algorithms in the CPath
context is lacking. This study aims to benchmark the effectiveness of 30 DG
algorithms on 3 CPath tasks of varying difficulty through 7,560
cross-validation runs. We evaluate these algorithms using a unified and robust
platform, incorporating modality-specific techniques and recent advances like
pretrained foundation models. Our extensive cross-validation experiments
provide insights into the relative performance of various DG strategies. We
observe that self-supervised learning and stain augmentation consistently
outperform other methods, highlighting the potential of pretrained models and
data augmentation. Furthermore, we introduce a new pan-cancer tumor detection
dataset (HISTOPANTUM) as a benchmark for future research. This study offers
valuable guidance to researchers in selecting appropriate DG approaches for
CPath tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Degradation-Guided One-Step Image Super-Resolution with Diffusion Priors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17058v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17058v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aiping Zhang, Zongsheng Yue, Renjing Pei, Wenqi Ren, Xiaochun Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion-based image super-resolution (SR) methods have achieved remarkable
success by leveraging large pre-trained text-to-image diffusion models as
priors. However, these methods still face two challenges: the requirement for
dozens of sampling steps to achieve satisfactory results, which limits
efficiency in real scenarios, and the neglect of degradation models, which are
critical auxiliary information in solving the SR problem. In this work, we
introduced a novel one-step SR model, which significantly addresses the
efficiency issue of diffusion-based SR methods. Unlike existing fine-tuning
strategies, we designed a degradation-guided Low-Rank Adaptation (LoRA) module
specifically for SR, which corrects the model parameters based on the
pre-estimated degradation information from low-resolution images. This module
not only facilitates a powerful data-dependent or degradation-dependent SR
model but also preserves the generative prior of the pre-trained diffusion
model as much as possible. Furthermore, we tailor a novel training pipeline by
introducing an online negative sample generation strategy. Combined with the
classifier-free guidance strategy during inference, it largely improves the
perceptual quality of the super-resolution results. Extensive experiments have
demonstrated the superior efficiency and effectiveness of the proposed model
compared to recent state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The code is available at https://github.com/ArcticHare105/S3Diff</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ControlCity: A Multimodal Diffusion Model Based Approach for Accurate
  Geospatial Data Generation and Urban Morphology Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17049v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17049v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangshuo Zhou, Huaxia Li, Rui Hu, Sensen Wu, Hailin Feng, Zhenhong Du, Liuchang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Volunteer Geographic Information (VGI), with its rich variety, large volume,
rapid updates, and diverse sources, has become a critical source of geospatial
data. However, VGI data from platforms like OSM exhibit significant quality
heterogeneity across different data types, particularly with urban building
data. To address this, we propose a multi-source geographic data transformation
solution, utilizing accessible and complete VGI data to assist in generating
urban building footprint data. We also employ a multimodal data generation
framework to improve accuracy. First, we introduce a pipeline for constructing
an 'image-text-metadata-building footprint' dataset, primarily based on road
network data and supplemented by other multimodal data. We then present
ControlCity, a geographic data transformation method based on a multimodal
diffusion model. This method first uses a pre-trained text-to-image model to
align text, metadata, and building footprint data. An improved ControlNet
further integrates road network and land-use imagery, producing refined
building footprint data. Experiments across 22 global cities demonstrate that
ControlCity successfully simulates real urban building patterns, achieving
state-of-the-art performance. Specifically, our method achieves an average FID
score of 50.94, reducing error by 71.01% compared to leading methods, and a
MIoU score of 0.36, an improvement of 38.46%. Additionally, our model excels in
tasks like urban morphology transfer, zero-shot city generation, and spatial
data completeness assessment. In the zero-shot city task, our method accurately
predicts and generates similar urban structures, demonstrating strong
generalization. This study confirms the effectiveness of our approach in
generating urban building footprint data and capturing complex city
characteristics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GeoBiked: A <span class="highlight-title">Dataset</span> with Geometric Features and Automated Labeling
  Techniques to Enable Deep Generative Models in Engineering Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17045v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17045v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Phillip Mueller, Sebastian Mueller, Lars Mikelsons
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We provide a dataset for enabling Deep Generative Models (DGMs) in
engineering design and propose methods to automate data labeling by utilizing
large-scale foundation models. GeoBiked is curated to contain 4 355 bicycle
images, annotated with structural and technical features and is used to
investigate two automated labeling techniques: The utilization of consolidated
latent features (Hyperfeatures) from image-generation models to detect
geometric correspondences (e.g. the position of the wheel center) in structural
images and the generation of diverse text descriptions for structural images.
GPT-4o, a vision-language-model (VLM), is instructed to analyze images and
produce diverse descriptions aligned with the system-prompt. By representing
technical images as Diffusion-Hyperfeatures, drawing geometric correspondences
between them is possible. The detection accuracy of geometric points in unseen
samples is improved by presenting multiple annotated source images. GPT-4o has
sufficient capabilities to generate accurate descriptions of technical images.
Grounding the generation only on images leads to diverse descriptions but
causes hallucinations, while grounding it on categorical labels restricts the
diversity. Using both as input balances creativity and accuracy. Successfully
using Hyperfeatures for geometric correspondence suggests that this approach
can be used for general point-detection and annotation tasks in technical
images. Labeling such images with text descriptions using VLMs is possible, but
dependent on the models detection capabilities, careful prompt-engineering and
the selection of input information. Applying foundation models in engineering
design is largely unexplored. We aim to bridge this gap with a dataset to
explore training, finetuning and conditioning DGMs in this field and suggesting
approaches to bootstrap foundation models to process technical images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EventHDR: from Event to High-Speed HDR Videos and Beyond 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17029v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17029v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunhao Zou, Ying Fu, Tsuyoshi Takatani, Yinqiang Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event cameras are innovative neuromorphic sensors that asynchronously capture
the scene dynamics. Due to the event-triggering mechanism, such cameras record
event streams with much shorter response latency and higher intensity
sensitivity compared to conventional cameras. On the basis of these features,
previous works have attempted to reconstruct high dynamic range (HDR) videos
from events, but have either suffered from unrealistic artifacts or failed to
provide sufficiently high frame rates. In this paper, we present a recurrent
convolutional neural network that reconstruct high-speed HDR videos from event
sequences, with a key frame guidance to prevent potential error accumulation
caused by the sparse event data. Additionally, to address the problem of
severely limited real dataset, we develop a new optical system to collect a
real-world dataset with paired high-speed HDR videos and event streams,
facilitating future research in this field. Our dataset provides the first real
paired dataset for event-to-HDR reconstruction, avoiding potential inaccuracies
from simulation strategies. Experimental results demonstrate that our method
can generate high-quality, high-speed HDR videos. We further explore the
potential of our work in cross-camera reconstruction and downstream computer
vision tasks, including object detection, panoramic segmentation, optical flow
estimation, and monocular depth estimation under HDR scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>TPAMI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Surgical Skill Assessment in Endoscopic Pituitary Surgery
  using Real-time Instrument Tracking on a High-fidelity Bench-top Phantom 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17025v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17025v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrito Das, Bilal Sidiqi, Laurent Mennillo, Zhehua Mao, Mikael Brudfors, Miguel Xochicale, Danyal Z. Khan, Nicola Newall, John G. Hanrahan, Matthew J. Clarkson, Danail Stoyanov, Hani J. Marcus, Sophia Bano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Improved surgical skill is generally associated with improved patient
outcomes, although assessment is subjective; labour-intensive; and requires
domain specific expertise. Automated data driven metrics can alleviate these
difficulties, as demonstrated by existing machine learning instrument tracking
models in minimally invasive surgery. However, these models have been tested on
limited datasets of laparoscopic surgery, with a focus on isolated tasks and
robotic surgery. In this paper, a new public dataset is introduced, focusing on
simulated surgery, using the nasal phase of endoscopic pituitary surgery as an
exemplar. Simulated surgery allows for a realistic yet repeatable environment,
meaning the insights gained from automated assessment can be used by novice
surgeons to hone their skills on the simulator before moving to real surgery.
PRINTNet (Pituitary Real-time INstrument Tracking Network) has been created as
a baseline model for this automated assessment. Consisting of DeepLabV3 for
classification and segmentation; StrongSORT for tracking; and the NVIDIA
Holoscan SDK for real-time performance, PRINTNet achieved 71.9% Multiple Object
Tracking Precision running at 22 Frames Per Second. Using this tracking output,
a Multilayer Perceptron achieved 87% accuracy in predicting surgical skill
level (novice or expert), with the "ratio of total procedure time to instrument
visible time" correlated with higher surgical skill. This therefore
demonstrates the feasibility of automated surgical skill assessment in
simulated endoscopic pituitary surgery. The new publicly available dataset can
be found here: https://doi.org/10.5522/04/26511049.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhanced Wavelet Scattering Network for image inpainting detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17023v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17023v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Barglazan Adrian-Alin, Brad Remus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of image inpainting tools, especially those aimed at
removing artifacts, has made digital image manipulation alarmingly accessible.
This paper proposes several innovative ideas for detecting inpainting forgeries
based on low level noise analysis by combining Dual-Tree Complex Wavelet
Transform (DT-CWT) for feature extraction with convolutional neural networks
(CNN) for forged area detection and localization, and lastly by employing an
innovative combination of texture segmentation with noise variance estimations.
The DT-CWT offers significant advantages due to its shift-invariance, enhancing
its robustness against subtle manipulations during the inpainting process.
Furthermore, its directional selectivity allows for the detection of subtle
artifacts introduced by inpainting within specific frequency bands and
orientations. Various neural network architectures were evaluated and proposed.
Lastly, we propose a fusion detection module that combines texture analysis
with noise variance estimation to give the forged area. Our approach was
benchmarked against state-of-the-art methods and demonstrated superior
performance over all cited alternatives. The training code (with pretrained
model weights) as long as the dataset will be available at
https://github.com/jmaba/Deep-dual-tree-complex-neural-network-for-image-inpainting-detection
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PTQ4RIS: Post-Training Quantization for Referring Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17020v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17020v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyan Jiang, Hang Yang, Kaiying Zhu, Xihe Qiu, Shibo Zhao, Sifan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Referring Image Segmentation (RIS), aims to segment the object referred by a
given sentence in an image by understanding both visual and linguistic
information. However, existing RIS methods tend to explore top-performance
models, disregarding considerations for practical applications on
resources-limited edge devices. This oversight poses a significant challenge
for on-device RIS inference. To this end, we propose an effective and efficient
post-training quantization framework termed PTQ4RIS. Specifically, we first
conduct an in-depth analysis of the root causes of performance degradation in
RIS model quantization and propose dual-region quantization (DRQ) and
reorder-based outlier-retained quantization (RORQ) to address the quantization
difficulties in visual and text encoders. Extensive experiments on three
benchmarks with different bits settings (from 8 to 4 bits) demonstrates its
superior performance. Importantly, we are the first PTQ method specifically
designed for the RIS task, highlighting the feasibility of PTQ in RIS
applications. Code will be available at {https://github.com/gugu511yy/PTQ4RIS}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CNN Mixture-of-Depths <span class="chip">ACCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17016v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17016v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rinor Cakaj, Jens Mehnert, Bin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Mixture-of-Depths (MoD) for Convolutional Neural Networks
(CNNs), a novel approach that enhances the computational efficiency of CNNs by
selectively processing channels based on their relevance to the current
prediction. This method optimizes computational resources by dynamically
selecting key channels in feature maps for focused processing within the
convolutional blocks (Conv-Blocks), while skipping less relevant channels.
Unlike conditional computation methods that require dynamic computation graphs,
CNN MoD uses a static computation graph with fixed tensor sizes which improve
hardware efficiency. It speeds up the training and inference processes without
the need for customized CUDA kernels, unique loss functions, or finetuning. CNN
MoD either matches the performance of traditional CNNs with reduced inference
times, GMACs, and parameters, or exceeds their performance while maintaining
similar inference times, GMACs, and parameters. For example, on ImageNet,
ResNet86-MoD exceeds the performance of the standard ResNet50 by 0.45% with a
6% speedup on CPU and 5% on GPU. Moreover, ResNet75-MoD achieves the same
performance as ResNet50 with a 25% speedup on CPU and 15% on GPU.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Conference Paper of the Asian Conference on Computer Vision (ACCV)
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adverse Weather Optical Flow: Cumulative Homogeneous-Heterogeneous
  Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17001v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17001v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanyu Zhou, Yi Chang, Zhiwei Shi, Wending Yan, Gang Chen, Yonghong Tian, Luxin Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optical flow has made great progress in clean scenes, while suffers
degradation under adverse weather due to the violation of the brightness
constancy and gradient continuity assumptions of optical flow. Typically,
existing methods mainly adopt domain adaptation to transfer motion knowledge
from clean to degraded domain through one-stage adaptation. However, this
direct adaptation is ineffective, since there exists a large gap due to adverse
weather and scene style between clean and real degraded domains. Moreover, even
within the degraded domain itself, static weather (e.g., fog) and dynamic
weather (e.g., rain) have different impacts on optical flow. To address above
issues, we explore synthetic degraded domain as an intermediate bridge between
clean and real degraded domains, and propose a cumulative
homogeneous-heterogeneous adaptation framework for real adverse weather optical
flow. Specifically, for clean-degraded transfer, our key insight is that static
weather possesses the depth-association homogeneous feature which does not
change the intrinsic motion of the scene, while dynamic weather additionally
introduces the heterogeneous feature which results in a significant boundary
discrepancy in warp errors between clean and degraded domains. For
synthetic-real transfer, we figure out that cost volume correlation shares a
similar statistical histogram between synthetic and real degraded domains,
benefiting to holistically aligning the homogeneous correlation distribution
for synthetic-real knowledge distillation. Under this unified framework, the
proposed method can progressively and explicitly transfer knowledge from clean
scenes to real adverse weather. In addition, we further collect a real adverse
weather dataset with manually annotated optical flow labels and perform
extensive experiments to verify the superiority of the proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WasteGAN: Data Augmentation for Robotic Waste Sorting through Generative
  Adversarial Networks <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16999v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16999v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alberto Bacchin, Leonardo Barcellona, Matteo Terreran, Stefano Ghidoni, Emanuele Menegatti, Takuya Kiyokawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic waste sorting poses significant challenges in both perception and
manipulation, given the extreme variability of objects that should be
recognized on a cluttered conveyor belt. While deep learning has proven
effective in solving complex tasks, the necessity for extensive data collection
and labeling limits its applicability in real-world scenarios like waste
sorting. To tackle this issue, we introduce a data augmentation method based on
a novel GAN architecture called wasteGAN. The proposed method allows to
increase the performance of semantic segmentation models, starting from a very
limited bunch of labeled examples, such as few as 100. The key innovations of
wasteGAN include a novel loss function, a novel activation function, and a
larger generator block. Overall, such innovations helps the network to learn
from limited number of examples and synthesize data that better mirrors
real-world distributions. We then leverage the higher-quality segmentation
masks predicted from models trained on the wasteGAN synthetic data to compute
semantic-aware grasp poses, enabling a robotic arm to effectively recognizing
contaminants and separating waste in a real-world scenario. Through
comprehensive evaluation encompassing dataset-based assessments and real-world
experiments, our methodology demonstrated promising potential for robotic waste
sorting, yielding performance gains of up to 5.8\% in picking contaminants. The
project page is available at https://github.com/bach05/wasteGAN.git
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 2024 IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PitRSDNet: Predicting Intra-operative Remaining Surgery Duration in
  Endoscopic Pituitary Surgery <span class="chip">MICCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16998v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16998v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anjana Wijekoon, Adrito Das, Roxana R. Herrera, Danyal Z. Khan, John Hanrahan, Eleanor Carter, Valpuri Luoma, Danail Stoyanov, Hani J. Marcus, Sophia Bano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate intra-operative Remaining Surgery Duration (RSD) predictions allow
for anaesthetists to more accurately decide when to administer anaesthetic
agents and drugs, as well as to notify hospital staff to send in the next
patient. Therefore RSD plays an important role in improving patient care and
minimising surgical theatre costs via efficient scheduling. In endoscopic
pituitary surgery, it is uniquely challenging due to variable workflow
sequences with a selection of optional steps contributing to high variability
in surgery duration. This paper presents PitRSDNet for predicting RSD during
pituitary surgery, a spatio-temporal neural network model that learns from
historical data focusing on workflow sequences. PitRSDNet integrates workflow
knowledge into RSD prediction in two forms: 1) multi-task learning for
concurrently predicting step and RSD; and 2) incorporating prior steps as
context in temporal learning and inference. PitRSDNet is trained and evaluated
on a new endoscopic pituitary surgery dataset with 88 videos to show
competitive performance improvements over previous statistical and machine
learning methods. The findings also highlight how PitRSDNet improve RSD
precision on outlier cases utilising the knowledge of prior steps.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the Augmented Environments for Computer-Assisted
  Interventions (AE-CAI) Workshop at the Medical Image Computing and
  Computer-Assisted Interventions (MICCAI) Conference 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Single Image, Any Face: Generalisable 3D Face Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16990v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16990v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqing Wang, Haosen Yang, Josef Kittler, Xiatian Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The creation of 3D human face avatars from a single unconstrained image is a
fundamental task that underlies numerous real-world vision and graphics
applications. Despite the significant progress made in generative models,
existing methods are either less suited in design for human faces or fail to
generalise from the restrictive training domain to unconstrained facial images.
To address these limitations, we propose a novel model, Gen3D-Face, which
generates 3D human faces with unconstrained single image input within a
multi-view consistent diffusion framework. Given a specific input image, our
model first produces multi-view images, followed by neural surface
construction. To incorporate face geometry information in a generalisable
manner, we utilise input-conditioned mesh estimation instead of ground-truth
mesh along with synthetic multi-view training data. Importantly, we introduce a
multi-view joint generation scheme to enhance appearance consistency among
different views. To the best of our knowledge, this is the first attempt and
benchmark for creating photorealistic 3D human face avatars from single images
for generic human subject across domains. Extensive experiments demonstrate the
superiority of our method over previous alternatives for out-of-domain singe
image 3D face generation and top competition for in-domain setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Robot Informative Path Planning for Efficient Target Mapping using
  Deep Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16967v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16967v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Apoorva Vashisth, Dipam Patel, Damon Conover, Aniket Bera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous robots are being employed in several mapping and data collection
tasks due to their efficiency and low labor costs. In these tasks, the robots
are required to map targets-of-interest in an unknown environment while
constrained to a given resource budget such as path length or mission time.
This is a challenging problem as each robot has to not only detect and avoid
collisions from static obstacles in the environment but also has to model other
robots' trajectories to avoid inter-robot collisions. We propose a novel deep
reinforcement learning approach for multi-robot informative path planning to
map targets-of-interest in an unknown 3D environment. A key aspect of our
approach is an augmented graph that models other robots' trajectories to enable
planning for communication and inter-robot collision avoidance. We train our
decentralized reinforcement learning policy via the centralized training and
decentralized execution paradigm. Once trained, our policy is also scalable to
varying number of robots and does not require re-training. Our approach
outperforms other state-of-the-art multi-robot target mapping approaches by
33.75% in terms of the number of discovered targets-of-interest. We open-source
our code and model at: https://github.com/AccGen99/marl_ipp
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2402.04894</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Path-adaptive Spatio-Temporal State Space Model for Event-based
  Recognition with Arbitrary Duration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16953v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16953v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiazhou Zhou, Kanghao Chen, Lei Zhang, Lin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event cameras are bio-inspired sensors that capture the intensity changes
asynchronously and output event streams with distinct advantages, such as high
temporal resolution. To exploit event cameras for object/action recognition,
existing methods predominantly sample and aggregate events in a second-level
duration at every fixed temporal interval (or frequency). However, they often
face difficulties in capturing the spatiotemporal relationships for longer,
e.g., minute-level, events and generalizing across varying temporal
frequencies. To fill the gap, we present a novel framework, dubbed PAST-SSM,
exhibiting superior capacity in recognizing events with arbitrary duration
(e.g., 0.1s to 4.5s) and generalizing to varying inference frequencies. Our key
insight is to learn the spatiotemporal relationships from the encoded event
features via the state space model (SSM) -- whose linear complexity makes it
ideal for modeling high temporal resolution events with longer sequences. To
achieve this goal, we first propose a Path-Adaptive Event Aggregation and Scan
(PEAS) module to encode events of varying duration into features with fixed
dimensions by adaptively scanning and selecting aggregated event frames. On top
of PEAS, we introduce a novel Multi-faceted Selection Guiding (MSG) loss to
minimize the randomness and redundancy of the encoded features. This subtly
enhances the model generalization across different inference frequencies.
Lastly, the SSM is employed to better learn the spatiotemporal properties from
the encoded features. Moreover, we build a minute-level event-based recognition
dataset, named ArDVS100, with arbitrary duration for the benefit of the
community. Extensive experiments prove that our method outperforms prior arts
by +3.45%, +0.38% and +8.31% on the DVS Action, SeAct and HARDVS datasets,
respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DALDA: Data Augmentation Leveraging Diffusion Model and LLM with
  Adaptive Guidance Scaling <span class="chip">ECCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16949v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16949v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyuheon Jung, Yongdeuk Seo, Seongwoo Cho, Jaeyoung Kim, Hyun-seok Min, Sungchul Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present an effective data augmentation framework leveraging
the Large Language Model (LLM) and Diffusion Model (DM) to tackle the
challenges inherent in data-scarce scenarios. Recently, DMs have opened up the
possibility of generating synthetic images to complement a few training images.
However, increasing the diversity of synthetic images also raises the risk of
generating samples outside the target distribution. Our approach addresses this
issue by embedding novel semantic information into text prompts via LLM and
utilizing real images as visual prompts, thus generating semantically rich
images. To ensure that the generated images remain within the target
distribution, we dynamically adjust the guidance weight based on each image's
CLIPScore to control the diversity. Experimental results show that our method
produces synthetic images with enhanced diversity while maintaining adherence
to the target distribution. Consequently, our approach proves to be more
efficient in the few-shot setting on several benchmarks. Our code is available
at https://github.com/kkyuhun94/dalda .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV Synthetic Data for Computer Vision Workshop (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NTIRE 2024 Challenge on Stereo Image Super-Resolution: Methods and
  Results 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16947v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16947v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Longguang Wang, Yulan Guo, Juncheng Li, Hongda Liu, Yang Zhao, Yingqian Wang, Zhi Jin, Shuhang Gu, Radu Timofte
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper summarizes the 3rd NTIRE challenge on stereo image
super-resolution (SR) with a focus on new solutions and results. The task of
this challenge is to super-resolve a low-resolution stereo image pair to a
high-resolution one with a magnification factor of x4 under a limited
computational budget. Compared with single image SR, the major challenge of
this challenge lies in how to exploit additional information in another
viewpoint and how to maintain stereo consistency in the results. This challenge
has 2 tracks, including one track on bicubic degradation and one track on real
degradations. In total, 108 and 70 participants were successfully registered
for each track, respectively. In the test phase, 14 and 13 teams successfully
submitted valid results with PSNR (RGB) scores better than the baseline. This
challenge establishes a new benchmark for stereo image SR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Face Forgery Detection with Elaborate Backbone 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16945v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16945v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zonghui Guo, Yingjie Liu, Jie Zhang, Haiyong Zheng, Shiguang Shan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Face Forgery Detection (FFD), or Deepfake detection, aims to determine
whether a digital face is real or fake. Due to different face synthesis
algorithms with diverse forgery patterns, FFD models often overfit specific
patterns in training datasets, resulting in poor generalization to other unseen
forgeries. This severe challenge requires FFD models to possess strong
capabilities in representing complex facial features and extracting subtle
forgery cues. Although previous FFD models directly employ existing backbones
to represent and extract facial forgery cues, the critical role of backbones is
often overlooked, particularly as their knowledge and capabilities are
insufficient to address FFD challenges, inevitably limiting generalization.
Therefore, it is essential to integrate the backbone pre-training
configurations and seek practical solutions by revisiting the complete FFD
workflow, from backbone pre-training and fine-tuning to inference of
discriminant results. Specifically, we analyze the crucial contributions of
backbones with different configurations in FFD task and propose leveraging the
ViT network with self-supervised learning on real-face datasets to pre-train a
backbone, equipping it with superior facial representation capabilities. We
then build a competitive backbone fine-tuning framework that strengthens the
backbone's ability to extract diverse forgery cues within a competitive
learning mechanism. Moreover, we devise a threshold optimization mechanism that
utilizes prediction confidence to improve the inference reliability.
Comprehensive experiments demonstrate that our FFD model with the elaborate
backbone achieves excellent performance in FFD and extra face-related tasks,
i.e., presentation attack detection. Code and models are available at
https://github.com/zhenglab/FFDBackbone.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Go-SLAM: Grounded Object Segmentation and Localization with Gaussian
  Splatting SLAM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16944v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16944v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Phu Pham, Dipam Patel, Damon Conover, Aniket Bera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Go-SLAM, a novel framework that utilizes 3D Gaussian Splatting
SLAM to reconstruct dynamic environments while embedding object-level
information within the scene representations. This framework employs advanced
object segmentation techniques, assigning a unique identifier to each Gaussian
splat that corresponds to the object it represents. Consequently, our system
facilitates open-vocabulary querying, allowing users to locate objects using
natural language descriptions. Furthermore, the framework features an optimal
path generation module that calculates efficient navigation paths for robots
toward queried objects, considering obstacles and environmental uncertainties.
Comprehensive evaluations in various scene settings demonstrate the
effectiveness of our approach in delivering high-fidelity scene
reconstructions, precise object segmentation, flexible object querying, and
efficient robot path planning. This work represents an additional step forward
in bridging the gap between 3D scene reconstruction, semantic object
understanding, and real-time environment interactions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Going Beyond U-Net: Assessing Vision <span class="highlight-title">Transformer</span>s for Semantic
  Segmentation in Microscopy Image Analysis <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16940v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16940v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Illia Tsiporenko, Pavel Chizhov, Dmytro Fishman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segmentation is a crucial step in microscopy image analysis. Numerous
approaches have been developed over the past years, ranging from classical
segmentation algorithms to advanced deep learning models. While U-Net remains
one of the most popular and well-established models for biomedical segmentation
tasks, recently developed transformer-based models promise to enhance the
segmentation process of microscopy images. In this work, we assess the efficacy
of transformers, including UNETR, the Segment Anything Model, and Swin-UPerNet,
and compare them with the well-established U-Net model across various image
modalities such as electron microscopy, brightfield, histopathology, and
phase-contrast. Our evaluation identifies several limitations in the original
Swin Transformer model, which we address through architectural modifications to
optimise its performance. The results demonstrate that these modifications
improve segmentation performance compared to the classical U-Net model and the
unmodified Swin-UPerNet. This comparative analysis highlights the promise of
transformer models for advancing biomedical image segmentation. It demonstrates
that their efficiency and applicability can be improved with careful
modifications, facilitating their future use in microscopy image analysis
tools.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to be published in ECCV 2024 BioImage Computing Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Object Insertion in Gaussian Splatting with a Multi-View
  Diffusion Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16938v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16938v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongliang Zhong, Can Wang, Jingbo Zhang, Jing Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating and inserting new objects into 3D content is a compelling approach
for achieving versatile scene recreation. Existing methods, which rely on SDS
optimization or single-view inpainting, often struggle to produce high-quality
results. To address this, we propose a novel method for object insertion in 3D
content represented by Gaussian Splatting. Our approach introduces a multi-view
diffusion model, dubbed MVInpainter, which is built upon a pre-trained stable
video diffusion model to facilitate view-consistent object inpainting. Within
MVInpainter, we incorporate a ControlNet-based conditional injection module to
enable controlled and more predictable multi-view generation. After generating
the multi-view inpainted results, we further propose a mask-aware 3D
reconstruction technique to refine Gaussian Splatting reconstruction from these
sparse inpainted views. By leveraging these fabricate techniques, our approach
yields diverse results, ensures view-consistent and harmonious insertions, and
produces better object quality. Extensive experiments demonstrate that our
approach outperforms existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://github.com/JiuTongBro/MultiView_Inpaint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Game4Loc: A UAV Geo-Localization Benchmark from Game Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16925v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16925v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiang Ji, Boyong He, Zhuoyue Tan, Liaoni Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The vision-based geo-localization technology for UAV, serving as a secondary
source of GPS information in addition to the global navigation satellite
systems (GNSS), can still operate independently in the GPS-denied environment.
Recent deep learning based methods attribute this as the task of image matching
and retrieval. By retrieving drone-view images in geo-tagged satellite image
database, approximate localization information can be obtained. However, due to
high costs and privacy concerns, it is usually difficult to obtain large
quantities of drone-view images from a continuous area. Existing drone-view
datasets are mostly composed of small-scale aerial photography with a strong
assumption that there exists a perfect one-to-one aligned reference image for
any query, leaving a significant gap from the practical localization scenario.
In this work, we construct a large-range contiguous area UAV geo-localization
dataset named GTA-UAV, featuring multiple flight altitudes, attitudes, scenes,
and targets using modern computer games. Based on this dataset, we introduce a
more practical UAV geo-localization task including partial matches of
cross-view paired data, and expand the image-level retrieval to the actual
localization in terms of distance (meters). For the construction of drone-view
and satellite-view pairs, we adopt a weight-based contrastive learning
approach, which allows for effective learning while avoiding additional
post-processing matching steps. Experiments demonstrate the effectiveness of
our data and training method for UAV geo-localization, as well as the
generalization capabilities to real-world scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://yux1angji.github.io/game4loc/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Moner: Motion Correction in Undersampled Radial MRI with Unsupervised
  Neural Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16921v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16921v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qing Wu, Chenhe Du, XuanYu Tian, Jingyi Yu, Yuyao Zhang, Hongjiang Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motion correction (MoCo) in radial MRI is a challenging problem due to the
unpredictability of subject's motion. Current state-of-the-art (SOTA) MoCo
algorithms often use extensive high-quality MR images to pre-train neural
networks, obtaining excellent reconstructions. However, the need for
large-scale datasets significantly increases costs and limits model
generalization. In this work, we propose Moner, an unsupervised MoCo method
that jointly solves artifact-free MR images and accurate motion from
undersampled, rigid motion-corrupted k-space data, without requiring training
data. Our core idea is to leverage the continuous prior of implicit neural
representation (INR) to constrain this ill-posed inverse problem, enabling
ideal solutions. Specifically, we incorporate a quasi-static motion model into
the INR, granting its ability to correct subject's motion. To stabilize model
optimization, we reformulate radial MRI as a back-projection problem using the
Fourier-slice theorem. Additionally, we propose a novel coarse-to-fine hash
encoding strategy, significantly enhancing MoCo accuracy. Experiments on
multiple MRI datasets show our Moner achieves performance comparable to SOTA
MoCo techniques on in-domain data, while demonstrating significant improvements
on out-of-domain data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Adaptive Screen-Space Meshing Approach for Normal Integration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16907v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16907v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moritz Heep, Eduard Zell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing surfaces from normals is a key component of photometric
stereo. This work introduces an adaptive surface triangulation in the image
domain and afterwards performs the normal integration on a triangle mesh. Our
key insight is that surface curvature can be computed from normals. Based on
the curvature, we identify flat areas and aggregate pixels into triangles. The
approximation quality is controlled by a single user parameter facilitating a
seamless generation of low- to high-resolution meshes. Compared to pixel grids,
our triangle meshes adapt locally to surface details and allow for a sparser
representation. Our new mesh-based formulation of the normal integration
problem is strictly derived from discrete differential geometry and leads to
well-conditioned linear systems. Results on real and synthetic data show that
10 to 100 times less vertices are required than pixels. Experiments suggest
that this sparsity translates into a sublinear runtime in the number of pixels.
For 64 MP normal maps, our meshing-first approach generates and integrates
meshes in minutes while pixel-based approaches require hours just for the
integration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Underwater Camouflaged Object Tracking: An Experimental
  Evaluation of SAM and SAM 2 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16902v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16902v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunhui Zhang, Li Liu, Guanjie Huang, Hao Wen, Xi Zhou, Yanfeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the past decade, significant progress has been made in visual object
tracking, largely due to the availability of large-scale training datasets.
However, existing tracking datasets are primarily focused on open-air
scenarios, which greatly limits the development of object tracking in
underwater environments. To address this issue, we take a step forward by
proposing the first large-scale underwater camouflaged object tracking dataset,
namely UW-COT. Based on the proposed dataset, this paper presents an
experimental evaluation of several advanced visual object tracking methods and
the latest advancements in image and video segmentation. Specifically, we
compare the performance of the Segment Anything Model (SAM) and its updated
version, SAM 2, in challenging underwater environments. Our findings highlight
the improvements in SAM 2 over SAM, demonstrating its enhanced capability to
handle the complexities of underwater camouflaged objects. Compared to current
advanced visual object tracking methods, the latest video segmentation
foundation model SAM 2 also exhibits significant advantages, providing valuable
insights into the development of more effective tracking technologies for
underwater scenarios. The dataset will be accessible at
\color{magenta}{https://github.com/983632847/Awesome-Multimodal-Object-Tracking}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Work in Progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HVT: A Comprehensive Vision Framework for Learning in Non-Euclidean
  Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16897v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16897v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacob Fein-Ashley, Ethan Feng, Minh Pham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data representation in non-Euclidean spaces has proven effective for
capturing hierarchical and complex relationships in real-world datasets.
Hyperbolic spaces, in particular, provide efficient embeddings for hierarchical
structures. This paper introduces the Hyperbolic Vision Transformer (HVT), a
novel extension of the Vision Transformer (ViT) that integrates hyperbolic
geometry. While traditional ViTs operate in Euclidean space, our method
enhances the self-attention mechanism by leveraging hyperbolic distance and
M\"obius transformations. This enables more effective modeling of hierarchical
and relational dependencies in image data. We present rigorous mathematical
formulations, showing how hyperbolic geometry can be incorporated into
attention layers, feed-forward networks, and optimization. We offer improved
performance for image classification using the ImageNet dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Linking in Style: Understanding learned features in deep learning models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16865v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16865v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maren H. Wehrheim, Pamela Osuna-Vargas, Matthias Kaschube
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional neural networks (CNNs) learn abstract features to perform
object classification, but understanding these features remains challenging due
to difficult-to-interpret results or high computational costs. We propose an
automatic method to visualize and systematically analyze learned features in
CNNs. Specifically, we introduce a linking network that maps the penultimate
layer of a pre-trained classifier to the latent space of a generative model
(StyleGAN-XL), thereby enabling an interpretable, human-friendly visualization
of the classifier's representations. Our findings indicate a congruent semantic
order in both spaces, enabling a direct linear mapping between them. Training
the linking network is computationally inexpensive and decoupled from training
both the GAN and the classifier. We introduce an automatic pipeline that
utilizes such GAN-based visualizations to quantify learned representations by
analyzing activation changes in the classifier in the image domain. This
quantification allows us to systematically study the learned representations in
several thousand units simultaneously and to extract and visualize units
selective for specific semantic concepts. Further, we illustrate how our method
can be used to quantify and interpret the classifier's decision boundary using
counterfactual examples. Overall, our method offers systematic and objective
perspectives on learned abstract representations in CNNs.
https://github.com/kaschube-lab/LinkingInStyle.git
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Unified 3D Hair Reconstruction from Single-View Portraits <span class="chip">SIGGRAPH</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16863v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16863v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujian Zheng, Yuda Qiu, Leyang Jin, Chongyang Ma, Haibin Huang, Di Zhang, Pengfei Wan, Xiaoguang Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Single-view 3D hair reconstruction is challenging, due to the wide range of
shape variations among diverse hairstyles. Current state-of-the-art methods are
specialized in recovering un-braided 3D hairs and often take braided styles as
their failure cases, because of the inherent difficulty to define priors for
complex hairstyles, whether rule-based or data-based. We propose a novel
strategy to enable single-view 3D reconstruction for a variety of hair types
via a unified pipeline. To achieve this, we first collect a large-scale
synthetic multi-view hair dataset SynMvHair with diverse 3D hair in both
braided and un-braided styles, and learn two diffusion priors specialized on
hair. Then we optimize 3D Gaussian-based hair from the priors with two
specially designed modules, i.e. view-wise and pixel-wise Gaussian refinement.
Our experiments demonstrate that reconstructing braided and un-braided 3D hair
from single-view images via a unified approach is possible and our method
achieves the state-of-the-art performance in recovering complex hairstyles. It
is worth to mention that our method shows good generalization ability to real
images, although it learns hair priors from synthetic data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGGRAPH Asia 2024, project page: https://unihair24.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Limitations of (Procrustes) Alignment in Assessing Multi-Person Human
  Pose and Shape Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16861v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16861v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Drazic Martin, Pierre Perrault
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We delve into the challenges of accurately estimating 3D human pose and shape
in video surveillance scenarios. Beginning with the advocacy for metrics like
W-MPJPE and W-PVE, which omit the (Procrustes) realignment step, to improve
model evaluation, we then introduce RotAvat. This technique aims to enhance
these metrics by refining the alignment of 3D meshes with the ground plane.
Through qualitative comparisons, we demonstrate RotAvat's effectiveness in
addressing the limitations of existing aproaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Role of Language Models in Modern Healthcare: A Comprehensive <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16860v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16860v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amna Khalid, Ayma Khalid, Umar Khalid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The application of large language models (LLMs) in healthcare has gained
significant attention due to their ability to process complex medical data and
provide insights for clinical decision-making. These models have demonstrated
substantial capabilities in understanding and generating natural language,
which is crucial for medical documentation, diagnostics, and patient
interaction. This review examines the trajectory of language models from their
early stages to the current state-of-the-art LLMs, highlighting their strengths
in healthcare applications and discussing challenges such as data privacy,
bias, and ethical considerations. The potential of LLMs to enhance healthcare
delivery is explored, alongside the necessary steps to ensure their ethical and
effective integration into medical practice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Versatile and Differentiable Hand-Object Interaction Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16855v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16855v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Théo Morales, Omid Taheri, Gerard Lacey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthesizing accurate hands-object interactions (HOI) is critical for
applications in Computer Vision, Augmented Reality (AR), and Mixed Reality
(MR). Despite recent advances, the accuracy of reconstructed or generated HOI
leaves room for refinement. Some techniques have improved the accuracy of dense
correspondences by shifting focus from generating explicit contacts to using
rich HOI fields. Still, they lack full differentiability or continuity and are
tailored to specific tasks. In contrast, we present a Coarse Hand-Object
Interaction Representation (CHOIR), a novel, versatile and fully differentiable
field for HOI modelling. CHOIR leverages discrete unsigned distances for
continuous shape and pose encoding, alongside multivariate Gaussian
distributions to represent dense contact maps with few parameters. To
demonstrate the versatility of CHOIR we design JointDiffusion, a diffusion
model to learn a grasp distribution conditioned on noisy hand-object
interactions or only object geometries, for both refinement and synthesis
applications. We demonstrate JointDiffusion's improvements over the SOTA in
both applications: it increases the contact F1 score by $5\%$ for refinement
and decreases the sim. displacement by $46\%$ for synthesis. Our experiments
show that JointDiffusion with CHOIR yield superior contact accuracy and
physical realism compared to SOTA methods designed for specific tasks. Our
models and code will be publicly available to the research community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the Winter Applications in Computer Vision 2025
  conference. 9 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Scene Change Detection Using Visual Foundation Models and
  Cross-Attention Mechanisms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16850v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16850v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chun-Jung Lin, Sourav Garg, Tat-Jun Chin, Feras Dayoub
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel method for scene change detection that leverages the
robust feature extraction capabilities of a visual foundational model, DINOv2,
and integrates full-image cross-attention to address key challenges such as
varying lighting, seasonal variations, and viewpoint differences. In order to
effectively learn correspondences and mis-correspondences between an image pair
for the change detection task, we propose to a) ``freeze'' the backbone in
order to retain the generality of dense foundation features, and b) employ
``full-image'' cross-attention to better tackle the viewpoint variations
between the image pair. We evaluate our approach on two benchmark datasets,
VL-CMU-CD and PSCD, along with their viewpoint-varied versions. Our experiments
demonstrate significant improvements in F1-score, particularly in scenarios
involving geometric changes between image pairs. The results indicate our
method's superior generalization capabilities over existing state-of-the-art
approaches, showing robustness against photometric and geometric variations as
well as better overall generalization when fine-tuned to adapt to new
environments. Detailed ablation studies further validate the contributions of
each component in our architecture. Source code will be made publicly available
upon acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IRASNet: Improved Feature-Level Clutter Reduction for Domain Generalized
  SAR-ATR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16845v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16845v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oh-Tae Jang, Hae-Kang Song, Min-Jun Kim, Kyung-Hwan Lee, Geon Lee, Sung-Ho Kim, Kyung-Tae Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, computer-aided design models and electromagnetic simulations have
been used to augment synthetic aperture radar (SAR) data for deep learning.
However, an automatic target recognition (ATR) model struggles with domain
shift when using synthetic data because the model learns specific clutter
patterns present in such data, which disturbs performance when applied to
measured data with different clutter distributions. This study proposes a
framework particularly designed for domain-generalized SAR-ATR called IRASNet,
enabling effective feature-level clutter reduction and domain-invariant feature
learning. First, we propose a clutter reduction module (CRM) that maximizes the
signal-to-clutter ratio on feature maps. The module reduces the impact of
clutter at the feature level while preserving target and shadow information,
thereby improving ATR performance. Second, we integrate adversarial learning
with CRM to extract clutter-reduced domain-invariant features. The integration
bridges the gap between synthetic and measured datasets without requiring
measured data during training. Third, we improve feature extraction from target
and shadow regions by implementing a positional supervision task using mask
ground truth encoding. The improvement enhances the ability of the model to
discriminate between classes. Our proposed IRASNet presents new
state-of-the-art public SAR datasets utilizing target and shadow information to
achieve superior performance across various test conditions. IRASNet not only
enhances generalization performance but also significantly improves
feature-level clutter reduction, making it a valuable advancement in the field
of radar image pattern recognition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explicitly Modeling Pre-Cortical Vision with a Neuro-Inspired Front-End
  Improves CNN Robustness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16838v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16838v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas Piper, Arlindo L. Oliveira, Tiago Marques
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While convolutional neural networks (CNNs) excel at clean image
classification, they struggle to classify images corrupted with different
common corruptions, limiting their real-world applicability. Recent work has
shown that incorporating a CNN front-end block that simulates some features of
the primate primary visual cortex (V1) can improve overall model robustness.
Here, we expand on this approach by introducing two novel biologically-inspired
CNN model families that incorporate a new front-end block designed to simulate
pre-cortical visual processing. RetinaNet, a hybrid architecture containing the
novel front-end followed by a standard CNN back-end, shows a relative
robustness improvement of 12.3% when compared to the standard model; and EVNet,
which further adds a V1 block after the pre-cortical front-end, shows a
relative gain of 18.5%. The improvement in robustness was observed for all the
different corruption categories, though accompanied by a small decrease in
clean image accuracy, and generalized to a different back-end architecture.
These findings show that simulating multiple stages of early visual processing
in CNN early layers provides cumulative benefits for model robustness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Focus Entirety and Perceive Environment for Arbitrary-Shaped Text
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xu Han, Junyu Gao, Chuang Yang, Yuan Yuan, Qi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the diversity of scene text in aspects such as font, color, shape, and
size, accurately and efficiently detecting text is still a formidable
challenge. Among the various detection approaches, segmentation-based
approaches have emerged as prominent contenders owing to their flexible
pixel-level predictions. However, these methods typically model text instances
in a bottom-up manner, which is highly susceptible to noise. In addition, the
prediction of pixels is isolated without introducing pixel-feature interaction,
which also influences the detection performance. To alleviate these problems,
we propose a multi-information level arbitrary-shaped text detector consisting
of a focus entirety module (FEM) and a perceive environment module (PEM). The
former extracts instance-level features and adopts a top-down scheme to model
texts to reduce the influence of noises. Specifically, it assigns consistent
entirety information to pixels within the same instance to improve their
cohesion. In addition, it emphasizes the scale information, enabling the model
to distinguish varying scale texts effectively. The latter extracts
region-level information and encourages the model to focus on the distribution
of positive samples in the vicinity of a pixel, which perceives environment
information. It treats the kernel pixels as positive samples and helps the
model differentiate text and kernel features. Extensive experiments demonstrate
the FEM's ability to efficiently support the model in handling different scale
texts and confirm the PEM can assist in perceiving pixels more accurately by
focusing on pixel vicinities. Comparisons show the proposed model outperforms
existing state-of-the-art approaches on four public datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ XAI-guided Insulator Anomaly Detection for Imbalanced <span class="highlight-title">Dataset</span>s <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16821v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16821v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximilian Andreas Hoefler, Karsten Mueller, Wojciech Samek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Power grids serve as a vital component in numerous industries, seamlessly
delivering electrical energy to industrial processes and technologies, making
their safe and reliable operation indispensable. However, powerlines can be
hard to inspect due to difficult terrain or harsh climatic conditions.
Therefore, unmanned aerial vehicles are increasingly deployed to inspect
powerlines, resulting in a substantial stream of visual data which requires
swift and accurate processing. Deep learning methods have become widely popular
for this task, proving to be a valuable asset in fault detection. In
particular, the detection of insulator defects is crucial for predicting
powerline failures, since their malfunction can lead to transmission
disruptions. It is therefore of great interest to continuously maintain and
rigorously inspect insulator components. In this work we propose a novel
pipeline to tackle this task. We utilize state-of-the-art object detection to
detect and subsequently classify individual insulator anomalies. Our approach
addresses dataset challenges such as imbalance and motion-blurred images
through a fine-tuning methodology which allows us to alter the classification
focus of the model by increasing the classification accuracy of anomalous
insulators. In addition, we employ explainable-AI tools for precise
localization and explanation of anomalies. This proposed method contributes to
the field of anomaly detection, particularly vision-based industrial inspection
and predictive maintenance. We significantly improve defect detection accuracy
by up to 13%, while also offering a detailed analysis of model
mis-classifications and localization quality, showcasing the potential of our
method on real-world data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a workshop paper at ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spotlight Text Detector: Spotlight on Candidate Regions Like a Camera 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16820v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16820v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xu Han, Junyu Gao, Chuang Yang, Yuan Yuan, Qi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The irregular contour representation is one of the tough challenges in scene
text detection. Although segmentation-based methods have achieved significant
progress with the help of flexible pixel prediction, the overlap of
geographically close texts hinders detecting them separately. To alleviate this
problem, some shrink-based methods predict text kernels and expand them to
restructure texts. However, the text kernel is an artificial object with
incomplete semantic features that are prone to incorrect or missing detection.
In addition, different from the general objects, the geometry features (aspect
ratio, scale, and shape) of scene texts vary significantly, which makes it
difficult to detect them accurately. To consider the above problems, we propose
an effective spotlight text detector (STD), which consists of a spotlight
calibration module (SCM) and a multivariate information extraction module
(MIEM). The former concentrates efforts on the candidate kernel, like a camera
focus on the target. It obtains candidate features through a mapping filter and
calibrates them precisely to eliminate some false positive samples. The latter
designs different shape schemes to explore multiple geometric features for
scene texts. It helps extract various spatial relationships to improve the
model's ability to recognize kernel regions. Ablation studies prove the
effectiveness of the designed SCM and MIEM. Extensive experiments verify that
our STD is superior to existing state-of-the-art methods on various datasets,
including ICDAR2015, CTW1500, MSRA-TD500, and Total-Text.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards General Text-guided Image Synthesis for Customized Multimodal
  Brain MRI Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16818v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16818v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulin Wang, Honglin Xiong, Kaicong Sun, Shuwei Bai, Ling Dai, Zhongxiang Ding, Jiameng Liu, Qian Wang, Qian Liu, Dinggang Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal brain magnetic resonance (MR) imaging is indispensable in
neuroscience and neurology. However, due to the accessibility of MRI scanners
and their lengthy acquisition time, multimodal MR images are not commonly
available. Current MR image synthesis approaches are typically trained on
independent datasets for specific tasks, leading to suboptimal performance when
applied to novel datasets and tasks. Here, we present TUMSyn, a Text-guided
Universal MR image Synthesis generalist model, which can flexibly generate
brain MR images with demanded imaging metadata from routinely acquired scans
guided by text prompts. To ensure TUMSyn's image synthesis precision,
versatility, and generalizability, we first construct a brain MR database
comprising 31,407 3D images with 7 MRI modalities from 13 centers. We then
pre-train an MRI-specific text encoder using contrastive learning to
effectively control MR image synthesis based on text prompts. Extensive
experiments on diverse datasets and physician assessments indicate that TUMSyn
can generate clinically meaningful MR images with specified imaging metadata in
supervised and zero-shot scenarios. Therefore, TUMSyn can be utilized along
with acquired MR scan(s) to facilitate large-scale MRI-based screening and
diagnosis of brain diseases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inline Photometrically Calibrated Hybrid Visual SLAM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16810v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16810v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Abboud, Malak Sayour, Imad H. Elhajj, John Zelek, Daniel Asmar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an integrated approach to Visual SLAM, merging online
sequential photometric calibration within a Hybrid direct-indirect visual SLAM
(H-SLAM). Photometric calibration helps normalize pixel intensity values under
different lighting conditions, and thereby improves the direct component of our
H-SLAM. A tangential benefit also results to the indirect component of H-SLAM
given that the detected features are more stable across variable lighting
conditions. Our proposed photometrically calibrated H-SLAM is tested on several
datasets, including the TUM monoVO as well as on a dataset we created.
Calibrated H-SLAM outperforms other state of the art direct, indirect, and
hybrid Visual SLAM systems in all the experiments. Furthermore, in online SLAM
tested at our site, it also significantly outperformed the other SLAM Systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Deep Learning Models for Object Detection on Edge Computing
  Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16808v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16808v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daghash K. Alqahtani, Aamir Cheema, Adel N. Toosi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern applications, such as autonomous vehicles, require deploying deep
learning algorithms on resource-constrained edge devices for real-time image
and video processing. However, there is limited understanding of the efficiency
and performance of various object detection models on these devices. In this
paper, we evaluate state-of-the-art object detection models, including YOLOv8
(Nano, Small, Medium), EfficientDet Lite (Lite0, Lite1, Lite2), and SSD (SSD
MobileNet V1, SSDLite MobileDet). We deployed these models on popular edge
devices like the Raspberry Pi 3, 4, and 5 with/without TPU accelerators, and
Jetson Orin Nano, collecting key performance metrics such as energy
consumption, inference time, and Mean Average Precision (mAP). Our findings
highlight that lower mAP models such as SSD MobileNet V1 are more
energy-efficient and faster in inference, whereas higher mAP models like YOLOv8
Medium generally consume more energy and have slower inference, though with
exceptions when accelerators like TPUs are used. Among the edge devices, Jetson
Orin Nano stands out as the fastest and most energy-efficient option for
request handling, despite having the highest idle energy consumption. These
results emphasize the need to balance accuracy, speed, and energy efficiency
when deploying deep learning models on edge devices, offering valuable guidance
for practitioners and researchers selecting models and devices for their
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Topological SLAM in colonoscopies leveraging deep features and
  topological priors <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16806v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16806v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Javier Morlana, Juan D. Tardós, José M. M. Montiel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce ColonSLAM, a system that combines classical multiple-map metric
SLAM with deep features and topological priors to create topological maps of
the whole colon. The SLAM pipeline by itself is able to create disconnected
individual metric submaps representing locations from short video subsections
of the colon, but is not able to merge covisible submaps due to deformations
and the limited performance of the SIFT descriptor in the medical domain.
ColonSLAM is guided by topological priors and combines a deep localization
network trained to distinguish if two images come from the same place or not
and the soft verification of a transformer-based matching network, being able
to relate far-in-time submaps during an exploration, grouping them in nodes
imaging the same colon place, building more complex maps than any other
approach in the literature. We demonstrate our approach in the Endomapper
dataset, showing its potential for producing maps of the whole colon in real
human explorations. Code and models are available at:
https://github.com/endomapper/ColonSLAM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable Ensemble Diversification for OOD Generalization and Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16797v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16797v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Rubinstein, Luca Scimeca, Damien Teney, Seong Joon Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training a diverse ensemble of models has several practical applications such
as providing candidates for model selection with better out-of-distribution
(OOD) generalization, and enabling the detection of OOD samples via Bayesian
principles. An existing approach to diverse ensemble training encourages the
models to disagree on provided OOD samples. However, the approach is
computationally expensive and it requires well-separated ID and OOD examples,
such that it has only been demonstrated in small-scale settings.
  $\textbf{Method.}$ This work presents a method for Scalable Ensemble
Diversification (SED) applicable to large-scale settings (e.g. ImageNet) that
does not require OOD samples. Instead, SED identifies hard training samples on
the fly and encourages the ensemble members to disagree on these. To improve
scaling, we show how to avoid the expensive computations in existing methods of
exhaustive pairwise disagreements across models.
  $\textbf{Results.}$ We evaluate the benefits of diversification with
experiments on ImageNet. First, for OOD generalization, we observe large
benefits from the diversification in multiple settings including output-space
(classical) ensembles and weight-space ensembles (model soups). Second, for OOD
detection, we turn the diversity of ensemble hypotheses into a novel
uncertainty score estimator that surpasses a large number of OOD detection
baselines.
  Code is available here:
https://github.com/AlexanderRubinstein/diverse-universe-public.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spacewalker: Traversing Representation Spaces for Fast Interactive
  Exploration and Annotation of Unstructured Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16793v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16793v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Heine, Fabian Hörst, Jana Fragemann, Gijs Luijten, Miriam Balzer, Jan Egger, Fin Bahnsen, M. Saquib Sarfraz, Jens Kleesiek, Constantin Seibold
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unstructured data in industries such as healthcare, finance, and
manufacturing presents significant challenges for efficient analysis and
decision making. Detecting patterns within this data and understanding their
impact is critical but complex without the right tools. Traditionally, these
tasks relied on the expertise of data analysts or labor-intensive manual
reviews. In response, we introduce Spacewalker, an interactive tool designed to
explore and annotate data across multiple modalities. Spacewalker allows users
to extract data representations and visualize them in low-dimensional spaces,
enabling the detection of semantic similarities. Through extensive user
studies, we assess Spacewalker's effectiveness in data annotation and integrity
verification. Results show that the tool's ability to traverse latent spaces
and perform multi-modal queries significantly enhances the user's capacity to
quickly identify relevant data. Moreover, Spacewalker allows for annotation
speed-ups far superior to conventional methods, making it a promising tool for
efficiently navigating unstructured data and improving decision making
processes. The code of this work is open-source and can be found at:
https://github.com/code-lukas/Spacewalker
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MixPolyp: Integrating Mask, Box and Scribble Supervision for Enhanced
  Polyp Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16774v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16774v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwen Hu, Jun Wei, Yuncheng Jiang, Haoyang Li, Shuguang Cui, Zhen Li, Song Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Limited by the expensive labeling, polyp segmentation models are plagued by
data shortages. To tackle this, we propose the mixed supervised polyp
segmentation paradigm (MixPolyp). Unlike traditional models relying on a single
type of annotation, MixPolyp combines diverse annotation types (mask, box, and
scribble) within a single model, thereby expanding the range of available data
and reducing labeling costs. To achieve this, MixPolyp introduces three novel
supervision losses to handle various annotations: Subspace Projection loss
(L_SP), Binary Minimum Entropy loss (L_BME), and Linear Regularization loss
(L_LR). For box annotations, L_SP eliminates shape inconsistencies between the
prediction and the supervision. For scribble annotations, L_BME provides
supervision for unlabeled pixels through minimum entropy constraint, thereby
alleviating supervision sparsity. Furthermore, L_LR provides dense supervision
by enforcing consistency among the predictions, thus reducing the
non-uniqueness. These losses are independent of the model structure, making
them generally applicable. They are used only during training, adding no
computational cost during inference. Extensive experiments on five datasets
demonstrate MixPolyp's effectiveness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in IEEE BIBM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Let There Be Light: Robust Lensless Imaging Under External Illumination
  With Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16766v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16766v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Bezzam, Stefan Peters, Martin Vetterli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lensless cameras relax the design constraints of traditional cameras by
shifting image formation from analog optics to digital post-processing. While
new camera designs and applications can be enabled, lensless imaging is very
sensitive to unwanted interference (other sources, noise, etc.). In this work,
we address a prevalent noise source that has not been studied for lensless
imaging: external illumination e.g. from ambient and direct lighting. Being
robust to a variety of lighting conditions would increase the practicality and
adoption of lensless imaging. To this end, we propose multiple recovery
approaches that account for external illumination by incorporating its estimate
into the image recovery process. At the core is a physics-based reconstruction
that combines learnable image recovery and denoisers, all of whose parameters
are trained using experimentally gathered data. Compared to standard
reconstruction methods, our approach yields significant qualitative and
quantitative improvements. We open-source our implementations and a 25K dataset
of measurements under multiple lighting conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, dataset: https://doi.org/10.57967/hf/2970</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MaViLS, a Benchmark <span class="highlight-title">Dataset</span> for Video-to-Slide Alignment, Assessing
  Baseline Accuracy with a Multimodal Alignment Algorithm Leveraging Speech,
  OCR, and Visual Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16765v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16765v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Katharina Anderer, Andreas Reich, Matthias Wölfel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a benchmark dataset for aligning lecture videos with
corresponding slides and introduces a novel multimodal algorithm leveraging
features from speech, text, and images. It achieves an average accuracy of 0.82
in comparison to SIFT (0.56) while being approximately 11 times faster. Using
dynamic programming the algorithm tries to determine the optimal slide
sequence. The results show that penalizing slide transitions increases
accuracy. Features obtained via optical character recognition (OCR) contribute
the most to a high matching accuracy, followed by image features. The findings
highlight that audio transcripts alone provide valuable information for
alignment and are beneficial if OCR data is lacking. Variations in matching
accuracy across different lectures highlight the challenges associated with
video quality and lecture style. The novel multimodal algorithm demonstrates
robustness to some of these challenges, underscoring the potential of the
approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Statewide Visual Geolocalization in the Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16763v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16763v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Fervers, Sebastian Bullinger, Christoph Bodensteiner, Michael Arens, Rainer Stiefelhagen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents a method that is able to predict the geolocation of a
street-view photo taken in the wild within a state-sized search region by
matching against a database of aerial reference imagery. We partition the
search region into geographical cells and train a model to map cells and
corresponding photos into a joint embedding space that is used to perform
retrieval at test time. The model utilizes aerial images for each cell at
multiple levels-of-detail to provide sufficient information about the
surrounding scene. We propose a novel layout of the search region with
consistent cell resolutions that allows scaling to large geographical regions.
Experiments demonstrate that the method successfully localizes 60.6% of all
non-panoramic street-view photos uploaded to the crowd-sourcing platform
Mapillary in the state of Massachusetts to within 50m of their ground-truth
location. Source code is available at
https://github.com/fferflo/statewide-visual-geolocalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Navigating the Maze of Explainable AI: A Systematic Approach to
  Evaluating Methods and Metrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Klein, Carsten T. Lüth, Udo Schlegel, Till J. Bungert, Mennatallah El-Assady, Paul F. Jäger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explainable AI (XAI) is a rapidly growing domain with a myriad of proposed
methods as well as metrics aiming to evaluate their efficacy. However, current
studies are often of limited scope, examining only a handful of XAI methods and
ignoring underlying design parameters for performance, such as the model
architecture or the nature of input data. Moreover, they often rely on one or a
few metrics and neglect thorough validation, increasing the risk of selection
bias and ignoring discrepancies among metrics. These shortcomings leave
practitioners confused about which method to choose for their problem. In
response, we introduce LATEC, a large-scale benchmark that critically evaluates
17 prominent XAI methods using 20 distinct metrics. We systematically
incorporate vital design parameters like varied architectures and diverse input
modalities, resulting in 7,560 examined combinations. Through LATEC, we
showcase the high risk of conflicting metrics leading to unreliable rankings
and consequently propose a more robust evaluation scheme. Further, we
comprehensively evaluate various XAI methods to assist practitioners in
selecting appropriate methods aligning with their needs. Curiously, the
emerging top-performing method, Expected Gradients, is not examined in any
relevant related study. LATEC reinforces its role in future XAI research by
publicly releasing all 326k saliency maps and 378k metric scores as a
(meta-)evaluation dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Commonly Interesting Images <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16736v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16736v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fitim Abdullahu, Helmut Grabner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Images tell stories, trigger emotions, and let us recall memories -- they
make us think. Thus, they have the ability to attract and hold one's attention,
which is the definition of being "interesting". Yet, the appeal of an image is
highly subjective. Looking at the image of my son taking his first steps will
always bring me back to this emotional moment, while it is just a blurry,
quickly taken snapshot to most others. Preferences vary widely: some adore
cats, others are dog enthusiasts, and a third group may not be fond of either.
We argue that every image can be interesting to a particular observer under
certain circumstances. This work particularly emphasizes subjective
preferences. However, our analysis of 2.5k image collections from diverse users
of the photo-sharing platform Flickr reveals that specific image
characteristics make them commonly more interesting. For instance, images,
including professionally taken landscapes, appeal broadly due to their
aesthetic qualities. In contrast, subjectively interesting images, such as
those depicting personal or niche community events, resonate on a more
individual level, often evoking personal memories and emotions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Effect of Lossy Compression on 3D Medical Images Segmentation with
  Deep Learning <span class="chip">MICCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16733v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16733v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anvar Kurmukov, Bogdan Zavolovich, Aleksandra Dalechina, Vladislav Proskurov, Boris Shirokikh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image compression is a critical tool in decreasing the cost of storage and
improving the speed of transmission over the internet. While deep learning
applications for natural images widely adopts the usage of lossy compression
techniques, it is not widespread for 3D medical images. Using three CT datasets
(17 tasks) and one MRI dataset (3 tasks) we demonstrate that lossy compression
up to 20 times have no negative impact on segmentation quality with deep neural
networks (DNN). In addition, we demonstrate the ability of DNN models trained
on compressed data to predict on uncompressed data and vice versa with no
quality deterioration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures, 2 tables; accepted on MICCAI Workshop on
  Advancing Data Solutions in Medical Imaging AI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Non-stationary <span class="highlight-title">BERT</span>: Exploring Augmented IMU Data For Robust Human
  Activity Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16730v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16730v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ning Sun, Yufei Wang, Yuwei Zhang, Jixiang Wan, Shenyue Wang, Ping Liu, Xudong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human Activity Recognition (HAR) has gained great attention from researchers
due to the popularity of mobile devices and the need to observe users' daily
activity data for better human-computer interaction. In this work, we collect a
human activity recognition dataset called OPPOHAR consisting of phone IMU data.
To facilitate the employment of HAR system in mobile phone and to achieve
user-specific activity recognition, we propose a novel light-weight network
called Non-stationary BERT with a two-stage training method. We also propose a
simple yet effective data augmentation method to explore the deeper
relationship between the accelerator and gyroscope data from the IMU. The
network achieves the state-of-the-art performance testing on various activity
recognition datasets and the data augmentation method demonstrates its wide
applicability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SDCL: Students Discrepancy-Informed Correction Learning for
  Semi-supervised Medical Image Segmentation <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16728v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16728v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bentao Song, Qingfeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised medical image segmentation (SSMIS) has been demonstrated the
potential to mitigate the issue of limited medical labeled data. However,
confirmation and cognitive biases may affect the prevalent teacher-student
based SSMIS methods due to erroneous pseudo-labels. To tackle this challenge,
we improve the mean teacher approach and propose the Students
Discrepancy-Informed Correction Learning (SDCL) framework that includes two
students and one non-trainable teacher, which utilizes the segmentation
difference between the two students to guide the self-correcting learning. The
essence of SDCL is to identify the areas of segmentation discrepancy as the
potential bias areas, and then encourage the model to review the correct
cognition and rectify their own biases in these areas. To facilitate the bias
correction learning with continuous review and rectification, two correction
loss functions are employed to minimize the correct segmentation voxel distance
and maximize the erroneous segmentation voxel entropy. We conducted experiments
on three public medical image datasets: two 3D datasets (CT and MRI) and one 2D
dataset (MRI). The results show that our SDCL surpasses the current
State-of-the-Art (SOTA) methods by 2.57\%, 3.04\%, and 2.34\% in the Dice score
on the Pancreas, LA, and ACDC datasets, respectively. In addition, the accuracy
of our method is very close to the fully supervised method on the ACDC dataset,
and even exceeds the fully supervised method on the Pancreas and LA dataset.
(Code available at \url{https://github.com/pascalcpp/SDCL}).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EAGLE: Towards Efficient Arbitrary Referring Visual <span class="highlight-title">Prompt</span>s
  Comprehension for Multimodal Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16723v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16723v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiacheng Zhang, Yang Jiao, Shaoxiang Chen, Jingjing Chen, Yu-Gang Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Multimodal Large Language Models (MLLMs) have sparked great
research interests owing to their exceptional content-reasoning and
instruction-following capabilities. To effectively instruct an MLLM, in
addition to conventional language expressions, the practice of referring to
objects by painting with brushes on images has emerged as a prevalent tool
(referred to as "referring visual prompts") due to its efficacy in aligning the
user's intention with specific image regions. To accommodate the most common
referring visual prompts, namely points, boxes, and masks, existing approaches
initially utilize specialized feature encoding modules to capture the semantics
of the highlighted areas indicated by these prompts. Subsequently, these
encoded region features are adapted to MLLMs through fine-tuning on a
meticulously curated multimodal instruction dataset. However, such designs
suffer from redundancy in architecture. Moreover, they face challenges in
effectively generalizing when encountering a diverse range of arbitrary
referring visual prompts in real-life scenarios. To address the above issues,
we propose EAGLE, a novel MLLM that empowers comprehension of arbitrary
referring visual prompts with less training efforts than existing approaches.
Specifically, our EAGLE maintains the innate format of the referring visual
prompts as colored patches rendered on the given image for conducting the
instruction tuning. Our approach embeds referring visual prompts as spatial
concepts conveying specific spatial areas comprehensible to the MLLM, with the
semantic comprehension of these regions originating from the MLLM itself.
Besides, we also propose a Geometry-Agnostic Learning paradigm (GAL) to further
disentangle the MLLM's region-level comprehension with the specific formats of
referring visual prompts. Extensive experiments are conducted to prove the
effectiveness of our proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vision-Language Model Fine-Tuning via Simple Parameter-Efficient
  Modification <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16718v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16718v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming Li, Jike Zhong, Chenxin Li, Liuzhuozheng Li, Nie Lin, Masashi Sugiyama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in fine-tuning Vision-Language Models (VLMs) have witnessed
the success of prompt tuning and adapter tuning, while the classic model
fine-tuning on inherent parameters seems to be overlooked. It is believed that
fine-tuning the parameters of VLMs with few-shot samples corrupts the
pre-trained knowledge since fine-tuning the CLIP model even degrades
performance. In this paper, we revisit this viewpoint, and propose a new
perspective: fine-tuning the specific parameters instead of all will uncover
the power of classic model fine-tuning on VLMs. Through our meticulous study,
we propose ClipFit, a simple yet effective method to fine-tune CLIP without
introducing any overhead of extra parameters. We demonstrate that by only
fine-tuning the specific bias terms and normalization layers, ClipFit can
improve the performance of zero-shot CLIP by 7.27\% average harmonic mean
accuracy. Lastly, to understand how fine-tuning in CLIPFit affects the
pre-trained models, we conducted extensive experimental analyses w.r.t. changes
in internal parameters and representations. We found that low-level text bias
layers and the first layer normalization layer change much more than other
layers. The code is available at \url{https://github.com/minglllli/CLIPFit}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pose-Guided Fine-Grained Sign Language Video Generation <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16709v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16709v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tongkai Shi, Lianyu Hu, Fanhua Shang, Jichao Feng, Peidong Liu, Wei Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sign language videos are an important medium for spreading and learning sign
language. However, most existing human image synthesis methods produce sign
language images with details that are distorted, blurred, or structurally
incorrect. They also produce sign language video frames with poor temporal
consistency, with anomalies such as flickering and abrupt detail changes
between the previous and next frames. To address these limitations, we propose
a novel Pose-Guided Motion Model (PGMM) for generating fine-grained and
motion-consistent sign language videos. Firstly, we propose a new Coarse Motion
Module (CMM), which completes the deformation of features by optical flow
warping, thus transfering the motion of coarse-grained structures without
changing the appearance; Secondly, we propose a new Pose Fusion Module (PFM),
which guides the modal fusion of RGB and pose features, thus completing the
fine-grained generation. Finally, we design a new metric, Temporal Consistency
Difference (TCD) to quantitatively assess the degree of temporal consistency of
a video by comparing the difference between the frames of the reconstructed
video and the previous and next frames of the target video. Extensive
qualitative and quantitative experiments show that our method outperforms
state-of-the-art methods in most benchmark tests, with visible improvements in
details and temporal consistency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pix2Next: Leveraging Vision Foundation Models for RGB to NIR Image
  Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16706v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16706v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youngwan Jin, Incheol Park, Hanbin Song, Hyeongjin Ju, Yagiz Nalcakan, Shiho Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes Pix2Next, a novel image-to-image translation framework
designed to address the challenge of generating high-quality Near-Infrared
(NIR) images from RGB inputs. Our approach leverages a state-of-the-art Vision
Foundation Model (VFM) within an encoder-decoder architecture, incorporating
cross-attention mechanisms to enhance feature integration. This design captures
detailed global representations and preserves essential spectral
characteristics, treating RGB-to-NIR translation as more than a simple domain
transfer problem. A multi-scale PatchGAN discriminator ensures realistic image
generation at various detail levels, while carefully designed loss functions
couple global context understanding with local feature preservation. We
performed experiments on the RANUS dataset to demonstrate Pix2Next's advantages
in quantitative metrics and visual quality, improving the FID score by 34.81%
compared to existing methods. Furthermore, we demonstrate the practical utility
of Pix2Next by showing improved performance on a downstream object detection
task using generated NIR data to augment limited real NIR datasets. The
proposed approach enables the scaling up of NIR datasets without additional
data acquisition or annotation efforts, potentially accelerating advancements
in NIR-based computer vision applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages,12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3DDX: Bone Surface Reconstruction from a Single Standard-Geometry
  Radiograph via Dual-Face Depth Estimation <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16702v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16702v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Gu, Yoshito Otake, Keisuke Uemura, Masaki Takao, Mazen Soufi, Seiji Okada, Nobuhiko Sugano, Hugues Talbot, Yoshinobu Sato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radiography is widely used in orthopedics for its affordability and low
radiation exposure. 3D reconstruction from a single radiograph, so-called 2D-3D
reconstruction, offers the possibility of various clinical applications, but
achieving clinically viable accuracy and computational efficiency is still an
unsolved challenge. Unlike other areas in computer vision, X-ray imaging's
unique properties, such as ray penetration and fixed geometry, have not been
fully exploited. We propose a novel approach that simultaneously learns
multiple depth maps (front- and back-surface of multiple bones) derived from
the X-ray image to computed tomography registration. The proposed method not
only leverages the fixed geometry characteristic of X-ray imaging but also
enhances the precision of the reconstruction of the whole surface. Our study
involved 600 CT and 2651 X-ray images (4 to 5 posed X-ray images per patient),
demonstrating our method's superiority over traditional approaches with a
surface reconstruction error reduction from 4.78 mm to 1.96 mm. This
significant accuracy improvement and enhanced computational efficiency suggest
our approach's potential for clinical application.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MICCAI 2024. 12 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Layout-Corrector: Alleviating Layout Sticking Phenomenon in Discrete
  Diffusion Model <span class="chip">ECCV2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16689v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16689v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shoma Iwai, Atsuki Osanai, Shunsuke Kitada, Shinichiro Omachi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Layout generation is a task to synthesize a harmonious layout with elements
characterized by attributes such as category, position, and size. Human
designers experiment with the placement and modification of elements to create
aesthetic layouts, however, we observed that current discrete diffusion models
(DDMs) struggle to correct inharmonious layouts after they have been generated.
In this paper, we first provide novel insights into layout sticking phenomenon
in DDMs and then propose a simple yet effective layout-assessment module
Layout-Corrector, which works in conjunction with existing DDMs to address the
layout sticking problem. We present a learning-based module capable of
identifying inharmonious elements within layouts, considering overall layout
harmony characterized by complex composition. During the generation process,
Layout-Corrector evaluates the correctness of each token in the generated
layout, reinitializing those with low scores to the ungenerated state. The DDM
then uses the high-scored tokens as clues to regenerate the harmonized tokens.
Layout-Corrector, tested on common benchmarks, consistently boosts
layout-generation performance when in conjunction with various state-of-the-art
DDMs. Furthermore, our extensive analysis demonstrates that the
Layout-Corrector (1) successfully identifies erroneous tokens, (2) facilitates
control over the fidelity-diversity trade-off, and (3) significantly mitigates
the performance drop associated with fast sampling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV2024, Project Page:
  https://iwa-shi.github.io/Layout-Corrector-Project-Page/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Skyeyes: Ground Roaming using Aerial View Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16685v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16685v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyuan Gao, Wenbin Teng, Gonglin Chen, Jinsen Wu, Ningli Xu, Rongjun Qin, Andrew Feng, Yajie Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrating aerial imagery-based scene generation into applications like
autonomous driving and gaming enhances realism in 3D environments, but
challenges remain in creating detailed content for occluded areas and ensuring
real-time, consistent rendering. In this paper, we introduce Skyeyes, a novel
framework that can generate photorealistic sequences of ground view images
using only aerial view inputs, thereby creating a ground roaming experience.
More specifically, we combine a 3D representation with a view consistent
generation model, which ensures coherence between generated images. This method
allows for the creation of geometrically consistent ground view images, even
with large view gaps. The images maintain improved spatial-temporal coherence
and realism, enhancing scene comprehension and visualization from aerial
perspectives. To the best of our knowledge, there are no publicly available
datasets that contain pairwise geo-aligned aerial and ground view imagery.
Therefore, we build a large, synthetic, and geo-aligned dataset using Unreal
Engine. Both qualitative and quantitative analyses on this synthetic dataset
display superior results compared to other leading synthesis approaches. See
the project page for more results:
https://chaoren2357.github.io/website-skyeyes/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TSBP: Improving Object Detection in Histology Images via Test-time
  Self-guided Bounding-box Propagation <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tingting Yang, Liang Xiao, Yizhe Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A global threshold (e.g., 0.5) is often applied to determine which bounding
boxes should be included in the final results for an object detection task. A
higher threshold reduces false positives but may result in missing a
significant portion of true positives. A lower threshold can increase detection
recall but may also result in more false positives. Because of this, using a
preset global threshold (e.g., 0.5) applied to all the bounding box candidates
may lead to suboptimal solutions. In this paper, we propose a Test-time
Self-guided Bounding-box Propagation (TSBP) method, leveraging Earth Mover's
Distance (EMD) to enhance object detection in histology images. TSBP utilizes
bounding boxes with high confidence to influence those with low confidence,
leveraging visual similarities between them. This propagation mechanism enables
bounding boxes to be selected in a controllable, explainable, and robust
manner, which surpasses the effectiveness of using simple thresholds and
uncertainty calibration methods. Importantly, TSBP does not necessitate
additional labeled samples for model training or parameter estimation, unlike
calibration methods. We conduct experiments on gland detection and cell
detection tasks in histology images. The results show that our proposed TSBP
significantly improves detection outcomes when working in conjunction with
state-of-the-art deep learning-based detection networks. Compared to other
methods such as uncertainty calibration, TSBP yields more robust and accurate
object detection predictions while using no additional labeled samples. The
code is available at https://github.com/jwhgdeu/TSBP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TalkinNeRF: Animatable Neural Fields for Full-Body Talking Humans <span class="chip">ECCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16666v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16666v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aggelina Chatziagapi, Bindita Chaudhuri, Amit Kumar, Rakesh Ranjan, Dimitris Samaras, Nikolaos Sarafianos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel framework that learns a dynamic neural radiance field
(NeRF) for full-body talking humans from monocular videos. Prior work
represents only the body pose or the face. However, humans communicate with
their full body, combining body pose, hand gestures, as well as facial
expressions. In this work, we propose TalkinNeRF, a unified NeRF-based network
that represents the holistic 4D human motion. Given a monocular video of a
subject, we learn corresponding modules for the body, face, and hands, that are
combined together to generate the final result. To capture complex finger
articulation, we learn an additional deformation field for the hands. Our
multi-identity representation enables simultaneous training for multiple
subjects, as well as robust animation under completely unseen poses. It can
also generalize to novel identities, given only a short video as input. We
demonstrate state-of-the-art performance for animating full-body talking
humans, with fine-grained hand articulation and facial expressions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCVW 2024. Project page:
  https://aggelinacha.github.io/TalkinNeRF/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Covariate Shift in Imitation Learning for Autonomous Vehicles
  Using Latent Space Generative World Models <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16663v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16663v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Popov, Alperen Degirmenci, David Wehr, Shashank Hegde, Ryan Oldja, Alexey Kamenev, Bertrand Douillard, David Nistér, Urs Muller, Ruchi Bhargava, Stan Birchfield, Nikolai Smolyanskiy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose the use of latent space generative world models to address the
covariate shift problem in autonomous driving. A world model is a neural
network capable of predicting an agent's next state given past states and
actions. By leveraging a world model during training, the driving policy
effectively mitigates covariate shift without requiring an excessive amount of
training data. During end-to-end training, our policy learns how to recover
from errors by aligning with states observed in human demonstrations, so that
at runtime it can recover from perturbations outside the training distribution.
Additionally, we introduce a novel transformer-based perception encoder that
employs multi-view cross-attention and a learned scene query. We present
qualitative and quantitative results, demonstrating significant improvements
upon prior state of the art in closed-loop testing in the CARLA simulator, as
well as showing the ability to handle perturbations in both CARLA and NVIDIA's
DRIVE Sim.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 6 figures, for ICRA 2025 conference, for associated video
  file, see https://youtu.be/9FpDFD9aiFU</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Progressive Representation Learning for Real-Time UAV Tracking <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16652v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16652v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changhong Fu, Xiang Lei, Haobo Zuo, Liangliang Yao, Guangze Zheng, Jia Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual object tracking has significantly promoted autonomous applications for
unmanned aerial vehicles (UAVs). However, learning robust object
representations for UAV tracking is especially challenging in complex dynamic
environments, when confronted with aspect ratio change and occlusion. These
challenges severely alter the original information of the object. To handle the
above issues, this work proposes a novel progressive representation learning
framework for UAV tracking, i.e., PRL-Track. Specifically, PRL-Track is divided
into coarse representation learning and fine representation learning. For
coarse representation learning, two innovative regulators, which rely on
appearance and semantic information, are designed to mitigate appearance
interference and capture semantic information. Furthermore, for fine
representation learning, a new hierarchical modeling generator is developed to
intertwine coarse object representations. Exhaustive experiments demonstrate
that the proposed PRL-Track delivers exceptional performance on three
authoritative UAV tracking benchmarks. Real-world tests indicate that the
proposed PRL-Track realizes superior tracking performance with 42.6 frames per
second on the typical UAV platform equipped with an edge smart camera. The
code, model, and demo videos are available at
\url{https://github.com/vision4robotics/PRL-Track}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the 2024 IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep-Learning Recognition of Scanning Transmission Electron Microscopy:
  Quantifying and Mitigating the Influence of Gaussian Noises 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanlei Zhang, Jincheng Bai, Xiabo Chen, Can Li, Chuanjian Zhong, Jiye Fang, Guangwen Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scanning transmission electron microscopy (STEM) is a powerful tool to reveal
the morphologies and structures of materials, thereby attracting intensive
interests from the scientific and industrial communities. The outstanding
spatial (atomic level) and temporal (ms level) resolutions of the STEM
techniques generate fruitful amounts of high-definition data, thereby enabling
the high-volume and high-speed analysis of materials. On the other hand,
processing of the big dataset generated by STEM is time-consuming and beyond
the capability of human-based manual work, which urgently calls for
computer-based automation. In this work, we present a deep-learning mask
region-based neural network (Mask R-CNN) for the recognition of nanoparticles
imaged by STEM, as well as generating the associated dimensional analysis. The
Mask R-CNN model was tested on simulated STEM-HAADF results with different
Gaussian noises, particle shapes and particle sizes, and the results indicated
that Gaussian noise has determining influence on the accuracy of recognition.
By applying Gaussian and Non-Local Means filters on the noise-containing
STEM-HAADF results, the influences of noises are largely mitigated, and
recognition accuracy is significantly improved. This filtering-recognition
approach was further applied to experimental STEM-HAADF results, which yields
satisfying accuracy compared with the traditional threshold methods. The
deep-learning-based method developed in this work has great potentials in
analysis of the complicated structures and large data generated by STEM-HAADF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Nighttime UAV Tracking with Light Distribution Suppression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16631v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16631v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liangliang Yao, Changhong Fu, Yiheng Wang, Haobo Zuo, Kunhan Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual object tracking has boosted extensive intelligent applications for
unmanned aerial vehicles (UAVs). However, the state-of-the-art (SOTA) enhancers
for nighttime UAV tracking always neglect the uneven light distribution in
low-light images, inevitably leading to excessive enhancement in scenarios with
complex illumination. To address these issues, this work proposes a novel
enhancer, i.e., LDEnhancer, enhancing nighttime UAV tracking with light
distribution suppression. Specifically, a novel image content refinement module
is developed to decompose the light distribution information and image content
information in the feature space, allowing for the targeted enhancement of the
image content information. Then this work designs a new light distribution
generation module to capture light distribution effectively. The features with
light distribution information and image content information are fed into the
different parameter estimation modules, respectively, for the parameter map
prediction. Finally, leveraging two parameter maps, an innovative interweave
iteration adjustment is proposed for the collaborative pixel-wise adjustment of
low-light images. Additionally, a challenging nighttime UAV tracking dataset
with uneven light distribution, namely NAT2024-2, is constructed to provide a
comprehensive evaluation, which contains 40 challenging sequences with over 74K
frames in total. Experimental results on the authoritative UAV benchmarks and
the proposed NAT2024-2 demonstrate that LDEnhancer outperforms other SOTA
low-light enhancers for nighttime UAV tracking. Furthermore, real-world tests
on a typical UAV platform with an NVIDIA Orin NX confirm the practicality and
efficiency of LDEnhancer. The code is available at
https://github.com/vision4robotics/LDEnhancer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stochastic Subsampling With Average Pooling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16630v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16630v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bum Jun Kim, Sang Woo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Regularization of deep neural networks has been an important issue to achieve
higher generalization performance without overfitting problems. Although the
popular method of Dropout provides a regularization effect, it causes
inconsistent properties in the output, which may degrade the performance of
deep neural networks. In this study, we propose a new module called stochastic
average pooling, which incorporates Dropout-like stochasticity in pooling. We
describe the properties of stochastic subsampling and average pooling and
leverage them to design a module without any inconsistency problem. The
stochastic average pooling achieves a regularization effect without any
potential performance degradation due to the inconsistency issue and can easily
be plugged into existing architectures of deep neural networks. Experiments
demonstrate that replacing existing average pooling with stochastic average
pooling yields consistent improvements across a variety of tasks, datasets, and
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeformStream: Deformation-based Adaptive Volumetric Video Streaming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16615v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16615v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyan Li, Yongting Chen, Dayou Zhang, Fangxin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Volumetric video streaming offers immersive 3D experiences but faces
significant challenges due to high bandwidth requirements and latency issues in
transmitting detailed content in real time. Traditional methods like point
cloud streaming compromise visual quality when zoomed in, and neural rendering
techniques are too computationally intensive for real-time use. Though
mesh-based streaming stands out by preserving surface detail and connectivity,
offering a more refined representation for 3D content, traditional mesh
streaming methods typically transmit data on a per-frame basis, failing to take
full advantage of temporal redundancies across frames. This results in
inefficient bandwidth usage and poor adaptability to fluctuating network
conditions. We introduce Deformation-based Adaptive Volumetric Video Streaming,
a novel framework that enhances volumetric video streaming performance by
leveraging the inherent deformability of mesh-based representations.
DeformStream uses embedded deformation to reconstruct subsequent frames from
inter-frame motion, significantly reducing bandwidth usage while ensuring
visual coherence between frames. To address frame reconstruction overhead and
network adaptability, we formulate a new QoE model that accounts for
client-side deformation latency and design a dynamic programming algorithm to
optimize the trade-off between visual quality and bandwidth consumption under
varying network conditions. Our evaluation demonstrates that Deformation-based
Adaptive Volumetric Video Streaming outperforms existing mesh-based streaming
systems in both bandwidth efficiency and visual quality, offering a robust
solution for real-time volumetric video applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semi-LLIE: Semi-supervised Contrastive Learning with Mamba-based
  Low-light Image Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16604v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16604v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanlin Li, Ke Zhang, Ting Wang, Ming Li, Bin Zhao, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the impressive advancements made in recent low-light image
enhancement techniques, the scarcity of paired data has emerged as a
significant obstacle to further advancements. This work proposes a
mean-teacher-based semi-supervised low-light enhancement (Semi-LLIE) framework
that integrates the unpaired data into model training. The mean-teacher
technique is a prominent semi-supervised learning method, successfully adopted
for addressing high-level and low-level vision tasks. However, two primary
issues hinder the naive mean-teacher method from attaining optimal performance
in low-light image enhancement. Firstly, pixel-wise consistency loss is
insufficient for transferring realistic illumination distribution from the
teacher to the student model, which results in color cast in the enhanced
images. Secondly, cutting-edge image enhancement approaches fail to effectively
cooperate with the mean-teacher framework to restore detailed information in
dark areas due to their tendency to overlook modeling structured information
within local regions. To mitigate the above issues, we first introduce a
semantic-aware contrastive loss to faithfully transfer the illumination
distribution, contributing to enhancing images with natural colors. Then, we
design a Mamba-based low-light image enhancement backbone to effectively
enhance Mamba's local region pixel relationship representation ability with a
multi-scale feature learning scheme, facilitating the generation of images with
rich textural details. Further, we propose novel perceptive loss based on the
large-scale vision-language Recognize Anything Model (RAM) to help generate
enhanced images with richer textual details. The experimental results indicate
that our Semi-LLIE surpasses existing methods in both quantitative and
qualitative metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FAFA: Frequency-Aware Flow-Aided Self-Supervision for Underwater Object
  Pose Estimation <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyi Tang, Gu Wang, Zeyu Chen, Shengquan Li, Xiu Li, Xiangyang Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although methods for estimating the pose of objects in indoor scenes have
achieved great success, the pose estimation of underwater objects remains
challenging due to difficulties brought by the complex underwater environment,
such as degraded illumination, blurring, and the substantial cost of obtaining
real annotations. In response, we introduce FAFA, a Frequency-Aware Flow-Aided
self-supervised framework for 6D pose estimation of unmanned underwater
vehicles (UUVs). Essentially, we first train a frequency-aware flow-based pose
estimator on synthetic data, where an FFT-based augmentation approach is
proposed to facilitate the network in capturing domain-invariant features and
target domain styles from a frequency perspective. Further, we perform
self-supervised training by enforcing flow-aided multi-level consistencies to
adapt it to the real-world underwater environment. Our framework relies solely
on the 3D model and RGB images, alleviating the need for any real pose
annotations or other-modality data like depths. We evaluate the effectiveness
of FAFA on common underwater object pose benchmarks and showcase significant
performance improvements compared to state-of-the-art methods. Code is
available at github.com/tjy0703/FAFA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EventHallusion: Diagnosing Event Hallucinations in Video LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16597v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16597v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiacheng Zhang, Yang Jiao, Shaoxiang Chen, Jingjing Chen, Yu-Gang Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Multimodal Large Language Models (MLLMs) have made significant
progress in the video comprehension field. Despite remarkable content reasoning
and instruction following capabilities they demonstrated, the hallucination
problem of these VideoLLMs is less explored compared with its counterpart in
the image domain. To mitigate this gap, we first propose EventHallusion, a
novel benchmark that focuses on assessing the VideoLMMs' hallucination
phenomenon on video event comprehension. Based on the observation that existing
VideoLLMs are entangled with the priors stemming from their foundation models,
our EventHallusion is curated by meticulously collecting videos and annotating
questions to intentionally mislead the VideoLLMs into interpreting events based
on these priors rather than accurately understanding the video content. On the
other hand, we also propose a simple yet effective method, called Temporal
Contrastive Decoding (TCD), to tackle the hallucination problems of VideoLLMs.
The proposed TCD suppresses the model's preference toward their priors by
comparing the original video with a constructed counterpart, whose temporal
cues are disrupted, during the autoregressive decoding stage. Through
comprehensive evaluation of eight open-source and two closed-source VideoLLMs
on the proposed EventHallusion benchmark, we find that the open-source models
suffer significantly from hallucination problems, whereas the closed-source
models perform markedly better. By further equipping open-sourced VideoLLMs
with the proposed TCD approach, evident performance improvements are achieved
across most metrics in the EventHallusion benchmark. Our codes and benchmark
data are available at https://github.com/Stevetich/EventHallusion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SelectiveKD: A semi-supervised framework for cancer detection in DBT
  through Knowledge Distillation and Pseudo-labeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16581v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16581v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laurent Dillard, Hyeonsoo Lee, Weonsuk Lee, Tae Soo Kim, Ali Diba, Thijs Kooi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When developing Computer Aided Detection (CAD) systems for Digital Breast
Tomosynthesis (DBT), the complexity arising from the volumetric nature of the
modality poses significant technical challenges for obtaining large-scale
accurate annotations. Without access to large-scale annotations, the resulting
model may not generalize to different domains. Given the costly nature of
obtaining DBT annotations, how to effectively increase the amount of data used
for training DBT CAD systems remains an open challenge.
  In this paper, we present SelectiveKD, a semi-supervised learning framework
for building cancer detection models for DBT, which only requires a limited
number of annotated slices to reach high performance. We achieve this by
utilizing unlabeled slices available in a DBT stack through a knowledge
distillation framework in which the teacher model provides a supervisory signal
to the student model for all slices in the DBT volume. Our framework mitigates
the potential noise in the supervisory signal from a sub-optimal teacher by
implementing a selective dataset expansion strategy using pseudo labels.
  We evaluate our approach with a large-scale real-world dataset of over 10,000
DBT exams collected from multiple device manufacturers and locations. The
resulting SelectiveKD process effectively utilizes unannotated slices from a
DBT stack, leading to significantly improved cancer classification performance
(AUC) and generalization performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FLaRe: Achieving Masterful and Adaptive Robot Policies with Large-Scale
  Reinforcement Learning Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16578v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16578v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaheng Hu, Rose Hendrix, Ali Farhadi, Aniruddha Kembhavi, Roberto Martin-Martin, Peter Stone, Kuo-Hao Zeng, Kiana Ehsan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the Robotics field has initiated several efforts toward
building generalist robot policies through large-scale multi-task Behavior
Cloning. However, direct deployments of these policies have led to
unsatisfactory performance, where the policy struggles with unseen states and
tasks. How can we break through the performance plateau of these models and
elevate their capabilities to new heights? In this paper, we propose FLaRe, a
large-scale Reinforcement Learning fine-tuning framework that integrates robust
pre-trained representations, large-scale training, and gradient stabilization
techniques. Our method aligns pre-trained policies towards task completion,
achieving state-of-the-art (SoTA) performance both on previously demonstrated
and on entirely novel tasks and embodiments. Specifically, on a set of
long-horizon mobile manipulation tasks, FLaRe achieves an average success rate
of 79.5% in unseen environments, with absolute improvements of +23.6% in
simulation and +30.7% on real robots over prior SoTA methods. By utilizing only
sparse rewards, our approach can enable generalizing to new capabilities beyond
the pretraining data with minimal human effort. Moreover, we demonstrate rapid
adaptation to new embodiments and behaviors with less than a day of
fine-tuning. Videos can be found on the project website at
https://robot-flare.github.io/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Source-Free Domain Adaptation for YOLO Object Detection <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16538v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16538v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Varailhon, Masih Aminbeidokhti, Marco Pedersoli, Eric Granger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Source-free domain adaptation (SFDA) is a challenging problem in object
detection, where a pre-trained source model is adapted to a new target domain
without using any source domain data for privacy and efficiency reasons. Most
state-of-the-art SFDA methods for object detection have been proposed for
Faster-RCNN, a detector that is known to have high computational complexity.
This paper focuses on domain adaptation techniques for real-world vision
systems, particularly for the YOLO family of single-shot detectors known for
their fast baselines and practical applications. Our proposed SFDA method -
Source-Free YOLO (SF-YOLO) - relies on a teacher-student framework in which the
student receives images with a learned, target domain-specific augmentation,
allowing the model to be trained with only unlabeled target data and without
requiring feature alignment. A challenge with self-training using a
mean-teacher architecture in the absence of labels is the rapid decline of
accuracy due to noisy or drifting pseudo-labels. To address this issue, a
teacher-to-student communication mechanism is introduced to help stabilize the
training and reduce the reliance on annotated target data for model selection.
Despite its simplicity, our approach is competitive with state-of-the-art
detectors on several challenging benchmark datasets, even sometimes
outperforming methods that use source data for adaptation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024: European Conference on Computer Vision - Workshop on
  Out-of-Distribution Generalization in Computer Vision Foundation Models,
  Milan Italy</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Prompt</span> Sliders for Fine-Grained Control, Editing and Erasing of Concepts
  in Diffusion Models <span class="chip">ECCV'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16535v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16535v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deepak Sridhar, Nuno Vasconcelos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have recently surpassed GANs in image synthesis and editing,
offering superior image quality and diversity. However, achieving precise
control over attributes in generated images remains a challenge. Concept
Sliders introduced a method for fine-grained image control and editing by
learning concepts (attributes/objects). However, this approach adds parameters
and increases inference time due to the loading and unloading of Low-Rank
Adapters (LoRAs) used for learning concepts. These adapters are model-specific
and require retraining for different architectures, such as Stable Diffusion
(SD) v1.5 and SD-XL. In this paper, we propose a straightforward textual
inversion method to learn concepts through text embeddings, which are
generalizable across models that share the same text encoder, including
different versions of the SD model. We refer to our method as Prompt Sliders.
Besides learning new concepts, we also show that Prompt Sliders can be used to
erase undesirable concepts such as artistic styles or mature content. Our
method is 30% faster than using LoRAs because it eliminates the need to load
and unload adapters and introduces no additional parameters aside from the
target concept text embedding. Each concept embedding only requires 3KB of
storage compared to the 8922KB or more required for each LoRA adapter, making
our approach more computationally efficient. Project Page:
https://deepaksridhar.github.io/promptsliders.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV'24 - Unlearning and Model Editing Workshop. Code:
  https://github.com/DeepakSridhar/promptsliders</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Pre-train</span>ed Language Models Do Not Help Auto-regressive Text-to-Image
  Generation <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.16201v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.16201v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhui Zhang, Brandon McKinzie, Zhe Gan, Vaishaal Shankar, Alexander Toshev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in image tokenizers, such as VQ-VAE, have enabled
text-to-image generation using auto-regressive methods, similar to language
modeling. However, these methods have yet to leverage pre-trained language
models, despite their adaptability to various downstream tasks. In this work,
we explore this gap by adapting a pre-trained language model for
auto-regressive text-to-image generation, and find that pre-trained language
models offer limited help. We provide a two-fold explanation by analyzing
tokens from each modality. First, we demonstrate that image tokens possess
significantly different semantics compared to text tokens, rendering
pre-trained language models no more effective in modeling them than randomly
initialized ones. Second, the text tokens in the image-text datasets are too
simple compared to normal language model pre-training data, which causes the
catastrophic degradation of language models' capability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at EMNLP 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simple Image Signal Processing using Global Context Guidance <span class="chip">ICIP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.11569v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.11569v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omar Elezabi, Marcos V. Conde, Radu Timofte
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In modern smartphone cameras, the Image Signal Processor (ISP) is the core
element that converts the RAW readings from the sensor into perceptually
pleasant RGB images for the end users. The ISP is typically proprietary and
handcrafted and consists of several blocks such as white balance, color
correction, and tone mapping. Deep learning-based ISPs aim to transform RAW
images into DSLR-like RGB images using deep neural networks. However, most
learned ISPs are trained using patches (small regions) due to computational
limitations. Such methods lack global context, which limits their efficacy on
full-resolution images and harms their ability to capture global properties
such as color constancy or illumination. First, we propose a novel module that
can be integrated into any neural ISP to capture the global context information
from the full RAW images. Second, we propose an efficient and simple neural ISP
that utilizes our proposed module. Our model achieves state-of-the-art results
on different benchmarks using diverse and real smartphone images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE International Conference on Image Processing (ICIP) 2024 - Oral
  Presentation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LingoQA: Video Question Answering for Autonomous Driving <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.14115v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.14115v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ana-Maria Marcu, Long Chen, Jan Hünermann, Alice Karnsund, Benoit Hanotte, Prajwal Chidananda, Saurabh Nair, Vijay Badrinarayanan, Alex Kendall, Jamie Shotton, Elahe Arani, Oleg Sinavski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce LingoQA, a novel dataset and benchmark for visual question
answering in autonomous driving. The dataset contains 28K unique short video
scenarios, and 419K annotations. Evaluating state-of-the-art vision-language
models on our benchmark shows that their performance is below human
capabilities, with GPT-4V responding truthfully to 59.6% of the questions
compared to 96.6% for humans. For evaluation, we propose a truthfulness
classifier, called Lingo-Judge, that achieves a 0.95 Spearman correlation
coefficient to human evaluations, surpassing existing techniques like METEOR,
BLEU, CIDEr, and GPT-4. We establish a baseline vision-language model and run
extensive ablation studies to understand its performance. We release our
dataset and benchmark https://github.com/wayveai/LingoQA as an evaluation
platform for vision-language models in autonomous driving.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2024. Benchmark and dataset are available at
  https://github.com/wayveai/LingoQA/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DilateQuant: Accurate and Efficient Diffusion Quantization via Weight
  Dilation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14307v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14307v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuewen Liu, Zhikai Li, Qingyi Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have shown excellent performance on various image generation
tasks, but the substantial computational costs and huge memory footprint hinder
their low-latency applications in real-world scenarios. Quantization is a
promising way to compress and accelerate models. Nevertheless, due to the wide
range and time-varying activations in diffusion models, existing methods cannot
maintain both accuracy and efficiency simultaneously for low-bit quantization.
To tackle this issue, we propose DilateQuant, a novel quantization framework
for diffusion models that offers comparable accuracy and high efficiency.
Specifically, we keenly aware of numerous unsaturated in-channel weights, which
can be cleverly exploited to reduce the range of activations without additional
computation cost. Based on this insight, we propose Weight Dilation (WD) that
maximally dilates the unsaturated in-channel weights to a constrained range
through a mathematically equivalent scaling. WD costlessly absorbs the
activation quantization errors into weight quantization. The range of
activations decreases, which makes activations quantization easy. The range of
weights remains constant, which makes model easy to converge in training stage.
Considering the temporal network leads to time-varying activations, we design a
Temporal Parallel Quantizer (TPQ), which sets time-step quantization parameters
and supports parallel quantization for different time steps, significantly
improving the performance and reducing time cost. To further enhance
performance while preserving efficiency, we introduce a Block-wise Knowledge
Distillation (BKD) to align the quantized models with the full-precision models
at a block level. The simultaneous training of time-step quantization
parameters and weights minimizes the time required, and the shorter
backpropagation paths decreases the memory footprint of the quantization
process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: http://github.com/BienLuky/DilateQuant</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Real-time estimation of overt attention from dynamic features of the
  face using deep-learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13084v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13084v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aimar Silvan Ortubay, Lucas C. Parra, Jens Madsen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Students often drift in and out of focus during class. Effective teachers
recognize this and re-engage them when necessary. With the shift to remote
learning, teachers have lost the visual feedback needed to adapt to varying
student engagement. We propose using readily available front-facing video to
infer attention levels based on movements of the eyes, head, and face. We train
a deep learning model to predict a measure of attention based on overt eye
movements. Specifically, we measure Inter-Subject Correlation of eye movements
in ten-second intervals while students watch the same educational videos. In 3
different experiments (N=83) we show that the trained model predicts this
objective metric of attention on unseen data with $R^2$=0.38, and on unseen
subjects with $R^2$=0.26-0.30. The deep network relies mostly on a student's
eye movements, but to some extent also on movements of the brows, cheeks, and
head. In contrast to Inter-Subject Correlation of the eyes, the model can
estimate attentional engagement from individual students' movements without
needing reference data from an attentive group. This enables a much broader set
of online applications. The solution is lightweight and can operate on the
client side, which mitigates some of the privacy concerns associated with
online attention monitoring. GitHub implementation is available at
https://github.com/asortubay/timeISC
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Guide-and-Rescale: Self-Guidance Mechanism for Effective Tuning-Free
  Real Image Editing <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01322v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01322v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vadim Titov, Madina Khalmatova, Alexandra Ivanova, Dmitry Vetrov, Aibek Alanov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent advances in large-scale text-to-image generative models,
manipulating real images with these models remains a challenging problem. The
main limitations of existing editing methods are that they either fail to
perform with consistent quality on a wide range of image edits or require
time-consuming hyperparameter tuning or fine-tuning of the diffusion model to
preserve the image-specific appearance of the input image. We propose a novel
approach that is built upon a modified diffusion sampling process via the
guidance mechanism. In this work, we explore the self-guidance technique to
preserve the overall structure of the input image and its local regions
appearance that should not be edited. In particular, we explicitly introduce
layout-preserving energy functions that are aimed to save local and global
structures of the source image. Additionally, we propose a noise rescaling
mechanism that allows to preserve noise distribution by balancing the norms of
classifier-free guidance and our proposed guiders during generation. Such a
guiding approach does not require fine-tuning the diffusion model and exact
inversion process. As a result, the proposed method provides a fast and
high-quality editing mechanism. In our experiments, we show through human
evaluation and quantitative analysis that the proposed method allows to produce
desired editing which is more preferable by humans and also achieves a better
trade-off between editing quality and preservation of the original image. Our
code is available at https://github.com/MACderRu/Guide-and-Rescale.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2024. The project page is available at
  https://macderru.github.io/Guide-and-Rescale</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RAP: Retrieval-Augmented Planner for Adaptive Procedure Planning in
  Instructional Videos <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18600v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18600v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Zare, Yulei Niu, Hammad Ayyubi, Shih-fu Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Procedure Planning in instructional videos entails generating a sequence of
action steps based on visual observations of the initial and target states.
Despite the rapid progress in this task, there remain several critical
challenges to be solved: (1) Adaptive procedures: Prior works hold an
unrealistic assumption that the number of action steps is known and fixed,
leading to non-generalizable models in real-world scenarios where the sequence
length varies. (2) Temporal relation: Understanding the step temporal relation
knowledge is essential in producing reasonable and executable plans. (3)
Annotation cost: Annotating instructional videos with step-level labels (i.e.,
timestamp) or sequence-level labels (i.e., action category) is demanding and
labor-intensive, limiting its generalizability to large-scale datasets. In this
work, we propose a new and practical setting, called adaptive procedure
planning in instructional videos, where the procedure length is not fixed or
pre-determined. To address these challenges, we introduce Retrieval-Augmented
Planner (RAP) model. Specifically, for adaptive procedures, RAP adaptively
determines the conclusion of actions using an auto-regressive model
architecture. For temporal relation, RAP establishes an external memory module
to explicitly retrieve the most relevant state-action pairs from the training
videos and revises the generated procedures. To tackle high annotation cost,
RAP utilizes a weakly-supervised learning manner to expand the training dataset
to other task-relevant, unannotated videos by generating pseudo labels for
action steps. Experiments on CrossTask and COIN benchmarks show the superiority
of RAP over traditional fixed-length models, establishing it as a strong
baseline solution for adaptive procedure planning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bits-to-Photon: End-to-End Learned Scalable Point Cloud Compression for
  Direct Rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05915v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05915v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yueyu Hu, Ran Gong, Yao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point cloud is a promising 3D representation for volumetric streaming in
emerging AR/VR applications. Despite recent advances in point cloud
compression, decoding and rendering high-quality images from lossy compressed
point clouds is still challenging in terms of quality and complexity, making it
a major roadblock to achieve real-time 6-Degree-of-Freedom video streaming. In
this paper, we address this problem by developing a point cloud compression
scheme that generates a bit stream that can be directly decoded to renderable
3D Gaussians. The encoder and decoder are jointly optimized to consider both
bit-rates and rendering quality. It significantly improves the rendering
quality while substantially reducing decoding and rendering time, compared to
existing point cloud compression methods. Furthermore, the proposed scheme
generates a scalable bit stream, allowing multiple levels of details at
different bit-rate ranges. Our method supports real-time color decoding and
rendering of high quality point clouds, thus paving the way for interactive 3D
streaming applications with free view points.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Huatuo<span class="highlight-title">GPT</span>-Vision, Towards Injecting Medical Visual Knowledge into
  Multimodal LLMs at Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19280v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19280v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junying Chen, Chi Gui, Ruyi Ouyang, Anningzhe Gao, Shunian Chen, Guiming Hardy Chen, Xidong Wang, Ruifei Zhang, Zhenyang Cai, Ke Ji, Guangjun Yu, Xiang Wan, Benyou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of multimodal large language models (MLLMs), such as
GPT-4V, has led to significant advancements. However, these models still face
challenges in medical multimodal capabilities due to limitations in the
quantity and quality of medical vision-text data, stemming from data privacy
concerns and high annotation costs. While pioneering approaches utilize
PubMed's large-scale, de-identified medical image-text pairs to address these
limitations, they still fall short due to inherent data noise. To tackle this,
we refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in
an 'unblinded' capacity to denoise and reformat the data, resulting in the
creation of the PubMedVision dataset with 1.3 million medical VQA samples. Our
validation demonstrates that: (1) PubMedVision can significantly enhance the
medical multimodal capabilities of current MLLMs, showing significant
improvement in benchmarks including the MMMU Health & Medicine track; (2)
manual checks by medical experts and empirical results validate the superior
data quality of our dataset compared to other data construction methods. Using
PubMedVision, we train a 34B medical MLLM HuatuoGPT-Vision, which shows
superior performance in medical multimodal scenarios among open-source MLLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ YCB-Ev 1.1: Event-vision <span class="highlight-title">dataset</span> for 6DoF object pose estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08482v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08482v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pavel Rojtberg, Thomas Pöllabauer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our work introduces the YCB-Ev dataset, which contains synchronized RGB-D
frames and event data that enables evaluating 6DoF object pose estimation
algorithms using these modalities.
  This dataset provides ground truth 6DoF object poses for the same 21 YCB
objects that were used in the YCB-Video (YCB-V) dataset, allowing for
cross-dataset algorithm performance evaluation.
  The dataset consists of 21 synchronized event and RGB-D sequences, totalling
13,851 frames (7 minutes and 43 seconds of event data). Notably, 12 of these
sequences feature the same object arrangement as the YCB-V subset used in the
BOP challenge.
  Ground truth poses are generated by detecting objects in the RGB-D frames,
interpolating the poses to align with the event timestamps, and then
transferring them to the event coordinate frame using extrinsic calibration.
  Our dataset is the first to provide ground truth 6DoF pose data for event
streams. Furthermore, we evaluate the generalization capabilities of two
state-of-the-art algorithms, which were pre-trained for the BOP challenge,
using our novel YCB-V sequences.
  The dataset is publicly available at https://github.com/paroj/ycbev.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Event-Free Moving Object Segmentation from Moving Ego Vehicle 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.00126v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.00126v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuyun Zhou, Zongwei Wu, Danda Pani Paudel, Rémi Boutteau, Fan Yang, Luc Van Gool, Radu Timofte, Dominique Ginhac
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Moving object segmentation (MOS) in dynamic scenes is an important,
challenging, but under-explored research topic for autonomous driving,
especially for sequences obtained from moving ego vehicles. Most segmentation
methods leverage motion cues obtained from optical flow maps. However, since
these methods are often based on optical flows that are pre-computed from
successive RGB frames, this neglects the temporal consideration of events
occurring within the inter-frame, consequently constraining its ability to
discern objects exhibiting relative staticity but genuinely in motion. To
address these limitations, we propose to exploit event cameras for better video
understanding, which provide rich motion cues without relying on optical flow.
To foster research in this area, we first introduce a novel large-scale dataset
called DSEC-MOS for moving object segmentation from moving ego vehicles, which
is the first of its kind. For benchmarking, we select various mainstream
methods and rigorously evaluate them on our dataset. Subsequently, we devise
EmoFormer, a novel network able to exploit the event data. For this purpose, we
fuse the event temporal prior with spatial semantic maps to distinguish
genuinely moving objects from the static background, adding another level of
dense supervision around our object of interest. Our proposed network relies
only on event data for training but does not require event input during
inference, making it directly comparable to frame-only methods in terms of
efficiency and more widely usable in many application cases. The exhaustive
comparison highlights a significant performance improvement of our method over
all other methods. The source code and dataset are publicly available at:
https://github.com/ZZY-Zhou/DSEC-MOS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StreetSurfaceVis: a <span class="highlight-title">dataset</span> of crowdsourced street-level imagery
  annotated by road surface type and quality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.21454v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.21454v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandra Kapp, Edith Hoffmann, Esther Weigmann, Helena Mihaljević
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Road unevenness significantly impacts the safety and comfort of traffic
participants, especially vulnerable groups such as cyclists and wheelchair
users. To train models for comprehensive road surface assessments, we introduce
StreetSurfaceVis, a novel dataset comprising 9,122 street-level images mostly
from Germany collected from a crowdsourcing platform and manually annotated by
road surface type and quality. By crafting a heterogeneous dataset, we aim to
enable robust models that maintain high accuracy across diverse image sources.
As the frequency distribution of road surface types and qualities is highly
imbalanced, we propose a sampling strategy incorporating various external label
prediction resources to ensure sufficient images per class while reducing
manual annotation. More precisely, we estimate the impact of (1) enriching the
image data with OpenStreetMap tags, (2) iterative training and application of a
custom surface type classification model, (3) amplifying underrepresented
classes through prompt-based classification with GPT-4o and (4) similarity
search using image embeddings. Combining these strategies effectively reduces
manual annotation workload while ensuring sufficient class representation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SimTxtSeg: Weakly-Supervised Medical Image Segmentation with Simple Text
  Cues <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19364v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19364v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Xie, Tao Zhou, Yi Zhou, Geng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weakly-supervised medical image segmentation is a challenging task that aims
to reduce the annotation cost while keep the segmentation performance. In this
paper, we present a novel framework, SimTxtSeg, that leverages simple text cues
to generate high-quality pseudo-labels and study the cross-modal fusion in
training segmentation models, simultaneously. Our contribution consists of two
key components: an effective Textual-to-Visual Cue Converter that produces
visual prompts from text prompts on medical images, and a text-guided
segmentation model with Text-Vision Hybrid Attention that fuses text and image
features. We evaluate our framework on two medical image segmentation tasks:
colonic polyp segmentation and MRI brain tumor segmentation, and achieve
consistent state-of-the-art performance. Source code is available at:
https://github.com/xyx1024/SimTxtSeg.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RoboSense: Large-scale <span class="highlight-title">Dataset</span> and Benchmark for Multi-sensor Low-speed
  Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.15503v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.15503v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haisheng Su, Feixiang Song, Cong Ma, Wei Wu, Junchi Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust object detection and tracking under arbitrary sight of view is
challenging yet essential for the development of Autonomous Vehicle technology.
With the growing demand of unmanned function vehicles, near-field scene
understanding becomes an important research topic in the areas of low-speed
autonomous driving. Due to the complexity of driving conditions and diversity
of near obstacles such as blind spots and high occlusion, the perception
capability of near-field environment is still inferior than its farther
counterpart. To further enhance the intelligent ability of unmanned vehicles,
in this paper, we construct a multimodal data collection platform based on 3
main types of sensors (Camera, LiDAR and Fisheye), which supports flexible
sensor configurations to enable dynamic sight of view for ego vehicle, either
global view or local view. Meanwhile, a large-scale multi-sensor dataset is
built, named RoboSense, to facilitate near-field scene understanding. RoboSense
contains more than 133K synchronized data with 1.4M 3D bounding box and IDs
annotated in the full $360^{\circ}$ view, forming 216K trajectories across 7.6K
temporal sequences. It has $270\times$ and $18\times$ as many annotations of
near-field obstacles within 5$m$ as the previous single-vehicle datasets such
as KITTI and nuScenes. Moreover, we define a novel matching criterion for
near-field 3D perception and prediction metrics. Based on RoboSense, we
formulate 6 popular tasks to facilitate the future development of related
research, where the detailed data analysis as well as benchmarks are also
provided accordingly. Code and dataset will be available at
https://github.com/suhaisheng/RoboSense.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Realism in Action: Anomaly-Aware Diagnosis of Brain Tumors from Medical
  Images Using YOLOv8 and DeiT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03302v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03302v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seyed Mohammad Hossein Hashemi, Leila Safari, Amirhossein Dadashzadeh Taromi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of medical sciences, reliable detection and classification of
brain tumors from images remains a formidable challenge due to the rarity of
tumors within the population of patients. Therefore, the ability to detect
tumors in anomaly scenarios is paramount for ensuring timely interventions and
improved patient outcomes. This study addresses the issue by leveraging deep
learning (DL) techniques to detect and classify brain tumors in challenging
situations. The curated data set from the National Brain Mapping Lab (NBML)
comprises 81 patients, including 30 Tumor cases and 51 Normal cases. The
detection and classification pipelines are separated into two consecutive
tasks. The detection phase involved comprehensive data analysis and
pre-processing to modify the number of image samples and the number of patients
of each class to anomaly distribution (9 Normal per 1 Tumor) to comply with
real world scenarios. Next, in addition to common evaluation metrics for the
testing, we employed a novel performance evaluation method called Patient to
Patient (PTP), focusing on the realistic evaluation of the model. In the
detection phase, we fine-tuned a YOLOv8n detection model to detect the tumor
region. Subsequent testing and evaluation yielded competitive performance both
in Common Evaluation Metrics and PTP metrics. Furthermore, using the Data
Efficient Image Transformer (DeiT) module, we distilled a Vision Transformer
(ViT) model from a fine-tuned ResNet152 as a teacher in the classification
phase. This approach demonstrates promising strides in reliable tumor detection
and classification, offering potential advancements in tumor diagnosis for
real-world medical imaging scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the Elsevier for possible
  publication. Copyright may be transferred without notice, after which this
  version may no longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Toward Tiny and High-quality Facial Makeup with Data Amplify Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15033v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15033v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiaoqiao Jin, Xuanhong Chen, Meiguang Jin, Ying Chen, Rui Shi, Yucheng Zheng, Yupeng Zhu, Bingbing Ni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contemporary makeup approaches primarily hinge on unpaired learning
paradigms, yet they grapple with the challenges of inaccurate supervision
(e.g., face misalignment) and sophisticated facial prompts (including face
parsing, and landmark detection). These challenges prohibit low-cost deployment
of facial makeup models, especially on mobile devices. To solve above problems,
we propose a brand-new learning paradigm, termed "Data Amplify Learning (DAL),"
alongside a compact makeup model named "TinyBeauty." The core idea of DAL lies
in employing a Diffusion-based Data Amplifier (DDA) to "amplify" limited images
for the model training, thereby enabling accurate pixel-to-pixel supervision
with merely a handful of annotations. Two pivotal innovations in DDA facilitate
the above training approach: (1) A Residual Diffusion Model (RDM) is designed
to generate high-fidelity detail and circumvent the detail vanishing problem in
the vanilla diffusion models; (2) A Fine-Grained Makeup Module (FGMM) is
proposed to achieve precise makeup control and combination while retaining face
identity. Coupled with DAL, TinyBeauty necessitates merely 80K parameters to
achieve a state-of-the-art performance without intricate face prompts.
Meanwhile, TinyBeauty achieves a remarkable inference speed of up to 460 fps on
the iPhone 13. Extensive experiments show that DAL can produce highly
competitive makeup models using only 5 image pairs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hyperbolic Metric Learning for Visual Outlier Detection <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15260v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15260v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alvaro Gonzalez-Jimenez, Simone Lionetti, Dena Bazazian, Philippe Gottfrois, Fabian Gröger, Marc Pouly, Alexander Navarini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-Of-Distribution (OOD) detection is critical to deploy deep learning
models in safety-critical applications. However, the inherent hierarchical
concept structure of visual data, which is instrumental to OOD detection, is
often poorly captured by conventional methods based on Euclidean geometry. This
work proposes a metric framework that leverages the strengths of Hyperbolic
geometry for OOD detection. Inspired by previous works that refine the decision
boundary for OOD data with synthetic outliers, we extend this method to
Hyperbolic space. Interestingly, we find that synthetic outliers do not benefit
OOD detection in Hyperbolic space as they do in Euclidean space. Furthermore we
explore the relationship between OOD detection performance and Hyperbolic
embedding dimension, addressing practical concerns in resource-constrained
environments. Extensive experiments show that our framework improves the FPR95
for OOD detection from 22\% to 15\% and from 49% to 28% on CIFAR-10 and
CIFAR-100 respectively compared to Euclidean methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>European Conference on Computer Vision ECCV 2024 BEW Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LiverUSRecon: Automatic 3D Reconstruction and Volumetry of the Liver
  with a Few Partial Ultrasound Scans <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19336v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19336v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaushalya Sivayogaraj, Sahan T. Guruge, Udari Liyanage, Jeevani Udupihille, Saroj Jayasinghe, Gerard Fernando, Ranga Rodrigo, M. Rukshani Liyanaarachchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D reconstruction of the liver for volumetry is important for qualitative
analysis and disease diagnosis. Liver volumetry using ultrasound (US) scans,
although advantageous due to less acquisition time and safety, is challenging
due to the inherent noisiness in US scans, blurry boundaries, and partial liver
visibility. We address these challenges by using the segmentation masks of a
few incomplete sagittal-plane US scans of the liver in conjunction with a
statistical shape model (SSM) built using a set of CT scans of the liver. We
compute the shape parameters needed to warp this canonical SSM to fit the US
scans through a parametric regression network. The resulting 3D liver
reconstruction is accurate and leads to automatic liver volume calculation. We
evaluate the accuracy of the estimated liver volumes with respect to CT
segmentation volumes using RMSE. Our volume computation is statistically much
closer to the volume estimated using CT scans than the volume computed using
Childs' method by radiologists: p-value of 0.094 (>0.05) says that there is no
significant difference between CT segmentation volumes and ours in contrast to
Childs' method. We validate our method using investigations (ablation studies)
on the US image resolution, the number of CT scans used for SSM, the number of
principal components, and the number of input US scans. To the best of our
knowledge, this is the first automatic liver volumetry system using a few
incomplete US scans given a set of CT scans of livers for SSM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, Accepted to MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interpretable Vision-Language Survival Analysis with Ordinal Inductive
  Bias for Computational Pathology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09369v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09369v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pei Liu, Luping Ji, Jiaxiang Gou, Bo Fu, Mao Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Histopathology Whole-Slide Images (WSIs) provide an important tool to assess
cancer prognosis in computational pathology (CPATH). While existing survival
analysis (SA) approaches have made exciting progress, they are generally
limited to adopting highly-expressive architectures and only coarse-grained
patient-level labels to learn prognostic visual representations from gigapixel
WSIs. Such learning paradigm suffers from important performance bottlenecks,
when facing present scarce training data and standard multi-instance learning
(MIL) framework in CPATH. To overcome it, this paper, for the first time,
proposes a new Vision-Language-based SA (VLSA) paradigm. Concretely, (1) VLSA
is driven by pathology VL foundation models. It no longer relies on
high-capability networks and shows the advantage of data efficiency. (2) In
vision-end, VLSA encodes prognostic language prior and then employs it as
auxiliary signals to guide the aggregating of prognostic visual features at
instance level, thereby compensating for the weak supervision in MIL. Moreover,
given the characteristics of SA, we propose i) ordinal survival prompt learning
to transform continuous survival labels into textual prompts; and ii) ordinal
incidence function as prediction target to make SA compatible with VL-based
prediction. Notably, VLSA's predictions can be interpreted intuitively by our
Shapley values-based method. The extensive experiments on five datasets confirm
the effectiveness of our scheme. Our VLSA could pave a new way for SA in CPATH
by offering weakly-supervised MIL an effective means to learn valuable
prognostic clues from gigapixel WSIs. Our source code is available at
https://github.com/liupei101/VLSA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 11 tables, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ScanTalk: 3D Talking Heads from Unregistered Scans <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10942v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10942v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Federico Nocentini, Thomas Besnier, Claudio Ferrari, Sylvain Arguillere, Stefano Berretti, Mohamed Daoudi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech-driven 3D talking heads generation has emerged as a significant area
of interest among researchers, presenting numerous challenges. Existing methods
are constrained by animating faces with fixed topologies, wherein point-wise
correspondence is established, and the number and order of points remains
consistent across all identities the model can animate. In this work, we
present \textbf{ScanTalk}, a novel framework capable of animating 3D faces in
arbitrary topologies including scanned data. Our approach relies on the
DiffusionNet architecture to overcome the fixed topology constraint, offering
promising avenues for more flexible and realistic 3D animations. By leveraging
the power of DiffusionNet, ScanTalk not only adapts to diverse facial
structures but also maintains fidelity when dealing with scanned data, thereby
enhancing the authenticity and versatility of generated 3D talking heads.
Through comprehensive comparisons with state-of-the-art methods, we validate
the efficacy of our approach, demonstrating its capacity to generate realistic
talking heads comparable to existing techniques. While our primary objective is
to develop a generic method free from topological constraints, all
state-of-the-art methodologies are bound by such limitations. Code for
reproducing our results, and the pre-trained model are available at
https://github.com/miccunifi/ScanTalk .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in the ECCV 2024 Proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ProbTalk3D: Non-Deterministic Emotion Controllable Speech-Driven 3D
  Facial Animation Synthesis Using VQ-VAE <span class="chip">SIGGRAPH</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07966v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07966v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sichun Wu, Kazi Injamamul Haque, Zerrin Yumak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-driven 3D facial animation synthesis has been an active field of
research with attention from both academia and industry. While there are
promising results in this area, recent approaches largely focus on lip-sync and
identity control, neglecting the role of emotions and emotion control in the
generative process. That is mainly due to the lack of emotionally rich facial
animation data and algorithms that can synthesize speech animations with
emotional expressions at the same time. In addition, majority of the models are
deterministic, meaning given the same audio input, they produce the same output
motion. We argue that emotions and non-determinism are crucial to generate
diverse and emotionally-rich facial animations. In this paper, we propose
ProbTalk3D a non-deterministic neural network approach for emotion controllable
speech-driven 3D facial animation synthesis using a two-stage VQ-VAE model and
an emotionally rich facial animation dataset 3DMEAD. We provide an extensive
comparative analysis of our model against the recent 3D facial animation
synthesis approaches, by evaluating the results objectively, qualitatively, and
with a perceptual user study. We highlight several objective metrics that are
more suitable for evaluating stochastic outputs and use both in-the-wild and
ground truth data for subjective evaluation. To our knowledge, that is the
first non-deterministic 3D facial animation synthesis method incorporating a
rich emotion dataset and emotion control with emotion labels and intensity
levels. Our evaluation demonstrates that the proposed model achieves superior
performance compared to state-of-the-art emotion-controlled, deterministic and
non-deterministic models. We recommend watching the supplementary video for
quality judgement. The entire codebase is publicly available
(https://github.com/uuembodiedsocialai/ProbTalk3D/).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 9 figures, 3 tables. Includes code. Accepted at ACM
  SIGGRAPH MIG 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Motion Prediction: A Lightweight & Accurate Trajectory
  Prediction Model With Fast Training and Inference Speed <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16154v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16154v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Prutsch, Horst Bischof, Horst Possegger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For efficient and safe autonomous driving, it is essential that autonomous
vehicles can predict the motion of other traffic agents. While highly accurate,
current motion prediction models often impose significant challenges in terms
of training resource requirements and deployment on embedded hardware. We
propose a new efficient motion prediction model, which achieves highly
competitive benchmark results while training only a few hours on a single GPU.
Due to our lightweight architectural choices and the focus on reducing the
required training resources, our model can easily be applied to custom
datasets. Furthermore, its low inference latency makes it particularly suitable
for deployment in autonomous applications with limited computing resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FlexiTex: Enhancing Texture Generation with Visual Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12431v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12431v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        DaDong Jiang, Xianghui Yang, Zibo Zhao, Sheng Zhang, Jiaao Yu, Zeqiang Lai, Shaoxiong Yang, Chunchao Guo, Xiaobo Zhou, Zhihui Ke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent texture generation methods achieve impressive results due to the
powerful generative prior they leverage from large-scale text-to-image
diffusion models. However, abstract textual prompts are limited in providing
global textural or shape information, which results in the texture generation
methods producing blurry or inconsistent patterns. To tackle this, we present
FlexiTex, embedding rich information via visual guidance to generate a
high-quality texture. The core of FlexiTex is the Visual Guidance Enhancement
module, which incorporates more specific information from visual guidance to
reduce ambiguity in the text prompt and preserve high-frequency details. To
further enhance the visual guidance, we introduce a Direction-Aware Adaptation
module that automatically designs direction prompts based on different camera
poses, avoiding the Janus problem and maintaining semantically global
consistency. Benefiting from the visual guidance, FlexiTex produces
quantitatively and qualitatively sound results, demonstrating its potential to
advance texture generation for real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://flexitex.github.io/FlexiTex/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TroL: Traversal of Layers for Large Language and Vision Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12246v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12246v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Byung-Kwan Lee, Sangyun Chung, Chae Won Kim, Beomchan Park, Yong Man Ro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language and vision models (LLVMs) have been driven by the
generalization power of large language models (LLMs) and the advent of visual
instruction tuning. Along with scaling them up directly, these models enable
LLVMs to showcase powerful vision language (VL) performances by covering
diverse tasks via natural language instructions. However, existing open-source
LLVMs that perform comparably to closed-source LLVMs such as GPT-4V are often
considered too large (e.g., 26B, 34B, and 110B parameters), having a larger
number of layers. These large models demand costly, high-end resources for both
training and inference. To address this issue, we present a new efficient LLVM
family with 1.8B, 3.8B, and 7B LLM model sizes, Traversal of Layers (TroL),
which enables the reuse of layers in a token-wise manner. This layer traversing
technique simulates the effect of looking back and retracing the answering
stream while increasing the number of forward propagation layers without
physically adding more layers. We demonstrate that TroL employs a simple layer
traversing approach yet efficiently outperforms the open-source LLVMs with
larger model sizes and rivals the performances of the closed-source LLVMs with
substantial sizes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024. Code is available in https://github.com/ByungKwanLee/TroL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TransUKAN:Computing-Efficient Hybrid KAN-<span class="highlight-title">Transformer</span> for Enhanced
  Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14676v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14676v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanlin Wu, Tao Li, Zhihong Wang, Hong Kang, Along He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  U-Net is currently the most widely used architecture for medical image
segmentation. Benefiting from its unique encoder-decoder architecture and skip
connections, it can effectively extract features from input images to segment
target regions. The commonly used U-Net is typically based on convolutional
operations or Transformers, modeling the dependencies between local or global
information to accomplish medical image analysis tasks. However, convolutional
layers, fully connected layers, and attention mechanisms used in this process
introduce a significant number of parameters, often requiring the stacking of
network layers to model complex nonlinear relationships, which can impact the
training process. To address these issues, we propose TransUKAN. Specifically,
we have improved the KAN to reduce memory usage and computational load. On this
basis, we explored an effective combination of KAN, Transformer, and U-Net
structures. This approach enhances the model's capability to capture nonlinear
relationships by introducing only a small number of additional parameters and
compensates for the Transformer structure's deficiency in local information
extraction. We validated TransUKAN on multiple medical image segmentation
tasks. Experimental results demonstrate that TransUKAN achieves excellent
performance with significantly reduced parameters. The code will be available
athttps://github.com/wuyanlin-wyl/TransUKAN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ One-Shot Machine Unlearning with Mnemonic Code 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05670v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05670v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomoya Yamashita, Masanori Yamada, Takashi Shibata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ethical and privacy issues inherent in artificial intelligence (AI)
applications have been a growing concern with the rapid spread of deep
learning. Machine unlearning (MU) is the research area that addresses these
issues by making a trained AI model forget about undesirable training data.
Unfortunately, most existing MU methods incur significant time and
computational costs for forgetting. Therefore, it is often difficult to apply
these methods to practical datasets and sophisticated architectures, e.g.,
ImageNet and Transformer. To tackle this problem, we propose a lightweight and
effective MU method. Our method identifies the model parameters sensitive to
the forgetting targets and adds perturbation to such model parameters. We
identify the sensitive parameters by calculating the Fisher Information Matrix
(FIM). This approach does not require time-consuming additional training for
forgetting. In addition, we introduce class-specific random signals called
mnemonic code to reduce the cost of FIM calculation, which generally requires
the entire training data and incurs significant computational costs. In our
method, we train the model with mnemonic code; when forgetting, we use a small
number of mnemonic codes to calculate the FIM and get the effective
perturbation for forgetting. Comprehensive experiments demonstrate that our
method is faster and better at forgetting than existing MU methods.
Furthermore, we show that our method can scale to more practical datasets and
sophisticated architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, welcome coments</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CVT-Occ: Cost Volume Temporal Fusion for 3D Occupancy Prediction <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13430v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13430v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhangchen Ye, Tao Jiang, Chenfeng Xu, Yiming Li, Hang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-based 3D occupancy prediction is significantly challenged by the
inherent limitations of monocular vision in depth estimation. This paper
introduces CVT-Occ, a novel approach that leverages temporal fusion through the
geometric correspondence of voxels over time to improve the accuracy of 3D
occupancy predictions. By sampling points along the line of sight of each voxel
and integrating the features of these points from historical frames, we
construct a cost volume feature map that refines current volume features for
improved prediction outcomes. Our method takes advantage of parallax cues from
historical observations and employs a data-driven approach to learn the cost
volume. We validate the effectiveness of CVT-Occ through rigorous experiments
on the Occ3D-Waymo dataset, where it outperforms state-of-the-art methods in 3D
occupancy prediction with minimal additional computational cost. The code is
released at \url{https://github.com/Tsinghua-MARS-Lab/CVT-Occ}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CodecNeRF: Toward Fast Encoding and Decoding, Compact, and High-quality
  Novel-view Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.04913v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.04913v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gyeongjin Kang, Younggeun Lee, Seungjun Oh, Eunbyung Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Radiance Fields (NeRF) have achieved huge success in effectively
capturing and representing 3D objects and scenes. However, to establish a
ubiquitous presence in everyday media formats, such as images and videos, we
need to fulfill three key objectives: 1. fast encoding and decoding time, 2.
compact model sizes, and 3. high-quality renderings. Despite recent
advancements, a comprehensive algorithm that adequately addresses all
objectives has yet to be fully realized. In this work, we present CodecNeRF, a
neural codec for NeRF representations, consisting of an encoder and decoder
architecture that can generate a NeRF representation in a single forward pass.
Furthermore, inspired by the recent parameter-efficient finetuning approaches,
we propose a finetuning method to efficiently adapt the generated NeRF
representations to a new test instance, leading to high-quality image
renderings and compact code sizes. The proposed CodecNeRF, a newly suggested
encoding-decoding-finetuning pipeline for NeRF, achieved unprecedented
compression performance of more than 100x and remarkable reduction in encoding
time while maintaining (or improving) the image quality on widely used 3D
object datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://gynjn.github.io/CodecNeRF/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ZoDi: Zero-Shot Domain Adaptation with Diffusion-Based Image Transfer <span class="chip">ECCV2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13652v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13652v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hiroki Azuma, Yusuke Matsui, Atsuto Maki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models achieve high accuracy in segmentation tasks among
others, yet domain shift often degrades the models' performance, which can be
critical in real-world scenarios where no target images are available. This
paper proposes a zero-shot domain adaptation method based on diffusion models,
called ZoDi, which is two-fold by the design: zero-shot image transfer and
model adaptation. First, we utilize an off-the-shelf diffusion model to
synthesize target-like images by transferring the domain of source images to
the target domain. In this we specifically try to maintain the layout and
content by utilising layout-to-image diffusion models with stochastic
inversion. Secondly, we train the model using both source images and
synthesized images with the original segmentation maps while maximizing the
feature similarity of images from the two domains to learn domain-robust
representations. Through experiments we show benefits of ZoDi in the task of
image segmentation over state-of-the-art methods. It is also more applicable
than existing CLIP-based methods because it assumes no specific backbone or
models, and it enables to estimate the model's performance without target
images by inspecting generated images. Our implementation will be publicly
available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV2024 Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Effective are <span class="highlight-title">Self-Supervised</span> Models for Contact Identification in
  Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00498v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00498v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Malitha Gunawardhana, Limalka Sadith, Liel David, Daniel Harari, Muhammad Haris Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The exploration of video content via Self-Supervised Learning (SSL) models
has unveiled a dynamic field of study, emphasizing both the complex challenges
and unique opportunities inherent in this area. Despite the growing body of
research, the ability of SSL models to detect physical contacts in videos
remains largely unexplored, particularly the effectiveness of methods such as
downstream supervision with linear probing or full fine-tuning. This work aims
to bridge this gap by employing eight different convolutional neural networks
(CNNs) based video SSL models to identify instances of physical contact within
video sequences specifically. The Something-Something v2 (SSv2) and
Epic-Kitchen (EK-100) datasets were chosen for evaluating these approaches due
to the promising results on UCF101 and HMDB51, coupled with their limited prior
assessment on SSv2 and EK-100. Additionally, these datasets feature diverse
environments and scenarios, essential for testing the robustness and accuracy
of video-based models. This approach not only examines the effectiveness of
each model in recognizing physical contacts but also explores the performance
in the action recognition downstream task. By doing so, valuable insights into
the adaptability of SSL models in interpreting complex, dynamic visual
information are contributed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Classification of Non-native Handwritten Characters Using Convolutional
  Neural Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04511v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04511v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        F. A. Mamun, S. A. H. Chowdhury, J. E. Giti, H. Sarker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of convolutional neural networks (CNNs) has accelerated the progress
of handwritten character classification/recognition. Handwritten character
recognition (HCR) has found applications in various domains, such as traffic
signal detection, language translation, and document information extraction.
However, the widespread use of existing HCR technology is yet to be seen as it
does not provide reliable character recognition with outstanding accuracy. One
of the reasons for unreliable HCR is that existing HCR methods do not take the
handwriting styles of non-native writers into account. Hence, further
improvement is needed to ensure the reliability and extensive deployment of
character recognition technologies for critical tasks. In this work, the
classification of English characters written by non-native users is performed
by proposing a custom-tailored CNN model. We train this CNN with a new dataset
called the handwritten isolated English character (HIEC) dataset. This dataset
consists of 16,496 images collected from 260 persons. This paper also includes
an ablation study of our CNN by adjusting hyperparameters to identify the best
model for the HIEC dataset. The proposed model with five convolutional layers
and one hidden layer outperforms state-of-the-art models in terms of character
recognition accuracy and achieves an accuracy of $\mathbf{97.04}$%. Compared
with the second-best model, the relative improvement of our model in terms of
classification accuracy is $\mathbf{4.38}$%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EF-Calib: Spatiotemporal Calibration of Event- and Frame-Based Cameras
  Using Continuous-Time Trajectories 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17278v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17278v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaoan Wang, Zhanhua Xin, Yaoqing Hu, Dongyue Li, Mingzhu Zhu, Junzhi Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event camera, a bio-inspired asynchronous triggered camera, offers promising
prospects for fusion with frame-based cameras owing to its low latency and high
dynamic range. However, calibrating stereo vision systems that incorporate both
event and frame-based cameras remains a significant challenge. In this letter,
we present EF-Calib, a spatiotemporal calibration framework for event- and
frame-based cameras using continuous-time trajectories. A novel calibration
pattern applicable to both camera types and the corresponding event recognition
algorithm is proposed. Leveraging the asynchronous nature of events, a
derivable piece-wise B-spline to represent camera pose continuously is
introduced, enabling calibration for intrinsic parameters, extrinsic
parameters, and time offset, with analytical Jacobians provided. Various
experiments are carried out to evaluate the calibration performance of
EF-Calib, including calibration experiments for intrinsic parameters, extrinsic
parameters, and time offset. Experimental results show that EF-Calib achieves
the most accurate intrinsic parameters compared to current SOTA, the close
accuracy of the extrinsic parameters compared to the frame-based results, and
accurate time offset estimation. EF-Calib provides a convenient and accurate
toolbox for calibrating the system that fuses events and frames. The code of
this paper will also be open-sourced at: https://github.com/wsakobe/EF-Calib.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Robotics and Automation Letters</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MiniDrive: More Efficient Vision-Language Models with Multi-Level 2D
  Features as Text Tokens for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07267v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07267v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enming Zhang, Xingyuan Dai, Yisheng Lv, Qinghai Miao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models (VLMs) serve as general-purpose end-to-end models in
autonomous driving, performing subtasks such as prediction, planning, and
perception through question-and-answer interactions. However, most existing
methods rely on computationally expensive visual encoders and large language
models (LLMs), making them difficult to deploy in real-world scenarios and
real-time applications. Meanwhile, most existing VLMs lack the ability to
process multiple images, making it difficult to adapt to multi-camera
perception in autonomous driving. To address these issues, we propose a novel
framework called MiniDrive, which incorporates our proposed Feature Engineering
Mixture of Experts (FE-MoE) module and Dynamic Instruction Adapter
(DI-Adapter). The FE-MoE effectively maps 2D features into visual token
embeddings before being input into the language model. The DI-Adapter enables
the visual token embeddings to dynamically change with the instruction text
embeddings, resolving the issue of static visual token embeddings for the same
image in previous approaches. Compared to previous works, MiniDrive achieves
state-of-the-art performance in terms of parameter size, floating point
operations, and response efficiency, with the smallest version containing only
83M parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OAPT: Offset-Aware Partition <span class="highlight-title">Transformer</span> for Double JPEG Artifacts
  Removal 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11480v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11480v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiao Mo, Yukang Ding, Jinhua Hao, Qiang Zhu, Ming Sun, Chao Zhou, Feiyu Chen, Shuyuan Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning-based methods have shown remarkable performance in single JPEG
artifacts removal task. However, existing methods tend to degrade on double
JPEG images, which are prevalent in real-world scenarios. To address this
issue, we propose Offset-Aware Partition Transformer for double JPEG artifacts
removal, termed as OAPT. We conduct an analysis of double JPEG compression that
results in up to four patterns within each 8x8 block and design our model to
cluster the similar patterns to remedy the difficulty of restoration. Our OAPT
consists of two components: compression offset predictor and image
reconstructor. Specifically, the predictor estimates pixel offsets between the
first and second compression, which are then utilized to divide different
patterns. The reconstructor is mainly based on several Hybrid Partition
Attention Blocks (HPAB), combining vanilla window-based self-attention and
sparse attention for clustered pattern features. Extensive experiments
demonstrate that OAPT outperforms the state-of-the-art method by more than
0.16dB in double JPEG image restoration task. Moreover, without increasing any
computation cost, the pattern clustering module in HPAB can serve as a plugin
to enhance other transformer-based image restoration methods. The code will be
available at https://github.com/QMoQ/OAPT.git .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 9 figures. Codes and models are available at
  https://github.com/QMoQ/OAPT.git</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ArtVLM: Attribute Recognition Through Vision-Based Prefix Language
  Modeling <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.04102v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.04102v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Yicheng Zhu, Keren Ye, Junjie Ke, Jiahui Yu, Leonidas Guibas, Peyman Milanfar, Feng Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recognizing and disentangling visual attributes from objects is a foundation
to many computer vision applications. While large vision language
representations like CLIP had largely resolved the task of zero-shot object
recognition, zero-shot visual attribute recognition remains a challenge because
CLIP's contrastively-learned vision-language representation cannot effectively
capture object-attribute dependencies. In this paper, we target this weakness
and propose a sentence generation-based retrieval formulation for attribute
recognition that is novel in 1) explicitly modeling a to-be-measured and
retrieved object-attribute relation as a conditional probability graph, which
converts the recognition problem into a dependency-sensitive language-modeling
problem, and 2) applying a large pretrained Vision-Language Model (VLM) on this
reformulation and naturally distilling its knowledge of image-object-attribute
relations to use towards attribute recognition. Specifically, for each
attribute to be recognized on an image, we measure the visual-conditioned
probability of generating a short sentence encoding the attribute's relation to
objects on the image. Unlike contrastive retrieval, which measures likelihood
by globally aligning elements of the sentence to the image, generative
retrieval is sensitive to the order and dependency of objects and attributes in
the sentence. We demonstrate through experiments that generative retrieval
consistently outperforms contrastive retrieval on two visual reasoning
datasets, Visual Attribute in the Wild (VAW), and our newly-proposed Visual
Genome Attribute Ranking (VGARank).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Precision Aquaculture: An Integrated Computer Vision and IoT Approach
  for Optimized Tilapia Feeding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08695v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08695v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rania Hossam, Ahmed Heakl, Walid Gomaa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional fish farming practices often lead to inefficient feeding,
resulting in environmental issues and reduced productivity. We developed an
innovative system combining computer vision and IoT technologies for precise
Tilapia feeding. Our solution uses real-time IoT sensors to monitor water
quality parameters and computer vision algorithms to analyze fish size and
count, determining optimal feed amounts. A mobile app enables remote monitoring
and control. We utilized YOLOv8 for keypoint detection to measure Tilapia
weight from length, achieving \textbf{94\%} precision on 3,500 annotated
images. Pixel-based measurements were converted to centimeters using depth
estimation for accurate feeding calculations. Our method, with data collection
mirroring inference conditions, significantly improved results. Preliminary
estimates suggest this approach could increase production up to 58 times
compared to traditional farms. Our models, code, and dataset are
open-source~\footnote{The code, dataset, and models are available upon
reasonable request.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, 3 tables, 21th International Conference on
  Informatics in Control, Automation, and Robotics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Object-Aware Query Perturbation for Cross-Modal Image-Text Retrieval <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12346v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12346v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naoya Sogi, Takashi Shibata, Makoto Terao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The pre-trained vision and language (V\&L) models have substantially improved
the performance of cross-modal image-text retrieval. In general, however, V\&L
models have limited retrieval performance for small objects because of the
rough alignment between words and the small objects in the image. In contrast,
it is known that human cognition is object-centric, and we pay more attention
to important objects, even if they are small. To bridge this gap between the
human cognition and the V\&L model's capability, we propose a cross-modal
image-text retrieval framework based on ``object-aware query perturbation.''
The proposed method generates a key feature subspace of the detected objects
and perturbs the corresponding queries using this subspace to improve the
object awareness in the image. In our proposed method, object-aware cross-modal
image-text retrieval is possible while keeping the rich expressive power and
retrieval performance of existing V\&L models without additional fine-tuning.
Comprehensive experiments on four public datasets show that our method
outperforms conventional algorithms. Our code is publicly available at
\url{https://github.com/NEC-N-SOGI/query-perturbation}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024. Code: https://github.com/NEC-N-SOGI/query-perturbation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving the Stability and Efficiency of Diffusion Models for Content
  Consistent Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00877v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00877v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingchen Sun, Rongyuan Wu, Jie Liang, Zhengqiang Zhang, Hongwei Yong, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The generative priors of pre-trained latent diffusion models (DMs) have
demonstrated great potential to enhance the visual quality of image
super-resolution (SR) results. However, the noise sampling process in DMs
introduces randomness in the SR outputs, and the generated contents can differ
a lot with different noise samples. The multi-step diffusion process can be
accelerated by distilling methods, but the generative capacity is difficult to
control. To address these issues, we analyze the respective advantages of DMs
and generative adversarial networks (GANs) and propose to partition the
generative SR process into two stages, where the DM is employed for
reconstructing image structures and the GAN is employed for improving
fine-grained details. Specifically, we propose a non-uniform timestep sampling
strategy in the first stage. A single timestep sampling is first applied to
extract the coarse information from the input image, then a few reverse steps
are used to reconstruct the main structures. In the second stage, we finetune
the decoder of the pre-trained variational auto-encoder by adversarial GAN
training for deterministic detail enhancement. Once trained, our proposed
method, namely content consistent super-resolution (CCSR),allows flexible use
of different diffusion steps in the inference stage without re-training.
Extensive experiments show that with 2 or even 1 diffusion step, CCSR can
significantly improve the content consistency of SR outputs while keeping high
perceptual quality. Codes and models can be found at
\href{https://github.com/csslc/CCSR}{https://github.com/csslc/CCSR}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distribution Backtracking Builds A Faster Convergence Trajectory for
  Diffusion Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.15991v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.15991v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengyuan Zhang, Ling Yang, Zejian Li, An Zhao, Chenye Meng, Changyuan Yang, Guang Yang, Zhiyuan Yang, Lingyun Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accelerating the sampling speed of diffusion models remains a significant
challenge. Recent score distillation methods distill a heavy teacher model into
a student generator to achieve one-step generation, which is optimized by
calculating the difference between the two score functions on the samples
generated by the student model. However, there is a score mismatch issue in the
early stage of the distillation process, because existing methods mainly focus
on using the endpoint of pre-trained diffusion models as teacher models,
overlooking the importance of the convergence trajectory between the student
generator and the teacher model. To address this issue, we extend the score
distillation process by introducing the entire convergence trajectory of
teacher models and propose Distribution Backtracking Distillation (DisBack).
DisBask is composed of two stages: Degradation Recording and Distribution
Backtracking. Degradation Recording is designed to obtain the convergence
trajectory of the teacher model, which records the degradation path from the
trained teacher model to the untrained initial student generator. The
degradation path implicitly represents the teacher model's intermediate
distributions, and its reverse can be viewed as the convergence trajectory from
the student generator to the teacher model. Then Distribution Backtracking
trains a student generator to backtrack the intermediate distributions along
the path to approximate the convergence trajectory of teacher models. Extensive
experiments show that DisBack achieves faster and better convergence than the
existing distillation method and accomplishes comparable generation
performance, with FID score of 1.38 on ImageNet 64x64 dataset. Notably, DisBack
is easy to implement and can be generalized to existing distillation methods to
boost performance. Our code is publicly available on
https://github.com/SYZhang0805/DisBack.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/SYZhang0805/DisBack</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Regional quality estimation for echocardiography using deep learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00591v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00591v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gilles Van De Vyver, Svein-Erik Måsøy, Håvard Dalen, Bjørnar Leangen Grenne, Espen Holte, Sindre Hellum Olaisen, John Nyberg, Andreas Østvik, Lasse Løvstakken, Erik Smistad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic estimation of cardiac ultrasound image quality can be beneficial
for guiding operators and ensuring the accuracy of clinical measurements.
Previous work often fails to distinguish the view correctness of the
echocardiogram from the image quality. Additionally, previous studies only
provide a global image quality value, which limits their practical utility. In
this work, we developed and compared three methods to estimate image quality:
1) classic pixel-based metrics like the generalized contrast-to-noise ratio
(gCNR) on myocardial segments as region of interest and left ventricle lumen as
background, obtained using a U-Net segmentation 2) local image coherence
derived from a U-Net model that predicts coherence from B-Mode images 3) a deep
convolutional network that predicts the quality of each region directly in an
end-to-end fashion. We evaluate each method against manual regional image
quality annotations by three experienced cardiologists. The results indicate
poor performance of the gCNR metric, with Spearman correlation to the
annotations of rho = 0.24. The end-to-end learning model obtains the best
result, rho = 0.69, comparable to the inter-observer correlation, rho = 0.63.
Finally, the coherence-based method, with rho = 0.58, outperformed the
classical metrics and is more generic than the end-to-end approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedRepOpt: Gradient Re-parameterized Optimizers in Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15898v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15898v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kin Wai Lau, Yasar Abbas Ur Rehman, Pedro Porto Buarque de Gusmão, Lai-Man Po, Lan Ma, Yuyang Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) has emerged as a privacy-preserving method for
training machine learning models in a distributed manner on edge devices.
However, on-device models face inherent computational power and memory
limitations, potentially resulting in constrained gradient updates. As the
model's size increases, the frequency of gradient updates on edge devices
decreases, ultimately leading to suboptimal training outcomes during any
particular FL round. This limits the feasibility of deploying advanced and
large-scale models on edge devices, hindering the potential for performance
enhancements. To address this issue, we propose FedRepOpt, a gradient
re-parameterized optimizer for FL. The gradient re-parameterized method allows
training a simple local model with a similar performance as a complex model by
modifying the optimizer's gradients according to a set of model-specific
hyperparameters obtained from the complex models. In this work, we focus on
VGG-style and Ghost-style models in the FL environment. Extensive experiments
demonstrate that models using FedRepOpt obtain a significant boost in
performance of 16.7% and 11.4% compared to the RepGhost-style and RepVGG-style
networks, while also demonstrating a faster convergence time of 11.7% and 57.4%
compared to their complex structure.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training-free Zero-shot Composed Image Retrieval via Weighted Modality
  Fusion and Similarity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04918v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04918v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ren-Di Wu, Yu-Yen Lin, Huei-Fang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Composed image retrieval (CIR), which formulates the query as a combination
of a reference image and modified text, has emerged as a new form of image
search due to its enhanced ability to capture users' intentions. However,
training a CIR model in a supervised manner typically requires labor-intensive
collection of (reference image, text modifier, target image) triplets. While
existing zero-shot CIR (ZS-CIR) methods eliminate the need for training on
specific downstream datasets, they still require additional pretraining on
large-scale image datasets. In this paper, we introduce a training-free
approach for ZS-CIR. Our approach, Weighted Modality fusion and similarity for
CIR (WeiMoCIR), operates under the assumption that image and text modalities
can be effectively combined using a simple weighted average. This allows the
query representation to be constructed directly from the reference image and
text modifier. To further enhance retrieval performance, we employ multimodal
large language models (MLLMs) to generate image captions for the database
images and incorporate these textual captions into the similarity computation
by combining them with image information using a weighted average. Our approach
is simple, easy to implement, and its effectiveness is validated through
experiments on the FashionIQ and CIRR datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ JourneyBench: A Challenging One-Stop Vision-Language Understanding
  Benchmark of Generated Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12953v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12953v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhecan Wang, Junzhang Liu, Chia-Wei Tang, Hani Alomari, Anushka Sivakumar, Rui Sun, Wenhao Li, Md. Atabuzzaman, Hammad Ayyubi, Haoxuan You, Alvi Ishmam, Kai-Wei Chang, Shih-Fu Chang, Chris Thomas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing vision-language understanding benchmarks largely consist of images
of objects in their usual contexts. As a consequence, recent multimodal large
language models can perform well with only a shallow visual understanding by
relying on background language biases. Thus, strong performance on these
benchmarks does not necessarily correlate with strong visual understanding. In
this paper, we release JourneyBench, a comprehensive human-annotated benchmark
of generated images designed to assess the model's fine-grained multimodal
reasoning abilities across five tasks: complementary multimodal chain of
thought, multi-image VQA, imaginary image captioning, VQA with hallucination
triggers, and fine-grained retrieval with sample-specific distractors. Unlike
existing benchmarks, JourneyBench explicitly requires fine-grained multimodal
reasoning in unusual imaginary scenarios where language bias and holistic image
gist are insufficient. We benchmark state-of-the-art models on JourneyBench and
analyze performance along a number of fine-grained dimensions. Results across
all five tasks show that JourneyBench is exceptionally challenging for even the
best models, indicating that models' visual reasoning abilities are not as
strong as they first appear. We discuss the implications of our findings and
propose avenues for further research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering
  of 3D Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13827v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13827v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joongho Jo, Hyeongwon Kim, Jongsun Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian splatting (3D-GS) is a new rendering approach that outperforms
the neural radiance field (NeRF) in terms of both speed and image quality.
3D-GS represents 3D scenes by utilizing millions of 3D Gaussians and projects
these Gaussians onto the 2D image plane for rendering. However, during the
rendering process, a substantial number of unnecessary 3D Gaussians exist for
the current view direction, resulting in significant computation costs
associated with their identification. In this paper, we propose a computational
reduction technique that quickly identifies unnecessary 3D Gaussians in
real-time for rendering the current view without compromising image quality.
This is accomplished through the offline clustering of 3D Gaussians that are
close in distance, followed by the projection of these clusters onto a 2D image
plane during runtime. Additionally, we analyze the bottleneck associated with
the proposed technique when executed on GPUs and propose an efficient hardware
architecture that seamlessly supports the proposed scheme. For the Mip-NeRF360
dataset, the proposed technique excludes 63% of 3D Gaussians on average before
the 2D image projection, which reduces the overall rendering computation by
almost 38.3% without sacrificing peak-signal-to-noise-ratio (PSNR). The
proposed accelerator also achieves a speedup of 10.7x compared to a GPU.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our claim that Step 1 of 3D Gaussian splatting accounts for ~50% of
  rendering (Fig. 2) was incorrect. Rerunning simulations showed it's only
  ~20%. Consequently, our method's performance decreased by ~40% from initial
  reports. We're exploring new directions but have no concrete plans yet. To
  avoid reader confusion, we're withdrawing the paper and will resubmit once
  revised</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Clinician Performance in Classification of EEG Patterns on the
  Ictal-Interictal-Injury Continuum using Interpretable Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.05207v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.05207v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alina Jade Barnett, Zhicheng Guo, Jin Jing, Wendong Ge, Peter W. Kaplan, Wan Yee Kong, Ioannis Karakis, Aline Herlopian, Lakshman Arcot Jayagopal, Olga Taraschenko, Olga Selioutski, Gamaleldin Osman, Daniel Goldenholz, Cynthia Rudin, M. Brandon Westover
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In intensive care units (ICUs), critically ill patients are monitored with
electroencephalograms (EEGs) to prevent serious brain injury. The number of
patients who can be monitored is constrained by the availability of trained
physicians to read EEGs, and EEG interpretation can be subjective and prone to
inter-observer variability. Automated deep learning systems for EEG could
reduce human bias and accelerate the diagnostic process. However, black box
deep learning models are untrustworthy, difficult to troubleshoot, and lack
accountability in real-world applications, leading to a lack of trust and
adoption by clinicians. To address these challenges, we propose a novel
interpretable deep learning model that not only predicts the presence of
harmful brainwave patterns but also provides high-quality case-based
explanations of its decisions. Our model performs better than the corresponding
black box model, despite being constrained to be interpretable. The learned 2D
embedded space provides the first global overview of the structure of
ictal-interictal-injury continuum brainwave patterns. The ability to understand
how our model arrived at its decisions will not only help clinicians to
diagnose and treat harmful brain activities more accurately but also increase
their trust and adoption of machine learning models in clinical practice; this
could be an integral component of the ICU neurologists' standard workflow.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages including appendices, 9 figures, published at NEJM AI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detecting Adversarial Data via Perturbation Forgery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16226v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16226v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Wang, Chen Li, Yuchen Luo, Hefei Ling, Ping Li, Jiazhong Chen, Shijuan Huang, Ning Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a defense strategy against adversarial attacks, adversarial detection aims
to identify and filter out adversarial data from the data flow based on
discrepancies in distribution and noise patterns between natural and
adversarial data. Although previous detection methods achieve high performance
in detecting gradient-based adversarial attacks, new attacks based on
generative models with imbalanced and anisotropic noise patterns evade
detection. Even worse, existing techniques either necessitate access to attack
data before deploying a defense or incur a significant time cost for inference,
rendering them impractical for defending against newly emerging attacks that
are unseen by defenders. In this paper, we explore the proximity relationship
between adversarial noise distributions and demonstrate the existence of an
open covering for them. By learning to distinguish this open covering from the
distribution of natural data, we can develop a detector with strong
generalization capabilities against all types of adversarial attacks. Based on
this insight, we heuristically propose Perturbation Forgery, which includes
noise distribution perturbation, sparse mask generation, and pseudo-adversarial
data production, to train an adversarial detector capable of detecting unseen
gradient-based, generative-model-based, and physical adversarial attacks, while
remaining agnostic to any specific models. Comprehensive experiments conducted
on multiple general and facial datasets, with a wide spectrum of attacks,
validate the strong generalization of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continual Adversarial Defense 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.09481v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.09481v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Wang, Yaoyao Liu, Hefei Ling, Yingwei Li, Qihao Liu, Ping Li, Jiazhong Chen, Alan Yuille, Ning Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In response to the rapidly evolving nature of adversarial attacks against
visual classifiers on a monthly basis, numerous defenses have been proposed to
generalize against as many known attacks as possible. However, designing a
defense method that generalizes to all types of attacks is not realistic
because the environment in which defense systems operate is dynamic and
comprises various unique attacks that emerge as time goes on. A well-matched
approach to the dynamic environment lies in a defense system that continuously
collects adversarial data online to quickly improve itself. Therefore, we put
forward a practical defense deployment against a challenging threat model and
propose, for the first time, the Continual Adversarial Defense (CAD) framework
that adapts to attack sequences under four principles: (1) continual adaptation
to new attacks without catastrophic forgetting, (2) few-shot adaptation, (3)
memory-efficient adaptation, and (4) high accuracy on both clean and
adversarial data. We explore and integrate cutting-edge continual learning,
few-shot learning, and ensemble learning techniques to qualify the principles.
Extensive experiments validate the effectiveness of our approach against
multiple stages of modern adversarial attacks and demonstrate significant
improvements over numerous baseline methods. In particular, CAD is capable of
quickly adapting with minimal budget and a low cost of defense failure while
maintaining good performance against previous attacks. Our research sheds light
on a brand-new paradigm for continual defense adaptation against dynamic and
evolving attacks.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">18</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spacewalker: Traversing Representation Spaces for Fast Interactive
  Exploration and Annotation of Unstructured Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16793v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16793v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Heine, Fabian Hörst, Jana Fragemann, Gijs Luijten, Miriam Balzer, Jan Egger, Fin Bahnsen, M. Saquib Sarfraz, Jens Kleesiek, Constantin Seibold
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unstructured data in industries such as healthcare, finance, and
manufacturing presents significant challenges for efficient analysis and
decision making. Detecting patterns within this data and understanding their
impact is critical but complex without the right tools. Traditionally, these
tasks relied on the expertise of data analysts or labor-intensive manual
reviews. In response, we introduce Spacewalker, an interactive tool designed to
explore and annotate data across multiple modalities. Spacewalker allows users
to extract data representations and visualize them in low-dimensional spaces,
enabling the detection of semantic similarities. Through extensive user
studies, we assess Spacewalker's effectiveness in data annotation and integrity
verification. Results show that the tool's ability to traverse latent spaces
and perform multi-modal queries significantly enhances the user's capacity to
quickly identify relevant data. Moreover, Spacewalker allows for annotation
speed-ups far superior to conventional methods, making it a promising tool for
efficiently navigating unstructured data and improving decision making
processes. The code of this work is open-source and can be found at:
https://github.com/code-lukas/Spacewalker
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Automatic Keyphrase Labelling with Text-to-Text Transfer
  <span class="highlight-title">Transformer</span> (T5) Architecture: A Framework for Keyphrase Generation and
  Filtering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16760v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16760v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jorge Gabín, M. Eduardo Ares, Javier Parapar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic keyphrase labelling stands for the ability of models to retrieve
words or short phrases that adequately describe documents' content. Previous
work has put much effort into exploring extractive techniques to address this
task; however, these methods cannot produce keyphrases not found in the text.
Given this limitation, keyphrase generation approaches have arisen lately. This
paper presents a keyphrase generation model based on the Text-to-Text Transfer
Transformer (T5) architecture. Having a document's title and abstract as input,
we learn a T5 model to generate keyphrases which adequately define its content.
We name this model docT5keywords. We not only perform the classic inference
approach, where the output sequence is directly selected as the predicted
values, but we also report results from a majority voting approach. In this
approach, multiple sequences are generated, and the keyphrases are ranked based
on their frequency of occurrence across these sequences. Along with this model,
we present a novel keyphrase filtering technique based on the T5 architecture.
We train a T5 model to learn whether a given keyphrase is relevant to a
document. We devise two evaluation methodologies to prove our model's
capability to filter inadequate keyphrases. First, we perform a binary
evaluation where our model has to predict if a keyphrase is relevant for a
given document. Second, we filter the predicted keyphrases by several AKG
models and check if the evaluation scores are improved. Experimental results
demonstrate that our keyphrase generation model significantly outperforms all
the baselines, with gains exceeding 100\% in some cases. The proposed filtering
technique also achieves near-perfect accuracy in eliminating false positives
across all datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Prompt</span>ing-Based Representation Learning Method for Recommendation with
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16674v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16674v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyi Chen, Toyotaro Suzumura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Recommender Systems (RS) have witnessed a transformative
shift with the advent of Large Language Models (LLMs) in the field of Natural
Language Processing (NLP). Models such as GPT-3.5/4, Llama, have demonstrated
unprecedented capabilities in understanding and generating human-like text. The
extensive information pre-trained by these LLMs allows for the potential to
capture a more profound semantic representation from different contextual
information of users and items.
  While the great potential lies behind the thriving of LLMs, the challenge of
leveraging user-item preferences from contextual information and its alignment
with the improvement of Recommender Systems needs to be addressed. Believing
that a better understanding of the user or item itself can be the key factor in
improving recommendation performance, we conduct research on generating
informative profiles using state-of-the-art LLMs.
  To boost the linguistic abilities of LLMs in Recommender Systems, we
introduce the Prompting-Based Representation Learning Method for Recommendation
(P4R). In our P4R framework, we utilize the LLM prompting strategy to create
personalized item profiles. These profiles are then transformed into semantic
representation spaces using a pre-trained BERT model for text embedding.
Furthermore, we incorporate a Graph Convolution Network (GCN) for collaborative
filtering representation. The P4R framework aligns these two embedding spaces
in order to address the general recommendation tasks. In our evaluation, we
compare P4R with state-of-the-art Recommender models and assess the quality of
prompt-based profile generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Risks: The 1st International Workshop on Risks, Opportunities, and
  Evaluation of Generative Models in Recommendation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System
  Inferences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16633v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16633v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pingyi Huo, Anusha Devulapally, Hasan Al Maruf, Minseo Park, Krishnakumar Nair, Meena Arunachalam, Gulsum Gudukbay Akbulut, Mahmut Taylan Kandemir, Vijaykrishnan Narayanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Learning Recommendation Models (DLRMs) have become increasingly popular
and prevalent in today's datacenters, consuming most of the AI inference
cycles. The performance of DLRMs is heavily influenced by available bandwidth
due to their large vector sizes in embedding tables and concurrent accesses. To
achieve substantial improvements over existing solutions, novel approaches
towards DLRM optimization are needed, especially, in the context of emerging
interconnect technologies like CXL. This study delves into exploring
CXL-enabled systems, implementing a process-in-fabric-switch (PIFS) solution to
accelerate DLRMs while optimizing their memory and bandwidth scalability. We
present an in-depth characterization of industry-scale DLRM workloads running
on CXL-ready systems, identifying the predominant bottlenecks in existing CXL
systems. We, therefore, propose PIFS-Rec, a PIFS-based scheme that implements
near-data processing through downstream ports of the fabric switch. PIFS-Rec
achieves a latency that is 3.89x lower than Pond, an industry-standard
CXL-based system, and also outperforms BEACON, a state-of-the-art scheme, by
2.03x.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Train Once, Deploy Anywhere: Matryoshka Representation Learning for
  Multimodal Recommendation <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16627v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16627v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yueqi Wang, Zhenrui Yue, Huimin Zeng, Dong Wang, Julian McAuley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent advancements in language and vision modeling, integrating rich
multimodal knowledge into recommender systems continues to pose significant
challenges. This is primarily due to the need for efficient recommendation,
which requires adaptive and interactive responses. In this study, we focus on
sequential recommendation and introduce a lightweight framework called
full-scale Matryoshka representation learning for multimodal recommendation
(fMRLRec). Our fMRLRec captures item features at different granularities,
learning informative representations for efficient recommendation across
multiple dimensions. To integrate item features from diverse modalities,
fMRLRec employs a simple mapping to project multimodal item features into an
aligned feature space. Additionally, we design an efficient linear
transformation that embeds smaller features into larger ones, substantially
reducing memory requirements for large-scale training on recommendation data.
Combined with improved state space modeling techniques, fMRLRec scales to
different dimensions and only requires one-time training to produce multiple
models tailored to various granularities. We demonstrate the effectiveness and
efficiency of fMRLRec on multiple benchmark datasets, which consistently
achieves superior performance over state-of-the-art baseline methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating and Enhancing Large Language Models for Novelty Assessment in
  Scholarly Publications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16605v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16605v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ethan Lin, Zhiyuan Peng, Yi Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have evaluated the creativity/novelty of large language models
(LLMs) primarily from a semantic perspective, using benchmarks from cognitive
science. However, accessing the novelty in scholarly publications is a largely
unexplored area in evaluating LLMs. In this paper, we introduce a scholarly
novelty benchmark (SchNovel) to evaluate LLMs' ability to assess novelty in
scholarly papers. SchNovel consists of 15000 pairs of papers across six fields
sampled from the arXiv dataset with publication dates spanning 2 to 10 years
apart. In each pair, the more recently published paper is assumed to be more
novel. Additionally, we propose RAG-Novelty, which simulates the review process
taken by human reviewers by leveraging the retrieval of similar papers to
assess novelty. Extensive experiments provide insights into the capabilities of
different LLMs to assess novelty and demonstrate that RAG-Novelty outperforms
recent baseline models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative <span class="highlight-title">Pre-train</span>ed Ranking Model with Over-parameterization at
  Web-Scale (Extended Abstract) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16594v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16594v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen Li, Haoyi Xiong, Linghe Kong, Jiang Bian, Shuaiqiang Wang, Guihai Chen, Dawei Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning to rank (LTR) is widely employed in web searches to prioritize
pertinent webpages from retrieved content based on input queries. However,
traditional LTR models encounter two principal obstacles that lead to
suboptimal performance: (1) the lack of well-annotated query-webpage pairs with
ranking scores covering a diverse range of search query popularities, which
hampers their ability to address queries across the popularity spectrum, and
(2) inadequately trained models that fail to induce generalized representations
for LTR, resulting in overfitting. To address these challenges, we propose a
\emph{\uline{G}enerative \uline{S}emi-\uline{S}upervised \uline{P}re-trained}
(GS2P) LTR model. We conduct extensive offline experiments on both a publicly
available dataset and a real-world dataset collected from a large-scale search
engine. Furthermore, we deploy GS2P in a large-scale web search engine with
realistic traffic, where we observe significant improvements in the real-world
application.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Pre-train</span>ed Graphformer-based Ranking at Web-scale Search (Extended
  Abstract) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16590v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16590v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen Li, Haoyi Xiong, Linghe Kong, Zeyi Sun, Hongyang Chen, Shuaiqiang Wang, Dawei Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Both Transformer and Graph Neural Networks (GNNs) have been employed in the
domain of learning to rank (LTR). However, these approaches adhere to two
distinct yet complementary problem formulations: ranking score regression based
on query-webpage pairs, and link prediction within query-webpage bipartite
graphs, respectively. While it is possible to pre-train GNNs or Transformers on
source datasets and subsequently fine-tune them on sparsely annotated LTR
datasets, the distributional shifts between the pair-based and bipartite graph
domains present significant challenges in integrating these heterogeneous
models into a unified LTR framework at web scale. To address this, we introduce
the novel MPGraf model, which leverages a modular and capsule-based
pre-training strategy, aiming to cohesively integrate the regression
capabilities of Transformers with the link prediction strengths of GNNs. We
conduct extensive offline and online experiments to rigorously evaluate the
performance of MPGraf.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FusionANNS: An Efficient CPU/GPU Cooperative Processing Architecture for
  Billion-scale Approximate Nearest Neighbor Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16576v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16576v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bing Tian, Haikun Liu, Yuhang Tang, Shihai Xiao, Zhuohui Duan, Xiaofei Liao, Xuecang Zhang, Junhua Zhu, Yu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Approximate nearest neighbor search (ANNS) has emerged as a crucial component
of database and AI infrastructure. Ever-increasing vector datasets pose
significant challenges in terms of performance, cost, and accuracy for ANNS
services. None of modern ANNS systems can address these issues simultaneously.
We present FusionANNS, a high-throughput, low-latency, cost-efficient, and
high-accuracy ANNS system for billion-scale datasets using SSDs and only one
entry-level GPU. The key idea of FusionANNS lies in CPU/GPU collaborative
filtering and re-ranking mechanisms, which significantly reduce I/O operations
across CPUs, GPU, and SSDs to break through the I/O performance bottleneck.
Specifically, we propose three novel designs: (1) multi-tiered indexing to
avoid data swapping between CPUs and GPU, (2) heuristic re-ranking to eliminate
unnecessary I/Os and computations while guaranteeing high accuracy, and (3)
redundant-aware I/O deduplication to further improve I/O efficiency. We
implement FusionANNS and compare it with the state-of-the-art SSD-based ANNS
system--SPANN and GPU-accelerated in-memory ANNS system--RUMMY. Experimental
results show that FusionANNS achieves 1) 9.4-13.1X higher query per second
(QPS) and 5.7-8.8X higher cost efficiency compared with SPANN; 2) and 2-4.9X
higher QPS and 2.3-6.8X higher cost efficiency compared with RUMMY, while
guaranteeing low latency and high accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 26 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unified Embedding Based Personalized Retrieval in Etsy Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04833v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04833v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rishikesh Jha, Siddharth Subramaniyam, Ethan Benjamin, Thrivikrama Taula
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embedding-based neural retrieval is a prevalent approach to address the
semantic gap problem which often arises in product search on tail queries. In
contrast, popular queries typically lack context and have a broad intent where
additional context from users historical interaction can be helpful. In this
paper, we share our novel approach to address both: the semantic gap problem
followed by an end to end trained model for personalized semantic retrieval. We
propose learning a unified embedding model incorporating graph, transformer and
term-based embeddings end to end and share our design choices for optimal
tradeoff between performance and efficiency. We share our learnings in feature
engineering, hard negative sampling strategy, and application of transformer
model, including a novel pre-training strategy and other tricks for improving
search relevance and deploying such a model at industry scale. Our personalized
retrieval model significantly improves the overall search experience, as
measured by a 5.58% increase in search purchase rate and a 2.63% increase in
site-wide conversion rate, aggregated across multiple A/B tests - on live
traffic.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at FMLDS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Trustworthy Reranking: A Simple yet Effective Abstention
  Mechanism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12997v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12997v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hippolyte Gisserot-Boukhlef, Manuel Faysse, Emmanuel Malherbe, Céline Hudelot, Pierre Colombo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Information Retrieval (NIR) has significantly improved upon
heuristic-based Information Retrieval (IR) systems. Yet, failures remain
frequent, the models used often being unable to retrieve documents relevant to
the user's query. We address this challenge by proposing a lightweight
abstention mechanism tailored for real-world constraints, with particular
emphasis placed on the reranking phase. We introduce a protocol for evaluating
abstention strategies in black-box scenarios (typically encountered when
relying on API services), demonstrating their efficacy, and propose a simple
yet effective data-driven mechanism. We provide open-source code for experiment
replication and abstention implementation, fostering wider adoption and
application in diverse contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Interaction-Based Relevance Modeling for Online e-Commerce Search <span class="chip">ECML-PKDD'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02135v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02135v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ben Chen, Huangyu Dai, Xiang Ma, Wen Jiang, Wei Ning
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic relevance calculation is crucial for e-commerce search engines, as
it ensures that the items selected closely align with customer intent.
Inadequate attention to this aspect can detrimentally affect user experience
and engagement. Traditional text-matching techniques are prevalent but often
fail to capture the nuances of search intent accurately, so neural networks now
have become a preferred solution to processing such complex text matching.
Existing methods predominantly employ representation-based architectures, which
strike a balance between high traffic capacity and low latency. However, they
exhibit significant shortcomings in generalization and robustness when compared
to interaction-based architectures. In this work, we introduce a robust
interaction-based modeling paradigm to address these shortcomings. It
encompasses 1) a dynamic length representation scheme for expedited inference,
2) a professional terms recognition method to identify subjects and core
attributes from complex sentence structures, and 3) a contrastive adversarial
training protocol to bolster the model's robustness and matching capabilities.
Extensive offline evaluations demonstrate the superior robustness and
effectiveness of our approach, and online A/B testing confirms its ability to
improve relevance in the same exposure position, resulting in more clicks and
conversions. To the best of our knowledge, this method is the first
interaction-based approach for large e-commerce search relevance calculation.
Notably, we have deployed it for the entire search traffic on alibaba.com, the
largest B2B e-commerce platform in the world.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECML-PKDD'24 as Outstanding Paper. 8 pages, 2 figures, 7
  tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Personality-Guided Preference Aggregator for Ephemeral Group
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.08851v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.08851v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangze Ye, Wen Wu, Liye Shi, Wenxin Hu, Xin Chen, Liang He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ephemeral group recommendation (EGR) aims to suggest items for a group of
users who come together for the first time. Existing work typically consider
individual preferences as the sole factor in aggregating group preferences.
However, they neglect to take into account the importance of the individual
inherent factors, such as personality, and thus fail to accurately simulate the
group decision-making process. Additionally, these methods often struggle due
to insufficient interactive records. To tackle these issues, a
Personality-Guided Preference Aggregator (PEGA) is proposed, which guides the
preference aggregation of group members based on their personalities, rather
than relying solely on their preferences. Specifically, implicit personalities
are first extracted from user reviews. Hyper-rectangles are then used to
aggregate individual personalities to obtain the "Group Personality", which
allows for the learning of personality distributions within the group.
Subsequently, a personality attention mechanism is employed to aggregate group
preferences, and a preference-based fine-tuning module is used to balance the
weights of personality and preferences. The role of personality in this
approach is twofold: (1) To estimate the importance of individual users in a
group and provide explainability; (2) To alleviate the data sparsity issue
encountered in ephemeral groups. Experimental results demonstrate that, on four
real-world datasets, the PEGA model significantly outperforms related baseline
models in terms of classification accuracy and interpretability. Moreover,
empirical evidence supports the idea that personality plays a pivotal role in
enhancing the performance of EGR tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DemoRank: Selecting Effective Demonstrations for Large Language Models
  in Ranking Task 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16332v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16332v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhan Liu, Yutao Zhu, Zhicheng Dou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, there has been increasing interest in applying large language
models (LLMs) as zero-shot passage rankers. However, few studies have explored
how to select appropriate in-context demonstrations for the passage ranking
task, which is the focus of this paper. Previous studies mainly use LLM's
feedback to train a retriever for demonstration selection. These studies apply
the LLM to score each demonstration independently, which ignores the
dependencies between demonstrations (especially important in ranking task),
leading to inferior performance of top-$k$ retrieved demonstrations. To
mitigate this issue, we introduce a demonstration reranker to rerank the
retrieved demonstrations so that top-$k$ ranked ones are more suitable for ICL.
However, generating training data for such reranker is quite challenging. On
the one hand, different from demonstration retriever, the training samples of
reranker need to incorporate demonstration dependencies. On the other hand,
obtaining the gold ranking from the retrieved demonstrations is an NP-hard
problem, which is hard to implement. To overcome these challenges, we propose a
method to approximate the optimal demonstration list iteratively and utilize
LLM to score demonstration lists of varying lengths. By doing so, the search
space is greatly reduced and demonstration dependencies are considered. Based
on these scored demonstration lists, we further design a list-pairwise training
approach which compares a pair of lists that only differ in the last
demonstration, to teach the reranker how to select the next demonstration given
a previous sequence. In this paper, we propose a demonstration selection
framework DemoRank for ranking task and conduct extensive experiments to prove
its strong ability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Study of Implicit Ranking Unfairness in Large Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07054v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07054v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Xu, Wenjie Wang, Yuxin Li, Liang Pang, Jun Xu, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Large Language Models (LLMs) have demonstrated a superior ability
to serve as ranking models. However, concerns have arisen as LLMs will exhibit
discriminatory ranking behaviors based on users' sensitive attributes (\eg
gender). Worse still, in this paper, we identify a subtler form of
discrimination in LLMs, termed \textit{implicit ranking unfairness}, where LLMs
exhibit discriminatory ranking patterns based solely on non-sensitive user
profiles, such as user names. Such implicit unfairness is more widespread but
less noticeable, threatening the ethical foundation. To comprehensively explore
such unfairness, our analysis will focus on three research aspects: (1) We
propose an evaluation method to investigate the severity of implicit ranking
unfairness. (2) We uncover the reasons for causing such unfairness. (3) To
mitigate such unfairness effectively, we utilize a pair-wise regression method
to conduct fair-aware data augmentation for LLM fine-tuning. The experiment
demonstrates that our method outperforms existing approaches in ranking
fairness, achieving this with only a small reduction in accuracy. Lastly, we
emphasize the need for the community to identify and mitigate the implicit
unfairness, aiming to avert the potential deterioration in the reinforced
human-LLMs ecosystem deterioration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in EMNLP 2024 findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards a Realistic Long-Term Benchmark for Open-Web Research Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14913v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14913v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Mühlbacher, Nikos I. Bosse, Lawrence Phillips
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present initial results of a forthcoming benchmark for evaluating LLM
agents on white-collar tasks of economic value. We evaluate agents on
real-world "messy" open-web research tasks of the type that are routine in
finance and consulting. In doing so, we lay the groundwork for an LLM agent
evaluation suite where good performance directly corresponds to a large
economic and societal impact. We built and tested several agent architectures
with o1-preview, GPT-4o, Claude-3.5 Sonnet, Llama 3.1 (405b), and GPT-4o-mini.
On average, LLM agents powered by Claude-3.5 Sonnet and o1-preview
substantially outperformed agents using GPT-4o, with agents based on Llama 3.1
(405b) and GPT-4o-mini lagging noticeably behind. Across LLMs, a ReAct
architecture with the ability to delegate subtasks to subagents performed best.
In addition to quantitative evaluations, we qualitatively assessed the
performance of the LLM agents by inspecting their traces and reflecting on
their observations. Our evaluation represents the first in-depth assessment of
agents' abilities to conduct challenging, economically valuable analyst-style
research on the real open web.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChatDiet: Empowering Personalized Nutrition-Oriented Food Recommender
  Chatbots through an LLM-Augmented Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00781v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00781v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongqi Yang, Elahe Khatibi, Nitish Nagesh, Mahyar Abbasian, Iman Azimi, Ramesh Jain, Amir M. Rahmani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The profound impact of food on health necessitates advanced
nutrition-oriented food recommendation services. Conventional methods often
lack the crucial elements of personalization, explainability, and
interactivity. While Large Language Models (LLMs) bring interpretability and
explainability, their standalone use falls short of achieving true
personalization. In this paper, we introduce ChatDiet, a novel LLM-powered
framework designed specifically for personalized nutrition-oriented food
recommendation chatbots. ChatDiet integrates personal and population models,
complemented by an orchestrator, to seamlessly retrieve and process pertinent
information. The personal model leverages causal discovery and inference
techniques to assess personalized nutritional effects for a specific user,
whereas the population model provides generalized information on food
nutritional content. The orchestrator retrieves, synergizes and delivers the
output of both models to the LLM, providing tailored food recommendations
designed to support targeted health outcomes. The result is a dynamic delivery
of personalized and explainable food recommendations, tailored to individual
user preferences. Our evaluation of ChatDiet includes a compelling case study,
where we establish a causal personal model to estimate individual nutrition
effects. Our assessments, including a food recommendation test showcasing a
92\% effectiveness rate, coupled with illustrative dialogue examples,
underscore ChatDiet's strengths in explainability, personalization, and
interactivity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published on Smart Health</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Object-Aware Query Perturbation for Cross-Modal Image-Text Retrieval <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12346v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12346v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naoya Sogi, Takashi Shibata, Makoto Terao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The pre-trained vision and language (V\&L) models have substantially improved
the performance of cross-modal image-text retrieval. In general, however, V\&L
models have limited retrieval performance for small objects because of the
rough alignment between words and the small objects in the image. In contrast,
it is known that human cognition is object-centric, and we pay more attention
to important objects, even if they are small. To bridge this gap between the
human cognition and the V\&L model's capability, we propose a cross-modal
image-text retrieval framework based on ``object-aware query perturbation.''
The proposed method generates a key feature subspace of the detected objects
and perturbs the corresponding queries using this subspace to improve the
object awareness in the image. In our proposed method, object-aware cross-modal
image-text retrieval is possible while keeping the rich expressive power and
retrieval performance of existing V\&L models without additional fine-tuning.
Comprehensive experiments on four public datasets show that our method
outperforms conventional algorithms. Our code is publicly available at
\url{https://github.com/NEC-N-SOGI/query-perturbation}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024. Code: https://github.com/NEC-N-SOGI/query-perturbation</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">144</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Molmo and PixMo: Open Weights and Open Data for State-of-the-Art
  Multimodal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17146v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17146v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, Jiasen Lu, Taira Anderson, Erin Bransom, Kiana Ehsani, Huong Ngo, YenSung Chen, Ajay Patel, Mark Yatskar, Chris Callison-Burch, Andrew Head, Rose Hendrix, Favyen Bastani, Eli VanderBilt, Nathan Lambert, Yvonne Chou, Arnavi Chheda, Jenna Sparks, Sam Skjonsberg, Michael Schmitz, Aaron Sarnat, Byron Bischoff, Pete Walsh, Chris Newell, Piper Wolters, Tanmay Gupta, Kuo-Hao Zeng, Jon Borchardt, Dirk Groeneveld, Jen Dumas, Crystal Nam, Sophie Lebrecht, Caitlin Wittlif, Carissa Schoenick, Oscar Michel, Ranjay Krishna, Luca Weihs, Noah A. Smith, Hannaneh Hajishirzi, Ross Girshick, Ali Farhadi, Aniruddha Kembhavi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Today's most advanced multimodal models remain proprietary. The strongest
open-weight models rely heavily on synthetic data from proprietary VLMs to
achieve good performance, effectively distilling these closed models into open
ones. As a result, the community is still missing foundational knowledge about
how to build performant VLMs from scratch. We present Molmo, a new family of
VLMs that are state-of-the-art in their class of openness. Our key innovation
is a novel, highly detailed image caption dataset collected entirely from human
annotators using speech-based descriptions. To enable a wide array of user
interactions, we also introduce a diverse dataset mixture for fine-tuning that
includes in-the-wild Q&A and innovative 2D pointing data. The success of our
approach relies on careful choices for the model architecture details, a
well-tuned training pipeline, and, most critically, the quality of our newly
collected datasets, all of which will be released. The best-in-class 72B model
within the Molmo family not only outperforms others in the class of open weight
and data models but also compares favorably against proprietary systems like
GPT-4o, Claude 3.5, and Gemini 1.5 on both academic benchmarks and human
evaluation.
  We will be releasing all of our model weights, captioning and fine-tuning
data, and source code in the near future. Select model weights, inference code,
and demo are available at https://molmo.allenai.org.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DreamWaltz-G: Expressive 3D Gaussian Avatars from Skeleton-Guided 2D
  Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17145v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17145v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yukun Huang, Jianan Wang, Ailing Zeng, Zheng-Jun Zha, Lei Zhang, Xihui Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging pretrained 2D diffusion models and score distillation sampling
(SDS), recent methods have shown promising results for text-to-3D avatar
generation. However, generating high-quality 3D avatars capable of expressive
animation remains challenging. In this work, we present DreamWaltz-G, a novel
learning framework for animatable 3D avatar generation from text. The core of
this framework lies in Skeleton-guided Score Distillation and Hybrid 3D
Gaussian Avatar representation. Specifically, the proposed skeleton-guided
score distillation integrates skeleton controls from 3D human templates into 2D
diffusion models, enhancing the consistency of SDS supervision in terms of view
and human pose. This facilitates the generation of high-quality avatars,
mitigating issues such as multiple faces, extra limbs, and blurring. The
proposed hybrid 3D Gaussian avatar representation builds on the efficient 3D
Gaussians, combining neural implicit fields and parameterized 3D meshes to
enable real-time rendering, stable SDS optimization, and expressive animation.
Extensive experiments demonstrate that DreamWaltz-G is highly effective in
generating and animating 3D avatars, outperforming existing methods in both
visual quality and animation expressiveness. Our framework further supports
diverse applications, including human video reenactment and multi-subject scene
composition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://yukun-huang.github.io/DreamWaltz-G/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differential Privacy Regularization: Protecting Training Data Through
  Loss Function Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17144v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17144v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francisco Aguilera-Martínez, Fernando Berzal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training machine learning models based on neural networks requires large
datasets, which may contain sensitive information. The models, however, should
not expose private information from these datasets. Differentially private SGD
[DP-SGD] requires the modification of the standard stochastic gradient descent
[SGD] algorithm for training new models. In this short paper, a novel
regularization strategy is proposed to achieve the same goal in a more
efficient manner.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FineZip : Pushing the Limits of Large Language Models for Practical
  Lossless Text Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17141v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17141v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fazal Mittu, Yihuan Bu, Akshat Gupta, Ashok Devireddy, Alp Eren Ozdarendeli, Anant Singh, Gopala Anumanchipalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While the language modeling objective has been shown to be deeply connected
with compression, it is surprising that modern LLMs are not employed in
practical text compression systems. In this paper, we provide an in-depth
analysis of neural network and transformer-based compression techniques to
answer this question. We compare traditional text compression systems with
neural network and LLM-based text compression methods. Although LLM-based
systems significantly outperform conventional compression methods, they are
highly impractical. Specifically, LLMZip, a recent text compression system
using Llama3-8B requires 9.5 days to compress just 10 MB of text, although with
huge improvements in compression ratios. To overcome this, we present FineZip -
a novel LLM-based text compression system that combines ideas of online
memorization and dynamic context to reduce the compression time immensely.
FineZip can compress the above corpus in approximately 4 hours compared to 9.5
days, a 54 times improvement over LLMZip and comparable performance. FineZip
outperforms traditional algorithmic compression methods with a large margin,
improving compression ratios by approximately 50\%. With this work, we take the
first step towards making lossless text compression with LLMs a reality. While
FineZip presents a significant step in that direction, LLMs are still not a
viable solution for large-scale text compression. We hope our work paves the
way for future research and innovation to solve this problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning with Dynamics: Autonomous Regulation of UAV Based Communication
  Networks with Dynamic UAV Crew 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17139v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17139v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ran Zhang, Bowei Li, Liyuan Zhang,  Jiang,  Xie, Miao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unmanned Aerial Vehicle (UAV) based communication networks (UCNs) are a key
component in future mobile networking. To handle the dynamic environments in
UCNs, reinforcement learning (RL) has been a promising solution attributed to
its strong capability of adaptive decision-making free of the environment
models. However, most existing RL-based research focus on control strategy
design assuming a fixed set of UAVs. Few works have investigated how UCNs
should be adaptively regulated when the serving UAVs change dynamically. This
article discusses RL-based strategy design for adaptive UCN regulation given a
dynamic UAV set, addressing both reactive strategies in general UCNs and
proactive strategies in solar-powered UCNs. An overview of the UCN and the RL
framework is first provided. Potential research directions with key challenges
and possible solutions are then elaborated. Some of our recent works are
presented as case studies to inspire innovative ways to handle dynamic UAV crew
with different RL algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 6 figures, magazine paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Landscape of Policy Optimization for Finite Horizon MDPs with General
  State and Action 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17138v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17138v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Chen, Yifan Hu, Minda Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Policy gradient methods are widely used in reinforcement learning. Yet, the
nonconvexity of policy optimization imposes significant challenges in
understanding the global convergence of policy gradient methods. For a class of
finite-horizon Markov Decision Processes (MDPs) with general state and action
spaces, we develop a framework that provides a set of easily verifiable
assumptions to ensure the Kurdyka-Lojasiewicz (KL) condition of the policy
optimization. Leveraging the KL condition, policy gradient methods converge to
the globally optimal policy with a non-asymptomatic rate despite nonconvexity.
Our results find applications in various control and operations models,
including entropy-regularized tabular MDPs, Linear Quadratic Regulator (LQR)
problems, stochastic inventory models, and stochastic cash balance problems,
for which we show an $\epsilon$-optimal policy can be obtained using a sample
size in $\tilde{\mathcal{O}}(\epsilon^{-1})$ and polynomial in terms of the
planning horizon by stochastic policy gradient methods. Our result establishes
the first sample complexity for multi-period inventory systems with
Markov-modulated demands and stochastic cash balance problems in the
literature.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PACE: marrying generalization in PArameter-efficient fine-tuning with
  Consistency rEgularization <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17137v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17137v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Ni, Shan Zhang, Piotr Koniusz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter-Efficient Fine-Tuning (PEFT) effectively adapts pre-trained vision
transformers to downstream tasks. However, the optimization for tasks
performance often comes at the cost of generalizability in fine-tuned models.
To address this issue, we theoretically connect smaller weight gradient norms
during training and larger datasets to the improved model generalization.
Motivated by this connection, we propose reducing gradient norms for enhanced
generalization and aligning fine-tuned model with the pre-trained counterpart
to retain knowledge from large-scale pre-training data. Yet, naive alignment
does not guarantee gradient reduction and can potentially cause gradient
explosion, complicating efforts to manage gradients. To address such issues, we
propose PACE, marrying generalization of PArameter-efficient fine-tuning with
Consistency rEgularization. We perturb features learned from the adapter with
the multiplicative noise and ensure the fine-tuned model remains consistent for
same sample under different perturbations. Theoretical analysis shows that PACE
not only implicitly regularizes gradients for enhanced generalization, but also
implicitly aligns the fine-tuned and pre-trained models to retain knowledge.
Experimental evidence supports our theories. PACE outperforms existing PEFT
methods in four visual adaptation tasks: VTAB-1k, FGVC, few-shot learning and
domain adaptation. Code will be available at
https://github.com/MaxwellYaoNi/PACE
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024 as a spotlight. This preliminary version
  will soon be extended with the experiments and analyses from the rebuttal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Blox-Net: Generative Design-for-Robot-Assembly Using VLM Supervision,
  Physics Simulation, and a Robot with Reset 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17126v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17126v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Goldberg, Kavish Kondap, Tianshuang Qiu, Zehan Ma, Letian Fu, Justin Kerr, Huang Huang, Kaiyuan Chen, Kuan Fang, Ken Goldberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative AI systems have shown impressive capabilities in creating text,
code, and images. Inspired by the rich history of research in industrial
''Design for Assembly'', we introduce a novel problem: Generative
Design-for-Robot-Assembly (GDfRA). The task is to generate an assembly based on
a natural language prompt (e.g., ''giraffe'') and an image of available
physical components, such as 3D-printed blocks. The output is an assembly, a
spatial arrangement of these components, and instructions for a robot to build
this assembly. The output must 1) resemble the requested object and 2) be
reliably assembled by a 6 DoF robot arm with a suction gripper. We then present
Blox-Net, a GDfRA system that combines generative vision language models with
well-established methods in computer vision, simulation, perturbation analysis,
motion planning, and physical robot experimentation to solve a class of GDfRA
problems with minimal human supervision. Blox-Net achieved a Top-1 accuracy of
63.5% in the ''recognizability'' of its designed assemblies (eg, resembling
giraffe as judged by a VLM). These designs, after automated perturbation
redesign, were reliably assembled by a robot, achieving near-perfect success
across 10 consecutive assembly iterations with human intervention only during
reset prior to assembly. Surprisingly, this entire design process from textual
word (''giraffe'') to reliable physical assembly is performed with zero human
intervention.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning and Machine Learning, Advancing Big Data Analytics and
  Management: Handy Appetizer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17120v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17120v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benji Peng, Xuanhe Pan, Yizhu Wen, Ziqian Bi, Keyu Chen, Ming Li, Ming Liu, Qian Niu, Junyu Liu, Jinlang Wang, Sen Zhang, Jiawei Xu, Pohsun Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This book explores the role of Artificial Intelligence (AI), Machine Learning
(ML), and Deep Learning (DL) in driving the progress of big data analytics and
management. The book focuses on simplifying the complex mathematical concepts
behind deep learning, offering intuitive visualizations and practical case
studies to help readers understand how neural networks and technologies like
Convolutional Neural Networks (CNNs) work. It introduces several classic models
and technologies such as Transformers, GPT, ResNet, BERT, and YOLO,
highlighting their applications in fields like natural language processing,
image recognition, and autonomous driving. The book also emphasizes the
importance of pre-trained models and how they can enhance model performance and
accuracy, with instructions on how to apply these models in various real-world
scenarios. Additionally, it provides an overview of key big data management
technologies like SQL and NoSQL databases, as well as distributed computing
frameworks such as Apache Hadoop and Spark, explaining their importance in
managing and processing vast amounts of data. Ultimately, the book underscores
the value of mastering deep learning and big data management skills as critical
tools for the future workforce, making it an essential resource for both
beginners and experienced professionals.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This book contains 93 pages and 60 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Programming Every Example: Lifting <span class="highlight-title">Pre-train</span>ing Data Quality like
  Experts at Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17115v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17115v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Zhou, Zengzhi Wang, Qian Liu, Junlong Li, Pengfei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language model pre-training has traditionally relied on human experts
to craft heuristics for improving the corpora quality, resulting in numerous
rules developed to date. However, these rules lack the flexibility to address
the unique characteristics of individual example effectively. Meanwhile,
applying tailored rules to every example is impractical for human experts. In
this paper, we demonstrate that even small language models, with as few as 0.3B
parameters, can exhibit substantial data refining capabilities comparable to
those of human experts. We introduce Programming Every Example (ProX), a novel
framework that treats data refinement as a programming task, enabling models to
refine corpora by generating and executing fine-grained operations, such as
string normalization, for each individual example at scale. Experimental
results show that models pre-trained on ProX-curated data outperform either
original data or data filtered by other selection methods by more than 2%
across various downstream benchmarks. Its effectiveness spans various model
sizes and pre-training corpora, including C4, RedPajama-V2, and FineWeb.
Furthermore, ProX exhibits significant potential in domain-specific continual
pre-training: without domain specific design, models trained on OpenWebMath
refined by ProX outperform human-crafted rule-based methods, improving average
accuracy by 7.6% over Mistral-7B, with 14.6% for Llama-2-7B and 20.3% for
CodeLlama-7B, all within 10B tokens to be comparable to models like Llemma-7B
trained on 200B tokens. Further analysis highlights that ProX significantly
saves training FLOPs, offering a promising path for efficient LLM
pre-training.We are open-sourcing ProX with >100B corpus, models, and sharing
all training and implementation details for reproducible research and future
innovation. Code: https://github.com/GAIR-NLP/ProX
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>45 pages, 13 figures, 34 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Characterizing stable regions in the residual stream of LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17113v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17113v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jett Janiak, Jacek Karwowski, Chatrik Singh Mangat, Giorgi Giglemiani, Nora Petrova, Stefan Heimersheim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We identify "stable regions" in the residual stream of Transformers, where
the model's output remains insensitive to small activation changes, but
exhibits high sensitivity at region boundaries. These regions emerge during
training and become more defined as training progresses or model size
increases. The regions appear to be much larger than previously studied
polytopes. Our analysis suggests that these stable regions align with semantic
distinctions, where similar prompts cluster within regions, and activations
from the same region lead to similar next token predictions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Non-asymptotic convergence analysis of the stochastic gradient
  Hamiltonian Monte Carlo algorithm with discontinuous stochastic gradient with
  applications to training of ReLU neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17107v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17107v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luxu Liang, Ariel Neufeld, Ying Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we provide a non-asymptotic analysis of the convergence of the
stochastic gradient Hamiltonian Monte Carlo (SGHMC) algorithm to a target
measure in Wasserstein-1 and Wasserstein-2 distance. Crucially, compared to the
existing literature on SGHMC, we allow its stochastic gradient to be
discontinuous. This allows us to provide explicit upper bounds, which can be
controlled to be arbitrarily small, for the expected excess risk of non-convex
stochastic optimization problems with discontinuous stochastic gradients,
including, among others, the training of neural networks with ReLU activation
function. To illustrate the applicability of our main results, we consider
numerical experiments on quantile estimation and on several optimization
problems involving ReLU neural networks relevant in finance and artificial
intelligence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accumulator-Aware Post-Training Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17092v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17092v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ian Colbert, Fabian Grob, Giuseppe Franco, Jinjie Zhang, Rayan Saab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Several recent studies have investigated low-precision accumulation,
reporting improvements in throughput, power, and area across various platforms.
However, the accompanying proposals have only considered the quantization-aware
training (QAT) paradigm, in which models are fine-tuned or trained from scratch
with quantization in the loop. As models continue to grow in size, QAT
techniques become increasingly more expensive, which has motivated the recent
surge in post-training quantization (PTQ) research. To the best of our
knowledge, ours marks the first formal study of accumulator-aware quantization
in the PTQ setting. To bridge this gap, we introduce AXE, a practical framework
of accumulator-aware extensions designed to endow overflow avoidance guarantees
to existing layer-wise PTQ algorithms. We theoretically motivate AXE and
demonstrate its flexibility by implementing it on top of two state-of-the-art
PTQ algorithms: GPFQ and OPTQ. We further generalize AXE to support multi-stage
accumulation for the first time, opening the door for full datapath
optimization and scaling to large language models (LLMs). We evaluate AXE
across image classification and language generation models, and observe
significant improvements in the trade-off between accumulator bit width and
model accuracy over baseline methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ctrl-GenAug: Controllable Generative Augmentation for Medical Sequence
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17091v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17091v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinrui Zhou, Yuhao Huang, Haoran Dou, Shijing Chen, Ao Chang, Jia Liu, Weiran Long, Jian Zheng, Erjiao Xu, Jie Ren, Ruobing Huang, Jun Cheng, Wufeng Xue, Dong Ni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the medical field, the limited availability of large-scale datasets and
labor-intensive annotation processes hinder the performance of deep models.
Diffusion-based generative augmentation approaches present a promising solution
to this issue, having been proven effective in advancing downstream medical
recognition tasks. Nevertheless, existing works lack sufficient semantic and
sequential steerability for challenging video/3D sequence generation, and
neglect quality control of noisy synthesized samples, resulting in unreliable
synthetic databases and severely limiting the performance of downstream tasks.
In this work, we present Ctrl-GenAug, a novel and general generative
augmentation framework that enables highly semantic- and sequential-customized
sequence synthesis and suppresses incorrectly synthesized samples, to aid
medical sequence classification. Specifically, we first design a multimodal
conditions-guided sequence generator for controllably synthesizing
diagnosis-promotive samples. A sequential augmentation module is integrated to
enhance the temporal/stereoscopic coherence of generated samples. Then, we
propose a noisy synthetic data filter to suppress unreliable cases at semantic
and sequential levels. Extensive experiments on 3 medical datasets, using 11
networks trained on 3 paradigms, comprehensively analyze the effectiveness and
generality of Ctrl-GenAug, particularly in underrepresented high-risk
populations and out-domain conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 7 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Locally Regularized Sparse Graph by Fast Proximal Gradient Descent <span class="chip">UAI2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17090v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17090v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongfang Sun, Yingzhen Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse graphs built by sparse representation has been demonstrated to be
effective in clustering high-dimensional data. Albeit the compelling empirical
performance, the vanilla sparse graph ignores the geometric information of the
data by performing sparse representation for each datum separately. In order to
obtain a sparse graph aligned with the local geometric structure of data, we
propose a novel Support Regularized Sparse Graph, abbreviated as SRSG, for data
clustering. SRSG encourages local smoothness on the neighborhoods of nearby
data points by a well-defined support regularization term. We propose a fast
proximal gradient descent method to solve the non-convex optimization problem
of SRSG with the convergence matching the Nesterov's optimal convergence rate
of first-order methods on smooth and convex objective function with Lipschitz
continuous gradient. Extensive experimental results on various real data sets
demonstrate the superiority of SRSG over other competing clustering methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by UAI2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SEN12-WATER: A New <span class="highlight-title">Dataset</span> for Hydrological Applications and its
  Benchmarking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17087v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17087v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luigi Russo, Francesco Mauro, Alessandro Sebastianelli, Paolo Gamba, Silvia Liberata Ullo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Climate change and increasing droughts pose significant challenges to water
resource management around the world. These problems lead to severe water
shortages that threaten ecosystems, agriculture, and human communities. To
advance the fight against these challenges, we present a new dataset,
SEN12-WATER, along with a benchmark using a novel end-to-end Deep Learning (DL)
framework for proactive drought-related analysis. The dataset, identified as a
spatiotemporal datacube, integrates SAR polarization, elevation, slope, and
multispectral optical bands. Our DL framework enables the analysis and
estimation of water losses over time in reservoirs of interest, revealing
significant insights into water dynamics for drought analysis by examining
temporal changes in physical quantities such as water volume. Our methodology
takes advantage of the multitemporal and multimodal characteristics of the
proposed dataset, enabling robust generalization and advancing understanding of
drought, contributing to climate change resilience and sustainable water
resource management. The proposed framework involves, among the several
components, speckle noise removal from SAR data, a water body segmentation
through a U-Net architecture, the time series analysis, and the predictive
capability of a Time-Distributed-Convolutional Neural Network (TD-CNN). Results
are validated through ground truth data acquired on-ground via dedicated
sensors and (tailored) metrics, such as Precision, Recall, Intersection over
Union, Mean Squared Error, Structural Similarity Index Measure and Peak
Signal-to-Noise Ratio.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Transactions on Geoscience and Remote Sensing.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Feature Interactions with <span class="highlight-title">Transformer</span>s: Improving User
  Spending Propensity Predictions in Gaming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17077v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17077v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ved Prakash, Kartavya Kothari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dream11 is a fantasy sports platform that allows users to create their own
virtual teams for real-life sports events. We host multiple sports and matches
for our 200M+ user base. In this RMG (real money gaming) setting, users pay an
entry amount to participate in various contest products that we provide to
users. In our current work, we discuss the problem of predicting the user's
propensity to spend in a gaming round, so it can be utilized for various
downstream applications. e.g. Upselling users by incentivizing them marginally
as per their spending propensity, or personalizing the product listing based on
the user's propensity to spend.
  We aim to model the spending propensity of each user based on past
transaction data. In this paper, we benchmark tree-based and deep-learning
models that show good results on structured data, and we propose a new
architecture change that is specifically designed to capture the rich
interactions among the input features. We show that our proposed architecture
outperforms the existing models on the task of predicting the user's propensity
to spend in a gaming round. Our new transformer model surpasses the
state-of-the-art FT-Transformer, improving MAE by 2.5\% and MSE by 21.8\%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Effect of Perceptual Metrics on Music Representation Learning for
  Genre Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17069v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17069v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tashi Namgyal, Alexander Hepburn, Raul Santos-Rodriguez, Valero Laparra, Jesus Malo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The subjective quality of natural signals can be approximated with objective
perceptual metrics. Designed to approximate the perceptual behaviour of human
observers, perceptual metrics often reflect structures found in natural signals
and neurological pathways. Models trained with perceptual metrics as loss
functions can capture perceptually meaningful features from the structures held
within these metrics. We demonstrate that using features extracted from
autoencoders trained with perceptual losses can improve performance on music
understanding tasks, i.e. genre classification, over using these metrics
directly as distances when learning a classifier. This result suggests improved
generalisation to novel signals when using perceptual metrics as loss functions
for representation learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2312.03455</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Domain Generalization Algorithms in Computational Pathology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17063v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17063v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neda Zamanitajeddin, Mostafa Jahanifar, Kesi Xu, Fouzia Siraj, Nasir Rajpoot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models have shown immense promise in computational pathology
(CPath) tasks, but their performance often suffers when applied to unseen data
due to domain shifts. Addressing this requires domain generalization (DG)
algorithms. However, a systematic evaluation of DG algorithms in the CPath
context is lacking. This study aims to benchmark the effectiveness of 30 DG
algorithms on 3 CPath tasks of varying difficulty through 7,560
cross-validation runs. We evaluate these algorithms using a unified and robust
platform, incorporating modality-specific techniques and recent advances like
pretrained foundation models. Our extensive cross-validation experiments
provide insights into the relative performance of various DG strategies. We
observe that self-supervised learning and stain augmentation consistently
outperform other methods, highlighting the potential of pretrained models and
data augmentation. Furthermore, we introduce a new pan-cancer tumor detection
dataset (HISTOPANTUM) as a benchmark for future research. This study offers
valuable guidance to researchers in selecting appropriate DG approaches for
CPath tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DRIM: Learning Disentangled Representations from Incomplete Multimodal
  Healthcare Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17055v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17055v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas Robinet, Ahmad Berjaoui, Ziad Kheil, Elizabeth Cohen-Jonathan Moyal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-life medical data is often multimodal and incomplete, fueling the
growing need for advanced deep learning models capable of integrating them
efficiently. The use of diverse modalities, including histopathology slides,
MRI, and genetic data, offers unprecedented opportunities to improve prognosis
prediction and to unveil new treatment pathways. Contrastive learning, widely
used for deriving representations from paired data in multimodal tasks, assumes
that different views contain the same task-relevant information and leverages
only shared information. This assumption becomes restrictive when handling
medical data since each modality also harbors specific knowledge relevant to
downstream tasks. We introduce DRIM, a new multimodal method for capturing
these shared and unique representations, despite data sparsity. More
specifically, given a set of modalities, we aim to encode a representation for
each one that can be divided into two components: one encapsulating
patient-related information common across modalities and the other,
encapsulating modality-specific details. This is achieved by increasing the
shared information among different patient modalities while minimizing the
overlap between shared and unique components within each modality. Our method
outperforms state-of-the-art algorithms on glioma patients survival prediction
tasks, while being robust to missing modalities. To promote reproducibility,
the code is made publicly available at https://github.com/Lucas-rbnt/DRIM
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predictive Covert Communication Against Multi-UAV Surveillance Using
  Graph Koopman Autoencoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17048v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17048v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sivaram Krishnan, Jihong Park, Gregory Sherman, Benjamin Campbell, Jinho Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low Probability of Detection (LPD) communication aims to obscure the presence
of radio frequency (RF) signals to evade surveillance. In the context of mobile
surveillance utilizing unmanned aerial vehicles (UAVs), achieving LPD
communication presents significant challenges due to the UAVs' rapid and
continuous movements, which are characterized by unknown nonlinear dynamics.
Therefore, accurately predicting future locations of UAVs is essential for
enabling real-time LPD communication. In this paper, we introduce a novel
framework termed predictive covert communication, aimed at minimizing
detectability in terrestrial ad-hoc networks under multi-UAV surveillance. Our
data-driven method synergistically integrates graph neural networks (GNN) with
Koopman theory to model the complex interactions within a multi-UAV network and
facilitating long-term predictions by linearizing the dynamics, even with
limited historical data. Extensive simulation results substantiate that the
predicted trajectories using our method result in at least 63%-75% lower
probability of detection when compared to well-known state-of-the-art baseline
approaches, showing promise in enabling low-latency covert operations in
practical scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How to Connect Speech Foundation Models and Large Language Models? What
  Matters and What Does Not 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17044v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17044v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Verdini, Pierfrancesco Melucci, Stefano Perna, Francesco Cariaggi, Marco Gaido, Sara Papi, Szymon Mazurek, Marek Kasztelnik, Luisa Bentivogli, Sébastien Bratières, Paolo Merialdo, Simone Scardapane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The remarkable performance achieved by Large Language Models (LLM) has driven
research efforts to leverage them for a wide range of tasks and input
modalities. In speech-to-text (S2T) tasks, the emerging solution consists of
projecting the output of the encoder of a Speech Foundational Model (SFM) into
the LLM embedding space through an adapter module. However, no work has yet
investigated how much the downstream-task performance depends on each component
(SFM, adapter, LLM) nor whether the best design of the adapter depends on the
chosen SFM and LLM. To fill this gap, we evaluate the combination of 5 adapter
modules, 2 LLMs (Mistral and Llama), and 2 SFMs (Whisper and SeamlessM4T) on
two widespread S2T tasks, namely Automatic Speech Recognition and Speech
Translation. Our results demonstrate that the SFM plays a pivotal role in
downstream performance, while the adapter choice has moderate impact and
depends on the SFM and LLM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Counterfactual Token Generation in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17027v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17027v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivi Chatzi, Nina Corvelo Benz, Eleni Straitouri, Stratis Tsirtsis, Manuel Gomez-Rodriguez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  "Sure, I am happy to generate a story for you: Captain Lyra stood at the helm
of her trusty ship, the Maelstrom's Fury, gazing out at the endless sea. [...]
Lyra's eyes welled up with tears as she realized the bitter truth - she had
sacrificed everything for fleeting riches, and lost the love of her crew, her
family, and herself." Although this story, generated by a large language model,
is captivating, one may wonder -- how would the story have unfolded if the
model had chosen "Captain Maeve" as the protagonist instead? We cannot know.
State-of-the-art large language models are stateless -- they maintain no
internal memory or state. Given a prompt, they generate a sequence of tokens as
an output using an autoregressive process. As a consequence, they cannot reason
about counterfactual alternatives to tokens they have generated in the past. In
this work, our goal is to enhance them with this functionality. To this end, we
develop a causal model of token generation that builds upon the Gumbel-Max
structural causal model. Our model allows any large language model to perform
counterfactual token generation at almost no cost in comparison with vanilla
token generation, it is embarrassingly simple to implement, and it does not
require any fine-tuning nor prompt engineering. We implement our model on Llama
3 8B-instruct and conduct both qualitative and quantitative analyses of
counterfactually generated text. We conclude with a demonstrative application
of counterfactual token generation for bias detection, unveiling interesting
insights about the model of the world constructed by large language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CombU: A Combined Unit Activation for Fitting Mathematical Expressions
  with Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17021v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17021v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayu Li, Zilong Zhao, Kevin Yee, Uzair Javaid, Biplab Sikdar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The activation functions are fundamental to neural networks as they introduce
non-linearity into data relationships, thereby enabling deep networks to
approximate complex data relations. Existing efforts to enhance neural network
performance have predominantly focused on developing new mathematical
functions. However, we find that a well-designed combination of existing
activation functions within a neural network can also achieve this objective.
In this paper, we introduce the Combined Units activation (CombU), which
employs different activation functions at various dimensions across different
layers. This approach can be theoretically proven to fit most mathematical
expressions accurately. The experiments conducted on four mathematical
expression datasets, compared against six State-Of-The-Art (SOTA) activation
function algorithms, demonstrate that CombU outperforms all SOTA algorithms in
10 out of 16 metrics and ranks in the top three for the remaining six metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CNN Mixture-of-Depths <span class="chip">ACCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17016v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17016v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rinor Cakaj, Jens Mehnert, Bin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Mixture-of-Depths (MoD) for Convolutional Neural Networks
(CNNs), a novel approach that enhances the computational efficiency of CNNs by
selectively processing channels based on their relevance to the current
prediction. This method optimizes computational resources by dynamically
selecting key channels in feature maps for focused processing within the
convolutional blocks (Conv-Blocks), while skipping less relevant channels.
Unlike conditional computation methods that require dynamic computation graphs,
CNN MoD uses a static computation graph with fixed tensor sizes which improve
hardware efficiency. It speeds up the training and inference processes without
the need for customized CUDA kernels, unique loss functions, or finetuning. CNN
MoD either matches the performance of traditional CNNs with reduced inference
times, GMACs, and parameters, or exceeds their performance while maintaining
similar inference times, GMACs, and parameters. For example, on ImageNet,
ResNet86-MoD exceeds the performance of the standard ResNet50 by 0.45% with a
6% speedup on CPU and 5% on GPU. Moreover, ResNet75-MoD achieves the same
performance as ResNet50 with a 25% speedup on CPU and 15% on GPU.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Conference Paper of the Asian Conference on Computer Vision (ACCV)
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PitRSDNet: Predicting Intra-operative Remaining Surgery Duration in
  Endoscopic Pituitary Surgery <span class="chip">MICCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16998v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16998v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anjana Wijekoon, Adrito Das, Roxana R. Herrera, Danyal Z. Khan, John Hanrahan, Eleanor Carter, Valpuri Luoma, Danail Stoyanov, Hani J. Marcus, Sophia Bano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate intra-operative Remaining Surgery Duration (RSD) predictions allow
for anaesthetists to more accurately decide when to administer anaesthetic
agents and drugs, as well as to notify hospital staff to send in the next
patient. Therefore RSD plays an important role in improving patient care and
minimising surgical theatre costs via efficient scheduling. In endoscopic
pituitary surgery, it is uniquely challenging due to variable workflow
sequences with a selection of optional steps contributing to high variability
in surgery duration. This paper presents PitRSDNet for predicting RSD during
pituitary surgery, a spatio-temporal neural network model that learns from
historical data focusing on workflow sequences. PitRSDNet integrates workflow
knowledge into RSD prediction in two forms: 1) multi-task learning for
concurrently predicting step and RSD; and 2) incorporating prior steps as
context in temporal learning and inference. PitRSDNet is trained and evaluated
on a new endoscopic pituitary surgery dataset with 88 videos to show
competitive performance improvements over previous statistical and machine
learning methods. The findings also highlight how PitRSDNet improve RSD
precision on outlier cases utilising the knowledge of prior steps.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the Augmented Environments for Computer-Assisted
  Interventions (AE-CAI) Workshop at the Medical Image Computing and
  Computer-Assisted Interventions (MICCAI) Conference 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ INT-FlashAttention: Enabling Flash Attention for INT8 Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16997v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16997v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shimao Chen, Zirui Liu, Zhiying Wu, Ce Zheng, Peizhuang Cong, Zihan Jiang, Lei Su, Tong Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the foundation of large language models (LLMs), self-attention module
faces the challenge of quadratic time and memory complexity with respect to
sequence length. FlashAttention accelerates attention computation and reduces
its memory usage by leveraging the GPU memory hierarchy. A promising research
direction is to integrate FlashAttention with quantization methods. This paper
introduces INT-FlashAttention, the first INT8 quantization architecture
compatible with the forward workflow of FlashAttention, which significantly
improves the inference speed of FlashAttention on Ampere GPUs. We implement our
INT-FlashAttention prototype with fully INT8 activations and general
matrix-multiplication (GEMM) kernels, making it the first attention operator
with fully INT8 input. As a general token-level post-training quantization
framework, INT-FlashAttention is also compatible with other data formats like
INT4, etc. Experimental results show INT-FlashAttention achieves 72% faster
inference speed and 82% smaller quantization error compared to standard
FlashAttention with FP16 and FP8 data format.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What is the relationship between Slow Feature Analysis and the Successor
  Representation? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16991v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16991v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eddie Seabrook, Laurenz Wiskott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  (This is a work in progress. Feedback is welcome) An analytical comparison is
made between slow feature analysis (SFA) and the successor representation (SR).
While SFA and the SR stem from distinct areas of machine learning, they share
important properties, both in terms of their mathematics and the types of
information they are sensitive to. This work studies their connection along
these two axes. In particular, multiple variants of the SFA algorithm are
explored analytically and then applied to the setting of an MDP, leading to a
family of eigenvalue problems involving the SR and other related quantities.
These resulting eigenvalue problems are then illustrated in the toy setting of
a gridworld, where it is demonstrated that the place- and grid-like fields
often associated to the SR can equally be generated using SFA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>52 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards User-Focused Research in Training Data Attribution for
  Human-Centered Explainable AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16978v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16978v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elisa Nguyen, Johannes Bertram, Evgenii Kortukov, Jean Y. Song, Seong Joon Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Explainable AI (XAI) aims to make AI understandable and useful to
humans, it has been criticised for relying too much on formalism and
solutionism, focusing more on mathematical soundness than user needs. We
propose an alternative to this bottom-up approach inspired by design thinking:
the XAI research community should adopt a top-down, user-focused perspective to
ensure user relevance. We illustrate this with a relatively young subfield of
XAI, Training Data Attribution (TDA). With the surge in TDA research and
growing competition, the field risks repeating the same patterns of
solutionism. We conducted a needfinding study with a diverse group of AI
practitioners to identify potential user needs related to TDA. Through
interviews (N=10) and a systematic survey (N=31), we uncovered new TDA tasks
that are currently largely overlooked. We invite the TDA and XAI communities to
consider these novel tasks and improve the user relevance of their research
outcomes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive <span class="highlight-title">Self-Supervised</span> Learning Strategies for Dynamic On-Device LLM
  Personalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16973v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16973v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafael Mendoza, Isabella Cruz, Richard Liu, Aarav Deshmukh, David Williams, Jesscia Peng, Rohan Iyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have revolutionized how we interact with
technology, but their personalization to individual user preferences remains a
significant challenge, particularly in on-device applications. Traditional
methods often depend heavily on labeled datasets and can be resource-intensive.
To address these issues, we present Adaptive Self-Supervised Learning
Strategies (ASLS), which utilizes self-supervised learning techniques to
personalize LLMs dynamically. The framework comprises a user profiling layer
for collecting interaction data and a neural adaptation layer for real-time
model fine-tuning. This innovative approach enables continuous learning from
user feedback, allowing the model to generate responses that align closely with
user-specific contexts. The adaptive mechanisms of ASLS minimize computational
demands and enhance personalization efficiency. Experimental results across
various user scenarios illustrate the superior performance of ASLS in boosting
user engagement and satisfaction, highlighting its potential to redefine LLMs
as highly responsive and context-aware systems on-device.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First ASLS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridge to Real Environment with Hardware-in-the-loop for Wireless
  Artificial Intelligence Paradigms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16968v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16968v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeffrey Redondo, Nauman Aslam, Juan Zhang, Zhenhui Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, many machine learning (ML) solutions to improve the wireless
standard IEEE802.11p for Vehicular Adhoc Network (VANET) are commonly evaluated
in the simulated world. At the same time, this approach could be cost-effective
compared to real-world testing due to the high cost of vehicles. There is a
risk of unexpected outcomes when these solutions are implemented in the real
world, potentially leading to wasted resources. To mitigate this challenge, the
hardware-in-the-loop is the way to move forward as it enables the opportunity
to test in the real world and simulated worlds together. Therefore, we have
developed what we believe is the pioneering hardware-in-the-loop for testing
artificial intelligence, multiple services, and HD map data (LiDAR), in both
simulated and real-world settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ABCFair: an Adaptable Benchmark approach for Comparing Fairness Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16965v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16965v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        MaryBeth Defrance, Maarten Buyl, Tijl De Bie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Numerous methods have been implemented that pursue fairness with respect to
sensitive features by mitigating biases in machine learning. Yet, the problem
settings that each method tackles vary significantly, including the stage of
intervention, the composition of sensitive features, the fairness notion, and
the distribution of the output. Even in binary classification, these subtle
differences make it highly complicated to benchmark fairness methods, as their
performance can strongly depend on exactly how the bias mitigation problem was
originally framed.
  Hence, we introduce ABCFair, a benchmark approach which allows adapting to
the desiderata of the real-world problem setting, enabling proper comparability
between methods for any use case. We apply ABCFair to a range of pre-, in-, and
postprocessing methods on both large-scale, traditional datasets and on a dual
label (biased and unbiased) dataset to sidestep the fairness-accuracy
trade-off.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Informed deep hierarchical classification: a non-standard analysis
  inspired approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16956v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16956v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenzo Fiaschi, Marco Cococcioni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work proposes a novel approach to the deep hierarchical classification
task, i.e., the problem of classifying data according to multiple labels
organized in a rigid parent-child structure. It consists in a multi-output deep
neural network equipped with specific projection operators placed before each
output layer. The design of such an architecture, called lexicographic hybrid
deep neural network (LH-DNN), has been possible by combining tools from
different and quite distant research fields: lexicographic multi-objective
optimization, non-standard analysis, and deep learning. To assess the efficacy
of the approach, the resulting network is compared against the B-CNN, a
convolutional neural network tailored for hierarchical classification tasks, on
the CIFAR10, CIFAR100 (where it has been originally and recently proposed
before being adopted and tuned for multiple real-world applications) and
Fashion-MNIST benchmarks. Evidence states that an LH-DNN can achieve comparable
if not superior performance, especially in the learning of the hierarchical
relations, in the face of a drastic reduction of the learning parameters,
training epochs, and computational time, without the need for ad-hoc loss
functions weighting values.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Obstacle Avoidance through Uncertainty-Based Adaptive Planning
  with Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16950v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16950v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vineet Punyamoorty, Pascal Jutras-Dubé, Ruqi Zhang, Vaneet Aggarwal, Damon Conover, Aniket Bera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  By framing reinforcement learning as a sequence modeling problem, recent work
has enabled the use of generative models, such as diffusion models, for
planning. While these models are effective in predicting long-horizon state
trajectories in deterministic environments, they face challenges in dynamic
settings with moving obstacles. Effective collision avoidance demands
continuous monitoring and adaptive decision-making. While replanning at every
timestep could ensure safety, it introduces substantial computational overhead
due to the repetitive prediction of overlapping state sequences -- a process
that is particularly costly with diffusion models, known for their intensive
iterative sampling procedure. We propose an adaptive generative planning
approach that dynamically adjusts replanning frequency based on the uncertainty
of action predictions. Our method minimizes the need for frequent,
computationally expensive, and redundant replanning while maintaining robust
collision avoidance performance. In experiments, we obtain a 13.5% increase in
the mean trajectory length and a 12.7% increase in mean reward over
long-horizon planning, indicating a reduction in collision rates and an
improved ability to navigate the environment safely.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decomposition of Equivariant Maps via Invariant Maps: Application to
  Universal Approximation under Symmetry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16922v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16922v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akiyoshi Sannai, Yuuki Takai, Matthieu Cordonnier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we develop a theory about the relationship between invariant
and equivariant maps with regard to a group $G$. We then leverage this theory
in the context of deep neural networks with group symmetries in order to obtain
novel insight into their mechanisms. More precisely, we establish a one-to-one
relationship between equivariant maps and certain invariant maps. This allows
us to reduce arguments for equivariant maps to those for invariant maps and
vice versa. As an application, we propose a construction of universal
equivariant architectures built from universal invariant networks. We, in turn,
explain how the universal architectures arising from our construction differ
from standard equivariant architectures known to be universal. Furthermore, we
explore the complexity, in terms of the number of free parameters, of our
models, and discuss the relation between invariant and equivariant networks'
complexity. Finally, we also give an approximation rate for G-equivariant deep
neural networks with ReLU activation functions for finite group G.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Discriminative Anchor Learning for Efficient Multi-view Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16904v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16904v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yalan Qin, Nan Pu, Hanzhou Wu, Nicu Sebe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-view clustering aims to study the complementary information across
views and discover the underlying structure. For solving the relatively high
computational cost for the existing approaches, works based on anchor have been
presented recently. Even with acceptable clustering performance, these methods
tend to map the original representation from multiple views into a fixed shared
graph based on the original dataset. However, most studies ignore the
discriminative property of the learned anchors, which ruin the representation
capability of the built model. Moreover, the complementary information among
anchors across views is neglected to be ensured by simply learning the shared
anchor graph without considering the quality of view-specific anchors. In this
paper, we propose discriminative anchor learning for multi-view clustering
(DALMC) for handling the above issues. We learn discriminative view-specific
feature representations according to the original dataset and build anchors
from different views based on these representations, which increase the quality
of the shared anchor graph. The discriminative feature learning and consensus
anchor graph construction are integrated into a unified framework to improve
each other for realizing the refinement. The optimal anchors from multiple
views and the consensus anchor graph are learned with the orthogonal
constraints. We give an iterative algorithm to deal with the formulated
problem. Extensive experiments on different datasets show the effectiveness and
efficiency of our method compared with other methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been accepted by TMM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Space Mission Planning: A Reinforcement Learning-Guided
  Approach for Multi-Debris Rendezvous 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16882v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16882v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Agni Bandyopadhyay, Guenther Waxenegger-Wilfing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research introduces a novel application of a masked Proximal Policy
Optimization (PPO) algorithm from the field of deep reinforcement learning
(RL), for determining the most efficient sequence of space debris visitation,
utilizing the Lambert solver as per Izzo's adaptation for individual
rendezvous. The aim is to optimize the sequence in which all the given debris
should be visited to get the least total time for rendezvous for the entire
mission. A neural network (NN) policy is developed, trained on simulated space
missions with varying debris fields. After training, the neural network
calculates approximately optimal paths using Izzo's adaptation of Lambert
maneuvers. Performance is evaluated against standard heuristics in mission
planning. The reinforcement learning approach demonstrates a significant
improvement in planning efficiency by optimizing the sequence for debris
rendezvous, reducing the total mission time by an average of approximately
{10.96\%} and {13.66\%} compared to the Genetic and Greedy algorithms,
respectively. The model on average identifies the most time-efficient sequence
for debris visitation across various simulated scenarios with the fastest
computational speed. This approach signifies a step forward in enhancing
mission planning strategies for space debris clearance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the 2024 International Conference on
  Space Robotics (iSpaRo)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Feedforward Controllers from Learned Dynamic Local Model Networks with
  Application to Excavator Assistance Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16875v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16875v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leon Greiser, Ozan Demir, Benjamin Hartmann, Henrik Hose, Sebastian Trimpe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Complicated first principles modelling and controller synthesis can be
prohibitively slow and expensive for high-mix, low-volume products such as
hydraulic excavators. Instead, in a data-driven approach, recorded trajectories
from the real system can be used to train local model networks (LMNs), for
which feedforward controllers are derived via feedback linearization. However,
previous works required LMNs without zero dynamics for feedback linearization,
which restricts the model structure and thus modelling capacity of LMNs. In
this paper, we overcome this restriction by providing a criterion for when
feedback linearization of LMNs with zero dynamics yields a valid controller. As
a criterion we propose the bounded-input bounded-output stability of the
resulting controller. In two additional contributions, we extend this approach
to consider measured disturbance signals and multiple inputs and outputs. We
illustrate the effectiveness of our contributions in a hydraulic excavator
control application with hardware experiments. To this end, we train LMNs from
recorded, noisy data and derive feedforward controllers used as part of a
leveling assistance system on the excavator. In our experiments, incorporating
disturbance signals and multiple inputs and outputs enhances tracking
performance of the learned controller. A video of our experiments is available
at https://youtu.be/lrrWBx2ASaE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ethical and Scalable Automation: A Governance and Compliance Framework
  for Business Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16872v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16872v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haocheng Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The popularisation of applying AI in businesses poses significant challenges
relating to ethical principles, governance, and legal compliance. Although
businesses have embedded AI into their day-to-day processes, they lack a
unified approach for mitigating its potential risks. This paper introduces a
framework ensuring that AI must be ethical, controllable, viable, and
desirable. Balancing these factors ensures the design of a framework that
addresses its trade-offs, such as balancing performance against explainability.
A successful framework provides practical advice for businesses to meet
regulatory requirements in sectors such as finance and healthcare, where it is
critical to comply with standards like GPDR and the EU AI Act. Different case
studies validate this framework by integrating AI in both academic and
practical environments. For instance, large language models are cost-effective
alternatives for generating synthetic opinions that emulate attitudes to
environmental issues. These case studies demonstrate how having a structured
framework could enhance transparency and maintain performance levels as shown
from the alignment between synthetic and expected distributions. This alignment
is quantified using metrics like Chi-test scores, normalized mutual
information, and Jaccard indexes. Future research should explore the
framework's empirical validation in diverse industrial settings further,
ensuring the model's scalability and adaptability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantifying Visual Properties of GAM Shape Plots: Impact on Perceived
  Cognitive Load and Interpretability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sven Kruschel, Lasse Bohlen, Julian Rosenberger, Patrick Zschech, Mathias Kraus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalized Additive Models (GAMs) offer a balance between performance and
interpretability in machine learning. The interpretability aspect of GAMs is
expressed through shape plots, representing the model's decision-making
process. However, the visual properties of these plots, e.g. number of kinks
(number of local maxima and minima), can impact their complexity and the
cognitive load imposed on the viewer, compromising interpretability. Our study,
including 57 participants, investigates the relationship between the visual
properties of GAM shape plots and cognitive load they induce. We quantify
various visual properties of shape plots and evaluate their alignment with
participants' perceived cognitive load, based on 144 plots. Our results
indicate that the number of kinks metric is the most effective, explaining
86.4% of the variance in users' ratings. We develop a simple model based on
number of kinks that provides a practical tool for predicting cognitive load,
enabling the assessment of one aspect of GAM interpretability without direct
user involvement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to be published in proceedings of the 58th Hawaii International
  Conference on System Sciences (HICSS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Risk-averse learning with delayed feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16866v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16866v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyi Wang, Zifan Wang, Karl Henrik Johansson, Sandra Hirche
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world scenarios, the impacts of decisions may not manifest
immediately. Taking these delays into account facilitates accurate assessment
and management of risk in real-world environments, thereby ensuring the
efficacy of strategies. In this paper, we investigate risk-averse learning
using Conditional Value at Risk (CVaR) as risk measure, while incorporating
delayed feedback with unknown but bounded delays. We develop two risk-averse
learning algorithms that rely on one-point and two-point zeroth-order
optimization approaches, respectively. The regret achieved by the algorithms is
analyzed in terms of the cumulative delay and the number of total samplings.
The results suggest that the two-point risk-averse learning achieves a smaller
regret bound than the one-point algorithm. Furthermore, the one-point
risk-averse learning algorithm attains sublinear regret under certain delay
conditions, and the two-point risk-averse learning algorithm achieves sublinear
regret with minimal restrictions on the delay. We provide numerical experiments
on a dynamic pricing problem to demonstrate the performance of the proposed
algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal starting point for time series forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16843v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16843v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Zhong, Yinuo Ren, Guangyao Cao, Feng Li, Haobo Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances on time series forecasting mainly focus on improving the
forecasting models themselves. However, managing the length of the input data
can also significantly enhance prediction performance. In this paper, we
introduce a novel approach called Optimal Starting Point Time Series Forecast
(OSP-TSP) to capture the intrinsic characteristics of time series data. By
adjusting the sequence length via leveraging the XGBoost and LightGBM models,
the proposed approach can determine optimal starting point (OSP) of the time
series and thus enhance the prediction performances. The performances of the
OSP-TSP approach are then evaluated across various frequencies on the M4
dataset and other real-world datasets. Empirical results indicate that
predictions based on the OSP-TSP approach consistently outperform those using
the complete dataset. Moreover, recognizing the necessity of sufficient data to
effectively train models for OSP identification, we further propose targeted
solutions to address the issue of data insufficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Demo2Vec: Learning Region Embedding with Demographic Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16837v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16837v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ya Wen, Yulun Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Demographic data, such as income, education level, and employment rate,
contain valuable information of urban regions, yet few studies have integrated
demographic information to generate region embedding. In this study, we show
how the simple and easy-to-access demographic data can improve the quality of
state-of-the-art region embedding and provide better predictive performances in
urban areas across three common urban tasks, namely check-in prediction, crime
rate prediction, and house price prediction. We find that existing pre-train
methods based on KL divergence are potentially biased towards mobility
information and propose to use Jenson-Shannon divergence as a more appropriate
loss function for multi-view representation learning. Experimental results from
both New York and Chicago show that mobility + income is the best pre-train
data combination, providing up to 10.22\% better predictive performances than
existing models. Considering that mobility big data can be hardly accessible in
many developing cities, we suggest geographic proximity + income to be a simple
but effective data combination for region embedding pre-training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Asynchronous Fractional Multi-Agent Deep Reinforcement Learning for
  Age-Minimal Mobile Edge Computing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16832v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16832v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lyudong Jin, Ming Tang, Jiayu Pan, Meng Zhang, Hao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of emerging real-time networked applications like cyber-physical
systems (CPS), the Age of Information (AoI) has merged as a pivotal metric for
evaluating the timeliness. To meet the high computational demands, such as
those in intelligent manufacturing within CPS, mobile edge computing (MEC)
presents a promising solution for optimizing computing and reducing AoI. In
this work, we study the timeliness of computational-intensive updates and
explores jointly optimize the task updating and offloading policies to minimize
AoI. Specifically, we consider edge load dynamics and formulate a task
scheduling problem to minimize the expected time-average AoI. The fractional
objective introduced by AoI and the semi-Markov game nature of the problem
render this challenge particularly difficult, with existing approaches not
directly applicable. To this end, we present a comprehensive framework to
fractional reinforcement learning (RL). We first introduce a fractional
single-agent RL framework and prove its linear convergence. We then extend this
to a fractional multi-agent RL framework with a convergence analysis. To tackle
the challenge of asynchronous control in semi-Markov game, we further design an
asynchronous model-free fractional multi-agent RL algorithm, where each device
makes scheduling decisions with the hybrid action space without knowing the
system dynamics and decisions of other devices. Experimental results show that
our proposed algorithms reduce the average AoI by up to 52.6% compared with the
best baseline algorithm in our experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning phase-space flows using time-discrete implicit Runge-Kutta
  PINNs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16826v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16826v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Álvaro Fernández Corral, Nicolás Mendoza, Armin Iske, Andrey Yachmenev, Jochen Küpper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a computational framework for obtaining multidimensional
phase-space solutions of systems of non-linear coupled differential equations,
using high-order implicit Runge-Kutta Physics- Informed Neural Networks
(IRK-PINNs) schemes. Building upon foundational work originally solving
differential equations for fields depending on coordinates [J. Comput. Phys.
378, 686 (2019)], we adapt the scheme to a context where the coordinates are
treated as functions. This modification enables us to efficiently solve
equations of motion for a particle in an external field. Our scheme is
particularly useful for explicitly time-independent and periodic fields. We
apply this approach to successfully solve the equations of motion for a mass
particle placed in a central force field and a charged particle in a periodic
electric field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures, published in the International Conference on
  Scientific Computing and Machine Learning, see http://scml.jp</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty Representations in State-Space Layers for Deep Reinforcement
  Learning under Partial Observability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16824v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16824v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlos E. Luis, Alessandro G. Bottero, Julia Vinogradska, Felix Berkenkamp, Jan Peters
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimal decision-making under partial observability requires reasoning about
the uncertainty of the environment's hidden state. However, most reinforcement
learning architectures handle partial observability with sequence models that
have no internal mechanism to incorporate uncertainty in their hidden state
representation, such as recurrent neural networks, deterministic state-space
models and transformers. Inspired by advances in probabilistic world models for
reinforcement learning, we propose a standalone Kalman filter layer that
performs closed-form Gaussian inference in linear state-space models and train
it end-to-end within a model-free architecture to maximize returns. Similar to
efficient linear recurrent layers, the Kalman filter layer processes sequential
data using a parallel scan, which scales logarithmically with the sequence
length. By design, Kalman filter layers are a drop-in replacement for other
recurrent layers in standard model-free architectures, but importantly they
include an explicit mechanism for probabilistic filtering of the latent state
representation. Experiments in a wide variety of tasks with partial
observability show that Kalman filter layers excel in problems where
uncertainty reasoning is key for decision-making, outperforming other stateful
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A parametric framework for kernel-based dynamic mode decomposition using
  deep learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16817v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16817v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konstantinos Kevopoulos, Dongwei Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surrogate modelling is widely applied in computational science and
engineering to mitigate computational efficiency issues for the real-time
simulations of complex and large-scale computational models or for many-query
scenarios, such as uncertainty quantification and design optimisation. In this
work, we propose a parametric framework for kernel-based dynamic mode
decomposition method based on the linear and nonlinear disambiguation
optimization (LANDO) algorithm. The proposed parametric framework consists of
two stages, offline and online. The offline stage prepares the essential
component for prediction, namely a series of LANDO models that emulate the
dynamics of the system with particular parameters from a training dataset. The
online stage leverages those LANDO models to generate new data at a desired
time instant, and approximate the mapping between parameters and the state with
the data using deep learning techniques. Moreover, dimensionality reduction
technique is applied to high-dimensional dynamical systems to reduce the
computational cost of training. Three numerical examples including
Lotka-Volterra model, heat equation and reaction-diffusion equation are
presented to demonstrate the efficiency and effectiveness of the proposed
framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accelerating TinyML Inference on Microcontrollers through Approximate
  Kernels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16815v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16815v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giorgos Armeniakos, Georgios Mentzos, Dimitrios Soudris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid growth of microcontroller-based IoT devices has opened up numerous
applications, from smart manufacturing to personalized healthcare. Despite the
widespread adoption of energy-efficient microcontroller units (MCUs) in the
Tiny Machine Learning (TinyML) domain, they still face significant limitations
in terms of performance and memory (RAM, Flash). In this work, we combine
approximate computing and software kernel design to accelerate the inference of
approximate CNN models on MCUs. Our kernel-based approximation framework
firstly unpacks the operands of each convolution layer and then conducts an
offline calculation to determine the significance of each operand.
Subsequently, through a design space exploration, it employs a computation
skipping approximation strategy based on the calculated significance. Our
evaluation on an STM32-Nucleo board and 2 popular CNNs trained on the CIFAR-10
dataset shows that, compared to state-of-the-art exact inference, our Pareto
optimal solutions can feature on average 21% latency reduction with no
degradation in Top-1 classification accuracy, while for lower accuracy
requirements, the corresponding reduction becomes even more pronounced.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Model Predicts Above Normal All India Summer Monsoon
  Rainfall in 2024 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16799v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16799v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ujjawal Sharma, Madhav Biyani, Akhil Dev Suresh, Debi Prasad Bhuyan, Saroj Kanta Mishra, Tanmoy Chakraborty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reliable prediction of the All India Summer Monsoon Rainfall (AISMR) is
pivotal for informed policymaking for the country, impacting the lives of
billions of people. However, accurate simulation of AISMR has been a persistent
challenge due to the complex interplay of various muti-scale factors and the
inherent variability of the monsoon system. This research focuses on adapting
and fine-tuning the latest LLM model, PatchTST, to accurately predict AISMR
with a lead time of three months. The fine-tuned PatchTST model, trained with
historical AISMR data, the Ni\~no3.4 index, and categorical Indian Ocean Dipole
values, outperforms several popular neural network models and statistical
models. This fine-tuned LLM model exhibits an exceptionally low RMSE percentage
of 0.07% and a Spearman correlation of 0.976. This is particularly impressive,
since it is nearly 80% more accurate than the best-performing NN models. The
model predicts an above-normal monsoon for the year 2024, with an accumulated
rainfall of 921.6 mm in the month of June-September for the entire country.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable Ensemble Diversification for OOD Generalization and Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16797v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16797v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Rubinstein, Luca Scimeca, Damien Teney, Seong Joon Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training a diverse ensemble of models has several practical applications such
as providing candidates for model selection with better out-of-distribution
(OOD) generalization, and enabling the detection of OOD samples via Bayesian
principles. An existing approach to diverse ensemble training encourages the
models to disagree on provided OOD samples. However, the approach is
computationally expensive and it requires well-separated ID and OOD examples,
such that it has only been demonstrated in small-scale settings.
  $\textbf{Method.}$ This work presents a method for Scalable Ensemble
Diversification (SED) applicable to large-scale settings (e.g. ImageNet) that
does not require OOD samples. Instead, SED identifies hard training samples on
the fly and encourages the ensemble members to disagree on these. To improve
scaling, we show how to avoid the expensive computations in existing methods of
exhaustive pairwise disagreements across models.
  $\textbf{Results.}$ We evaluate the benefits of diversification with
experiments on ImageNet. First, for OOD generalization, we observe large
benefits from the diversification in multiple settings including output-space
(classical) ensembles and weight-space ensembles (model soups). Second, for OOD
detection, we turn the diversity of ensemble hypotheses into a novel
uncertainty score estimator that surpasses a large number of OOD detection
baselines.
  Code is available here:
https://github.com/AlexanderRubinstein/diverse-universe-public.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Symbolic State Partition for Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16791v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16791v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohsen Ghaffari, Mahsa Varshosaz, Einar Broch Johnsen, Andrzej Wąsowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tabular reinforcement learning methods cannot operate directly on continuous
state spaces. One solution for this problem is to partition the state space. A
good partitioning enables generalization during learning and more efficient
exploitation of prior experiences. Consequently, the learning process becomes
faster and produces more reliable policies. However, partitioning introduces
approximation, which is particularly harmful in the presence of nonlinear
relations between state components. An ideal partition should be as coarse as
possible, while capturing the key structure of the state space for the given
problem. This work extracts partitions from the environment dynamics by
symbolic execution. We show that symbolic partitioning improves state space
coverage with respect to environmental behavior and allows reinforcement
learning to perform better for sparse rewards. We evaluate symbolic state space
partitioning with respect to precision, scalability, learning agent performance
and state space coverage for the learnt policies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Feature Selection and Interpretability in AI Regression Tasks
  Through Feature Attribution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16787v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16787v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Hinterleitner, Thomas Bartz-Beielstein, Richard Schulz, Sebastian Spengler, Thomas Winter, Christoph Leitenmeier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research in Explainable Artificial Intelligence (XAI) is increasing, aiming
to make deep learning models more transparent. Most XAI methods focus on
justifying the decisions made by Artificial Intelligence (AI) systems in
security-relevant applications. However, relatively little attention has been
given to using these methods to improve the performance and robustness of deep
learning algorithms. Additionally, much of the existing XAI work primarily
addresses classification problems. In this study, we investigate the potential
of feature attribution methods to filter out uninformative features in input
data for regression problems, thereby improving the accuracy and stability of
predictions. We introduce a feature selection pipeline that combines Integrated
Gradients with k-means clustering to select an optimal set of variables from
the initial data space. To validate the effectiveness of this approach, we
apply it to a real-world industrial problem - blade vibration analysis in the
development process of turbo machinery.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ World Model-based Perception for Visual Legged Locomotion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16784v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16784v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Lai, Jiahang Cao, Jiafeng Xu, Hongtao Wu, Yunfeng Lin, Tao Kong, Yong Yu, Weinan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Legged locomotion over various terrains is challenging and requires precise
perception of the robot and its surroundings from both proprioception and
vision. However, learning directly from high-dimensional visual input is often
data-inefficient and intricate. To address this issue, traditional methods
attempt to learn a teacher policy with access to privileged information first
and then learn a student policy to imitate the teacher's behavior with visual
input. Despite some progress, this imitation framework prevents the student
policy from achieving optimal performance due to the information gap between
inputs. Furthermore, the learning process is unnatural since animals
intuitively learn to traverse different terrains based on their understanding
of the world without privileged knowledge. Inspired by this natural ability, we
propose a simple yet effective method, World Model-based Perception (WMP),
which builds a world model of the environment and learns a policy based on the
world model. We illustrate that though completely trained in simulation, the
world model can make accurate predictions of real-world trajectories, thus
providing informative signals for the policy controller. Extensive simulated
and real-world experiments demonstrate that WMP outperforms state-of-the-art
baselines in traversability and robustness. Videos and Code are available at:
https://wmp-loco.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Super Level Sets and Exponential Decay: A Synergistic Approach to Stable
  Neural Network Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16769v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16769v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jatin Chaudhary, Dipak Nidhi, Jukka Heikkonen, Haari Merisaari, Rajiv Kanth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The objective of this paper is to enhance the optimization process for neural
networks by developing a dynamic learning rate algorithm that effectively
integrates exponential decay and advanced anti-overfitting strategies. Our
primary contribution is the establishment of a theoretical framework where we
demonstrate that the optimization landscape, under the influence of our
algorithm, exhibits unique stability characteristics defined by Lyapunov
stability principles. Specifically, we prove that the superlevel sets of the
loss function, as influenced by our adaptive learning rate, are always
connected, ensuring consistent training dynamics. Furthermore, we establish the
"equiconnectedness" property of these superlevel sets, which maintains uniform
stability across varying training conditions and epochs. This paper contributes
to the theoretical understanding of dynamic learning rate mechanisms in neural
networks and also pave the way for the development of more efficient and
reliable neural optimization techniques. This study intends to formalize and
validate the equiconnectedness of loss function as superlevel sets in the
context of neural network training, opening newer avenues for future research
in adaptive machine learning algorithms. We leverage previous theoretical
discoveries to propose training mechanisms that can effectively handle complex
and high-dimensional data landscapes, particularly in applications requiring
high precision and reliability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpreting Deep Neural Network-Based Receiver Under Varying
  Signal-To-Noise Ratios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16768v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16768v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marko Tuononen, Dani Korpi, Ville Hautamäki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel method for interpreting neural networks, focusing on
convolutional neural network-based receiver model. The method identifies which
unit or units of the model contain most (or least) information about the
channel parameter(s) of the interest, providing insights at both global and
local levels -- with global explanations aggregating local ones. Experiments on
link-level simulations demonstrate the method's effectiveness in identifying
units that contribute most (and least) to signal-to-noise ratio processing.
Although we focus on a radio receiver model, the method generalizes to other
neural network architectures and applications, offering robust estimation even
in high-dimensional settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7+1 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Information-Theoretic Metrics Associated with Neural Collapse
  in Supervised Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16767v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16767v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Song, Zhiquan Tan, Bochao Zou, Jiansheng Chen, Huimin Ma, Weiran Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we utilize information-theoretic metrics like matrix entropy
and mutual information to analyze supervised learning. We explore the
information content of data representations and classification head weights and
their information interplay during supervised training. Experiments show that
matrix entropy cannot solely describe the interaction of the information
content of data representation and classification head weights but it can
effectively reflect the similarity and clustering behavior of the data.
Inspired by this, we propose a cross-modal alignment loss to improve the
alignment between the representations of the same class from different
modalities. Moreover, in order to assess the interaction of the information
content of data representation and classification head weights more accurately,
we utilize new metrics like matrix mutual information ratio (MIR) and matrix
information entropy difference ratio (HDR). Through theory and experiment, we
show that HDR and MIR can not only effectively describe the information
interplay of supervised training but also improve the performance of supervised
and semi-supervised learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2406.03999</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MaViLS, a Benchmark <span class="highlight-title">Dataset</span> for Video-to-Slide Alignment, Assessing
  Baseline Accuracy with a Multimodal Alignment Algorithm Leveraging Speech,
  OCR, and Visual Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16765v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16765v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Katharina Anderer, Andreas Reich, Matthias Wölfel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a benchmark dataset for aligning lecture videos with
corresponding slides and introduces a novel multimodal algorithm leveraging
features from speech, text, and images. It achieves an average accuracy of 0.82
in comparison to SIFT (0.56) while being approximately 11 times faster. Using
dynamic programming the algorithm tries to determine the optimal slide
sequence. The results show that penalizing slide transitions increases
accuracy. Features obtained via optical character recognition (OCR) contribute
the most to a high matching accuracy, followed by image features. The findings
highlight that audio transcripts alone provide valuable information for
alignment and are beneficial if OCR data is lacking. Variations in matching
accuracy across different lectures highlight the challenges associated with
video quality and lecture style. The novel multimodal algorithm demonstrates
robustness to some of these challenges, underscoring the potential of the
approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Offline and Distributional Reinforcement Learning for Radio Resource
  Management 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16764v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16764v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eslam Eldeeb, Hirley Alves
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) has proved to have a promising role in future
intelligent wireless networks. Online RL has been adopted for radio resource
management (RRM), taking over traditional schemes. However, due to its reliance
on online interaction with the environment, its role becomes limited in
practical, real-world problems where online interaction is not feasible. In
addition, traditional RL stands short in front of the uncertainties and risks
in real-world stochastic environments. In this manner, we propose an offline
and distributional RL scheme for the RRM problem, enabling offline training
using a static dataset without any interaction with the environment and
considering the sources of uncertainties using the distributions of the return.
Simulation results demonstrate that the proposed scheme outperforms
conventional resource management models. In addition, it is the only scheme
that surpasses online RL and achieves a $16 \%$ gain over online RL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GB-RVFL: Fusion of Randomized Neural Network and Granular Ball Computing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16735v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16735v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        M. Sajid, A. Quadir, M. Tanveer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The random vector functional link (RVFL) network is a prominent
classification model with strong generalization ability. However, RVFL treats
all samples uniformly, ignoring whether they are pure or noisy, and its
scalability is limited due to the need for inverting the entire training
matrix. To address these issues, we propose granular ball RVFL (GB-RVFL) model,
which uses granular balls (GBs) as inputs instead of training samples. This
approach enhances scalability by requiring only the inverse of the GB center
matrix and improves robustness against noise and outliers through the coarse
granularity of GBs. Furthermore, RVFL overlooks the dataset's geometric
structure. To address this, we propose graph embedding GB-RVFL (GE-GB-RVFL)
model, which fuses granular computing and graph embedding (GE) to preserve the
topological structure of GBs. The proposed GB-RVFL and GE-GB-RVFL models are
evaluated on KEEL, UCI, NDC and biomedical datasets, demonstrating superior
performance compared to baseline models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Verified Relative Safety Margins for Neural Network Twins 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16726v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16726v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anahita Baninajjar, Kamran Hosseini, Ahmed Rezine, Amir Aminifar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given two Deep Neural Network (DNN) classifiers with the same input and
output domains, our goal is to quantify the robustness of the two networks in
relation to each other. Towards this, we introduce the notion of Relative
Safety Margins (RSMs). Intuitively, given two classes and a common input, RSM
of one classifier with respect to another reflects the relative margins with
which decisions are made. The proposed notion is relevant in the context of
several applications domains, including to compare a trained network and its
corresponding compact network (e.g., pruned, quantized, distilled network). Not
only can RSMs establish whether decisions are preserved, but they can also
quantify their qualities. We also propose a framework to establish safe bounds
on RSM gains or losses given an input and a family of perturbations. We
evaluate our approach using the MNIST, CIFAR10, and two real-world medical
datasets, to show the relevance of our results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PMSS: <span class="highlight-title">Pretrain</span>ed Matrices Skeleton Selection for LLM Fine-tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16722v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16722v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qibin Wang, Xiaolin Hu, Weikai Xu, Wei Liu, Jian Luan, Bin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-rank adaptation (LoRA) and its variants have recently gained much
interest due to their ability to avoid excessive inference costs. However, LoRA
still encounters the following challenges: (1) Limitation of low-rank
assumption; and (2) Its initialization method may be suboptimal. To this end,
we propose PMSS(Pre-trained Matrices Skeleton Selection), which enables
high-rank updates with low costs while leveraging semantic and linguistic
information inherent in pre-trained weight. It achieves this by selecting
skeletons from the pre-trained weight matrix and only learning a small matrix
instead. Experiments demonstrate that PMSS outperforms LoRA and other
fine-tuning methods across tasks with much less trainable parameters. We
demonstrate its effectiveness, especially in handling complex tasks such as
DROP benchmark(+3.4%/+5.9% on LLaMA2-7B/13B) and math
reasoning(+12.89%/+5.61%/+3.11% on LLaMA2-7B, Mistral-7B and Gemma-7B of
GSM8K). The code and model will be released soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dashing for the Golden Snitch: Multi-Drone Time-Optimal Motion Planning
  with Multi-Agent Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16720v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16720v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xian Wang, Jin Zhou, Yuanli Feng, Jiahao Mei, Jiming Chen, Shuo Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent innovations in autonomous drones have facilitated time-optimal flight
in single-drone configurations and enhanced maneuverability in multi-drone
systems through the application of optimal control and learning-based methods.
However, few studies have achieved time-optimal motion planning for multi-drone
systems, particularly during highly agile maneuvers or in dynamic scenarios.
This paper presents a decentralized policy network for time-optimal multi-drone
flight using multi-agent reinforcement learning. To strike a balance between
flight efficiency and collision avoidance, we introduce a soft collision
penalty inspired by optimization-based methods. By customizing PPO in a
centralized training, decentralized execution (CTDE) fashion, we unlock higher
efficiency and stability in training, while ensuring lightweight
implementation. Extensive simulations show that, despite slight performance
trade-offs compared to single-drone systems, our multi-drone approach maintains
near-time-optimal performance with low collision rates. Real-world experiments
validate our method, with two quadrotors using the same network as simulation
achieving a maximum speed of 13.65 m/s and a maximum body rate of 13.4 rad/s in
a 5.5 m * 5.5 m * 2.0 m space across various tracks, relying entirely on
onboard computation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vision-Language Model Fine-Tuning via Simple Parameter-Efficient
  Modification <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16718v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16718v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming Li, Jike Zhong, Chenxin Li, Liuzhuozheng Li, Nie Lin, Masashi Sugiyama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in fine-tuning Vision-Language Models (VLMs) have witnessed
the success of prompt tuning and adapter tuning, while the classic model
fine-tuning on inherent parameters seems to be overlooked. It is believed that
fine-tuning the parameters of VLMs with few-shot samples corrupts the
pre-trained knowledge since fine-tuning the CLIP model even degrades
performance. In this paper, we revisit this viewpoint, and propose a new
perspective: fine-tuning the specific parameters instead of all will uncover
the power of classic model fine-tuning on VLMs. Through our meticulous study,
we propose ClipFit, a simple yet effective method to fine-tune CLIP without
introducing any overhead of extra parameters. We demonstrate that by only
fine-tuning the specific bias terms and normalization layers, ClipFit can
improve the performance of zero-shot CLIP by 7.27\% average harmonic mean
accuracy. Lastly, to understand how fine-tuning in CLIPFit affects the
pre-trained models, we conducted extensive experimental analyses w.r.t. changes
in internal parameters and representations. We found that low-level text bias
layers and the first layer normalization layer change much more than other
layers. The code is available at \url{https://github.com/minglllli/CLIPFit}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Numerical Approximation Capacity of Neural Networks with Bounded
  Parameters: Do Limits Exist, and How Can They Be Measured? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16697v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16697v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Liu, Tengchao Yu, Heng Yong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Universal Approximation Theorem posits that neural networks can
theoretically possess unlimited approximation capacity with a suitable
activation function and a freely chosen or trained set of parameters. However,
a more practical scenario arises when these neural parameters, especially the
nonlinear weights and biases, are bounded. This leads us to question:
\textbf{Does the approximation capacity of a neural network remain universal,
or does it have a limit when the parameters are practically bounded? And if it
has a limit, how can it be measured?}
  Our theoretical study indicates that while universal approximation is
theoretically feasible, in practical numerical scenarios, Deep Neural Networks
(DNNs) with any analytic activation functions (such as Tanh and Sigmoid) can
only be approximated by a finite-dimensional vector space under a bounded
nonlinear parameter space (NP space), whether in a continuous or discrete
sense. Based on this study, we introduce the concepts of \textit{$\epsilon$
outer measure} and \textit{Numerical Span Dimension (NSdim)} to quantify the
approximation capacity limit of a family of networks both theoretically and
practically.
  Furthermore, drawing on our new theoretical study and adopting a fresh
perspective, we strive to understand the relationship between back-propagation
neural networks and random parameter networks (such as the Extreme Learning
Machine (ELM)) with both finite and infinite width. We also aim to provide
fresh insights into regularization, the trade-off between width and depth,
parameter space, width redundancy, condensation, and other related important
issues.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Universal Approximation; Bounded Weights; Analytic Function;
  Numerical Span Dimension; Infinite Width Neural Network}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> of Low-bit Large Language Models: Basics, Systems, and
  Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16694v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16694v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruihao Gong, Yifu Ding, Zining Wang, Chengtao Lv, Xingyu Zheng, Jinyang Du, Haotong Qin, Jinyang Guo, Michele Magno, Xianglong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved remarkable advancements in natural
language processing, showcasing exceptional performance across various tasks.
However, the expensive memory and computational requirements present
significant challenges for their practical deployment. Low-bit quantization has
emerged as a critical approach to mitigate these challenges by reducing the
bit-width of model parameters, activations, and gradients, thus decreasing
memory usage and computational demands. This paper presents a comprehensive
survey of low-bit quantization methods tailored for LLMs, covering the
fundamental principles, system implementations, and algorithmic strategies. An
overview of basic concepts and new data formats specific to low-bit LLMs is
first introduced, followed by a review of frameworks and systems that
facilitate low-bit LLMs across various hardware platforms. Then, we categorize
and analyze techniques and toolkits for efficient low-bit training and
inference of LLMs. Finally, we conclude with a discussion of future trends and
potential advancements of low-bit LLMs. Our systematic overview from basic,
system, and algorithm perspectives can offer valuable insights and guidelines
for future works to enhance the efficiency and applicability of LLMs through
low-bit quantization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ruihao Gong leads the overall organization of the survey, with Yifu
  Ding and Jinyang Du contributing to Sections 2 and 3. Xingyu Zheng is
  responsible for authoring Section 4, while Chengtao Lv and Zining Wang
  collaborate on Section 5. Haotong Qin, Jinyang Guo, Michele Magno, and
  Xianglong Liu provide guidance during the whole process and assist in
  refining the final manuscript</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Layout-Corrector: Alleviating Layout Sticking Phenomenon in Discrete
  Diffusion Model <span class="chip">ECCV2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16689v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16689v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shoma Iwai, Atsuki Osanai, Shunsuke Kitada, Shinichiro Omachi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Layout generation is a task to synthesize a harmonious layout with elements
characterized by attributes such as category, position, and size. Human
designers experiment with the placement and modification of elements to create
aesthetic layouts, however, we observed that current discrete diffusion models
(DDMs) struggle to correct inharmonious layouts after they have been generated.
In this paper, we first provide novel insights into layout sticking phenomenon
in DDMs and then propose a simple yet effective layout-assessment module
Layout-Corrector, which works in conjunction with existing DDMs to address the
layout sticking problem. We present a learning-based module capable of
identifying inharmonious elements within layouts, considering overall layout
harmony characterized by complex composition. During the generation process,
Layout-Corrector evaluates the correctness of each token in the generated
layout, reinitializing those with low scores to the ungenerated state. The DDM
then uses the high-scored tokens as clues to regenerate the harmonized tokens.
Layout-Corrector, tested on common benchmarks, consistently boosts
layout-generation performance when in conjunction with various state-of-the-art
DDMs. Furthermore, our extensive analysis demonstrates that the
Layout-Corrector (1) successfully identifies erroneous tokens, (2) facilitates
control over the fidelity-diversity trade-off, and (3) significantly mitigates
the performance drop associated with fast sampling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV2024, Project Page:
  https://iwa-shi.github.io/Layout-Corrector-Project-Page/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Erase then Rectify: A Training-Free Parameter Editing Approach for
  Cost-Effective Graph Unlearning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16684v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16684v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe-Rui Yang, Jindong Han, Chang-Dong Wang, Hao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph unlearning, which aims to eliminate the influence of specific nodes,
edges, or attributes from a trained Graph Neural Network (GNN), is essential in
applications where privacy, bias, or data obsolescence is a concern. However,
existing graph unlearning techniques often necessitate additional training on
the remaining data, leading to significant computational costs, particularly
with large-scale graphs. To address these challenges, we propose a two-stage
training-free approach, Erase then Rectify (ETR), designed for efficient and
scalable graph unlearning while preserving the model utility. Specifically, we
first build a theoretical foundation showing that masking parameters critical
for unlearned samples enables effective unlearning. Building on this insight,
the Erase stage strategically edits model parameters to eliminate the impact of
unlearned samples and their propagated influence on intercorrelated nodes. To
further ensure the GNN's utility, the Rectify stage devises a gradient
approximation method to estimate the model's gradient on the remaining dataset,
which is then used to enhance model performance. Overall, ETR achieves graph
unlearning without additional training or full training data access,
significantly reducing computational overhead and preserving data privacy.
Extensive experiments on seven public datasets demonstrate the consistent
superiority of ETR in model utility, unlearning efficiency, and unlearning
effectiveness, establishing it as a promising solution for real-world graph
unlearning challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TSBP: Improving Object Detection in Histology Images via Test-time
  Self-guided Bounding-box Propagation <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tingting Yang, Liang Xiao, Yizhe Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A global threshold (e.g., 0.5) is often applied to determine which bounding
boxes should be included in the final results for an object detection task. A
higher threshold reduces false positives but may result in missing a
significant portion of true positives. A lower threshold can increase detection
recall but may also result in more false positives. Because of this, using a
preset global threshold (e.g., 0.5) applied to all the bounding box candidates
may lead to suboptimal solutions. In this paper, we propose a Test-time
Self-guided Bounding-box Propagation (TSBP) method, leveraging Earth Mover's
Distance (EMD) to enhance object detection in histology images. TSBP utilizes
bounding boxes with high confidence to influence those with low confidence,
leveraging visual similarities between them. This propagation mechanism enables
bounding boxes to be selected in a controllable, explainable, and robust
manner, which surpasses the effectiveness of using simple thresholds and
uncertainty calibration methods. Importantly, TSBP does not necessitate
additional labeled samples for model training or parameter estimation, unlike
calibration methods. We conduct experiments on gland detection and cell
detection tasks in histology images. The results show that our proposed TSBP
significantly improves detection outcomes when working in conjunction with
state-of-the-art deep learning-based detection networks. Compared to other
methods such as uncertainty calibration, TSBP yields more robust and accurate
object detection predictions while using no additional labeled samples. The
code is available at https://github.com/jwhgdeu/TSBP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CryptoTrain: Fast Secure Training on Encrypted Datase <span class="chip">CCS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16675v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16675v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Xue, Yancheng Zhang, Yanshan Wang, Xueqiang Wang, Hao Zheng, Qian Lou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Secure training, while protecting the confidentiality of both data and model
weights, typically incurs significant training overhead. Traditional Fully
Homomorphic Encryption (FHE)-based non-inter-active training models are heavily
burdened by computationally demanding bootstrapping. To develop an efficient
secure training system, we established a foundational framework, CryptoTrain-B,
utilizing a hybrid cryptographic protocol that merges FHE with Oblivious
Transfer (OT) for handling linear and non-linear operations, respectively. This
integration eliminates the need for costly bootstrapping. Although
CryptoTrain-B sets a new baseline in performance, reducing its training
overhead remains essential. We found that ciphertext-ciphertext multiplication
(CCMul) is a critical bottleneck in operations involving encrypted inputs and
models. Our solution, the CCMul-Precompute technique, involves precomputing
CCMul offline and resorting to the less resource-intensive ciphertext-plaintext
multiplication (CPMul) during private training. Furthermore, conventional
polynomial convolution in FHE systems tends to encode irrelevant and redundant
values into polynomial slots, necessitating additional polynomials and
ciphertexts for input representation and leading to extra multiplications.
Addressing this, we introduce correlated polynomial convolution, which encodes
only related input values into polynomials, thus drastically reducing the
number of computations and overheads. By integrating CCMul-Precompute and
correlated polynomial convolution into CryptoTrain-B, we facilitate a rapid and
efficient secure training framework, CryptoTrain. Extensive experiments
demonstrate that CryptoTrain achieves a ~5.3X training time reduction compared
to prior methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CCS-LAMPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SWE2: SubWord Enriched and Significant Word Emphasized Framework for
  Hate Speech Detection <span class="chip">CIKM 2020</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16673v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16673v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanyi Mou, Pengyi Ye, Kyumin Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hate speech detection on online social networks has become one of the
emerging hot topics in recent years. With the broad spread and fast propagation
speed across online social networks, hate speech makes significant impacts on
society by increasing prejudice and hurting people. Therefore, there are
aroused attention and concern from both industry and academia. In this paper,
we address the hate speech problem and propose a novel hate speech detection
framework called SWE2, which only relies on the content of messages and
automatically identifies hate speech. In particular, our framework exploits
both word-level semantic information and sub-word knowledge. It is intuitively
persuasive and also practically performs well under a situation with/without
character-level adversarial attack. Experimental results show that our proposed
model achieves 0.975 accuracy and 0.953 macro F1, outperforming 7
state-of-the-art baselines under no adversarial attack. Our model robustly and
significantly performed well under extreme adversarial attack (manipulation of
50% messages), achieving 0.967 accuracy and 0.934 macro F1.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in CIKM 2020</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Wildlife Product Trading in Online Social Networks: A Case Study on
  Ivory-Related Product Sales Promotion Posts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16671v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16671v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanyi Mou, Yun Yue, Kyumin Lee, Ziming Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Wildlife trafficking (WLT) has emerged as a global issue, with traffickers
expanding their operations from offline to online platforms, utilizing
e-commerce websites and social networks to enhance their illicit trade. This
paper addresses the challenge of detecting and recognizing wildlife product
sales promotion behaviors in online social networks, a crucial aspect in
combating these environmentally harmful activities. To counter these
environmentally damaging illegal operations, in this research, we focus on
wildlife product sales promotion behaviors in online social networks.
Specifically, 1) A scalable dataset related to wildlife product trading is
collected using a network-based approach. This dataset is labeled through a
human-in-the-loop machine learning process, distinguishing positive class
samples containing wildlife product selling posts and hard-negatives
representing normal posts misclassified as potential WLT posts, subsequently
corrected by human annotators. 2) We benchmark the machine learning results on
the proposed dataset and build a practical framework that automatically
identifies suspicious wildlife selling posts and accounts, sufficiently
leveraging the multi-modal nature of online social networks. 3) This research
delves into an in-depth analysis of trading posts, shedding light on the
systematic and organized selling behaviors prevalent in the current landscape.
We provide detailed insights into the nature of these behaviors, contributing
valuable information for understanding and countering illegal wildlife product
trading.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICWSM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GraphLoRA: Structure-Aware Contrastive Low-Rank Adaptation for
  Cross-Graph Transfer Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16670v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16670v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe-Rui Yang, Jindong Han, Chang-Dong Wang, Hao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have demonstrated remarkable proficiency in
handling a range of graph analytical tasks across various domains, such as
e-commerce and social networks. Despite their versatility, GNNs face
significant challenges in transferability, limiting their utility in real-world
applications. Existing research in GNN transfer learning overlooks
discrepancies in distribution among various graph datasets, facing challenges
when transferring across different distributions. How to effectively adopt a
well-trained GNN to new graphs with varying feature and structural
distributions remains an under-explored problem. Taking inspiration from the
success of Low-Rank Adaptation (LoRA) in adapting large language models to
various domains, we propose GraphLoRA, an effective and parameter-efficient
method for transferring well-trained GNNs to diverse graph domains.
Specifically, we first propose a Structure-aware Maximum Mean Discrepancy
(SMMD) to align divergent node feature distributions across source and target
graphs. Moreover, we introduce low-rank adaptation by injecting a small
trainable GNN alongside the pre-trained one, effectively bridging structural
distribution gaps while mitigating the catastrophic forgetting. Additionally, a
structure-aware regularization objective is proposed to enhance the
adaptability of the pre-trained GNN to target graph with scarce supervision
labels. Extensive experiments on six real-world datasets demonstrate the
effectiveness of GraphLoRA against eleven baselines by tuning only 20% of
parameters, even across disparate graph domains. The code is available at
https://anonymous.4open.science/r/GraphLoRA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Covariate Shift in Imitation Learning for Autonomous Vehicles
  Using Latent Space Generative World Models <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16663v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16663v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Popov, Alperen Degirmenci, David Wehr, Shashank Hegde, Ryan Oldja, Alexey Kamenev, Bertrand Douillard, David Nistér, Urs Muller, Ruchi Bhargava, Stan Birchfield, Nikolai Smolyanskiy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose the use of latent space generative world models to address the
covariate shift problem in autonomous driving. A world model is a neural
network capable of predicting an agent's next state given past states and
actions. By leveraging a world model during training, the driving policy
effectively mitigates covariate shift without requiring an excessive amount of
training data. During end-to-end training, our policy learns how to recover
from errors by aligning with states observed in human demonstrations, so that
at runtime it can recover from perturbations outside the training distribution.
Additionally, we introduce a novel transformer-based perception encoder that
employs multi-view cross-attention and a learned scene query. We present
qualitative and quantitative results, demonstrating significant improvements
upon prior state of the art in closed-loop testing in the CARLA simulator, as
well as showing the ability to handle perturbations in both CARLA and NVIDIA's
DRIVE Sim.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 6 figures, for ICRA 2025 conference, for associated video
  file, see https://youtu.be/9FpDFD9aiFU</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Credibility <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16653v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16653v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ronald Richman, Salvatore Scognamiglio, Mario V. Wüthrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspired by the large success of Transformers in Large Language Models, these
architectures are increasingly applied to tabular data. This is achieved by
embedding tabular data into low-dimensional Euclidean spaces resulting in
similar structures as time-series data. We introduce a novel credibility
mechanism to this Transformer architecture. This credibility mechanism is based
on a special token that should be seen as an encoder that consists of a
credibility weighted average of prior information and observation based
information. We demonstrate that this novel credibility mechanism is very
beneficial to stabilize training, and our Credibility Transformer leads to
predictive models that are superior to state-of-the-art deep learning models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Representation for Multitask learning through Self Supervised
  Auxiliary learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16651v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16651v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seokwon Shin, Hyungrok Do, Youngdoo Son
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-task learning is a popular machine learning approach that enables
simultaneous learning of multiple related tasks, improving algorithmic
efficiency and effectiveness. In the hard parameter sharing approach, an
encoder shared through multiple tasks generates data representations passed to
task-specific predictors. Therefore, it is crucial to have a shared encoder
that provides decent representations for every and each task. However, despite
recent advances in multi-task learning, the question of how to improve the
quality of representations generated by the shared encoder remains open. To
address this gap, we propose a novel approach called Dummy Gradient norm
Regularization that aims to improve the universality of the representations
generated by the shared encoder. Specifically, the method decreases the norm of
the gradient of the loss function with repect to dummy task-specific predictors
to improve the universality of the shared encoder's representations. Through
experiments on multiple multi-task learning benchmark datasets, we demonstrate
that DGR effectively improves the quality of the shared representations,
leading to better multi-task prediction performances. Applied to various
classifiers, the shared representations generated by DGR also show superior
performance compared to existing multi-task learning methods. Moreover, our
approach takes advantage of computational efficiency due to its simplicity. The
simplicity also allows us to seamlessly integrate DGR with the existing
multi-task learning algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Domain-Independent Automatic Generation of Descriptive Texts for
  Time-Series Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16647v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16647v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kota Dohi, Aoi Ito, Harsh Purohit, Tomoya Nishida, Takashi Endo, Yohei Kawaguchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to scarcity of time-series data annotated with descriptive texts,
training a model to generate descriptive texts for time-series data is
challenging. In this study, we propose a method to systematically generate
domain-independent descriptive texts from time-series data. We identify two
distinct approaches for creating pairs of time-series data and descriptive
texts: the forward approach and the backward approach. By implementing the
novel backward approach, we create the Temporal Automated Captions for
Observations (TACO) dataset. Experimental results demonstrate that a
contrastive learning based model trained using the TACO dataset is capable of
generating descriptive texts for time-series data in novel domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Task Addition in Multi-Task Learning by Geometrical Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16645v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16645v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soorin Yim, Dae-Woong Jeong, Sung Moon Ko, Sumin Lee, Hyunseung Kim, Chanhui Lee, Sehui Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training deep learning models on limited data while maintaining
generalization is one of the fundamental challenges in molecular property
prediction. One effective solution is transferring knowledge extracted from
abundant datasets to those with scarce data. Recently, a novel algorithm called
Geometrically Aligned Transfer Encoder (GATE) has been introduced, which uses
soft parameter sharing by aligning the geometrical shapes of task-specific
latent spaces. However, GATE faces limitations in scaling to multiple tasks due
to computational costs. In this study, we propose a task addition approach for
GATE to improve performance on target tasks with limited data while minimizing
computational complexity. It is achieved through supervised multi-task
pre-training on a large dataset, followed by the addition and training of
task-specific modules for each target task. Our experiments demonstrate the
superior performance of the task addition strategy for GATE over conventional
multi-task methods, with comparable computational costs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 5 figures, Accepted at AI for Science Workshop at 41st
  International Conference on Machine Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Examining the Rat in the Tunnel: Interpretable Multi-Label
  Classification of Tor-based Malware 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16639v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16639v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ishan Karunanayake, Mashael AlSabah, Nadeem Ahmed, Sanjay Jha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite being the most popular privacy-enhancing network, Tor is increasingly
adopted by cybercriminals to obfuscate malicious traffic, hindering the
identification of malware-related communications between compromised devices
and Command and Control (C&C) servers. This malicious traffic can induce
congestion and reduce Tor's performance, while encouraging network
administrators to block Tor traffic. Recent research, however, demonstrates the
potential for accurately classifying captured Tor traffic as malicious or
benign. While existing efforts have addressed malware class identification,
their performance remains limited, with micro-average precision and recall
values around 70%. Accurately classifying specific malware classes is crucial
for effective attack prevention and mitigation. Furthermore, understanding the
unique patterns and attack vectors employed by different malware classes helps
the development of robust and adaptable defence mechanisms.
  We utilise a multi-label classification technique based on Message-Passing
Neural Networks, demonstrating its superiority over previous approaches such as
Binary Relevance, Classifier Chains, and Label Powerset, by achieving
micro-average precision (MAP) and recall (MAR) exceeding 90%. Compared to
previous work, we significantly improve performance by 19.98%, 10.15%, and
59.21% in MAP, MAR, and Hamming Loss, respectively. Next, we employ Explainable
Artificial Intelligence (XAI) techniques to interpret the decision-making
process within these models. Finally, we assess the robustness of all
techniques by crafting adversarial perturbations capable of manipulating
classifier predictions and generating false positives and negatives.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System
  Inferences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16633v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16633v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pingyi Huo, Anusha Devulapally, Hasan Al Maruf, Minseo Park, Krishnakumar Nair, Meena Arunachalam, Gulsum Gudukbay Akbulut, Mahmut Taylan Kandemir, Vijaykrishnan Narayanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Learning Recommendation Models (DLRMs) have become increasingly popular
and prevalent in today's datacenters, consuming most of the AI inference
cycles. The performance of DLRMs is heavily influenced by available bandwidth
due to their large vector sizes in embedding tables and concurrent accesses. To
achieve substantial improvements over existing solutions, novel approaches
towards DLRM optimization are needed, especially, in the context of emerging
interconnect technologies like CXL. This study delves into exploring
CXL-enabled systems, implementing a process-in-fabric-switch (PIFS) solution to
accelerate DLRMs while optimizing their memory and bandwidth scalability. We
present an in-depth characterization of industry-scale DLRM workloads running
on CXL-ready systems, identifying the predominant bottlenecks in existing CXL
systems. We, therefore, propose PIFS-Rec, a PIFS-based scheme that implements
near-data processing through downstream ports of the fabric switch. PIFS-Rec
achieves a latency that is 3.89x lower than Pond, an industry-standard
CXL-based system, and also outperforms BEACON, a state-of-the-art scheme, by
2.03x.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Functional Stochastic Gradient MCMC for Bayesian Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16632v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16632v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengjing Wu, Junyu Xuan, Jie Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classical variational inference for Bayesian neural networks (BNNs) in
parameter space usually suffers from unresolved prior issues such as knowledge
encoding intractability and pathological behaviors in deep networks, which
could lead to an improper posterior inference. Hence, functional variational
inference has been proposed recently to resolve these issues via stochastic
process priors. Beyond variational inference, stochastic gradient Markov Chain
Monte Carlo (SGMCMC) is another scalable and effective inference method for
BNNs to asymptotically generate samples from true posterior by simulating a
continuous dynamic. However, the existing SGMCMC methods only work in
parametric space, which has the same issues of parameter-space variational
inference, and extending the parameter-space dynamics to function-space
dynamics is not a trivial undertaking. In this paper, we introduce a new
functional SGMCMC scheme via newly designed diffusion dynamics, which can
incorporate more informative functional priors. Moreover, we prove that the
stationary distribution of these functional dynamics is the target posterior
distribution over functions. We demonstrate better performance in both accuracy
and uncertainty quantification of our functional SGMCMC on several tasks
compared with naive SGMCMC and functional variational inference methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stochastic Subsampling With Average Pooling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16630v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16630v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bum Jun Kim, Sang Woo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Regularization of deep neural networks has been an important issue to achieve
higher generalization performance without overfitting problems. Although the
popular method of Dropout provides a regularization effect, it causes
inconsistent properties in the output, which may degrade the performance of
deep neural networks. In this study, we propose a new module called stochastic
average pooling, which incorporates Dropout-like stochasticity in pooling. We
describe the properties of stochastic subsampling and average pooling and
leverage them to design a module without any inconsistency problem. The
stochastic average pooling achieves a regularization effect without any
potential performance degradation due to the inconsistency issue and can easily
be plugged into existing architectures of deep neural networks. Experiments
demonstrate that replacing existing average pooling with stochastic average
pooling yields consistent improvements across a variety of tasks, datasets, and
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ascend HiFloat8 Format for Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16626v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16626v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanyong Luo, Zhongxing Zhang, Richard Wu, Hu Liu, Ying Jin, Kai Zheng, Minmin Wang, Zhanying He, Guipeng Hu, Luyao Chen, Tianchi Hu, Junsong Wang, Minqi Chen, Mikhaylov Dmitry, Korviakov Vladimir, Bobrin Maxim, Yuhao Hu, Guanfu Chen, Zeyi Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This preliminary white paper proposes a novel 8-bit floating-point data
format HiFloat8 (abbreviated as HiF8) for deep learning. HiF8 features tapered
precision. For normal value encoding, it provides 7 exponents with 3-bit
mantissa, 8 exponents with 2-bit mantissa, and 16 exponents with 1-bit
mantissa. For denormal or subnormal value encoding, it extends the dynamic
range by 7 extra powers of 2, from 31 to 38 binades (notice that FP16 covers 40
binades). Meanwhile, HiF8 encodes all the special values except that positive
zero and negative zero are represented by only one bit-pattern. Thanks to the
better balance between precision and dynamic range, HiF8 can be simultaneously
used in both forward and backward passes of AI training. In this paper, we will
describe the definition and rounding methods of HiF8, as well as the tentative
training and inference solutions. To demonstrate the efficacy of HiF8 format,
massive simulation results on various neural networks, including traditional
neural networks and large language models (LLMs), will also be presented.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 Pages, 4 Figures, 9 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Random Forest Regression Feature Importance for Climate Impact Pathway
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16609v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16609v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meredith G. L. Brown, Matt Peterson, Irina Tezaur, Kara Peterson, Diana Bull
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Disturbances to the climate system, both natural and anthropogenic, have far
reaching impacts that are not always easy to identify or quantify using
traditional climate science analyses or causal modeling techniques. In this
paper, we develop a novel technique for discovering and ranking the chain of
spatio-temporal downstream impacts of a climate source, referred to herein as a
source-impact pathway, using Random Forest Regression (RFR) and SHapley
Additive exPlanation (SHAP) feature importances. Rather than utilizing RFR for
classification or regression tasks (the most common use case for RFR), we
propose a fundamentally new RFR-based workflow in which we: (i) train random
forest (RF) regressors on a set of spatio-temporal features of interest, (ii)
calculate their pair-wise feature importances using the SHAP weights associated
with those features, and (iii) translate these feature importances into a
weighted pathway network (i.e., a weighted directed graph), which can be used
to trace out and rank interdependencies between climate features and/or
modalities. We adopt a tiered verification approach to verify our new pathway
identification methodology. In this approach, we apply our method to ensembles
of data generated by running two increasingly complex benchmarks: (i) a set of
synthetic coupled equations, and (ii) a fully coupled simulation of the 1991
eruption of Mount Pinatubo in the Philippines performed using a modified
version 2 of the U.S. Department of Energy's Energy Exascale Earth System Model
(E3SMv2). We find that our RFR feature importance-based approach can accurately
detect known pathways of impact for both test cases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating and Enhancing Large Language Models for Novelty Assessment in
  Scholarly Publications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16605v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16605v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ethan Lin, Zhiyuan Peng, Yi Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have evaluated the creativity/novelty of large language models
(LLMs) primarily from a semantic perspective, using benchmarks from cognitive
science. However, accessing the novelty in scholarly publications is a largely
unexplored area in evaluating LLMs. In this paper, we introduce a scholarly
novelty benchmark (SchNovel) to evaluate LLMs' ability to assess novelty in
scholarly papers. SchNovel consists of 15000 pairs of papers across six fields
sampled from the arXiv dataset with publication dates spanning 2 to 10 years
apart. In each pair, the more recently published paper is assumed to be more
novel. Additionally, we propose RAG-Novelty, which simulates the review process
taken by human reviewers by leveraging the retrieval of similar papers to
assess novelty. Extensive experiments provide insights into the capabilities of
different LLMs to assess novelty and demonstrate that RAG-Novelty outperforms
recent baseline models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative <span class="highlight-title">Pre-train</span>ed Ranking Model with Over-parameterization at
  Web-Scale (Extended Abstract) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16594v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16594v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen Li, Haoyi Xiong, Linghe Kong, Jiang Bian, Shuaiqiang Wang, Guihai Chen, Dawei Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning to rank (LTR) is widely employed in web searches to prioritize
pertinent webpages from retrieved content based on input queries. However,
traditional LTR models encounter two principal obstacles that lead to
suboptimal performance: (1) the lack of well-annotated query-webpage pairs with
ranking scores covering a diverse range of search query popularities, which
hampers their ability to address queries across the popularity spectrum, and
(2) inadequately trained models that fail to induce generalized representations
for LTR, resulting in overfitting. To address these challenges, we propose a
\emph{\uline{G}enerative \uline{S}emi-\uline{S}upervised \uline{P}re-trained}
(GS2P) LTR model. We conduct extensive offline experiments on both a publicly
available dataset and a real-world dataset collected from a large-scale search
engine. Furthermore, we deploy GS2P in a large-scale web search engine with
realistic traffic, where we observe significant improvements in the real-world
application.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MambaJSCC: Adaptive Deep Joint Source-Channel Coding with Generalized
  State Space Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16592v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16592v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tong Wu, Zhiyong Chen, Meixia Tao, Yaping Sun, Xiaodong Xu, Wenjun Zhang, Ping Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lightweight and efficient neural network models for deep joint source-channel
coding (JSCC) are crucial for semantic communications. In this paper, we
propose a novel JSCC architecture, named MambaJSCC, that achieves
state-of-the-art performance with low computational and parameter overhead.
MambaJSCC utilizes the visual state space model with channel adaptation
(VSSM-CA) blocks as its backbone for transmitting images over wireless
channels, where the VSSM-CA primarily consists of the generalized state space
models (GSSM) and the zero-parameter, zero-computational channel adaptation
method (CSI-ReST). We design the GSSM module, leveraging reversible matrix
transformations to express generalized scan expanding operations, and
theoretically prove that two GSSM modules can effectively capture global
information. We discover that GSSM inherently possesses the ability to adapt to
channels, a form of endogenous intelligence. Based on this, we design the
CSI-ReST method, which injects channel state information (CSI) into the initial
state of GSSM to utilize its native response, and into the residual state to
mitigate CSI forgetting, enabling effective channel adaptation without
introducing additional computational and parameter overhead. Experimental
results show that MambaJSCC not only outperforms existing JSCC methods (e.g.,
SwinJSCC) across various scenarios but also significantly reduces parameter
size, computational overhead, and inference delay.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IEEE Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Pre-train</span>ed Graphformer-based Ranking at Web-scale Search (Extended
  Abstract) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16590v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16590v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen Li, Haoyi Xiong, Linghe Kong, Zeyi Sun, Hongyang Chen, Shuaiqiang Wang, Dawei Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Both Transformer and Graph Neural Networks (GNNs) have been employed in the
domain of learning to rank (LTR). However, these approaches adhere to two
distinct yet complementary problem formulations: ranking score regression based
on query-webpage pairs, and link prediction within query-webpage bipartite
graphs, respectively. While it is possible to pre-train GNNs or Transformers on
source datasets and subsequently fine-tune them on sparsely annotated LTR
datasets, the distributional shifts between the pair-based and bipartite graph
domains present significant challenges in integrating these heterogeneous
models into a unified LTR framework at web scale. To address this, we introduce
the novel MPGraf model, which leverages a modular and capsule-based
pre-training strategy, aiming to cohesively integrate the regression
capabilities of Transformers with the link prediction strengths of GNNs. We
conduct extensive offline and online experiments to rigorously evaluate the
performance of MPGraf.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoSTF: Decoupled Neural Architecture Search for Cost-Effective
  Automated Spatio-Temporal Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16586v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16586v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tengfei Lyu, Weijia Zhang, Jinliang Deng, Hao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatio-temporal forecasting is a critical component of various smart city
applications, such as transportation optimization, energy management, and
socio-economic analysis. Recently, several automated spatio-temporal
forecasting methods have been proposed to automatically search the optimal
neural network architecture for capturing complex spatio-temporal dependencies.
However, the existing automated approaches suffer from expensive neural
architecture search overhead, which hinders their practical use and the further
exploration of diverse spatio-temporal operators in a finer granularity. In
this paper, we propose AutoSTF, a decoupled automatic neural architecture
search framework for cost-effective automated spatio-temporal forecasting. From
the efficiency perspective, we first decouple the mixed search space into
temporal space and spatial space and respectively devise representation
compression and parameter-sharing schemes to mitigate the parameter explosion.
The decoupled spatio-temporal search not only expedites the model optimization
process but also leaves new room for more effective spatio-temporal dependency
modeling. From the effectiveness perspective, we propose a multi-patch transfer
module to jointly capture multi-granularity temporal dependencies and extend
the spatial search space to enable finer-grained layer-wise spatial dependency
search. Extensive experiments on eight datasets demonstrate the superiority of
AutoSTF in terms of both accuracy and efficiency. Specifically, our proposed
method achieves up to 13.48x speed-up compared to state-of-the-art automatic
spatio-temporal forecasting methods while maintaining the best forecasting
accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FLaRe: Achieving Masterful and Adaptive Robot Policies with Large-Scale
  Reinforcement Learning Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16578v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16578v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaheng Hu, Rose Hendrix, Ali Farhadi, Aniruddha Kembhavi, Roberto Martin-Martin, Peter Stone, Kuo-Hao Zeng, Kiana Ehsan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the Robotics field has initiated several efforts toward
building generalist robot policies through large-scale multi-task Behavior
Cloning. However, direct deployments of these policies have led to
unsatisfactory performance, where the policy struggles with unseen states and
tasks. How can we break through the performance plateau of these models and
elevate their capabilities to new heights? In this paper, we propose FLaRe, a
large-scale Reinforcement Learning fine-tuning framework that integrates robust
pre-trained representations, large-scale training, and gradient stabilization
techniques. Our method aligns pre-trained policies towards task completion,
achieving state-of-the-art (SoTA) performance both on previously demonstrated
and on entirely novel tasks and embodiments. Specifically, on a set of
long-horizon mobile manipulation tasks, FLaRe achieves an average success rate
of 79.5% in unseen environments, with absolute improvements of +23.6% in
simulation and +30.7% on real robots over prior SoTA methods. By utilizing only
sparse rewards, our approach can enable generalizing to new capabilities beyond
the pretraining data with minimal human effort. Moreover, we demonstrate rapid
adaptation to new embodiments and behaviors with less than a day of
fine-tuning. Videos can be found on the project website at
https://robot-flare.github.io/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient and generalizable nested Fourier-DeepONet for
  three-dimensional geological carbon sequestration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16572v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16572v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan E. Lee, Min Zhu, Ziqiao Xi, Kun Wang, Yanhua O. Yuan, Lu Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Geological carbon sequestration (GCS) involves injecting CO$_2$ into
subsurface geological formations for permanent storage. Numerical simulations
could guide decisions in GCS projects by predicting CO$_2$ migration pathways
and the pressure distribution in storage formation. However, these simulations
are often computationally expensive due to highly coupled physics and large
spatial-temporal simulation domains. Surrogate modeling with data-driven
machine learning has become a promising alternative to accelerate physics-based
simulations. Among these, the Fourier neural operator (FNO) has been applied to
three-dimensional synthetic subsurface models. Here, to further improve
performance, we have developed a nested Fourier-DeepONet by combining the
expressiveness of the FNO with the modularity of a deep operator network
(DeepONet). This new framework is twice as efficient as a nested FNO for
training and has at least 80% lower GPU memory requirement due to its
flexibility to treat temporal coordinates separately. These performance
improvements are achieved without compromising prediction accuracy. In
addition, the generalization and extrapolation ability of nested
Fourier-DeepONet beyond the training range has been thoroughly evaluated.
Nested Fourier-DeepONet outperformed the nested FNO for extrapolation in time
with more than 50% reduced error. It also exhibited good extrapolation accuracy
beyond the training range in terms of reservoir properties, number of wells,
and injection rate.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EMIT- Event-Based Masked Auto Encoding for Irregular Time Series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16554v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16554v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hrishikesh Patel, Ruihong Qiu, Adam Irwin, Shazia Sadiq, Sen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Irregular time series, where data points are recorded at uneven intervals,
are prevalent in healthcare settings, such as emergency wards where vital signs
and laboratory results are captured at varying times. This variability, which
reflects critical fluctuations in patient health, is essential for informed
clinical decision-making. Existing self-supervised learning research on
irregular time series often relies on generic pretext tasks like forecasting,
which may not fully utilise the signal provided by irregular time series. There
is a significant need for specialised pretext tasks designed for the
characteristics of irregular time series to enhance model performance and
robustness, especially in scenarios with limited data availability. This paper
proposes a novel pretraining framework, EMIT, an event-based masking for
irregular time series. EMIT focuses on masking-based reconstruction in the
latent space, selecting masking points based on the rate of change in the data.
This method preserves the natural variability and timing of measurements while
enhancing the model's ability to process irregular intervals without losing
essential information. Extensive experiments on the MIMIC-III and PhysioNet
Challenge datasets demonstrate the superior performance of our event-based
masking strategy. The code has been released at
https://github.com/hrishi-ds/EMIT .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned
  Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16546v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16546v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Tan, Haoze Wang, Chao Yan, Yangdong Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model quantization has become a crucial technique to address the issues of
large memory consumption and long inference times associated with LLMs.
Mixed-precision quantization, which distinguishes between important and
unimportant parameters, stands out among numerous quantization schemes as it
achieves a balance between precision and compression rate. However, existing
approaches can only identify important parameters through qualitative analysis
and manual experiments without quantitatively analyzing how their importance is
determined. We propose a new criterion, so-called 'precision alignment', to
build a quantitative framework to holistically evaluate the importance of
parameters in mixed-precision quantization. Our observations on floating point
addition under various real-world scenarios suggest that two addends should
have identical precision, otherwise the information in the higher-precision
number will be wasted. Such an observation offers an essential principle to
determine the precision of each parameter in matrix multiplication operation.
As the first step towards applying the above discovery to large model
inference, we develop a dynamic KV-Cache quantization technique to effectively
reduce memory access latency. Different from existing quantization approaches
that focus on memory saving, this work directly aims to accelerate LLM
inference through quantifying floating numbers. The proposed technique attains
a 25% saving of memory access and delivers up to 1.3x speedup in the
computation of attention in the decoding phase of LLM, with almost no loss of
precision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Monge-Kantorovich Fitting With Sobolev Budgets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16541v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16541v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Forest Kobayashi, Jonathan Hayase, Young-Heon Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of finding the ``best'' approximation of an
$n$-dimensional probability measure $\rho$ using a measure $\nu$ whose support
is parametrized by $f : \mathbb{R}^m \to \mathbb{R}^n$ where $m < n$. We
quantify the performance of the approximation with the Monge-Kantorovich
$p$-cost (also called the Wasserstein $p$-cost) $\mathbb{W}_p^p(\rho, \nu)$,
and constrain the complexity of the approximation by bounding the $W^{k,q}$
Sobolev norm of $f$, which acts as a ``budget.'' We may then reformulate the
problem as minimizing a functional $\mathscr{J}_p(f)$ under a constraint on the
Sobolev budget.
  We treat general $k \geq 1$ for the Sobolev differentiability order (though
$q, m$ are chosen to restrict $W^{k,q}$ to the supercritical regime $k q > m$
to guarantee existence of optimizers). The problem is closely related to (but
distinct from) principal curves with length constraints when $m=1, k = 1$ and
smoothing splines when $k > 1$. New aspects and challenges arise from the
higher order differentiability condition.
  We study the gradient of $\mathscr{J}_p$, which is given by a vector field
along $f$ we call the barycenter field. We use it to construct improvements to
a given $f$, which gives a nontrivial (almost) strict monotonicty relation
between the functional $\mathscr{J}_p$ and the Sobolev budget. We also provide
a natural discretization scheme and establish its consistency. We use this
scheme to model a generative learning task; in particular, we demonstrate that
adding a constraint like ours as a soft penalty yields substantial improvement
in training a GAN to produce images of handwritten digits, with performance
competitive with weight-decay.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>68 pages, 23 figures, 50 pages without figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Source-Free Domain Adaptation for YOLO Object Detection <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16538v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16538v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Varailhon, Masih Aminbeidokhti, Marco Pedersoli, Eric Granger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Source-free domain adaptation (SFDA) is a challenging problem in object
detection, where a pre-trained source model is adapted to a new target domain
without using any source domain data for privacy and efficiency reasons. Most
state-of-the-art SFDA methods for object detection have been proposed for
Faster-RCNN, a detector that is known to have high computational complexity.
This paper focuses on domain adaptation techniques for real-world vision
systems, particularly for the YOLO family of single-shot detectors known for
their fast baselines and practical applications. Our proposed SFDA method -
Source-Free YOLO (SF-YOLO) - relies on a teacher-student framework in which the
student receives images with a learned, target domain-specific augmentation,
allowing the model to be trained with only unlabeled target data and without
requiring feature alignment. A challenge with self-training using a
mean-teacher architecture in the absence of labels is the rapid decline of
accuracy due to noisy or drifting pseudo-labels. To address this issue, a
teacher-to-student communication mechanism is introduced to help stabilize the
training and reduce the reliance on annotated target data for model selection.
Despite its simplicity, our approach is competitive with state-of-the-art
detectors on several challenging benchmark datasets, even sometimes
outperforming methods that use source data for adaptation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024: European Conference on Computer Vision - Workshop on
  Out-of-Distribution Generalization in Computer Vision Foundation Models,
  Milan Italy</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A QoE-Aware Split Inference Accelerating Algorithm for NOMA-based Edge
  Intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Yuan, Ning Li, Quan Chen, Wenchao Xu, Zhaoxin Zhang, Song Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Even the AI has been widely used and significantly changed our life,
deploying the large AI models on resource limited edge devices directly is not
appropriate. Thus, the model split inference is proposed to improve the
performance of edge intelligence, in which the AI model is divided into
different sub models and the resource-intensive sub model is offloaded to edge
server wirelessly for reducing resource requirements and inference latency.
However, the previous works mainly concentrate on improving and optimizing the
system QoS, ignore the effect of QoE which is another critical item for the
users except for QoS. Even the QoE has been widely learned in EC, considering
the differences between task offloading in EC and split inference in EI, and
the specific issues in QoE which are still not addressed in EC and EI, these
algorithms cannot work effectively in edge split inference scenarios. Thus, an
effective resource allocation algorithm is proposed in this paper, for
accelerating split inference in EI and achieving the tradeoff between inference
delay, QoE, and resource consumption, abbreviated as ERA. Specifically, the ERA
takes the resource consumption, QoE, and inference latency into account to find
the optimal model split strategy and resource allocation strategy. Since the
minimum inference delay and resource consumption, and maximum QoE cannot be
satisfied simultaneously, the gradient descent based algorithm is adopted to
find the optimal tradeoff between them. Moreover, the loop iteration GD
approach is developed to reduce the complexity of the GD algorithm caused by
parameter discretization. Additionally, the properties of the proposed
algorithms are investigated, including convergence, complexity, and
approximation error. The experimental results demonstrate that the performance
of ERA is much better than that of the previous studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16pages, 19figures. arXiv admin note: substantial text overlap with
  arXiv:2312.15850</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Concise Mathematical Description of Active Inference in Discrete Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.07726v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.07726v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jesse van Oostrum, Carlotta Langer, Nihat Ay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we present a concise mathematical description of active
inference in discrete time. The main part of the paper serves as a basic
introduction to the topic, including a detailed example illustrating the theory
on action selection. In the appendix the more subtle mathematical details are
discussed. This part is aimed at readers who have already studied the active
inference literature but struggle to make sense of the mathematical details and
derivations. Throughout the whole manuscript, special attention has been paid
to adopting notation that is both precise and in line with standard
mathematical texts. All equations and derivations are linked to specific
equation numbers in other popular text on the topic. Furthermore, Python code
is provided that implements the action selection mechanism described in this
paper and is compatible with pymdp environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Pre-train</span>ed Language Models Do Not Help Auto-regressive Text-to-Image
  Generation <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.16201v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.16201v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhui Zhang, Brandon McKinzie, Zhe Gan, Vaishaal Shankar, Alexander Toshev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in image tokenizers, such as VQ-VAE, have enabled
text-to-image generation using auto-regressive methods, similar to language
modeling. However, these methods have yet to leverage pre-trained language
models, despite their adaptability to various downstream tasks. In this work,
we explore this gap by adapting a pre-trained language model for
auto-regressive text-to-image generation, and find that pre-trained language
models offer limited help. We provide a two-fold explanation by analyzing
tokens from each modality. First, we demonstrate that image tokens possess
significantly different semantics compared to text tokens, rendering
pre-trained language models no more effective in modeling them than randomly
initialized ones. Second, the text tokens in the image-text datasets are too
simple compared to normal language model pre-training data, which causes the
catastrophic degradation of language models' capability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at EMNLP 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simple Image Signal Processing using Global Context Guidance <span class="chip">ICIP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.11569v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.11569v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omar Elezabi, Marcos V. Conde, Radu Timofte
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In modern smartphone cameras, the Image Signal Processor (ISP) is the core
element that converts the RAW readings from the sensor into perceptually
pleasant RGB images for the end users. The ISP is typically proprietary and
handcrafted and consists of several blocks such as white balance, color
correction, and tone mapping. Deep learning-based ISPs aim to transform RAW
images into DSLR-like RGB images using deep neural networks. However, most
learned ISPs are trained using patches (small regions) due to computational
limitations. Such methods lack global context, which limits their efficacy on
full-resolution images and harms their ability to capture global properties
such as color constancy or illumination. First, we propose a novel module that
can be integrated into any neural ISP to capture the global context information
from the full RAW images. Second, we propose an efficient and simple neural ISP
that utilizes our proposed module. Our model achieves state-of-the-art results
on different benchmarks using diverse and real smartphone images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE International Conference on Image Processing (ICIP) 2024 - Oral
  Presentation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data-Driven Room Acoustic Modeling Via Differentiable Feedback Delay
  Networks With Learnable Delay Lines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.00082v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.00082v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Ilic Mezza, Riccardo Giampiccolo, Enzo De Sena, Alberto Bernardini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the past few decades, extensive research has been devoted to the design
of artificial reverberation algorithms aimed at emulating the room acoustics of
physical environments. Despite significant advancements, automatic parameter
tuning of delay-network models remains an open challenge. We introduce a novel
method for finding the parameters of a Feedback Delay Network (FDN) such that
its output renders target attributes of a measured room impulse response. The
proposed approach involves the implementation of a differentiable FDN with
trainable delay lines, which, for the first time, allows us to simultaneously
learn each and every delay-network parameter via backpropagation. The iterative
optimization process seeks to minimize a perceptually-motivated time-domain
loss function incorporating differentiable terms accounting for energy decay
and echo density. Through experimental validation, we show that the proposed
method yields time-invariant frequency-independent FDNs capable of closely
matching the desired acoustical characteristics, and outperforms existing
methods based on genetic algorithms and analytical FDN design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The article is scheduled to be published in EURASIP Journal on Audio,
  Speech, and Music Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Benchmarking Cognitive Biases in Large Language Models as Evaluators <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.17012v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.17012v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park, Zae Myung Kim, Dongyeop Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models are cognitively biased judges. Large Language Models
(LLMs) have recently been shown to be effective as automatic evaluators with
simple prompting and in-context learning. In this work, we assemble 15 LLMs of
four different size ranges and evaluate their output responses by preference
ranking from the other LLMs as evaluators, such as System Star is better than
System Square. We then evaluate the quality of ranking outputs introducing the
Cognitive Bias Benchmark for LLMs as Evaluators (CoBBLEr), a benchmark to
measure six different cognitive biases in LLM evaluation outputs, such as the
Egocentric bias where a model prefers to rank its own outputs highly in
evaluation. We find that LLMs are biased text quality evaluators, exhibiting
strong indications on our bias benchmark (average of 40% of comparisons across
all models) within each of their evaluations that question their robustness as
evaluators. Furthermore, we examine the correlation between human and machine
preferences and calculate the average Rank-Biased Overlap (RBO) score to be
49.6%, indicating that machine preferences are misaligned with humans.
According to our findings, LLMs may still be unable to be utilized for
automatic annotation aligned with human preferences. Our project page is at:
https://minnesotanlp.github.io/cobbler.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Publishsed at ACL 2024. 29 pages, 9 figures, 14 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable Learning of Segment-Level Traffic Congestion Functions <span class="chip">SC 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.06080v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.06080v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shushman Choudhury, Abdul Rahman Kreidieh, Iveel Tsogsuren, Neha Arora, Carolina Osorio, Alexandre Bayen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose and study a data-driven framework for identifying traffic
congestion functions (numerical relationships between observations of traffic
variables) at global scale and segment-level granularity. In contrast to
methods that estimate a separate set of parameters for each roadway, ours
learns a single black-box function over all roadways in a metropolitan area.
First, we pool traffic data from all segments into one dataset, combining
static attributes with dynamic time-dependent features. Second, we train a
feed-forward neural network on this dataset, which we can then use on any
segment in the area. We evaluate how well our framework identifies congestion
functions on observed segments and how it generalizes to unobserved segments
and predicts segment attributes on a large dataset covering multiple cities
worldwide. For identification error on observed segments, our single
data-driven congestion function compares favorably to segment-specific
model-based functions on highway roads, but has room to improve on arterial
roads. For generalization, our approach shows strong performance across cities
and road types: both on unobserved segments in the same city and on zero-shot
transfer learning between cities. Finally, for predicting segment attributes,
we find that our approach can approximate critical densities for individual
segments using their static properties.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at IEEE ITSC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Error-Bounded Hierarchical Matrices for Efficient Neural
  Network Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07028v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07028v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Mango, Ronald Katende
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a dynamic, error-bounded hierarchical matrix (H-matrix)
compression method tailored for Physics-Informed Neural Networks (PINNs). The
proposed approach reduces the computational complexity and memory demands of
large-scale physics-based models while preserving the essential properties of
the Neural Tangent Kernel (NTK). By adaptively refining hierarchical matrix
approximations based on local error estimates, our method ensures efficient
training and robust model performance. Empirical results demonstrate that this
technique outperforms traditional compression methods, such as Singular Value
Decomposition (SVD), pruning, and quantization, by maintaining high accuracy
and improving generalization capabilities. Additionally, the dynamic H-matrix
method enhances inference speed, making it suitable for real-time applications.
This approach offers a scalable and efficient solution for deploying PINNs in
complex scientific and engineering domains, bridging the gap between
computational feasibility and real-world applicability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Looped <span class="highlight-title">Transformer</span>s for Length Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15647v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15647v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ying Fan, Yilun Du, Kannan Ramchandran, Kangwook Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has shown that Transformers trained from scratch can successfully
solve various arithmetic and algorithmic tasks, such as adding numbers and
computing parity. While these Transformers generalize well on unseen inputs of
the same length, they struggle with length generalization, i.e., handling
inputs of unseen lengths. In this work, we demonstrate that looped Transformers
with an adaptive number of steps significantly improve length generalization.
We focus on tasks with a known iterative solution, involving multiple
iterations of a RASP-L operation - a length-generalizable operation that can be
expressed by a finite-sized Transformer. We train looped Transformers using our
proposed learning algorithm and observe that they learn highly
length-generalizable solutions for various tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Force-Guided Bridge Matching for Full-Atom Time-Coarsened Dynamics of
  Peptides 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.15126v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.15126v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Yu, Wenbing Huang, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Molecular Dynamics (MD) is crucial in various fields such as materials
science, chemistry, and pharmacology to name a few. Conventional MD software
struggles with the balance between time cost and prediction accuracy, which
restricts its wider application. Recently, data-driven approaches based on deep
generative models have been devised for time-coarsened dynamics, which aim at
learning dynamics of diverse molecular systems over a long timestep, enjoying
both universality and efficiency. Nevertheless, most current methods are
designed solely to learn from the data distribution regardless of the
underlying Boltzmann distribution, and the physics priors such as energies and
forces are constantly overlooked. In this work, we propose a conditional
generative model called Force-guided Bridge Matching (FBM), which learns
full-atom time-coarsened dynamics and targets the Boltzmann-constrained
distribution. With the guidance of our delicately-designed intermediate force
field, FBM leverages favourable physics priors into the generation process,
giving rise to enhanced simulations. Experiments on two datasets consisting of
peptides verify our superiority in terms of comprehensive metrics and
demonstrate transferability to unseen systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The $μ\mathcal{G}$ Language for Programming Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.09441v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.09441v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Belenchia, Flavio Corradini, Michela Quadrini, Michele Loreti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks form a class of deep learning architectures
specifically designed to work with graph-structured data. As such, they share
the inherent limitations and problems of deep learning, especially regarding
the issues of explainability and trustworthiness. We propose $\mu\mathcal{G}$,
an original domain-specific language for the specification of graph neural
networks that aims to overcome these issues. The language's syntax is
introduced, and its meaning is rigorously defined by a denotational semantics.
An equivalent characterization in the form of an operational semantics is also
provided and, together with a type system, is used to prove the type soundness
of $\mu\mathcal{G}$. We show how $\mu\mathcal{G}$ programs can be represented
in a more user-friendly graphical visualization, and provide examples of its
generality by showing how it can be used to define some of the most popular
graph neural network models, or to develop any custom graph processing
application.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fourier neural operators for spatiotemporal dynamics in two-dimensional
  turbulence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14660v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14660v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Atif, Pulkit Dubey, Pratik P. Aghor, Vanessa Lopez-Marrero, Tao Zhang, Abdullah Sharfuddin, Kwangmin Yu, Fan Yang, Foluso Ladeinde, Yangang Liu, Meifeng Lin, Lingda Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-fidelity direct numerical simulation of turbulent flows for most
real-world applications remains an outstanding computational challenge. Several
machine learning approaches have recently been proposed to alleviate the
computational cost even though they become unstable or unphysical for long time
predictions. We identify that the Fourier neural operator (FNO) based models
combined with a partial differential equation (PDE) solver can accelerate fluid
dynamic simulations and thus address computational expense of large-scale
turbulence simulations. We treat the FNO model on the same footing as a PDE
solver and answer important questions about the volume and temporal resolution
of data required to build pre-trained models for turbulence. We also discuss
the pitfalls of purely data-driven approaches that need to be avoided by the
machine learning models to become viable and competitive tools for long time
simulations of turbulence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-UAV Pursuit-Evasion with Online Planning in Unknown Environments
  by Deep Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15866v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15866v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayu Chen, Chao Yu, Guosheng Li, Wenhao Tang, Xinyi Yang, Botian Xu, Huazhong Yang, Yu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-UAV pursuit-evasion, where pursuers aim to capture evaders, poses a key
challenge for UAV swarm intelligence. Multi-agent reinforcement learning (MARL)
has demonstrated potential in modeling cooperative behaviors, but most RL-based
approaches remain constrained to simplified simulations with limited dynamics
or fixed scenarios. Previous attempts to deploy RL policy to real-world
pursuit-evasion are largely restricted to two-dimensional scenarios, such as
ground vehicles or UAVs at fixed altitudes. In this paper, we address multi-UAV
pursuit-evasion by considering UAV dynamics and physical constraints. We
introduce an evader prediction-enhanced network to tackle partial observability
in cooperative strategy learning. Additionally, we propose an adaptive
environment generator within MARL training, enabling higher exploration
efficiency and better policy generalization across diverse scenarios.
Simulations show our method significantly outperforms all baselines in
challenging scenarios, generalizing to unseen scenarios with a 100% capture
rate. Finally, we derive a feasible policy via a two-stage reward refinement
and deploy the policy on real quadrotors in a zero-shot manner. To our
knowledge, this is the first work to derive and deploy an RL-based policy using
collective thrust and body rates control commands for multi-UAV pursuit-evasion
in unknown environments. The open-source code and videos are available at
https://sites.google.com/view/pursuit-evasion-rl.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Huatuo<span class="highlight-title">GPT</span>-Vision, Towards Injecting Medical Visual Knowledge into
  Multimodal LLMs at Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19280v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19280v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junying Chen, Chi Gui, Ruyi Ouyang, Anningzhe Gao, Shunian Chen, Guiming Hardy Chen, Xidong Wang, Ruifei Zhang, Zhenyang Cai, Ke Ji, Guangjun Yu, Xiang Wan, Benyou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of multimodal large language models (MLLMs), such as
GPT-4V, has led to significant advancements. However, these models still face
challenges in medical multimodal capabilities due to limitations in the
quantity and quality of medical vision-text data, stemming from data privacy
concerns and high annotation costs. While pioneering approaches utilize
PubMed's large-scale, de-identified medical image-text pairs to address these
limitations, they still fall short due to inherent data noise. To tackle this,
we refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in
an 'unblinded' capacity to denoise and reformat the data, resulting in the
creation of the PubMedVision dataset with 1.3 million medical VQA samples. Our
validation demonstrates that: (1) PubMedVision can significantly enhance the
medical multimodal capabilities of current MLLMs, showing significant
improvement in benchmarks including the MMMU Health & Medicine track; (2)
manual checks by medical experts and empirical results validate the superior
data quality of our dataset compared to other data construction methods. Using
PubMedVision, we train a 34B medical MLLM HuatuoGPT-Vision, which shows
superior performance in medical multimodal scenarios among open-source MLLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fair Mixed Effects Support Vector Machine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.06433v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.06433v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        João Vitor Pamplona, Jan Pablo Burgard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To ensure unbiased and ethical automated predictions, fairness must be a core
principle in machine learning applications. Fairness in machine learning aims
to mitigate biases present in the training data and model imperfections that
could lead to discriminatory outcomes. This is achieved by preventing the model
from making decisions based on sensitive characteristics like ethnicity or
sexual orientation. A fundamental assumption in machine learning is the
independence of observations. However, this assumption often does not hold true
for data describing social phenomena, where data points are often clustered
based. Hence, if the machine learning models do not account for the cluster
correlations, the results may be biased. Especially high is the bias in cases
where the cluster assignment is correlated to the variable of interest. We
present a fair mixed effects support vector machine algorithm that can handle
both problems simultaneously. With a reproducible simulation study we
demonstrate the impact of clustered data on the quality of fair machine
learning predictions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MAPF-<span class="highlight-title">GPT</span>: Imitation Learning for Multi-Agent Pathfinding at Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00134v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00134v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anton Andreychuk, Konstantin Yakovlev, Aleksandr Panov, Alexey Skrynnik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-agent pathfinding (MAPF) is a challenging computational problem that
typically requires to find collision-free paths for multiple agents in a shared
environment. Solving MAPF optimally is NP-hard, yet efficient solutions are
critical for numerous applications, including automated warehouses and
transportation systems. Recently, learning-based approaches to MAPF have gained
attention, particularly those leveraging deep reinforcement learning. Following
current trends in machine learning, we have created a foundation model for the
MAPF problems called MAPF-GPT. Using imitation learning, we have trained a
policy on a set of pre-collected sub-optimal expert trajectories that can
generate actions in conditions of partial observability without additional
heuristics, reward functions, or communication with other agents. The resulting
MAPF-GPT model demonstrates zero-shot learning abilities when solving the MAPF
problem instances that were not present in the training dataset. We show that
MAPF-GPT notably outperforms the current best-performing learnable-MAPF solvers
on a diverse range of problem instances and is efficient in terms of
computation (in the inference mode).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hierarchical Tree-structured Knowledge Graph For Academic Insight <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.04854v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.04854v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinghong Li, Huy Phan, Wen Gu, Koichi Ota, Shinobu Hasegawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research surveys have always posed a challenge for beginner researchers who
lack of research training. These researchers struggle to understand the
directions within their research topic, and the discovery of new research
findings within a short time. One way to provide intuitive assistance to
beginner researchers is by offering relevant knowledge graphs(KG) and
recommending related academic papers. However, existing navigation knowledge
graphs primarily rely on keywords in the research field and often fail to
present the logical hierarchy among multiple related papers clearly. Moreover,
most recommendation systems for academic papers simply rely on high text
similarity, which can leave researchers confused as to why a particular article
is being recommended. They may lack of grasp important information about the
insight connection between "Issue resolved" and "Issue finding" that they hope
to obtain. To address these issues, this study aims to support research insight
surveys for beginner researchers by establishing a hierarchical tree-structured
knowledge graph that reflects the inheritance insight of research topics and
the relevance insight among the academic papers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been published by 'The 18TH International Conference
  on INnovations in Intelligent SysTems and Applications (INISTA 2024)'</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentiating and Integrating ZX Diagrams with Applications to Quantum
  Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.13250v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.13250v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quanlong Wang, Richie Yeung, Mark Koch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ZX-calculus has proved to be a useful tool for quantum technology with a wide
range of successful applications. Most of these applications are of an
algebraic nature. However, other tasks that involve differentiation and
integration remain unreachable with current ZX techniques. Here we elevate ZX
to an analytical perspective by realising differentiation and integration
entirely within the framework of ZX-calculus. We explicitly illustrate the new
analytic framework of ZX-calculus by applying it in context of quantum machine
learning for the analysis of barren plateaus.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Kolmogorov-Arnold Networks (KANs) for Time Series Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.08790v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.08790v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cristian J. Vaca-Rubio, Luis Blanco, Roberto Pereira, Màrius Caus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel application of Kolmogorov-Arnold Networks
(KANs) to time series forecasting, leveraging their adaptive activation
functions for enhanced predictive modeling. Inspired by the Kolmogorov-Arnold
representation theorem, KANs replace traditional linear weights with
spline-parametrized univariate functions, allowing them to learn activation
patterns dynamically. We demonstrate that KANs outperforms conventional
Multi-Layer Perceptrons (MLPs) in a real-world satellite traffic forecasting
task, providing more accurate results with considerably fewer number of
learnable parameters. We also provide an ablation study of KAN-specific
parameters impact on performance. The proposed approach opens new avenues for
adaptive forecasting models, emphasizing the potential of KANs as a powerful
tool in predictive analytics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LSR-IGRU: Stock Trend Prediction Based on Long Short-Term Relationships
  and Improved GRU 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08282v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08282v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Zhu, Yuante Li, Yifan Hu, Qinyuan Liu, Dawei Cheng, Yuqi Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stock price prediction is a challenging problem in the field of finance and
receives widespread attention. In recent years, with the rapid development of
technologies such as deep learning and graph neural networks, more research
methods have begun to focus on exploring the interrelationships between stocks.
However, existing methods mostly focus on the short-term dynamic relationships
of stocks and directly integrating relationship information with temporal
information. They often overlook the complex nonlinear dynamic characteristics
and potential higher-order interaction relationships among stocks in the stock
market. Therefore, we propose a stock price trend prediction model named
LSR-IGRU in this paper, which is based on long short-term stock relationships
and an improved GRU input. Firstly, we construct a long short-term relationship
matrix between stocks, where secondary industry information is employed for the
first time to capture long-term relationships of stocks, and overnight price
information is utilized to establish short-term relationships. Next, we improve
the inputs of the GRU model at each step, enabling the model to more
effectively integrate temporal information and long short-term relationship
information, thereby significantly improving the accuracy of predicting stock
trend changes. Finally, through extensive experiments on multiple datasets from
stock markets in China and the United States, we validate the superiority of
the proposed LSR-IGRU model over the current state-of-the-art baseline models.
We also apply the proposed model to the algorithmic trading system of a
financial company, achieving significantly higher cumulative portfolio returns
compared to other baseline methods. Our sources are released at
https://github.com/ZP1481616577/Baselines_LSR-IGRU.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Are LLMs Ready for Real-World Materials Discovery? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.05200v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.05200v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Santiago Miret, N M Anoop Krishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) create exciting possibilities for powerful
language processing tools to accelerate research in materials science. While
LLMs have great potential to accelerate materials understanding and discovery,
they currently fall short in being practical materials science tools. In this
position paper, we show relevant failure cases of LLMs in materials science
that reveal current limitations of LLMs related to comprehending and reasoning
over complex, interconnected materials science knowledge. Given those
shortcomings, we outline a framework for developing Materials Science LLMs
(MatSci-LLMs) that are grounded in materials science knowledge and hypothesis
generation followed by hypothesis testing. The path to attaining performant
MatSci-LLMs rests in large part on building high-quality, multi-modal datasets
sourced from scientific literature where various information extraction
challenges persist. As such, we describe key materials science information
extraction challenges which need to be overcome in order to build large-scale,
multi-modal datasets that capture valuable materials science knowledge.
Finally, we outline a roadmap for applying future MatSci-LLMs for real-world
materials discovery via: 1. Automated Knowledge Base Generation; 2. Automated
In-Silico Material Design; and 3. MatSci-LLM Integrated Self-Driving Materials
Laboratories.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Non-backtracking Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07430v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07430v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seonghyun Park, Narae Ryu, Gahee Kim, Dongyeop Woo, Se-Young Yun, Sungsoo Ahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The celebrated message-passing updates for graph neural networks allow
representing large-scale graphs with local and computationally tractable
updates. However, the updates suffer from backtracking, i.e., a message flowing
through the same edge twice and revisiting the previously visited node. Since
the number of message flows increases exponentially with the number of updates,
the redundancy in local updates prevents the graph neural network from
accurately recognizing a particular message flow relevant for downstream tasks.
In this work, we propose to resolve such a redundancy issue via the
non-backtracking graph neural network (NBA-GNN) that updates a message without
incorporating the message from the previously visited node. We theoretically
investigate how NBA-GNN alleviates the over-squashing of GNNs, and establish a
connection between NBA-GNN and the impressive performance of non-backtracking
updates for stochastic block model recovery. Furthermore, we empirically verify
the effectiveness of our NBA-GNN on the long-range graph benchmark and
transductive node classification problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Realism in Action: Anomaly-Aware Diagnosis of Brain Tumors from Medical
  Images Using YOLOv8 and DeiT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03302v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03302v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seyed Mohammad Hossein Hashemi, Leila Safari, Amirhossein Dadashzadeh Taromi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of medical sciences, reliable detection and classification of
brain tumors from images remains a formidable challenge due to the rarity of
tumors within the population of patients. Therefore, the ability to detect
tumors in anomaly scenarios is paramount for ensuring timely interventions and
improved patient outcomes. This study addresses the issue by leveraging deep
learning (DL) techniques to detect and classify brain tumors in challenging
situations. The curated data set from the National Brain Mapping Lab (NBML)
comprises 81 patients, including 30 Tumor cases and 51 Normal cases. The
detection and classification pipelines are separated into two consecutive
tasks. The detection phase involved comprehensive data analysis and
pre-processing to modify the number of image samples and the number of patients
of each class to anomaly distribution (9 Normal per 1 Tumor) to comply with
real world scenarios. Next, in addition to common evaluation metrics for the
testing, we employed a novel performance evaluation method called Patient to
Patient (PTP), focusing on the realistic evaluation of the model. In the
detection phase, we fine-tuned a YOLOv8n detection model to detect the tumor
region. Subsequent testing and evaluation yielded competitive performance both
in Common Evaluation Metrics and PTP metrics. Furthermore, using the Data
Efficient Image Transformer (DeiT) module, we distilled a Vision Transformer
(ViT) model from a fine-tuned ResNet152 as a teacher in the classification
phase. This approach demonstrates promising strides in reliable tumor detection
and classification, offering potential advancements in tumor diagnosis for
real-world medical imaging scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the Elsevier for possible
  publication. Copyright may be transferred without notice, after which this
  version may no longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ultra-low latency quantum-inspired machine learning predictors
  implemented on FPGA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16075v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16075v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenzo Borella, Alberto Coppi, Jacopo Pazzini, Andrea Stanco, Marco Trenti, Andrea Triossi, Marco Zanetti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tensor Networks (TNs) are a computational paradigm used for representing
quantum many-body systems. Recent works have shown how TNs can also be applied
to perform Machine Learning (ML) tasks, yielding comparable results to standard
supervised learning techniques. In this work, we study the use of Tree Tensor
Networks (TTNs) in high-frequency real-time applications by exploiting the
low-latency hardware of the Field-Programmable Gate Array (FPGA) technology. We
present different implementations of TTN classifiers, capable of performing
inference on classical ML datasets as well as on complex physics data. A
preparatory analysis of bond dimensions and weight quantization is realized in
the training phase, together with entanglement entropy and correlation
measurements, that help setting the choice of the TTN architecture. The
generated TTNs are then deployed on a hardware accelerator; using an FPGA
integrated into a server, the inference of the TTN is completely offloaded.
Eventually, a classifier for High Energy Physics (HEP) applications is
implemented and executed fully pipelined with sub-microsecond latency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards a Realistic Long-Term Benchmark for Open-Web Research Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14913v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14913v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Mühlbacher, Nikos I. Bosse, Lawrence Phillips
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present initial results of a forthcoming benchmark for evaluating LLM
agents on white-collar tasks of economic value. We evaluate agents on
real-world "messy" open-web research tasks of the type that are routine in
finance and consulting. In doing so, we lay the groundwork for an LLM agent
evaluation suite where good performance directly corresponds to a large
economic and societal impact. We built and tested several agent architectures
with o1-preview, GPT-4o, Claude-3.5 Sonnet, Llama 3.1 (405b), and GPT-4o-mini.
On average, LLM agents powered by Claude-3.5 Sonnet and o1-preview
substantially outperformed agents using GPT-4o, with agents based on Llama 3.1
(405b) and GPT-4o-mini lagging noticeably behind. Across LLMs, a ReAct
architecture with the ability to delegate subtasks to subagents performed best.
In addition to quantitative evaluations, we qualitatively assessed the
performance of the LLM agents by inspecting their traces and reflecting on
their observations. Our evaluation represents the first in-depth assessment of
agents' abilities to conduct challenging, economically valuable analyst-style
research on the real open web.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TroL: Traversal of Layers for Large Language and Vision Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12246v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12246v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Byung-Kwan Lee, Sangyun Chung, Chae Won Kim, Beomchan Park, Yong Man Ro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language and vision models (LLVMs) have been driven by the
generalization power of large language models (LLMs) and the advent of visual
instruction tuning. Along with scaling them up directly, these models enable
LLVMs to showcase powerful vision language (VL) performances by covering
diverse tasks via natural language instructions. However, existing open-source
LLVMs that perform comparably to closed-source LLVMs such as GPT-4V are often
considered too large (e.g., 26B, 34B, and 110B parameters), having a larger
number of layers. These large models demand costly, high-end resources for both
training and inference. To address this issue, we present a new efficient LLVM
family with 1.8B, 3.8B, and 7B LLM model sizes, Traversal of Layers (TroL),
which enables the reuse of layers in a token-wise manner. This layer traversing
technique simulates the effect of looking back and retracing the answering
stream while increasing the number of forward propagation layers without
physically adding more layers. We demonstrate that TroL employs a simple layer
traversing approach yet efficiently outperforms the open-source LLVMs with
larger model sizes and rivals the performances of the closed-source LLVMs with
substantial sizes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024. Code is available in https://github.com/ByungKwanLee/TroL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ One-Shot Machine Unlearning with Mnemonic Code 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05670v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05670v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomoya Yamashita, Masanori Yamada, Takashi Shibata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ethical and privacy issues inherent in artificial intelligence (AI)
applications have been a growing concern with the rapid spread of deep
learning. Machine unlearning (MU) is the research area that addresses these
issues by making a trained AI model forget about undesirable training data.
Unfortunately, most existing MU methods incur significant time and
computational costs for forgetting. Therefore, it is often difficult to apply
these methods to practical datasets and sophisticated architectures, e.g.,
ImageNet and Transformer. To tackle this problem, we propose a lightweight and
effective MU method. Our method identifies the model parameters sensitive to
the forgetting targets and adds perturbation to such model parameters. We
identify the sensitive parameters by calculating the Fisher Information Matrix
(FIM). This approach does not require time-consuming additional training for
forgetting. In addition, we introduce class-specific random signals called
mnemonic code to reduce the cost of FIM calculation, which generally requires
the entire training data and incurs significant computational costs. In our
method, we train the model with mnemonic code; when forgetting, we use a small
number of mnemonic codes to calculate the FIM and get the effective
perturbation for forgetting. Comprehensive experiments demonstrate that our
method is faster and better at forgetting than existing MU methods.
Furthermore, we show that our method can scale to more practical datasets and
sophisticated architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, welcome coments</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Source Attribution for Large Language Model-Generated Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00646v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00646v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingtan Wang, Xinyang Lu, Zitong Zhao, Zhongxiang Dai, Chuan-Sheng Foo, See-Kiong Ng, Bryan Kian Hsiang Low
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The impressive performances of Large Language Models (LLMs) and their immense
potential for commercialization have given rise to serious concerns over the
Intellectual Property (IP) of their training data. In particular, the synthetic
texts generated by LLMs may infringe the IP of the data being used to train the
LLMs. To this end, it is imperative to be able to perform source attribution by
identifying the data provider who contributed to the generation of a synthetic
text by an LLM. In this paper, we show that this problem can be tackled by
watermarking, i.e., by enabling an LLM to generate synthetic texts with
embedded watermarks that contain information about their source(s). We identify
the key properties of such watermarking frameworks (e.g., source attribution
accuracy, robustness against adversaries), and propose a source attribution
framework that satisfies these key properties due to our algorithmic designs.
Our framework enables an LLM to learn an accurate mapping from the generated
texts to data providers, which sets the foundation for effective source
attribution. Extensive empirical evaluations show that our framework achieves
effective source attribution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The loss landscape of deep linear neural networks: a second-order
  analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.13289v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.13289v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        El Mehdi Achour, François Malgouyres, Sébastien Gerchinovitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the optimization landscape of deep linear neural networks with the
square loss. It is known that, under weak assumptions, there are no spurious
local minima and no local maxima. However, the existence and diversity of
non-strict saddle points, which can play a role in first-order algorithms'
dynamics, have only been lightly studied. We go a step further with a full
analysis of the optimization landscape at order 2. We characterize, among all
critical points, which are global minimizers, strict saddle points, and
non-strict saddle points. We enumerate all the associated critical values. The
characterization is simple, involves conditions on the ranks of partial matrix
products, and sheds some light on global convergence or implicit regularization
that have been proved or observed when optimizing linear neural networks. In
passing, we provide an explicit parameterization of the set of all global
minimizers and exhibit large sets of strict and non-strict saddle points.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantum Long Short-Term Memory (QLSTM) vs Classical LSTM in Time Series
  Forecasting: A Comparative Study in Solar Power Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17032v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17032v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saad Zafar Khan, Nazeefa Muzammil, Salman Ghafoor, Haibat Khan, Syed Mohammad Hasan Zaidi, Abdulah Jeza Aljohani, Imran Aziz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate solar power forecasting is pivotal for the global transition towards
sustainable energy systems. This study conducts a meticulous comparison between
Quantum Long Short-Term Memory (QLSTM) and classical Long Short-Term Memory
(LSTM) models for solar power production forecasting. The primary objective is
to evaluate the potential advantages of QLSTMs, leveraging their exponential
representational capabilities, in capturing the intricate spatiotemporal
patterns inherent in renewable energy data. Through controlled experiments on
real-world photovoltaic datasets, our findings reveal promising improvements
offered by QLSTMs, including accelerated training convergence and substantially
reduced test loss within the initial epoch compared to classical LSTMs. These
empirical results demonstrate QLSTM's potential to swiftly assimilate complex
time series relationships, enabled by quantum phenomena like superposition.
However, realizing QLSTM's full capabilities necessitates further research into
model validation across diverse conditions, systematic hyperparameter
optimization, hardware noise resilience, and applications to correlated
renewable forecasting problems. With continued progress, quantum machine
learning can offer a paradigm shift in renewable energy time series prediction,
potentially ushering in an era of unprecedented accuracy and reliability in
solar power forecasting worldwide. This pioneering work provides initial
evidence substantiating quantum advantages over classical LSTM models while
acknowledging present limitations. Through rigorous benchmarking grounded in
real-world data, our study illustrates a promising trajectory for quantum
learning in renewable forecasting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Privacy Evaluation Benchmarks for NLP Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15868v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15868v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Huang, Yinggui Wang, Cen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  By inducing privacy attacks on NLP models, attackers can obtain sensitive
information such as training data and model parameters, etc. Although
researchers have studied, in-depth, several kinds of attacks in NLP models,
they are non-systematic analyses. It lacks a comprehensive understanding of the
impact caused by the attacks. For example, we must consider which scenarios can
apply to which attacks, what the common factors are that affect the performance
of different attacks, the nature of the relationships between different
attacks, and the influence of various datasets and models on the effectiveness
of the attacks, etc. Therefore, we need a benchmark to holistically assess the
privacy risks faced by NLP models. In this paper, we present a privacy attack
and defense evaluation benchmark in the field of NLP, which includes the
conventional/small models and large language models (LLMs). This benchmark
supports a variety of models, datasets, and protocols, along with standardized
modules for comprehensive evaluation of attacks and defense strategies. Based
on the above framework, we present a study on the association between auxiliary
data from different domains and the strength of privacy attacks. And we provide
an improved attack method in this scenario with the help of Knowledge
Distillation (KD). Furthermore, we propose a chained framework for privacy
attacks. Allowing a practitioner to chain multiple attacks to achieve a
higher-level attack objective. Based on this, we provide some defense and
enhanced attack strategies. The code for reproducing the results can be found
at https://github.com/user2311717757/nlp_doctor.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Needs further optimization</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advancing Cyber Incident Timeline Analysis Through Rule Based AI and
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02572v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02572v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fatma Yasmine Loumachi, Mohamed Chahine Ghanem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Timeline Analysis (TA) plays a crucial role in Timeline Forensics (TF) within
the field of Digital Forensics (DF). It focuses on examining and analyzing
time-based digital artefacts, such as timestamps derived from event logs, file
metadata, and other relevant data, to correlate events linked to cyber
incidents and reconstruct their chronological sequence. Traditional tools often
struggle to efficiently handle the large volume and variety of data generated
during DF investigations and Incident Response (IR) processes. This paper
introduces a novel framework, GenDFIR, which combines Rule-Based Artificial
Intelligence (R-BAI) algorithms with Large Language Models (LLMs) to enhance
and automate the TA process. The proposed approach consists of two key stages:
(1) R-BAI is used to identify and select anomalous digital artefacts based on
predefined rules. (2) The selected artefacts are then transformed into
embeddings for processing by an LLM with the assistance of a
Retrieval-Augmented Generation (RAG) agent. The LLM uses its capabilities to
perform automated TA on the artefacts and predict potential incident outcomes.
To validate the framework, we evaluated its performance, efficiency, and
reliability. Several metrics were applied to simulated cyber incident
scenarios, which were presented as forensic case documents. Our findings
demonstrate the significant potential of integrating R-BAI and LLMs for TA.
This innovative approach underscores the power of Generative AI (GenAI),
particularly LLMs, and opens up new possibilities for advanced threat detection
and incident reconstruction, marking a significant advancement in the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages V3.1</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Block-Attention for Efficient RAG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15355v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15355v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        East Sun, Yan Wang, Lan Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Block-Attention, an attention mechanism designed to address the
increased inference latency and cost in Retrieval-Augmented Generation (RAG)
scenarios. Unlike existing works that encodes the whole context, its main idea
lies in dividing the retrieved documents into blocks, where each block
calculates key-value (KV) states independently except for the final block. In
RAG scenarios, by defining each passage as a block, Block-Attention enables us
to pre-compute the KV states for all passages and cache them in memory,
significantly reducing the latency and the computation cost during inference.
The implementation involves block segmentation, positional encoding
calculation, and fine-tuning the LLM to adapt to the Block-Attention mechanism.
Experiments on four RAG benchmarks demonstrate that after block fine-tuning,
the Block Attention model can achieve performance comparable to (68.4\% vs
67.9\% on Llama3) or even better (62.8\% vs 59.6\% on Mistral) than
self-attention models. Notably, Block-Attention reduces the TTFT (the time to
first token) and FLOPs (floating point operations) to a very low level. It only
takes 45 ms to output the first token for an input sequence with a total length
of 32K. Compared with the self-attention model, the time consumption and
corresponding FLOPs are reduced by 98.7\% and 99.8\%, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Group-Feature (Sensor) Selection With Controlled Redundancy Using Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.20524v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.20524v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aytijhya Saha, Nikhil R. Pal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a novel embedded feature selection method based on
a Multi-layer Perceptron (MLP) network and generalize it for group-feature or
sensor selection problems, which can control the level of redundancy among the
selected features or groups. Additionally, we have generalized the group lasso
penalty for feature selection to encompass a mechanism for selecting valuable
group features while simultaneously maintaining a control over redundancy. We
establish the monotonicity and convergence of the proposed algorithm, with a
smoothed version of the penalty terms, under suitable assumptions. Experimental
results on several benchmark datasets demonstrate the promising performance of
the proposed methodology for both feature selection and group feature selection
over some state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChatDiet: Empowering Personalized Nutrition-Oriented Food Recommender
  Chatbots through an LLM-Augmented Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00781v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00781v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongqi Yang, Elahe Khatibi, Nitish Nagesh, Mahyar Abbasian, Iman Azimi, Ramesh Jain, Amir M. Rahmani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The profound impact of food on health necessitates advanced
nutrition-oriented food recommendation services. Conventional methods often
lack the crucial elements of personalization, explainability, and
interactivity. While Large Language Models (LLMs) bring interpretability and
explainability, their standalone use falls short of achieving true
personalization. In this paper, we introduce ChatDiet, a novel LLM-powered
framework designed specifically for personalized nutrition-oriented food
recommendation chatbots. ChatDiet integrates personal and population models,
complemented by an orchestrator, to seamlessly retrieve and process pertinent
information. The personal model leverages causal discovery and inference
techniques to assess personalized nutritional effects for a specific user,
whereas the population model provides generalized information on food
nutritional content. The orchestrator retrieves, synergizes and delivers the
output of both models to the LLM, providing tailored food recommendations
designed to support targeted health outcomes. The result is a dynamic delivery
of personalized and explainable food recommendations, tailored to individual
user preferences. Our evaluation of ChatDiet includes a compelling case study,
where we establish a causal personal model to estimate individual nutrition
effects. Our assessments, including a food recommendation test showcasing a
92\% effectiveness rate, coupled with illustrative dialogue examples,
underscore ChatDiet's strengths in explainability, personalization, and
interactivity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published on Smart Health</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MLLM Is a Strong Reranker: Advancing Multimodal Retrieval-augmented
  Generation via Knowledge-enhanced Reranking and Noise-injected Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.21439v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.21439v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhanpeng Chen, Chengjin Xu, Yiyan Qi, Jian Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in processing and generating content across multiple data
modalities. However, a significant drawback of MLLMs is their reliance on
static training data, leading to outdated information and limited contextual
awareness. This static nature hampers their ability to provide accurate and
up-to-date responses, particularly in dynamic or rapidly evolving contexts.
Though integrating Multimodal Retrieval-augmented Generation (Multimodal RAG)
offers a promising solution, the system would inevitably encounter the
multi-granularity noisy correspondence (MNC) problem, which hinders accurate
retrieval and generation. In this work, we propose RagVL, a novel framework
with knowledge-enhanced reranking and noise-injected training, to address these
limitations. We instruction-tune the MLLM with a simple yet effective
instruction template to induce its ranking ability and serve it as a reranker
to precisely filter the top-k retrieved images. For generation, we inject
visual noise during training at the data and token levels to enhance the
generator's robustness. Extensive experiments on the subsets of two datasets
that require retrieving and reasoning over images to answer a given query
verify the effectiveness of our method. Code and models are available at
https://github.com/IDEA-FinAI/RagVL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Historical Trajectory Assisted Zeroth-Order Federated Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15955v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15955v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu He, Chenlin Wu, Zike Li, Zibin Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning is a distributed learning framework which enables clients
to train models individually and to upload their model updates for aggregation.
The local training process heavily relies on distributed gradient descent
techniques. In the situation where gradient information is not available, the
gradients need to be estimated from zeroth-order information, which typically
involves computing finite-differences along isotropic random directions. This
method suffers from high estimation errors, as the geometric features of the
objective landscape may be overlooked during the isotropic sampling. In this
work, we propose a non-isotropic sampling method to improve the gradient
estimation procedure. Gradients in our method are estimated in a subspace
spanned by historical trajectories of solutions, aiming to encourage the
exploration of promising regions and hence improve the convergence. We
implement this method in zeroth-order federated settings, and show that the
convergence rate aligns with existing ones while introducing no significant
overheads in communication or local computation. The effectiveness of our
proposal is verified on several numerical experiments in comparison to several
commonly-used zeroth-order federated optimization algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages with theoretical proof</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast Distributed Inference Serving for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.05920v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.05920v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingyang Wu, Yinmin Zhong, Zili Zhang, Shengyu Liu, Fangyue Liu, Yuanhang Sun, Gang Huang, Xuanzhe Liu, Xin Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) power a new generation of interactive AI
applications exemplified by ChatGPT. The interactive nature of these
applications demands low latency for LLM inference. Existing LLM serving
systems use run-to-completion processing for inference jobs, which suffers from
head-of-line blocking and long latency.
  We present FastServe, a distributed inference serving system for LLMs.
FastServe exploits the autoregressive pattern of LLM inference to enable
preemption at the granularity of each output token. FastServe uses preemptive
scheduling to minimize latency with a novel skip-join Multi-Level Feedback
Queue scheduler. Based on the new semi-information-agnostic setting of LLM
inference, the scheduler leverages the input length information to assign an
appropriate initial queue for each arrival job to join. The higher priority
queues than the joined queue are skipped to reduce demotions. We design an
efficient GPU memory management mechanism that proactively offloads and uploads
intermediate state between GPU memory and host memory for LLM inference. We
build a system prototype of FastServe and experimental results show that
compared to the state-of-the-art solution vLLM, FastServe improves the
throughput by up to 31.4x and 17.9x under the same average and tail latency
requirements, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Higher-order-ReLU-KANs (HRKANs) for solving physics-informed neural
  networks (PINNs) more accurately, robustly and faster 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14248v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14248v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chi Chiu So, Siu Pang Yung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Finding solutions to partial differential equations (PDEs) is an important
and essential component in many scientific and engineering discoveries. One of
the common approaches empowered by deep learning is Physics-informed Neural
Networks (PINNs). Recently, a new type of fundamental neural network model,
Kolmogorov-Arnold Networks (KANs), has been proposed as a substitute of
Multilayer Perceptions (MLPs), and possesses trainable activation functions. To
enhance KANs in fitting accuracy, a modification of KANs, so called ReLU-KANs,
using "square of ReLU" as the basis of its activation functions, has been
suggested. In this work, we propose another basis of activation functions,
namely, Higherorder-ReLU (HR), which is simpler than the basis of activation
functions used in KANs, namely, Bsplines; allows efficient KAN matrix
operations; and possesses smooth and non-zero higher-order derivatives,
essential to physicsinformed neural networks. We name such KANs with
Higher-order-ReLU (HR) as their activations, HRKANs. Our detailed experiments
on two famous and representative PDEs, namely, the linear Poisson equation and
nonlinear Burgers' equation with viscosity, reveal that our proposed
Higher-order-ReLU-KANs (HRKANs) achieve the highest fitting accuracy and
training robustness and lowest training time significantly among KANs,
ReLU-KANs and HRKANs. The codes to replicate our experiments are available at
https://github.com/kelvinhkcs/HRKAN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Precision Aquaculture: An Integrated Computer Vision and IoT Approach
  for Optimized Tilapia Feeding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08695v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08695v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rania Hossam, Ahmed Heakl, Walid Gomaa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional fish farming practices often lead to inefficient feeding,
resulting in environmental issues and reduced productivity. We developed an
innovative system combining computer vision and IoT technologies for precise
Tilapia feeding. Our solution uses real-time IoT sensors to monitor water
quality parameters and computer vision algorithms to analyze fish size and
count, determining optimal feed amounts. A mobile app enables remote monitoring
and control. We utilized YOLOv8 for keypoint detection to measure Tilapia
weight from length, achieving \textbf{94\%} precision on 3,500 annotated
images. Pixel-based measurements were converted to centimeters using depth
estimation for accurate feeding calculations. Our method, with data collection
mirroring inference conditions, significantly improved results. Preliminary
estimates suggest this approach could increase production up to 58 times
compared to traditional farms. Our models, code, and dataset are
open-source~\footnote{The code, dataset, and models are available upon
reasonable request.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, 3 tables, 21th International Conference on
  Informatics in Control, Automation, and Robotics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ M^2PT: Multimodal <span class="highlight-title">Prompt</span> Tuning for Zero-shot Instruction Learning <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15657v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15657v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taowen Wang, Yiyang Liu, James Chenhao Liang, junhan zhao, Yiming Cui, Yuning Mao, Shaoliang Nie, Jiahao Liu, Fuli Feng, Zenglin Xu, Cheng Han, Lifu Huang, Qifan Wang, Dongfang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) demonstrate remarkable performance
across a wide range of domains, with increasing emphasis on enhancing their
zero-shot generalization capabilities for unseen tasks across various
modalities. Instruction tuning has emerged as an effective strategy for
achieving zero-shot generalization by finetuning pretrained models on diverse
multimodal tasks. As the scale of MLLMs continues to grow, parameter-efficient
finetuning becomes increasingly critical. However, most existing
parameter-efficient approaches focus only on single modalities and often
overlook the multimodal characteristics during finetuning. In this work, we
introduce a novel Multimodal Prompt Tuning (M$^2$PT) approach for efficient
instruction tuning of MLLMs. M$^2$PT effectively integrates visual and textual
prompts into the vision encoder and language processor respectively during
finetuning, facilitating the extraction and alignment of features across
modalities. Empirical results on various multimodal evaluation datasets
demonstrate the superior performance of our approach compared to several
state-of-the-art baselines. A comprehensive set of ablation studies validates
the effectiveness of our prompt design and the efficiency of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Object-Aware Query Perturbation for Cross-Modal Image-Text Retrieval <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12346v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12346v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naoya Sogi, Takashi Shibata, Makoto Terao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The pre-trained vision and language (V\&L) models have substantially improved
the performance of cross-modal image-text retrieval. In general, however, V\&L
models have limited retrieval performance for small objects because of the
rough alignment between words and the small objects in the image. In contrast,
it is known that human cognition is object-centric, and we pay more attention
to important objects, even if they are small. To bridge this gap between the
human cognition and the V\&L model's capability, we propose a cross-modal
image-text retrieval framework based on ``object-aware query perturbation.''
The proposed method generates a key feature subspace of the detected objects
and perturbs the corresponding queries using this subspace to improve the
object awareness in the image. In our proposed method, object-aware cross-modal
image-text retrieval is possible while keeping the rich expressive power and
retrieval performance of existing V\&L models without additional fine-tuning.
Comprehensive experiments on four public datasets show that our method
outperforms conventional algorithms. Our code is publicly available at
\url{https://github.com/NEC-N-SOGI/query-perturbation}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024. Code: https://github.com/NEC-N-SOGI/query-perturbation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Symbolic Music Generation with Non-Differentiable Rule Guided Diffusion <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14285v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14285v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujia Huang, Adishree Ghatare, Yuanzhe Liu, Ziniu Hu, Qinsheng Zhang, Chandramouli S Sastry, Siddharth Gururani, Sageev Oore, Yisong Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of symbolic music generation (e.g., generating piano
rolls), with a technical focus on non-differentiable rule guidance. Musical
rules are often expressed in symbolic form on note characteristics, such as
note density or chord progression, many of which are non-differentiable which
pose a challenge when using them for guided diffusion. We propose Stochastic
Control Guidance (SCG), a novel guidance method that only requires forward
evaluation of rule functions that can work with pre-trained diffusion models in
a plug-and-play way, thus achieving training-free guidance for
non-differentiable rules for the first time. Additionally, we introduce a
latent diffusion architecture for symbolic music generation with high time
resolution, which can be composed with SCG in a plug-and-play fashion. Compared
to standard strong baselines in symbolic music generation, this framework
demonstrates marked advancements in music quality and rule-based
controllability, outperforming current state-of-the-art generators in a variety
of settings. For detailed demonstrations, code and model checkpoints, please
visit our project website: https://scg-rule-guided-music.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2024 (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedRepOpt: Gradient Re-parameterized Optimizers in Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15898v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15898v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kin Wai Lau, Yasar Abbas Ur Rehman, Pedro Porto Buarque de Gusmão, Lai-Man Po, Lan Ma, Yuyang Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) has emerged as a privacy-preserving method for
training machine learning models in a distributed manner on edge devices.
However, on-device models face inherent computational power and memory
limitations, potentially resulting in constrained gradient updates. As the
model's size increases, the frequency of gradient updates on edge devices
decreases, ultimately leading to suboptimal training outcomes during any
particular FL round. This limits the feasibility of deploying advanced and
large-scale models on edge devices, hindering the potential for performance
enhancements. To address this issue, we propose FedRepOpt, a gradient
re-parameterized optimizer for FL. The gradient re-parameterized method allows
training a simple local model with a similar performance as a complex model by
modifying the optimizer's gradients according to a set of model-specific
hyperparameters obtained from the complex models. In this work, we focus on
VGG-style and Ghost-style models in the FL environment. Extensive experiments
demonstrate that models using FedRepOpt obtain a significant boost in
performance of 16.7% and 11.4% compared to the RepGhost-style and RepVGG-style
networks, while also demonstrating a faster convergence time of 11.7% and 57.4%
compared to their complex structure.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Metric Entropy-Free Sample Complexity Bounds for Sample Average
  Approximation in Convex Stochastic Programming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00664v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00664v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongcheng Liu, Jindong Tong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies sample average approximation (SAA) in solving convex or
strongly convex stochastic programming (SP) problems. Under some common
regularity conditions, we show -- perhaps for the first time -- that SAA's
sample complexity can be completely free from any quantification of metric
entropy (such as the logarithm of the covering number), leading to a
significantly more efficient rate with dimensionality $d$ than most existing
results. From the newly established complexity bounds, an important revelation
is that SAA and the canonical stochastic mirror descent (SMD) method, two
mainstream solution approaches to SP, entail almost identical rates of sample
efficiency, rectifying a persistent theoretical discrepancy of SAA from SMD by
the order of $O(d)$. Furthermore, this paper explores non-Lipschitzian
scenarios where SAA maintains provable efficacy but the corresponding results
for SMD remain mostly unexplored, indicating the potential of SAA's better
applicability in some irregular settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Analysis of Centrifugal Clutches in Two-Speed Automatic Transmissions
  with Deep Learning-Based Engagement Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09755v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09755v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo-Yi Lin, Kai Chun Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a comprehensive numerical analysis of centrifugal clutch
systems integrated with a two-speed automatic transmission, a key component in
automotive torque transfer. Centrifugal clutches enable torque transmission
based on rotational speed without external controls. The study systematically
examines various clutch configurations effects on transmission dynamics,
focusing on torque transfer, upshifting, and downshifting behaviors under
different conditions. A Deep Neural Network (DNN) model predicts clutch
engagement using parameters such as spring preload and shoe mass, offering an
efficient alternative to complex simulations. The integration of deep learning
and numerical modeling provides critical insights for optimizing clutch
designs, enhancing transmission performance and efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Clinician Performance in Classification of EEG Patterns on the
  Ictal-Interictal-Injury Continuum using Interpretable Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.05207v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.05207v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alina Jade Barnett, Zhicheng Guo, Jin Jing, Wendong Ge, Peter W. Kaplan, Wan Yee Kong, Ioannis Karakis, Aline Herlopian, Lakshman Arcot Jayagopal, Olga Taraschenko, Olga Selioutski, Gamaleldin Osman, Daniel Goldenholz, Cynthia Rudin, M. Brandon Westover
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In intensive care units (ICUs), critically ill patients are monitored with
electroencephalograms (EEGs) to prevent serious brain injury. The number of
patients who can be monitored is constrained by the availability of trained
physicians to read EEGs, and EEG interpretation can be subjective and prone to
inter-observer variability. Automated deep learning systems for EEG could
reduce human bias and accelerate the diagnostic process. However, black box
deep learning models are untrustworthy, difficult to troubleshoot, and lack
accountability in real-world applications, leading to a lack of trust and
adoption by clinicians. To address these challenges, we propose a novel
interpretable deep learning model that not only predicts the presence of
harmful brainwave patterns but also provides high-quality case-based
explanations of its decisions. Our model performs better than the corresponding
black box model, despite being constrained to be interpretable. The learned 2D
embedded space provides the first global overview of the structure of
ictal-interictal-injury continuum brainwave patterns. The ability to understand
how our model arrived at its decisions will not only help clinicians to
diagnose and treat harmful brain activities more accurately but also increase
their trust and adoption of machine learning models in clinical practice; this
could be an integral component of the ICU neurologists' standard workflow.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages including appendices, 9 figures, published at NEJM AI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Speech Robust Bench: A Robustness Benchmark For Speech Recognition <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07937v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07937v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad A. Shah, David Solans Noguero, Mikko A. Heikkila, Bhiksha Raj, Nicolas Kourtellis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Automatic Speech Recognition (ASR) models become ever more pervasive, it
is important to ensure that they make reliable predictions under corruptions
present in the physical and digital world. We propose Speech Robust Bench
(SRB), a comprehensive benchmark for evaluating the robustness of ASR models to
diverse corruptions. SRB is composed of 114 input perturbations which simulate
an heterogeneous range of corruptions that ASR models may encounter when
deployed in the wild. We use SRB to evaluate the robustness of several
state-of-the-art ASR models and observe that model size and certain modeling
choices such as the use of discrete representations, or self-training appear to
be conducive to robustness. We extend this analysis to measure the robustness
of ASR models on data from various demographic subgroups, namely English and
Spanish speakers, and males and females. Our results revealed noticeable
disparities in the model's robustness across subgroups. We believe that SRB
will significantly facilitate future research towards robust ASR models, by
making it easier to conduct comprehensive and comparable robustness
evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to NeurIPS datasets and benchmark track 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detecting Adversarial Data via Perturbation Forgery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16226v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16226v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Wang, Chen Li, Yuchen Luo, Hefei Ling, Ping Li, Jiazhong Chen, Shijuan Huang, Ning Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a defense strategy against adversarial attacks, adversarial detection aims
to identify and filter out adversarial data from the data flow based on
discrepancies in distribution and noise patterns between natural and
adversarial data. Although previous detection methods achieve high performance
in detecting gradient-based adversarial attacks, new attacks based on
generative models with imbalanced and anisotropic noise patterns evade
detection. Even worse, existing techniques either necessitate access to attack
data before deploying a defense or incur a significant time cost for inference,
rendering them impractical for defending against newly emerging attacks that
are unseen by defenders. In this paper, we explore the proximity relationship
between adversarial noise distributions and demonstrate the existence of an
open covering for them. By learning to distinguish this open covering from the
distribution of natural data, we can develop a detector with strong
generalization capabilities against all types of adversarial attacks. Based on
this insight, we heuristically propose Perturbation Forgery, which includes
noise distribution perturbation, sparse mask generation, and pseudo-adversarial
data production, to train an adversarial detector capable of detecting unseen
gradient-based, generative-model-based, and physical adversarial attacks, while
remaining agnostic to any specific models. Comprehensive experiments conducted
on multiple general and facial datasets, with a wide spectrum of attacks,
validate the strong generalization of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continual Adversarial Defense 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.09481v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.09481v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Wang, Yaoyao Liu, Hefei Ling, Yingwei Li, Qihao Liu, Ping Li, Jiazhong Chen, Alan Yuille, Ning Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In response to the rapidly evolving nature of adversarial attacks against
visual classifiers on a monthly basis, numerous defenses have been proposed to
generalize against as many known attacks as possible. However, designing a
defense method that generalizes to all types of attacks is not realistic
because the environment in which defense systems operate is dynamic and
comprises various unique attacks that emerge as time goes on. A well-matched
approach to the dynamic environment lies in a defense system that continuously
collects adversarial data online to quickly improve itself. Therefore, we put
forward a practical defense deployment against a challenging threat model and
propose, for the first time, the Continual Adversarial Defense (CAD) framework
that adapts to attack sequences under four principles: (1) continual adaptation
to new attacks without catastrophic forgetting, (2) few-shot adaptation, (3)
memory-efficient adaptation, and (4) high accuracy on both clean and
adversarial data. We explore and integrate cutting-edge continual learning,
few-shot learning, and ensemble learning techniques to qualify the principles.
Extensive experiments validate the effectiveness of our approach against
multiple stages of modern adversarial attacks and demonstrate significant
improvements over numerous baseline methods. In particular, CAD is capable of
quickly adapting with minimal budget and a low cost of defense failure while
maintaining good performance against previous attacks. Our research sheds light
on a brand-new paradigm for continual defense adaptation against dynamic and
evolving attacks.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language-oriented Semantic Communication for Image Transmission with
  Fine-Tuned Diffusion Model <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17104v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17104v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinfeng Wei, Haonan Tong, Nuocheng Yang, Changchuan Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ubiquitous image transmission in emerging applications brings huge overheads
to limited wireless resources. Since that text has the characteristic of
conveying a large amount of information with very little data, the transmission
of the descriptive text of an image can reduce the amount of transmitted data.
In this context, this paper develops a novel semantic communication framework
based on a text-2-image generative model (Gen-SC). In particular, a transmitter
converts the input image to textual modality data. Then the text is transmitted
through a noisy channel to the receiver. The receiver then uses the received
text to generate images. Additionally, to improve the robustness of text
transmission over noisy channels, we designed a transformer-based text
transmission codec model. Moreover, we obtained a personalized knowledge base
by fine-tuning the diffusion model to meet the requirements of task-oriented
transmission scenarios. Simulation results show that the proposed framework can
achieve high perceptual quality with reducing the transmitted data volume by up
to 99% and is robust to wireless channel noise in terms of portrait image
transmission.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 9 figures, accepted by Wireless Communications and Signal
  Processing (WCSP) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semi-Supervised Cognitive State Classification from Speech with
  Multi-View Pseudo-Labeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16937v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16937v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanchao Li, Zixing Zhang, Jing Han, Peter Bell, Catherine Lai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The lack of labeled data is a common challenge in speech classification
tasks, particularly those requiring extensive subjective assessment, such as
cognitive state classification. In this work, we propose a Semi-Supervised
Learning (SSL) framework, introducing a novel multi-view pseudo-labeling method
that leverages both acoustic and linguistic characteristics to select the most
confident data for training the classification model. Acoustically, unlabeled
data are compared to labeled data using the Frechet audio distance, calculated
from embeddings generated by multiple audio encoders. Linguistically, large
language models are prompted to revise automatic speech recognition
transcriptions and predict labels based on our proposed task-specific
knowledge. High-confidence data are identified when pseudo-labels from both
sources align, while mismatches are treated as low-confidence data. A bimodal
classifier is then trained to iteratively label the low-confidence data until a
predefined criterion is met. We evaluate our SSL framework on emotion
recognition and dementia detection tasks. Experimental results demonstrate that
our method achieves competitive performance compared to fully supervised
learning using only 30% of the labeled data and significantly outperforms two
selected baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Navigating Weight Prediction with Diet Diary <span class="chip">ACM MM'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.05445v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.05445v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinxuan Gui, Bin Zhu, Jingjing Chen, Chong-Wah Ngo, Yu-Gang Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current research in food analysis primarily concentrates on tasks such as
food recognition, recipe retrieval and nutrition estimation from a single
image. Nevertheless, there is a significant gap in exploring the impact of food
intake on physiological indicators (e.g., weight) over time. This paper
addresses this gap by introducing the DietDiary dataset, which encompasses
daily dietary diaries and corresponding weight measurements of real users.
Furthermore, we propose a novel task of weight prediction with a dietary diary
that aims to leverage historical food intake and weight to predict future
weights. To tackle this task, we propose a model-agnostic time series
forecasting framework. Specifically, we introduce a Unified Meal Representation
Learning (UMRL) module to extract representations for each meal. Additionally,
we design a diet-aware loss function to associate food intake with weight
variations. By conducting experiments on the DietDiary dataset with two
state-of-the-art time series forecasting models, NLinear and iTransformer, we
demonstrate that our proposed framework achieves superior performance compared
to the original models. We make our dataset, code, and models publicly
available at: https://yxg1005.github.io/weight-prediction/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACM MM'24 oral</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChatDiet: Empowering Personalized Nutrition-Oriented Food Recommender
  Chatbots through an LLM-Augmented Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00781v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00781v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongqi Yang, Elahe Khatibi, Nitish Nagesh, Mahyar Abbasian, Iman Azimi, Ramesh Jain, Amir M. Rahmani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The profound impact of food on health necessitates advanced
nutrition-oriented food recommendation services. Conventional methods often
lack the crucial elements of personalization, explainability, and
interactivity. While Large Language Models (LLMs) bring interpretability and
explainability, their standalone use falls short of achieving true
personalization. In this paper, we introduce ChatDiet, a novel LLM-powered
framework designed specifically for personalized nutrition-oriented food
recommendation chatbots. ChatDiet integrates personal and population models,
complemented by an orchestrator, to seamlessly retrieve and process pertinent
information. The personal model leverages causal discovery and inference
techniques to assess personalized nutritional effects for a specific user,
whereas the population model provides generalized information on food
nutritional content. The orchestrator retrieves, synergizes and delivers the
output of both models to the LLM, providing tailored food recommendations
designed to support targeted health outcomes. The result is a dynamic delivery
of personalized and explainable food recommendations, tailored to individual
user preferences. Our evaluation of ChatDiet includes a compelling case study,
where we establish a causal personal model to estimate individual nutrition
effects. Our assessments, including a food recommendation test showcasing a
92\% effectiveness rate, coupled with illustrative dialogue examples,
underscore ChatDiet's strengths in explainability, personalization, and
interactivity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published on Smart Health</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HybridVC: Efficient Voice Style Conversion with Text and Audio <span class="highlight-title">Prompt</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.15637v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.15637v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinlei Niu, Jing Zhang, Charles Patrick Martin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce HybridVC, a voice conversion (VC) framework built upon a
pre-trained conditional variational autoencoder (CVAE) that combines the
strengths of a latent model with contrastive learning. HybridVC supports text
and audio prompts, enabling more flexible voice style conversion. HybridVC
models a latent distribution conditioned on speaker embeddings acquired by a
pretrained speaker encoder and optimises style text embeddings to align with
the speaker style information through contrastive learning in parallel.
Therefore, HybridVC can be efficiently trained under limited computational
resources. Our experiments demonstrate HybridVC's superior training efficiency
and its capability for advanced multi-modal voice style conversion. This
underscores its potential for widespread applications such as user-defined
personalised voice in various social media platforms. A comprehensive ablation
study further validates the effectiveness of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of Interspeech</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-09-24T00:00:00Z">2024-09-24</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">58</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Unified Hallucination Mitigation Framework for Large Vision-Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16494v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16494v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Chang, Liqiang Jing, Xiaopeng Zhang, Yue Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hallucination is a common problem for Large Vision-Language Models (LVLMs)
with long generations which is difficult to eradicate. The generation with
hallucinations is partially inconsistent with the image content. To mitigate
hallucination, current studies either focus on the process of model inference
or the results of model generation, but the solutions they design sometimes do
not deal appropriately with various types of queries and the hallucinations of
the generations about these queries. To accurately deal with various
hallucinations, we present a unified framework, Dentist, for hallucination
mitigation. The core step is to first classify the queries, then perform
different processes of hallucination mitigation based on the classification
result, just like a dentist first observes the teeth and then makes a plan. In
a simple deployment, Dentist can classify queries as perception or reasoning
and easily mitigate potential hallucinations in answers which has been
demonstrated in our experiments. On MMbench, we achieve a 13.44%/10.2%/15.8%
improvement in accuracy on Image Quality, a Coarse Perception visual question
answering (VQA) task, over the baseline InstructBLIP/LLaVA/VisualGLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by TMLR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Knowledge Tracing in Tutor-Student Dialogues 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16490v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16490v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Scarlatos, Andrew Lan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in large language models (LLMs) have led to the development
of artificial intelligence (AI)-powered tutoring chatbots, showing promise in
providing broad access to high-quality personalized education. Existing works
have primarily studied how to make LLMs follow tutoring principles but not how
to model student behavior in dialogues. However, analyzing student dialogue
turns can serve as a formative assessment, since open-ended student discourse
may indicate their knowledge levels and reveal specific misconceptions. In this
work, we present a first attempt at performing knowledge tracing (KT) in
tutor-student dialogues. We propose LLM prompting methods to identify the
knowledge components/skills involved in each dialogue turn and diagnose whether
the student responds correctly to the tutor, and verify the LLM's effectiveness
via an expert human evaluation. We then apply a range of KT methods on the
resulting labeled data to track student knowledge levels over an entire
dialogue. We conduct experiments on two tutoring dialogue datasets, and show
that a novel yet simple LLM-based method, LLMKT, significantly outperforms
existing KT methods in predicting student response correctness in dialogues. We
perform extensive qualitative analyses to highlight the challenges in dialogue
KT and outline multiple avenues for future work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spelling Correction through Rewriting of Non-Autoregressive ASR Lattices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16469v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16469v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonid Velikovich, Christopher Li, Diamantino Caseiro, Shankar Kumar, Pat Rondon, Kandarp Joshi, Xavier Velez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For end-to-end Automatic Speech Recognition (ASR) models, recognizing
personal or rare phrases can be hard. A promising way to improve accuracy is
through spelling correction (or rewriting) of the ASR lattice, where
potentially misrecognized phrases are replaced with acoustically similar and
contextually relevant alternatives. However, rewriting is challenging for ASR
models trained with connectionist temporal classification (CTC) due to noisy
hypotheses produced by a non-autoregressive, context-independent beam search.
  We present a finite-state transducer (FST) technique for rewriting wordpiece
lattices generated by Transformer-based CTC models. Our algorithm performs
grapheme-to-phoneme (G2P) conversion directly from wordpieces into phonemes,
avoiding explicit word representations and exploiting the richness of the CTC
lattice. Our approach requires no retraining or modification of the ASR model.
We achieved up to a 15.2% relative reduction in sentence error rate (SER) on a
test set with contextually relevant entities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Strategies for Improving NL-to-FOL Translation with LLMs: Data
  Generation, Incremental Fine-Tuning, and Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16461v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16461v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ramya Keerthy Thatikonda, Jiuzhou Han, Wray Buntine, Ehsan Shareghi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Logical reasoning is a fundamental task in natural language processing that
presents significant challenges to Large Language Models (LLMs). The inherent
characteristics of logical reasoning makes it well-suited for symbolic
representations such as first-order logic (FOL). Research in symbolic logical
reasoning explored FOL generation using state-of-the-art LLMs (i.e., GPT-4) to
produce FOL translations of natural language (NL) statements, but errors in
translation are usually not the focus. We address this by categorizing the
translation errors in FOL statements generated by LLMs. To make progress
towards improving the quality of FOL translations for smaller language models
such as LLaMA-2 13B and Mistral 7B, we create ProofFOL, a high-quality
FOL-annotated subset of ProofWriter dataset using GPT-4o. The models fine-tuned
on this silver standard data achieve a significant gain in performance when
compared to larger language models such as LLaMA-2 70B. In addition to
improving the model using large data, we also tackle the issue of data scarcity
and introduce an incremental framework encompassing of data augmentation and
verification steps. In the augmentation process, a single pair of (premises,
conclusion) is split into multiple new instances based on the predicates and
FOLs. This data is used for fine-tuning, and the inference on this model
generates FOLs with fewer errors over the model trained on the original data.
Our investigation on the translation errors leads to generation of a
perturbation dataset, which is used to train a verifier that corrects potential
syntactic and semantic FOL translation errors. We demonstrate an efficient
method for making the most of a limited existing human-annotated dataset. Our
results show state-of-the-art performance for ProofWriter and ProntoQA datasets
using ProofFOL on LLaMA-2 and Mistral models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FMDLlama: Financial Misinformation Detection based on Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16452v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16452v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiwei Liu, Xin Zhang, Kailai Yang, Qianqian Xie, Jimin Huang, Sophia Ananiadou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of social media has made the spread of misinformation easier.
In the financial domain, the accuracy of information is crucial for various
aspects of financial market, which has made financial misinformation detection
(FMD) an urgent problem that needs to be addressed. Large language models
(LLMs) have demonstrated outstanding performance in various fields. However,
current studies mostly rely on traditional methods and have not explored the
application of LLMs in the field of FMD. The main reason is the lack of FMD
instruction tuning datasets and evaluation benchmarks. In this paper, we
propose FMDLlama, the first open-sourced instruction-following LLMs for FMD
task based on fine-tuning Llama3.1 with instruction data, the first multi-task
FMD instruction dataset (FMDID) to support LLM instruction tuning, and a
comprehensive FMD evaluation benchmark (FMD-B) with classification and
explanation generation tasks to test the FMD ability of LLMs. We compare our
models with a variety of LLMs on FMD-B, where our model outperforms all other
open-sourced LLMs as well as ChatGPT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comprehensive <span class="highlight-title">Survey</span> of Bias in LLMs: Current Landscape and Future
  Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16430v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16430v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rajesh Ranjan, Shailja Gupta, Surya Narayan Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models(LLMs) have revolutionized various applications in
natural language processing (NLP) by providing unprecedented text generation,
translation, and comprehension capabilities. However, their widespread
deployment has brought to light significant concerns regarding biases embedded
within these models. This paper presents a comprehensive survey of biases in
LLMs, aiming to provide an extensive review of the types, sources, impacts, and
mitigation strategies related to these biases. We systematically categorize
biases into several dimensions. Our survey synthesizes current research
findings and discusses the implications of biases in real-world applications.
Additionally, we critically assess existing bias mitigation techniques and
propose future research directions to enhance fairness and equity in LLMs. This
survey serves as a foundational resource for researchers, practitioners, and
policymakers concerned with addressing and understanding biases in LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2 Tables, 1 Figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Acoustic Features for Robust ASR <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16399v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16399v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad A. Shah, Bhiksha Raj
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic Speech Recognition (ASR) systems must be robust to the myriad types
of noises present in real-world environments including environmental noise,
room impulse response, special effects as well as attacks by malicious actors
(adversarial attacks). Recent works seek to improve accuracy and robustness by
developing novel Deep Neural Networks (DNNs) and curating diverse training
datasets for them, while using relatively simple acoustic features. While this
approach improves robustness to the types of noise present in the training
data, it confers limited robustness against unseen noises and negligible
robustness to adversarial attacks. In this paper, we revisit the approach of
earlier works that developed acoustic features inspired by biological auditory
perception that could be used to perform accurate and robust ASR. In contrast,
Specifically, we evaluate the ASR accuracy and robustness of several
biologically inspired acoustic features. In addition to several features from
prior works, such as gammatone filterbank features (GammSpec), we also propose
two new acoustic features called frequency masked spectrogram (FreqMask) and
difference of gammatones spectrogram (DoGSpec) to simulate the
neuro-psychological phenomena of frequency masking and lateral suppression.
Experiments on diverse models and datasets show that (1) DoGSpec achieves
significantly better robustness than the highly popular log mel spectrogram
(LogMelSpec) with minimal accuracy degradation, and (2) GammSpec achieves
better accuracy and robustness to non-adversarial noises from the Speech Robust
Bench benchmark, but it is outperformed by DoGSpec against adversarial attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RISCORE: Enhancing In-Context Riddle Solving in Language Models through
  Context-Reconstructed Example Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16383v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16383v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ioannis Panagiotopoulos, Giorgos Filandrianos, Maria Lymperaiou, Giorgos Stamou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Riddle-solving requires advanced reasoning skills, pushing LLMs to engage in
abstract thinking and creative problem-solving, often revealing limitations in
their cognitive abilities. In this paper, we examine the riddle-solving
capabilities of LLMs using a multiple-choice format, exploring how different
prompting techniques impact performance on riddles that demand diverse
reasoning skills. To enhance results, we introduce RISCORE (RIddle Solving with
COntext REcontruciton) a novel fully automated prompting method that generates
and utilizes contextually reconstructed sentence-based puzzles in conjunction
with the original examples to create few-shot exemplars. Our experiments
demonstrate that RISCORE significantly improves the performance of language
models in both vertical and lateral thinking tasks, surpassing traditional
exemplar selection strategies across a variety of few-shot settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do the Right Thing, Just Debias! Multi-Category Bias Mitigation Using
  LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16371v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16371v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amartya Roy, Danush Khanna, Devanshu Mahapatra,  Vasanthakumar, Avirup Das, Kripabandhu Ghosh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper tackles the challenge of building robust and generalizable bias
mitigation models for language. Recognizing the limitations of existing
datasets, we introduce ANUBIS, a novel dataset with 1507 carefully curated
sentence pairs encompassing nine social bias categories. We evaluate
state-of-the-art models like T5, utilizing Supervised Fine-Tuning (SFT),
Reinforcement Learning (PPO, DPO), and In-Context Learning (ICL) for effective
bias mitigation. Our analysis focuses on multi-class social bias reduction,
cross-dataset generalizability, and environmental impact of the trained models.
ANUBIS and our findings offer valuable resources for building more equitable AI
systems and contribute to the development of responsible and unbiased
technologies with broad societal impact.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 5 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quality Matters: Evaluating Synthetic Data for Tool-Using LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16341v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16341v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shadi Iskander, Nachshon Cohen, Zohar Karnin, Ori Shapira, Sofia Tolmach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training large language models (LLMs) for external tool usage is a rapidly
expanding field, with recent research focusing on generating synthetic data to
address the shortage of available data. However, the absence of systematic data
quality checks poses complications for properly training and testing models. To
that end, we propose two approaches for assessing the reliability of data for
training LLMs to use external tools. The first approach uses intuitive,
human-defined correctness criteria. The second approach uses a model-driven
assessment with in-context evaluation. We conduct a thorough evaluation of data
quality on two popular benchmarks, followed by an extrinsic evaluation that
showcases the impact of data quality on model performance. Our results
demonstrate that models trained on high-quality data outperform those trained
on unvalidated data, even when trained with a smaller quantity of data. These
findings empirically support the significance of assessing and ensuring the
reliability of training data for tool-using LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A fast and sound tagging method for discontinuous named-entity
  recognition <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16243v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16243v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Caio Corro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel tagging scheme for discontinuous named entity
recognition based on an explicit description of the inner structure of
discontinuous mentions. We rely on a weighted finite state automaton for both
marginal and maximum a posteriori inference. As such, our method is sound in
the sense that (1) well-formedness of predicted tag sequences is ensured via
the automaton structure and (2) there is an unambiguous mapping between
well-formed sequences of tags and (discontinuous) mentions. We evaluate our
approach on three English datasets in the biomedical domain, and report
comparable results to state-of-the-art while having a way simpler and faster
model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EuroLLM: Multilingual Language Models for Europe 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16235v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16235v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro Henrique Martins, Patrick Fernandes, João Alves, Nuno M. Guerreiro, Ricardo Rei, Duarte M. Alves, José Pombal, Amin Farajian, Manuel Faysse, Mateusz Klimaszewski, Pierre Colombo, Barry Haddow, José G. C. de Souza, Alexandra Birch, André F. T. Martins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The quality of open-weight LLMs has seen significant improvement, yet they
remain predominantly focused on English. In this paper, we introduce the
EuroLLM project, aimed at developing a suite of open-weight multilingual LLMs
capable of understanding and generating text in all official European Union
languages, as well as several additional relevant languages. We outline the
progress made to date, detailing our data collection and filtering process, the
development of scaling laws, the creation of our multilingual tokenizer, and
the data mix and modeling configurations. Additionally, we release our initial
models: EuroLLM-1.7B and EuroLLM-1.7B-Instruct and report their performance on
multilingual general benchmarks and machine translation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Enhancing Linked Data Retrieval in Conversational UIs using
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16220v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16220v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omar Mussa, Omer Rana, Benoît Goossens, Pablo Orozco-Terwengel, Charith Perera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the recent broad adoption of Large Language Models (LLMs) across
various domains, their potential for enriching information systems in
extracting and exploring Linked Data (LD) and Resource Description Framework
(RDF) triplestores has not been extensively explored. This paper examines the
integration of LLMs within existing systems, emphasising the enhancement of
conversational user interfaces (UIs) and their capabilities for data extraction
by producing more accurate SPARQL queries without the requirement for model
retraining. Typically, conversational UI models necessitate retraining with the
introduction of new datasets or updates, limiting their functionality as
general-purpose extraction tools. Our approach addresses this limitation by
incorporating LLMs into the conversational UI workflow, significantly enhancing
their ability to comprehend and process user queries effectively. By leveraging
the advanced natural language understanding capabilities of LLMs, our method
improves RDF entity extraction within web systems employing conventional
chatbots. This integration facilitates a more nuanced and context-aware
interaction model, critical for handling the complex query patterns often
encountered in RDF datasets and Linked Open Data (LOD) endpoints. The
evaluation of this methodology shows a marked enhancement in system
expressivity and the accuracy of responses to user queries, indicating a
promising direction for future research in this area. This investigation not
only underscores the versatility of LLMs in enhancing existing information
systems but also sets the stage for further explorations into their potential
applications within more specialised domains of web information systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted at the 25th International Web
  Information Systems Engineering Conference (WISE 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HelloBench: Evaluating Long Text Generation Capabilities of Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16191v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16191v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Que, Feiyu Duan, Liqun He, Yutao Mou, Wangchunshu Zhou, Jiaheng Liu, Wenge Rong, Zekun Moore Wang, Jian Yang, Ge Zhang, Junran Peng, Zhaoxiang Zhang, Songyang Zhang, Kai Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Large Language Models (LLMs) have demonstrated remarkable
capabilities in various tasks (e.g., long-context understanding), and many
benchmarks have been proposed. However, we observe that long text generation
capabilities are not well investigated. Therefore, we introduce the
Hierarchical Long Text Generation Benchmark (HelloBench), a comprehensive,
in-the-wild, and open-ended benchmark to evaluate LLMs' performance in
generating long text. Based on Bloom's Taxonomy, HelloBench categorizes long
text generation tasks into five subtasks: open-ended QA, summarization, chat,
text completion, and heuristic text generation. Besides, we propose
Hierarchical Long Text Evaluation (HelloEval), a human-aligned evaluation
method that significantly reduces the time and effort required for human
evaluation while maintaining a high correlation with human evaluation. We have
conducted extensive experiments across around 30 mainstream LLMs and observed
that the current LLMs lack long text generation capabilities. Specifically,
first, regardless of whether the instructions include explicit or implicit
length constraints, we observe that most LLMs cannot generate text that is
longer than 4000 words. Second, we observe that while some LLMs can generate
longer text, many issues exist (e.g., severe repetition and quality
degradation). Third, to demonstrate the effectiveness of HelloEval, we compare
HelloEval with traditional metrics (e.g., ROUGE, BLEU, etc.) and LLM-as-a-Judge
methods, which show that HelloEval has the highest correlation with human
evaluation. We release our code in https://github.com/Quehry/HelloBench.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Merging LoRAs like Playing LEGO: Pushing the Modularity of LoRA to
  Extremes Through Rank-Wise Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16167v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16167v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyu Zhao, Tao Shen, Didi Zhu, Zexi Li, Jing Su, Xuwu Wang, Kun Kuang, Fei Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-Rank Adaptation (LoRA) has emerged as a popular technique for fine-tuning
large language models (LLMs) to various domains due to its modular design and
widespread availability on platforms like Huggingface. This modularity has
sparked interest in combining multiple LoRAs to enhance LLM capabilities.
However, existing methods for LoRA composition primarily focus on task-specific
adaptations that require additional training, and current model merging
techniques often fail to fully leverage LoRA's modular nature, leading to
parameter interference and performance degradation. In this paper, we
investigate the feasibility of disassembling and reassembling multiple LoRAs at
a finer granularity, analogous to assembling LEGO blocks. We introduce the
concept of Minimal Semantic Units (MSUs), where the parameters corresponding to
each rank in LoRA function as independent units. These MSUs demonstrate
permutation invariance and concatenation-summation equivalence properties,
enabling flexible combinations to create new LoRAs. Building on these insights,
we propose the LoRA-LEGO framework. This framework conducts rank-wise parameter
clustering by grouping MSUs from different LoRAs into $k$ clusters. The
centroid of each cluster serves as a representative MSU, enabling the assembly
of a merged LoRA with an adjusted rank of $k$. Additionally, we apply a dual
reweighting strategy to optimize the scale of the merged LoRA. Experiments
across various benchmarks demonstrate that our method outperforms existing
approaches in LoRA merging.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Controlling Risk of Retrieval-augmented Generation: A Counterfactual
  <span class="highlight-title">Prompt</span>ing Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16146v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16146v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lu Chen, Ruqing Zhang, Jiafeng Guo, Yixing Fan, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) has emerged as a popular solution to
mitigate the hallucination issues of large language models. However, existing
studies on RAG seldom address the issue of predictive uncertainty, i.e., how
likely it is that a RAG model's prediction is incorrect, resulting in
uncontrollable risks in real-world applications. In this work, we emphasize the
importance of risk control, ensuring that RAG models proactively refuse to
answer questions with low confidence. Our research identifies two critical
latent factors affecting RAG's confidence in its predictions: the quality of
the retrieved results and the manner in which these results are utilized. To
guide RAG models in assessing their own confidence based on these two latent
factors, we develop a counterfactual prompting framework that induces the
models to alter these factors and analyzes the effect on their answers. We also
introduce a benchmarking procedure to collect answers with the option to
abstain, facilitating a series of experiments. For evaluation, we introduce
several risk-related metrics and the experimental results demonstrate the
effectiveness of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HA-FGOVD: Highlighting Fine-grained Attributes via Explicit Linear
  Composition for Open-Vocabulary Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16136v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16136v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqi Ma, Mengyin Liu, Chao Zhu, Xu-Cheng Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-vocabulary object detection (OVD) models are considered to be Large
Multi-modal Models (LMM), due to their extensive training data and a large
number of parameters. Mainstream OVD models prioritize object coarse-grained
category rather than focus on their fine-grained attributes, e.g., colors or
materials, thus failed to identify objects specified with certain attributes.
However, OVD models are pretrained on large-scale image-text pairs with rich
attribute words, whose latent feature space can represent the global text
feature as a linear composition of fine-grained attribute tokens without
highlighting them. Therefore, we propose in this paper a universal and explicit
approach for frozen mainstream OVD models that boosts their attribute-level
detection capabilities by highlighting fine-grained attributes in explicit
linear space. Firstly, a LLM is leveraged to highlight attribute words within
the input text as a zero-shot prompted task. Secondly, by strategically
adjusting the token masks, the text encoders of OVD models extract both global
text and attribute-specific features, which are then explicitly composited as
two vectors in linear space to form the new attribute-highlighted feature for
detection tasks, where corresponding scalars are hand-crafted or learned to
reweight both two vectors. Notably, these scalars can be seamlessly transferred
among different OVD models, which proves that such an explicit linear
composition is universal. Empirical evaluation on the FG-OVD dataset
demonstrates that our proposed method uniformly improves fine-grained
attribute-level OVD of various mainstream models and achieves new
state-of-the-art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Implicit assessment of language learning during practice as accurate as
  explicit testing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16133v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16133v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jue Hou, Anisia Katinskaia, Anh-Duc Vu, Roman Yangarber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assessment of proficiency of the learner is an essential part of Intelligent
Tutoring Systems (ITS). We use Item Response Theory (IRT) in computer-aided
language learning for assessment of student ability in two contexts: in test
sessions, and in exercises during practice sessions. Exhaustive testing across
a wide range of skills can provide a detailed picture of proficiency, but may
be undesirable for a number of reasons. Therefore, we first aim to replace
exhaustive tests with efficient but accurate adaptive tests. We use learner
data collected from exhaustive tests under imperfect conditions, to train an
IRT model to guide adaptive tests. Simulations and experiments with real
learner data confirm that this approach is efficient and accurate. Second, we
explore whether we can accurately estimate learner ability directly from the
context of practice with exercises, without testing. We transform learner data
collected from exercise sessions into a form that can be used for IRT modeling.
This is done by linking the exercises to {\em linguistic constructs}; the
constructs are then treated as "items" within IRT. We present results from
large-scale studies with thousands of learners. Using teacher assessments of
student ability as "ground truth," we compare the estimates obtained from tests
vs. those from exercises. The experiments confirm that the IRT models can
produce accurate ability estimation based on exercises.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MOSS: Enabling Code-Driven Evolution and Context Management for AI
  Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16120v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16120v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming Zhu, Yi Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing AI agents powered by large language models (LLMs) faces
significant challenges in achieving true Turing completeness and adaptive,
code-driven evolution. Current approaches often generate code independently of
its runtime context, relying heavily on the LLM's memory, which results in
inefficiencies and limits adaptability. Manual protocol development in sandbox
environments further constrains the agent's autonomous adaptability. Crucially,
achieving consistency in code and context across multi-turn interactions and
ensuring isolation of local variables within each interaction remains an
unsolved problem.
  We introduce MOSS (llM-oriented Operating System Simulation), a novel
framework that addresses these challenges by integrating code generation with a
dynamic context management system. MOSS ensures consistency and adaptability by
using a mechanism that maintains the Python context across interactions,
including isolation of local variables and preservation of runtime integrity.
At its core, the framework employs an Inversion of Control (IoC) container in
conjunction with decorators to enforce the least knowledge principle, allowing
agents to focus on abstract interfaces rather than concrete implementations.
This facilitates seamless integration of new tools and libraries, enables
runtime instance replacement, and reduces prompt complexity, providing a "what
you see is what you get" environment for the agent.
  Through a series of case studies, we show how this framework can enhance the
efficiency and capabilities of agent development and highlight its advantages
in moving towards Turing-complete agents capable of evolving through code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Hint Generation Approaches in Open-Domain Question Answering <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16096v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16096v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jamshid Mozafari, Abdelrahman Abdallah, Bhawna Piryani, Adam Jatowt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic Question Answering (QA) systems rely on contextual information to
provide accurate answers. Commonly, contexts are prepared through either
retrieval-based or generation-based methods. The former involves retrieving
relevant documents from a corpus like Wikipedia, whereas the latter uses
generative models such as Large Language Models (LLMs) to generate the context.
In this paper, we introduce a novel context preparation approach called HINTQA,
which employs Automatic Hint Generation (HG) techniques. Unlike traditional
methods, HINTQA prompts LLMs to produce hints about potential answers for the
question rather than generating relevant context. We evaluate our approach
across three QA datasets including TriviaQA, NaturalQuestions, and Web
Questions, examining how the number and order of hints impact performance. Our
findings show that the HINTQA surpasses both retrieval-based and
generation-based approaches. We demonstrate that hints enhance the accuracy of
answers more than retrieved and generated contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unlocking Markets: A Multilingual Benchmark to Cross-Market Question
  Answering <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16025v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16025v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Yuan, Yang Deng, Anders Søgaard, Mohammad Aliannejadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Users post numerous product-related questions on e-commerce platforms,
affecting their purchase decisions. Product-related question answering (PQA)
entails utilizing product-related resources to provide precise responses to
users. We propose a novel task of Multilingual Cross-market Product-based
Question Answering (MCPQA) and define the task as providing answers to
product-related questions in a main marketplace by utilizing information from
another resource-rich auxiliary marketplace in a multilingual context. We
introduce a large-scale dataset comprising over 7 million questions from 17
marketplaces across 11 languages. We then perform automatic translation on the
Electronics category of our dataset, naming it as McMarket. We focus on two
subtasks: review-based answer generation and product-related question ranking.
For each subtask, we label a subset of McMarket using an LLM and further
evaluate the quality of the annotations via human assessment. We then conduct
experiments to benchmark our dataset, using models ranging from traditional
lexical models to LLMs in both single-market and cross-market scenarios across
McMarket and the corresponding LLM subset. Results show that incorporating
cross-market information significantly enhances performance in both tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI Can Be Cognitively Biased: An Exploratory Study on Threshold Priming
  in LLM-Based Batch Relevance Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16022v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16022v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nuo Chen, Jiqun Liu, Xiaoyu Dong, Qijiong Liu, Tetsuya Sakai, Xiao-Ming Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cognitive biases are systematic deviations in thinking that lead to
irrational judgments and problematic decision-making, extensively studied
across various fields. Recently, large language models (LLMs) have shown
advanced understanding capabilities but may inherit human biases from their
training data. While social biases in LLMs have been well-studied, cognitive
biases have received less attention, with existing research focusing on
specific scenarios. The broader impact of cognitive biases on LLMs in various
decision-making contexts remains underexplored. We investigated whether LLMs
are influenced by the threshold priming effect in relevance judgments, a core
task and widely-discussed research topic in the Information Retrieval (IR)
coummunity. The priming effect occurs when exposure to certain stimuli
unconsciously affects subsequent behavior and decisions. Our experiment
employed 10 topics from the TREC 2019 Deep Learning passage track collection,
and tested AI judgments under different document relevance scores, batch
lengths, and LLM models, including GPT-3.5, GPT-4, LLaMa2-13B and LLaMa2-70B.
Results showed that LLMs tend to give lower scores to later documents if
earlier ones have high relevance, and vice versa, regardless of the combination
and model used. Our finding demonstrates that LLM%u2019s judgments, similar to
human judgments, are also influenced by threshold priming biases, and suggests
that researchers and system engineers should take into account potential
human-like cognitive biases in designing, evaluating, and auditing LLMs in IR
tasks and beyond.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging Speech and Text: Enhancing ASR with Pinyin-to-Character
  <span class="highlight-title">Pre-train</span>ing in LLMs <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16005v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16005v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Yuhang, Peng Yizhou, Eng Siong Chng, Xionghu Zhong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of large language models (LLMs) with pre-trained speech
models has opened up new avenues in automatic speech recognition (ASR). While
LLMs excel in multimodal understanding tasks, effectively leveraging their
capabilities for ASR remains a significant challenge. This paper presents a
novel training approach to enhance LLM performance in ASR tasks. We propose
pre-training LLMs on Pinyin embedding sequences, which represent pronunciation
features, to generate corresponding Chinese characters. This step enables the
LLM to adapt to generating text from pronunciation features before encountering
real speech data. Furthermore, we fine-tune the LoRA parameters to enhance the
LLM's understanding of speech modality information. In AISHELL-1 corpus, our
approach yields a 9.5% relative improvement in ASR tasks compared to the
baseline without Pinyi-to-Character pre-training. Additionally, incorporating
auxiliary text data for Pinyi-to-Character pre-training further boosts
performance, achieving a 19.0% relative improvement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ISCSLP2024-Special session-Speech Processing in LLM Era</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Finetuning LLMs for Comparative Assessment Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15979v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15979v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vatsal Raina, Adian Liusie, Mark Gales
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated assessment in natural language generation is a challenging task.
Instruction-tuned large language models (LLMs) have shown promise in
reference-free evaluation, particularly through comparative assessment.
However, the quadratic computational complexity of pairwise comparisons limits
its scalability. To address this, efficient comparative assessment has been
explored by applying comparative strategies on zero-shot LLM probabilities. We
propose a framework for finetuning LLMs for comparative assessment to align the
model's output with the target distribution of comparative probabilities. By
training on soft probabilities, our approach improves state-of-the-art
performance while maintaining high performance with an efficient subset of
comparisons.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ StyleSinger 2: Zero-Shot Singing Voice Synthesis with Style Transfer and
  Multi-Level Style Control <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15977v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15977v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Zhang, Ziyue Jiang, Ruiqi Li, Changhao Pan, Jinzheng He, Rongjie Huang, Chuxin Wang, Zhou Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-shot singing voice synthesis (SVS) with style transfer and style control
aims to generate high-quality singing voices with unseen timbres and styles
(including singing method, emotion, rhythm, technique, and pronunciation) from
audio and text prompts. However, the multifaceted nature of singing styles
poses a significant challenge for effective modeling, transfer, and control.
Furthermore, current SVS models often fail to generate singing voices rich in
stylistic nuances for unseen singers. To address these challenges, we introduce
StyleSinger 2, the first zero-shot SVS model for style transfer across
cross-lingual speech and singing styles, along with multi-level style control.
Specifically, StyleSinger 2 proposes three primary modules: 1) the clustering
style encoder employs a clustering vector quantization model to stably condense
style information into a compact latent space; 2) the Style and Duration
Language Model (S\&D-LM) concurrently predicts style information and phoneme
duration, which benefits both; 3) the style adaptive decoder uses a novel
mel-style adaptive normalization method to generate singing voices with
enhanced details. Experimental results show that StyleSinger 2 outperforms all
baseline models in synthesis quality, singer similarity, and style
controllability across various tasks, including zero-shot style transfer,
multi-level style control, cross-lingual style transfer, and speech-to-singing
style transfer. Singing voice samples can be accessed at
https://stylesinger2.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beats of Bias: Analyzing Lyrics with Topic Modeling and Gender Bias
  Measurements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15949v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15949v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danqing Chen, Adithi Satish, Rasul Khanbayov, Carolin M. Schuster, Georg Groh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper uses topic modeling and bias measurement techniques to analyze and
determine gender bias in English song lyrics. We utilize BERTopic to cluster
537,553 English songs into distinct topics and chart their development over
time. Our analysis shows the thematic shift in song lyrics over the years, from
themes of romance to the increasing sexualization of women in songs. We observe
large amounts of profanity and misogynistic lyrics on various topics,
especially in the overall biggest cluster. Furthermore, to analyze gender bias
across topics and genres, we employ the Single Category Word Embedding
Association Test (SC-WEAT) to compute bias scores for the word embeddings
trained on the most popular topics as well as for each genre. We find that
words related to intelligence and strength tend to show a male bias across
genres, as opposed to appearance and weakness words, which are more
female-biased; however, a closer look also reveals differences in biases across
topics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted and presented at the 17th International Conference on Social
  Computing, Behavioral-Cultural Modeling, & Prediction and Behavior
  Representation in Modeling and Simulation (see
  https://sbp-brims.org/2024/papers/working-papers/Chen_SBP-BRiMS2024_Final_31.pdf
  )</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated test generation to evaluate tool-augmented LLMs as
  conversational AI agents <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15934v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15934v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Arcadinho, David Aparicio, Mariana Almeida
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tool-augmented LLMs are a promising approach to create AI agents that can
have realistic conversations, follow procedures, and call appropriate
functions. However, evaluating them is challenging due to the diversity of
possible conversations, and existing datasets focus only on single interactions
and function-calling. We present a test generation pipeline to evaluate LLMs as
conversational AI agents. Our framework uses LLMs to generate diverse tests
grounded on user-defined procedures. For that, we use intermediate graphs to
limit the LLM test generator's tendency to hallucinate content that is not
grounded on input procedures, and enforces high coverage of the possible
conversations. Additionally, we put forward ALMITA, a manually curated dataset
for evaluating AI agents in customer support, and use it to evaluate existing
LLMs. Our results show that while tool-augmented LLMs perform well in single
interactions, they often struggle to handle complete conversations. While our
focus is on customer support, our method is general and capable of AI agents
for different domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures, Submitted to GenBench@EMNLP2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SLIMER-IT: Zero-Shot NER on Italian Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15933v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15933v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Zamai, Leonardo Rigutini, Marco Maggini, Andrea Zugarini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional approaches to Named Entity Recognition (NER) frame the task into
a BIO sequence labeling problem. Although these systems often excel in the
downstream task at hand, they require extensive annotated data and struggle to
generalize to out-of-distribution input domains and unseen entity types. On the
contrary, Large Language Models (LLMs) have demonstrated strong zero-shot
capabilities. While several works address Zero-Shot NER in English, little has
been done in other languages. In this paper, we define an evaluation framework
for Zero-Shot NER, applying it to the Italian language. Furthermore, we
introduce SLIMER-IT, the Italian version of SLIMER, an instruction-tuning
approach for zero-shot NER leveraging prompts enriched with definition and
guidelines. Comparisons with other state-of-the-art models, demonstrate the
superiority of SLIMER-IT on never-seen-before entity tags.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multilingual Transfer and Domain Adaptation for Low-Resource Languages
  of Spain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15924v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15924v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanchang Luo, Zhanglin Wu, Daimeng Wei, Hengchao Shang, Zongyao Li, Jiaxin Guo, Zhiqiang Rao, Shaojun Li, Jinlong Yang, Yuhao Xie, Jiawei Zheng Bin Wei, Hao Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article introduces the submission status of the Translation into
Low-Resource Languages of Spain task at (WMT 2024) by Huawei Translation
Service Center (HW-TSC). We participated in three translation tasks: spanish to
aragonese (es-arg), spanish to aranese (es-arn), and spanish to asturian
(es-ast). For these three translation tasks, we use training strategies such as
multilingual transfer, regularized dropout, forward translation and back
translation, labse denoising, transduction ensemble learning and other
strategies to neural machine translation (NMT) model based on training deep
transformer-big architecture. By using these enhancement strategies, our
submission achieved a competitive result in the final evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages,wmt24. arXiv admin note: substantial text overlap with
  arXiv:2409.14842; text overlap with arXiv:2409.14800</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explaining word embeddings with perfect fidelity: Case study in research
  impact prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15912v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15912v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucie Dvorackova, Marcin P. Joachimiak, Michal Cerny, Adriana Kubecova, Vilem Sklenak, Tomas Kliegr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Best performing approaches for scholarly document quality prediction are
based on embedding models, which do not allow direct explanation of classifiers
as distinct words no longer correspond to the input features for model
training. Although model-agnostic explanation methods such as Local
interpretable model-agnostic explanations (LIME) can be applied, these produce
results with questionable correspondence to the ML model. We introduce a new
feature importance method, Self-model Rated Entities (SMER), for logistic
regression-based classification models trained on word embeddings. We show that
SMER has theoretically perfect fidelity with the explained model, as its
prediction corresponds exactly to the average of predictions for individual
words in the text. SMER allows us to reliably determine which words or entities
positively contribute to predicting impactful articles. Quantitative and
qualitative evaluation is performed through five diverse experiments conducted
on 50.000 research papers from the CORD-19 corpus. Through an AOPC curve
analysis, we experimentally demonstrate that SMER produces better explanations
than LIME for logistic regression.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Modular-based Strategy for Mitigating Gradient Conflicts in
  Simultaneous Speech Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15911v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15911v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoqian Liu, Yangfan Du, Jianjin Wang, Yuan Ge, Chen Xu, Tong Xiao, Guocheng Chen, Jingbo Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simultaneous Speech Translation (SimulST) involves generating target language
text while continuously processing streaming speech input, presenting
significant real-time challenges. Multi-task learning is often employed to
enhance SimulST performance but introduces optimization conflicts between
primary and auxiliary tasks, potentially compromising overall efficiency. The
existing model-level conflict resolution methods are not well-suited for this
task which exacerbates inefficiencies and leads to high GPU memory consumption.
To address these challenges, we propose a Modular Gradient Conflict Mitigation
(MGCM) strategy that detects conflicts at a finer-grained modular level and
resolves them utilizing gradient projection. Experimental results demonstrate
that MGCM significantly improves SimulST performance, particularly under medium
and high latency conditions, achieving a 0.68 BLEU score gain in offline tasks.
Additionally, MGCM reduces GPU memory consumption by over 95\% compared to
other conflict mitigation methods, establishing it as a robust solution for
SimulST tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Text-to-SQL Capabilities of Large Language Models via Domain
  Database Knowledge Injection <span class="chip">ECAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15907v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15907v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyu Ma, Xin Tian, Lingxiang Wu, Xuepeng Wang, Xueming Tang, Jinqiao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-SQL is a subtask in semantic parsing that has seen rapid progress
with the evolution of Large Language Models (LLMs). However, LLMs face
challenges due to hallucination issues and a lack of domain-specific database
knowledge(such as table schema and cell values). As a result, they can make
errors in generating table names, columns, and matching values to the correct
columns in SQL statements. This paper introduces a method of knowledge
injection to enhance LLMs' ability to understand schema contents by
incorporating prior knowledge. This approach improves their performance in
Text-to-SQL tasks. Experimental results show that pre-training LLMs on
domain-specific database knowledge and fine-tuning them on downstream
Text-to-SQL tasks significantly improves the Execution Match (EX) and Exact
Match (EM) metrics across various models. This effectively reduces errors in
generating column names and matching values to the columns. Furthermore, the
knowledge-injected models can be applied to many downstream Text-to-SQL tasks,
demonstrating the generalizability of the approach presented in this paper.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by ECAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Konstruktor: A Strong Baseline for Simple Knowledge Graph Question
  Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15902v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15902v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maria Lysyuk, Mikhail Salnikov, Pavel Braslavski, Alexander Panchenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While being one of the most popular question types, simple questions such as
"Who is the author of Cinderella?", are still not completely solved.
Surprisingly, even the most powerful modern Large Language Models are prone to
errors when dealing with such questions, especially when dealing with rare
entities. At the same time, as an answer may be one hop away from the question
entity, one can try to develop a method that uses structured knowledge graphs
(KGs) to answer such questions. In this paper, we introduce Konstruktor - an
efficient and robust approach that breaks down the problem into three steps:
(i) entity extraction and entity linking, (ii) relation prediction, and (iii)
querying the knowledge graph. Our approach integrates language models and
knowledge graphs, exploiting the power of the former and the interpretability
of the latter. We experiment with two named entity recognition and entity
linking methods and several relation detection techniques. We show that for
relation detection, the most challenging step of the workflow, a combination of
relation classification/generation and ranking outperforms other methods. We
report Konstruktor's strong results on four datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 2 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contextual Evaluation of Large Language Models for Classifying Tropical
  and Infectious Diseases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09201v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09201v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mercy Asiedu, Nenad Tomasev, Chintan Ghate, Tiya Tiyasirichokchai, Awa Dieng, Oluwatosin Akande, Geoffrey Siwo, Steve Adudans, Sylvanus Aitkins, Odianosen Ehiakhamen, Eric Ndombi, Katherine Heller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models (LLMs) have shown promise for medical question
answering, there is limited work focused on tropical and infectious
disease-specific exploration. We build on an opensource tropical and infectious
diseases (TRINDs) dataset, expanding it to include demographic and semantic
clinical and consumer augmentations yielding 11000+ prompts. We evaluate LLM
performance on these, comparing generalist and medical LLMs, as well as LLM
outcomes to human experts. We demonstrate through systematic experimentation,
the benefit of contextual information such as demographics, location, gender,
risk factors for optimal LLM response. Finally we develop a prototype of
TRINDs-LM, a research tool that provides a playground to navigate how context
impacts LLM outputs for health.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SurGen: Text-Guided Diffusion Model for Surgical Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.14028v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.14028v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joseph Cho, Samuel Schmidgall, Cyril Zakka, Mrudang Mathur, Dhamanpreet Kaur, Rohan Shad, William Hiesinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion-based video generation models have made significant strides,
producing outputs with improved visual fidelity, temporal coherence, and user
control. These advancements hold great promise for improving surgical education
by enabling more realistic, diverse, and interactive simulation environments.
In this study, we introduce SurGen, a text-guided diffusion model tailored for
surgical video synthesis. SurGen produces videos with the highest resolution
and longest duration among existing surgical video generation models. We
validate the visual and temporal quality of the outputs using standard image
and video generation metrics. Additionally, we assess their alignment to the
corresponding text prompts through a deep learning classifier trained on
surgical data. Our results demonstrate the potential of diffusion models to
serve as valuable educational tools for surgical trainees.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WeQA: A Benchmark for Retrieval Augmented Generation in Wind Energy
  Domain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11800v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11800v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rounak Meyur, Hung Phan, Sridevi Wagle, Jan Strube, Mahantesh Halappanavar, Sameera Horawalavithana, Anurag Acharya, Sai Munikoti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the rapidly evolving landscape of Natural Language Processing (NLP) and
text generation, the emergence of Retrieval Augmented Generation (RAG) presents
a promising avenue for improving the quality and reliability of generated text
by leveraging information retrieved from user specified database. Benchmarking
is essential to evaluate and compare the performance of the different RAG
configurations in terms of retriever and generator, providing insights into
their effectiveness, scalability, and suitability for the specific domain and
applications. In this paper, we present a comprehensive framework to generate a
domain relevant RAG benchmark. Our framework is based on automatic
question-answer generation with Human (domain experts)-AI Large Language Model
(LLM) teaming. As a case study, we demonstrate the framework by introducing
WeQA, a first-of-its-kind benchmark on the wind energy domain which comprises
of multiple scientific documents/reports related to environmental impact of
wind energy projects. Our framework systematically evaluates RAG performance
using diverse metrics and multiple question types with varying complexity
level. We also demonstrate the performance of different models on our
benchmark.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can we teach language models to gloss endangered languages? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18895v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18895v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Ginn, Mans Hulden, Alexis Palmer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interlinear glossed text (IGT) is a popular format in language documentation
projects, where each morpheme is labeled with a descriptive annotation.
Automating the creation of interlinear glossed text can be desirable to reduce
annotator effort and maintain consistency across annotated corpora. Prior
research has explored a number of statistical and neural methods for
automatically producing IGT.
  As large language models (LLMs) have showed promising results across
multilingual tasks, even for rare, endangered languages, it is natural to
wonder whether they can be utilized for the task of generating IGT. We explore
whether LLMs can be effective at the task of interlinear glossing with
in-context learning, without any traditional training. We propose new
approaches for selecting examples to provide in-context, observing that
targeted selection can significantly improve performance. We find that
LLM-based methods beat standard transformer baselines, despite requiring no
training at all. These approaches still underperform state-of-the-art
supervised systems for the task, but are highly practical for researchers
outside of the NLP community, requiring minimal effort to use.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neuron-Level Knowledge Attribution in Large Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.12141v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.12141v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeping Yu, Sophia Ananiadou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying important neurons for final predictions is essential for
understanding the mechanisms of large language models. Due to computational
constraints, current attribution techniques struggle to operate at neuron
level. In this paper, we propose a static method for pinpointing significant
neurons. Compared to seven other methods, our approach demonstrates superior
performance across three metrics. Additionally, since most static methods
typically only identify "value neurons" directly contributing to the final
prediction, we propose a method for identifying "query neurons" which activate
these "value neurons". Finally, we apply our methods to analyze six types of
knowledge across both attention and feed-forward network (FFN) layers. Our
method and analysis are helpful for understanding the mechanisms of knowledge
storage and set the stage for future research in knowledge editing. The code is
available on https://github.com/zepingyu0512/neuron-attribution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2024 main. This paper aims to identify the
  important neurons in large language models</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How do Large Language Models Learn In-Context? Query and Key Matrices of
  In-Context Heads are Two Towers for Metric Learning <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02872v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02872v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeping Yu, Sophia Ananiadou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the mechanism of in-context learning (ICL) on sentence
classification tasks with semantically-unrelated labels ("foo"/"bar"). We find
intervening in only 1\% heads (named "in-context heads") significantly affects
ICL accuracy from 87.6\% to 24.4\%. To understand this phenomenon, we analyze
the value-output vectors in these heads and discover that the vectors at each
label position contain substantial information about the corresponding labels.
Furthermore, we observe that the prediction shift from "foo" to "bar" is due to
the respective reduction and increase in these heads' attention scores at "foo"
and "bar" positions. Therefore, we propose a hypothesis for ICL: in in-context
heads, the value-output matrices extract label features, while the query-key
matrices compute the similarity between the features at the last position and
those at each label position. The query and key matrices can be considered as
two towers that learn the similarity metric between the last position's
features and each demonstration at label positions. Using this hypothesis, we
explain the majority label bias and recency bias in ICL and propose two methods
to reduce these biases by 22\% and 17\%, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2024 main. Mechanistic interpretability for
  in-contexting in large language models</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond the binary: Limitations and possibilities of gender-related
  speech technology research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13335v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13335v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ariadna Sanchez, Alice Ross, Nina Markl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a review of 107 research papers relating to speech and
sex or gender in ISCA Interspeech publications between 2013 and 2023. We note
the scarcity of work on this topic and find that terminology, particularly the
word gender, is used in ways that are underspecified and often out of step with
the prevailing view in social sciences that gender is socially constructed and
is a spectrum as opposed to a binary category. We draw attention to the
potential problems that this can cause for already marginalised groups, and
suggest some questions for researchers to ask themselves when undertaking work
on speech and gender.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Spoken Language Technology (SLT) Workshop 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can We Count on LLMs? The Fixed-Effect Fallacy and Claims of <span class="highlight-title">GPT</span>-4
  Capabilities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07638v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07638v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Ball, Shuo Chen, Cormac Herley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we explore evaluation of LLM capabilities. We present
measurements of GPT-4 performance on several deterministic tasks; each task
involves a basic calculation and takes as input parameter some element drawn
from a large well-defined population (e.g., count elements in a list, multiply
two k-digit numbers, etc). We examine several conditions per-task and perform
enough trials so that statistically significant differences can be detected.
This allows us to investigate the sensitivity of task-accuracy both to query
phrasing and input parameter population. We find that seemingly trivial
modifications in the task-prompt or input population can yield differences far
larger than can be explained by sampling effects. For example, performance on a
simple list-counting task varies with query-phrasing and list-length, but also
with list composition (i.e., the thing-to-be-counted) and object frequency
(e.g., success when an element accounts for $\approx$ 50\% of a list is
different from when it accounts for $\approx$ 70\% etc).
  We conclude that efforts to quantify LLM capabilities easily succumb to the
language-as-fixed-effect fallacy, where experimental observations are
improperly generalized beyond what the data supports. A consequence appears to
be that intuitions that have been formed based on interactions with humans form
a very unreliable guide as to which input modifications should ``make no
difference'' to LLM performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OmniBench: Towards The Future of Universal Omni-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15272v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15272v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhi Li, Ge Zhang, Yinghao Ma, Ruibin Yuan, Kang Zhu, Hangyu Guo, Yiming Liang, Jiaheng Liu, Jian Yang, Siwei Wu, Xingwei Qu, Jinjie Shi, Xinyue Zhang, Zhenzhu Yang, Xiangzhou Wang, Zhaoxiang Zhang, Zachary Liu, Emmanouil Benetos, Wenhao Huang, Chenghua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in multimodal large language models (MLLMs) have aimed to
integrate and interpret data across diverse modalities. However, the capacity
of these models to concurrently process and reason about multiple modalities
remains inadequately explored, partly due to the lack of comprehensive
modality-wise benchmarks. We introduce OmniBench, a novel benchmark designed to
rigorously evaluate models' ability to recognize, interpret, and reason across
visual, acoustic, and textual inputs simultaneously. We define models capable
of such tri-modal processing as omni-language models (OLMs). OmniBench is
distinguished by high-quality human annotations, ensuring that accurate
responses require integrated understanding and reasoning across all three
modalities. Our main findings reveal that: i) most OLMs exhibit critical
limitations in instruction-following and reasoning capabilities within
tri-modal contexts; and ii) most baselines models perform poorly (below 50\%
accuracy) even when provided with alternative textual representations of images
or/and audio. These results suggest that the ability to construct a consistent
context from text, image, and audio is often overlooked in existing MLLM
training paradigms. We advocate for future research to focus on developing more
robust tri-modal integration techniques and training strategies to enhance OLM
performance across diverse modalities. The codes and live leaderboard could be
found at https://m-a-p.ai/OmniBench.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DuQuant: Distributing Outliers via Dual Transformation Makes Stronger
  Quantized LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.01721v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.01721v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haokun Lin, Haobo Xu, Yichen Wu, Jingzhi Cui, Yingtao Zhang, Linzhan Mou, Linqi Song, Zhenan Sun, Ying Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantization of large language models (LLMs) faces significant challenges,
particularly due to the presence of outlier activations that impede efficient
low-bit representation. Traditional approaches predominantly address
$\textit{Normal Outliers}$, which are activations across all tokens with
relatively large magnitudes. However, these methods struggle with smoothing
$\textit{Massive Outliers}$ that display significantly larger values, which
leads to significant performance degradation in low-bit quantization. In this
paper, we introduce DuQuant, a novel approach that utilizes rotation and
permutation transformations to more effectively mitigate both massive and
normal outliers. First, DuQuant starts by constructing rotation matrices, using
specific outlier dimensions as prior knowledge, to redistribute outliers to
adjacent channels by block-wise rotation. Second, We further employ a zigzag
permutation to balance the distribution of outliers across blocks, thereby
reducing block-wise variance. A subsequent rotation further smooths the
activation landscape, enhancing model performance. DuQuant simplifies the
quantization process and excels in managing outliers, outperforming the
state-of-the-art baselines across various sizes and types of LLMs on multiple
tasks, even with 4-bit weight-activation quantization. Our code is available at
https://github.com/Hsu1023/DuQuant.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 13 figures, Website at https://duquant.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MINERS: Multilingual Language Models as Semantic Retrievers <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.07424v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.07424v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Genta Indra Winata, Ruochen Zhang, David Ifeoluwa Adelani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Words have been represented in a high-dimensional vector space that encodes
their semantic similarities, enabling downstream applications such as
retrieving synonyms, antonyms, and relevant contexts. However, despite recent
advances in multilingual language models (LMs), the effectiveness of these
models' representations in semantic retrieval contexts has not been
comprehensively explored. To fill this gap, this paper introduces the MINERS, a
benchmark designed to evaluate the ability of multilingual LMs in semantic
retrieval tasks, including bitext mining and classification via
retrieval-augmented contexts. We create a comprehensive framework to assess the
robustness of LMs in retrieving samples across over 200 diverse languages,
including extremely low-resource languages in challenging cross-lingual and
code-switching settings. Our results demonstrate that by solely retrieving
semantically similar embeddings yields performance competitive with
state-of-the-art approaches, without requiring any fine-tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learn and Don't Forget: Adding a New Language to ASR Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.06800v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.06800v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengjie Qian, Siyuan Tang, Rao Ma, Kate M. Knill, Mark J. F. Gales
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation ASR models often support many languages, e.g. 100 languages in
Whisper. However, there has been limited work on integrating an additional,
typically low-resource, language, while maintaining performance on the original
language set. Fine-tuning, while simple, may degrade the accuracy of the
original set. We compare three approaches that exploit adaptation parameters:
soft language code tuning, train only the language code; soft prompt tuning,
train prepended tokens; and LoRA where a small set of additional parameters are
optimised. Elastic Weight Consolidation (EWC) offers an alternative compromise
with the potential to maintain performance in specific target languages.
Results show that direct fine-tuning yields the best performance for the new
language but degrades existing language capabilities. EWC can address this
issue for specific languages. If only adaptation parameters are used, the
language capabilities are maintained but at the cost of performance in the new
language.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of Interspeech</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nine-year-old children outperformed Chat<span class="highlight-title">GPT</span> in emotion: Evidence from
  Chinese writing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00578v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00578v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyi Cao, Yizhong Xu, Tongquan Zhou, Siruo Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ChatGPT has been demonstrated to possess significant capabilities in
generating intricate, human-like text, and recent studies have established that
its performance in theory of mind tasks is comparable to that of a
nine-year-old child. However, it remains uncertain whether ChatGPT surpasses
nine-year-old children in Chinese writing proficiency. To explore this, our
study juxtaposed the Chinese writing performance of ChatGPT and nine-year-old
children on both narrative and scientific topics, aiming to uncover the
relative strengths and weaknesses of ChatGPT in writing.
  The collected data were analyzed across five linguistic dimensions: fluency,
accuracy, complexity, cohesion, and emotion. Each dimension underwent
assessment through precise indices. The findings revealed that nine-year-old
children excelled beyond ChatGPT in terms of fluency and cohesion within their
writing. In contrast, ChatGPT manifested a superior performance in accuracy
compared to the children. Concerning complexity, children exhibited superior
skills in science-themed writing, while ChatGPT prevailed in nature-themed
writing. Significantly, this research is pioneering in revealing that
nine-year-old children convey stronger emotions than ChatGPT in their Chinese
compositions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mixture of Tokens: Continuous MoE through Cross-Example Aggregation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15961v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15961v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Szymon Antoniak, Michał Krutul, Maciej Pióro, Jakub Krajewski, Jan Ludziejewski, Kamil Ciebiera, Krystian Król, Tomasz Odrzygóźdź, Marek Cygan, Sebastian Jaszczur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixture of Experts (MoE) models based on Transformer architecture are pushing
the boundaries of language and vision tasks. The allure of these models lies in
their ability to substantially increase the parameter count without a
corresponding increase in FLOPs. Most widely adopted MoE models are
discontinuous with respect to their parameters - often referred to as sparse.
At the same time, existing continuous MoE designs either lag behind their
sparse counterparts or are incompatible with autoregressive decoding. Motivated
by the observation that the adaptation of fully continuous methods has been an
overarching trend in deep learning, we develop Mixture of Tokens (MoT), a
simple, continuous architecture that is capable of scaling the number of
parameters similarly to sparse MoE models. Unlike conventional methods, MoT
assigns mixtures of tokens from different examples to each expert. This
architecture is fully compatible with autoregressive training and generation.
Our best models not only achieve a 3x increase in training speed over dense
Transformer models in language pretraining but also match the performance of
state-of-the-art MoE architectures. Additionally, a close connection between
MoT and MoE is demonstrated through a novel technique we call transition
tuning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do LLMs Know When to NOT Answer? Investigating Abstention Abilities of
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.16221v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.16221v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nishanth Madhusudhan, Sathwik Tejaswi Madhusudhan, Vikas Yadav, Masoud Hashemi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Abstention Ability (AA) is a critical aspect of Large Language Model (LLM)
reliability, referring to an LLM's capability to withhold responses when
uncertain or lacking a definitive answer, without compromising performance.
Although previous studies have attempted to improve AA, they lack a
standardised evaluation method and remain unsuitable for black-box models where
token prediction probabilities are inaccessible. This makes comparative
analysis challenging, especially for state-of-the-art closed-source commercial
LLMs. This paper bridges this gap by introducing a black-box evaluation
approach and a new dataset, Abstain-QA, crafted to rigorously assess AA across
varied question types (answerable and unanswerable), domains (well-represented
and under-represented), and task types (fact centric and reasoning). We also
propose a new confusion matrix, the ''Answerable-Unanswerable Confusion
Matrix'' (AUCM) which serves as the basis for evaluating AA, by offering a
structured and precise approach for assessment. Finally, we explore the impact
of three prompting strategies-Strict Prompting, Verbal Confidence Thresholding,
and Chain-of-Thought (CoT)-on improving AA. Our results indicate that even
powerful models like GPT-4, Mixtral 8x22b encounter difficulties with
abstention; however, strategic approaches such as Strict prompting and CoT can
enhance this capability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages (excluding limitations, references and appendix) and 5
  figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EffiQA: Efficient Question-Answering with Strategic Multi-Model
  Collaboration on Knowledge Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.01238v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.01238v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixuan Dong, Baoyun Peng, Yufei Wang, Jia Fu, Xiaodong Wang, Yongxue Shan, Xin Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models (LLMs) have shown remarkable capabilities in
natural language processing, they struggle with complex, multi-step reasoning
tasks involving knowledge graphs (KGs). Existing approaches that integrate LLMs
and KGs either underutilize the reasoning abilities of LLMs or suffer from
prohibitive computational costs due to tight coupling. To address these
limitations, we propose a novel collaborative framework named EffiQA that can
strike a balance between performance and efficiency via an iterative paradigm.
EffiQA consists of three stages: global planning, efficient KG exploration, and
self-reflection. Specifically, EffiQA leverages the commonsense capability of
LLMs to explore potential reasoning pathways through global planning. Then, it
offloads semantic pruning to a small plug-in model for efficient KG
exploration. Finally, the exploration results are fed to LLMs for
self-reflection to further improve the global planning and efficient KG
exploration. Empirical evidence on multiple KBQA benchmarks shows EffiQA's
effectiveness, achieving an optimal balance between reasoning accuracy and
computational costs. We hope the proposed new framework will pave the way for
efficient, knowledge-intensive querying by redefining the integration of LLMs
and KGs, fostering future research on knowledge-based question answering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Open Conversational LLMs do not know most Spanish words 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15491v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15491v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Javier Conde, Miguel González, Nina Melero, Raquel Ferrando, Gonzalo Martínez, Elena Merino-Gómez, José Alberto Hernández, Pedro Reviriego
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing interest in Large Language Models (LLMs) and in particular in
conversational models with which users can interact has led to the development
of a large number of open-source chat LLMs. These models are evaluated on a
wide range of benchmarks to assess their capabilities in answering questions or
solving problems on almost any possible topic or to test their ability to
reason or interpret texts. Instead, the evaluation of the knowledge that these
models have of the languages has received much less attention. For example, the
words that they can recognize and use in different languages. In this paper, we
evaluate the knowledge that open-source chat LLMs have of Spanish words by
testing a sample of words in a reference dictionary. The results show that
open-source chat LLMs produce incorrect meanings for an important fraction of
the words and are not able to use most of the words correctly to write
sentences with context. These results show how Spanish is left behind in the
open-source LLM race and highlight the need to push for linguistic fairness in
conversational LLMs ensuring that they provide similar performance across
languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Procesamiento del Lenguaje Natural, 73, 95-108</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PALLM: Evaluating and Enhancing PALLiative Care Conversations with Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15188v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15188v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyuan Wang, Fangxu Yuan, Virginia LeBaron, Tabor Flickinger, Laura E. Barnes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective patient-provider communication is crucial in clinical care,
directly impacting patient outcomes and quality of life. Traditional evaluation
methods, such as human ratings, patient feedback, and provider
self-assessments, are often limited by high costs and scalability issues.
Although existing natural language processing (NLP) techniques show promise,
they struggle with the nuances of clinical communication and require sensitive
clinical data for training, reducing their effectiveness in real-world
applications. Emerging large language models (LLMs) offer a new approach to
assessing complex communication metrics, with the potential to advance the
field through integration into passive sensing and just-in-time intervention
systems. This study explores LLMs as evaluators of palliative care
communication quality, leveraging their linguistic, in-context learning, and
reasoning capabilities. Specifically, using simulated scripts crafted and
labeled by healthcare professionals, we test proprietary models (e.g., GPT-4)
and fine-tune open-source LLMs (e.g., LLaMA2) with a synthetic dataset
generated by GPT-4 to evaluate clinical conversations, to identify key metrics
such as `understanding' and `empathy'. Our findings demonstrated LLMs' superior
performance in evaluating clinical communication, providing actionable feedback
with reasoning, and demonstrating the feasibility and practical viability of
developing in-house LLMs. This research highlights LLMs' potential to enhance
patient-provider interactions and lays the groundwork for downstream steps in
developing LLM-empowered clinical health systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM Transactions on Computing for Healthcare, Special
  Issue on Large Language Models, Conversational Systems, and Generative AI in
  Health, pending minor revisions</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficiently Dispatching Flash Attention For Partially Filled Attention
  Masks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15097v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15097v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Agniv Sharma, Jonas Geiping
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers are widely used across various applications, many of which yield
sparse or partially filled attention matrices. Examples include attention masks
designed to reduce the quadratic complexity of attention, sequence packing
techniques, and recent innovations like tree masking for fast validation in
MEDUSA. Despite the inherent sparsity in these matrices, the state-of-the-art
algorithm Flash Attention still processes them with quadratic complexity as
though they were dense. In this paper, we introduce Binary Block Masking, a
highly efficient modification that enhances Flash Attention by making it
mask-aware. We further propose two optimizations: one tailored for masks with
contiguous non-zero patterns and another for extremely sparse masks. Our
experiments on attention masks derived from real-world scenarios demonstrate up
to a 9x runtime improvement. The implementation will be publicly released to
foster further research and application.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models as Carriers of Hidden Messages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02481v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02481v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jakub Hoscilowicz, Pawel Popiolek, Jan Rudkowski, Jedrzej Bieniasz, Artur Janicki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simple fine-tuning can embed hidden text into large language models (LLMs),
which is revealed only when triggered by a specific query. Applications include
LLM fingerprinting, where a unique identifier is embedded to verify licensing
compliance, and steganography, where the LLM carries hidden messages disclosed
through a trigger query.
  Our work demonstrates that embedding hidden text via fine-tuning, although
seemingly secure due to the vast number of potential triggers, is vulnerable to
extraction through analysis of the LLM's output decoding process. We introduce
an extraction attack called Unconditional Token Forcing (UTF), which
iteratively feeds tokens from the LLM's vocabulary to reveal sequences with
high token probabilities, indicating hidden text candidates. We also present
Unconditional Token Forcing Confusion (UTFC), a defense paradigm that makes
hidden text resistant to all known extraction attacks without degrading the
general performance of LLMs compared to standard fine-tuning. UTFC has both
benign (improving LLM fingerprinting) and malign applications (using LLMs to
create covert communication channels).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress. Code is available at
  https://github.com/j-hoscilowic/zurek-stegano</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FaaF: Facts as a Function for the evaluation of generated text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03888v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03888v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vasileios Katranidis, Gabor Barany
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The demand for accurate and efficient verification of information in texts
generated by large language models (LMs) is at an all-time high, but remains
unresolved. Recent efforts have focused on extracting and verifying atomic
facts from these texts via prompting LM evaluators. However, we demonstrate
that this method of prompting is unreliable when faced with incomplete or
inaccurate reference information. We introduce Facts as a Function (FaaF), a
new approach to the fact verification task that leverages the function-calling
capabilities of LMs. FaaF significantly enhances the ability of LMs to identify
unsupported facts in texts, while also improving efficiency and significantly
lowering costs compared to prompt-based methods. Additionally, we propose a
framework for evaluating factual recall in Retrieval Augmented Generation (RAG)
systems, which we employ to compare prompt-based and FaaF methods using various
LMs under challenging conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ tinyCLAP: Distilling Constrastive Language-Audio <span class="highlight-title">Pretrain</span>ed Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.14517v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.14517v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Paissan, Elisabetta Farella
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive Language-Audio Pretraining (CLAP) became of crucial importance in
the field of audio and speech processing. Its employment ranges from sound
event detection to text-to-audio generation. However, one of the main
limitations is the considerable amount of data required in the training process
and the overall computational complexity during inference. This paper
investigates how we can reduce the complexity of contrastive language-audio
pre-trained models, yielding an efficient model that we call tinyCLAP. We
derive an unimodal distillation loss from first principles and explore how the
dimensionality of the shared, multimodal latent space can be reduced via
pruning. TinyCLAP uses only 6% of the original Microsoft CLAP parameters with a
minimal reduction (less than 5%) in zero-shot classification performance across
the three sound event detection datasets on which it was tested
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of Interspeech. Please use the citation available at
  https://www.isca-archive.org/interspeech_2024/paissan24_interspeech.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Watch Every Step! LLM Agent Learning via Iterative Step-Level Process
  Refinement <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11176v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11176v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weimin Xiong, Yifan Song, Xiutian Zhao, Wenhao Wu, Xun Wang, Ke Wang, Cheng Li, Wei Peng, Sujian Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language model agents have exhibited exceptional performance across a
range of complex interactive tasks. Recent approaches have utilized tuning with
expert trajectories to enhance agent performance, yet they primarily
concentrate on outcome rewards, which may lead to errors or suboptimal actions
due to the absence of process supervision signals. In this paper, we introduce
the Iterative step-level Process Refinement (IPR) framework, which provides
detailed step-by-step guidance to enhance agent training. Specifically, we
adopt the Monte Carlo method to estimate step-level rewards. During each
iteration, the agent explores along the expert trajectory and generates new
actions. These actions are then evaluated against the corresponding step of
expert trajectory using step-level rewards. Such comparison helps identify
discrepancies, yielding contrastive action pairs that serve as training data
for the agent. Our experiments on three complex agent tasks demonstrate that
our framework outperforms a variety of strong baselines. Moreover, our
analytical findings highlight the effectiveness of IPR in augmenting action
efficiency and its applicability to diverse models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 (Main Conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knowledge Editing in Language Models via Adapted Direct Preference
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09920v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09920v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amit Rozner, Barak Battash, Lior Wolf, Ofir Lindenbaum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) can become outdated over time as they may lack
updated world knowledge, leading to factual knowledge errors and gaps.
Knowledge Editing (KE) aims to overcome this challenge using weight updates
that do not require expensive retraining. We propose treating KE as an LLM
alignment problem. Toward this goal, we introduce Knowledge Direct Preference
Optimization (KDPO), a variation of the Direct Preference Optimization (DPO)
that is more effective for knowledge modifications. Our method is based on an
online approach that continually updates the knowledge stored in the model. We
use the current knowledge as a negative sample and the new knowledge we want to
introduce as a positive sample in a process called DPO. We also use
teacher-forcing for negative sample generation and optimize using the positive
sample, which helps maintain localized changes. We tested our KE method on
various datasets and models, comparing it to several cutting-edge methods, with
100 and 500 sequential edits. Additionally, we conducted an ablation study
comparing our method to the standard DPO approach. Our experimental results
show that our modified DPO method allows for more refined KE, achieving similar
or better performance compared to previous methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quest: Query-centric Data Synthesis Approach for Long-context Scaling of
  Large Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19846v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19846v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaochen Gao, Xing Wu, Qi Fu, Songlin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models, initially pre-trained with a limited context length,
can better handle longer texts by continuing training on a corpus with extended
contexts. However, obtaining effective long-context data is challenging due to
the scarcity and uneven distribution of long documents across different
domains. To address this issue, we propose a Query-centric data synthesis
method, abbreviated as Quest. Quest is an interpretable method based on the
observation that documents retrieved by similar queries are relevant but
low-redundant, thus well-suited for synthesizing long-context data. The method
is also scalable and capable of constructing large amounts of long-context
data. Using Quest, we synthesize a long-context dataset up to 128k context
length, significantly outperforming other data synthesis methods on multiple
long-context benchmark datasets. In addition, we further verify that the Quest
method is predictable through scaling law experiments, making it a reliable
solution for advancing long-context models.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">13</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Low Latency Point Cloud Rendering with Learned Splatting <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yueyu Hu, Ran Gong, Qi Sun, Yao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point cloud is a critical 3D representation with many emerging applications.
Because of the point sparsity and irregularity, high-quality rendering of point
clouds is challenging and often requires complex computations to recover the
continuous surface representation. On the other hand, to avoid visual
discomfort, the motion-to-photon latency has to be very short, under 10 ms.
Existing rendering solutions lack in either quality or speed. To tackle these
challenges, we present a framework that unlocks interactive, free-viewing and
high-fidelity point cloud rendering. We train a generic neural network to
estimate 3D elliptical Gaussians from arbitrary point clouds and use
differentiable surface splatting to render smooth texture and surface normal
for arbitrary views. Our approach does not require per-scene optimization, and
enable real-time rendering of dynamic point cloud. Experimental results
demonstrate the proposed solution enjoys superior visual quality and speed, as
well as generalizability to different scene content and robustness to
compression artifacts. The code is available at
https://github.com/huzi96/gaussian-pcloud-render .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at CVPR 2024 Workshop on AIS: Vision, Graphics and AI for
  Streaming (https://ai4streaming-workshop.github.io/)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GSplatLoc: Grounding Keypoint Descriptors into 3D Gaussian Splatting for
  Improved Visual Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gennady Sidorov, Malik Mohrat, Ksenia Lebedeva, Ruslan Rakhimov, Sergey Kolyubin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although various visual localization approaches exist, such as scene
coordinate and pose regression, these methods often struggle with high memory
consumption or extensive optimization requirements. To address these
challenges, we utilize recent advancements in novel view synthesis,
particularly 3D Gaussian Splatting (3DGS), to enhance localization. 3DGS allows
for the compact encoding of both 3D geometry and scene appearance with its
spatial features. Our method leverages the dense description maps produced by
XFeat's lightweight keypoint detection and description model. We propose
distilling these dense keypoint descriptors into 3DGS to improve the model's
spatial understanding, leading to more accurate camera pose predictions through
2D-3D correspondences. After estimating an initial pose, we refine it using a
photometric warping loss. Benchmarking on popular indoor and outdoor datasets
shows that our approach surpasses state-of-the-art Neural Render Pose (NRP)
methods, including NeRFMatch and PNeRFLoc.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website at https://gsplatloc.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-Time Detection of Electronic Components in Waste Printed Circuit
  Boards: A <span class="highlight-title">Transformer</span>-Based Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16496v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16496v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Mohsin, Stefano Rovetta, Francesco Masulli, Alberto Cabri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Critical Raw Materials (CRMs) such as copper, manganese, gallium, and various
rare earths have great importance for the electronic industry. To increase the
concentration of individual CRMs and thus make their extraction from Waste
Printed Circuit Boards (WPCBs) convenient, we have proposed a practical
approach that involves selective disassembling of the different types of
electronic components from WPCBs using mechatronic systems guided by artificial
vision techniques. In this paper we evaluate the real-time accuracy of
electronic component detection and localization of the Real-Time DEtection
TRansformer model architecture. Transformers have recently become very popular
for the extraordinary results obtained in natural language processing and
machine translation. Also in this case, the transformer model achieves very
good performances, often superior to those of the latest state of the art
object detection and localization models YOLOv8 and YOLOv9.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Conference on Applications in Electronics Pervading
  Industry, Environment and Society (ApplePies2024). Proceedings are published
  in the Springer Lecture Notes in Electrical Engineering</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Unified Hallucination Mitigation Framework for Large Vision-Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16494v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16494v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Chang, Liqiang Jing, Xiaopeng Zhang, Yue Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hallucination is a common problem for Large Vision-Language Models (LVLMs)
with long generations which is difficult to eradicate. The generation with
hallucinations is partially inconsistent with the image content. To mitigate
hallucination, current studies either focus on the process of model inference
or the results of model generation, but the solutions they design sometimes do
not deal appropriately with various types of queries and the hallucinations of
the generations about these queries. To accurately deal with various
hallucinations, we present a unified framework, Dentist, for hallucination
mitigation. The core step is to first classify the queries, then perform
different processes of hallucination mitigation based on the classification
result, just like a dentist first observes the teeth and then makes a plan. In
a simple deployment, Dentist can classify queries as perception or reasoning
and easily mitigate potential hallucinations in answers which has been
demonstrated in our experiments. On MMbench, we achieve a 13.44%/10.2%/15.8%
improvement in accuracy on Image Quality, a Coarse Perception visual question
answering (VQA) task, over the baseline InstructBLIP/LLaVA/VisualGLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by TMLR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Proactive Schemes: A <span class="highlight-title">Survey</span> of Adversarial Attacks for Social Good 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16491v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16491v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vishal Asnani, Xi Yin, Xiaoming Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial attacks in computer vision exploit the vulnerabilities of machine
learning models by introducing subtle perturbations to input data, often
leading to incorrect predictions or classifications. These attacks have evolved
in sophistication with the advent of deep learning, presenting significant
challenges in critical applications, which can be harmful for society. However,
there is also a rich line of research from a transformative perspective that
leverages adversarial techniques for social good. Specifically, we examine the
rise of proactive schemes-methods that encrypt input data using additional
signals termed templates, to enhance the performance of deep learning models.
By embedding these imperceptible templates into digital media, proactive
schemes are applied across various applications, from simple image enhancements
to complicated deep learning frameworks to aid performance, as compared to the
passive schemes, which don't change the input data distribution for their
framework. The survey delves into the methodologies behind these proactive
schemes, the encryption and learning processes, and their application to modern
computer vision and natural language processing applications. Additionally, it
discusses the challenges, potential vulnerabilities, and future directions for
proactive schemes, ultimately highlighting their potential to foster the
responsible and secure advancement of deep learning technologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted for review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion Models to Enhance the Resolution of Microscopy Images: A
  Tutorial 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16488v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16488v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harshith Bachimanchi, Giovanni Volpe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have emerged as a prominent technique in generative modeling
with neural networks, making their mark in tasks like text-to-image translation
and super-resolution. In this tutorial, we provide a comprehensive guide to
build denoising diffusion probabilistic models (DDPMs) from scratch, with a
specific focus on transforming low-resolution microscopy images into their
corresponding high-resolution versions. We provide the theoretical background,
mathematical derivations, and a detailed Python code implementation using
PyTorch, along with techniques to enhance model performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>45 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Frequency-based View Selection in Gaussian Splatting Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16470v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16470v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Monica M. Q. Li, Pierre-Yves Lajoie, Giovanni Beltrame
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Three-dimensional reconstruction is a fundamental problem in robotics
perception. We examine the problem of active view selection to perform 3D
Gaussian Splatting reconstructions with as few input images as possible.
Although 3D Gaussian Splatting has made significant progress in image rendering
and 3D reconstruction, the quality of the reconstruction is strongly impacted
by the selection of 2D images and the estimation of camera poses through
Structure-from-Motion (SfM) algorithms. Current methods to select views that
rely on uncertainties from occlusions, depth ambiguities, or neural network
predictions directly are insufficient to handle the issue and struggle to
generalize to new scenes. By ranking the potential views in the frequency
domain, we are able to effectively estimate the potential information gain of
new viewpoints without ground truth data. By overcoming current constraints on
model architecture and efficacy, our method achieves state-of-the-art results
in view selection, demonstrating its potential for efficient image-based 3D
reconstruction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Initialization of Monocular Visual Navigation for Autonomous Agents
  Using Modified Structure from Small Motion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16465v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16465v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan-Diego Florez, Mehregan Dor, Panagiotis Tsiotras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a standalone monocular visual Simultaneous Localization and
Mapping (vSLAM) initialization pipeline for autonomous robots in space. Our
method, a state-of-the-art factor graph optimization pipeline, enhances
classical Structure from Small Motion (SfSM) to robustly initialize a monocular
agent in weak-perspective projection scenes. Furthermore, it overcomes visual
estimation challenges introduced by spacecraft inspection trajectories, such
as: center-pointing motion, which exacerbates the bas-relief ambiguity, and the
presence of a dominant plane in the scene, which causes motion estimation
degeneracies in classical Structure from Motion (SfM). We validate our method
on realistic, simulated satellite inspection images exhibiting weak-perspective
projection, and we demonstrate its effectiveness and improved performance
compared to other monocular initialization procedures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 1 page for references, 6 figures, 1 table, IEEEtran format
  This work has been submitted to the IEEE for possible publication. Copyright
  may be transferred without notice, after which this version may no longer be
  accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Underground Mapping and Localization Based on Ground-Penetrating Radar 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16446v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16446v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinchang Zhang, Guoyu Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D object reconstruction based on deep neural networks has gained increasing
attention in recent years. However, 3D reconstruction of underground objects to
generate point cloud maps remains a challenge. Ground Penetrating Radar (GPR)
is one of the most powerful and extensively used tools for detecting and
locating underground objects such as plant root systems and pipelines, with its
cost-effectiveness and continuously evolving technology. This paper introduces
a parabolic signal detection network based on deep convolutional neural
networks, utilizing B-scan images from GPR sensors. The detected keypoints can
aid in accurately fitting parabolic curves used to interpret the original GPR
B-scan images as cross-sections of the object model. Additionally, a multi-task
point cloud network was designed to perform both point cloud segmentation and
completion simultaneously, filling in sparse point cloud maps. For unknown
locations, GPR A-scan data can be used to match corresponding A-scan data in
the constructed map, pinpointing the position to verify the accuracy of the map
construction by the model. Experimental results demonstrate the effectiveness
of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The RoboDepth Challenge: Methods and Advancements Towards Robust Depth
  Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.15061v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.15061v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingdong Kong, Yaru Niu, Shaoyuan Xie, Hanjiang Hu, Lai Xing Ng, Benoit R. Cottereau, Liangjun Zhang, Hesheng Wang, Wei Tsang Ooi, Ruijie Zhu, Ziyang Song, Li Liu, Tianzhu Zhang, Jun Yu, Mohan Jing, Pengwei Li, Xiaohua Qi, Cheng Jin, Yingfeng Chen, Jie Hou, Jie Zhang, Zhen Kan, Qiang Ling, Liang Peng, Minglei Li, Di Xu, Changpeng Yang, Yuanqi Yao, Gang Wu, Jian Kuai, Xianming Liu, Junjun Jiang, Jiamian Huang, Baojun Li, Jiale Chen, Shuang Zhang, Sun Ao, Zhenyu Li, Runze Chen, Haiyong Luo, Fang Zhao, Jingze Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate depth estimation under out-of-distribution (OoD) scenarios, such as
adverse weather conditions, sensor failure, and noise contamination, is
desirable for safety-critical applications. Existing depth estimation systems,
however, suffer inevitably from real-world corruptions and perturbations and
are struggled to provide reliable depth predictions under such cases. In this
paper, we summarize the winning solutions from the RoboDepth Challenge -- an
academic competition designed to facilitate and advance robust OoD depth
estimation. This challenge was developed based on the newly established KITTI-C
and NYUDepth2-C benchmarks. We hosted two stand-alone tracks, with an emphasis
on robust self-supervised and robust fully-supervised depth estimation,
respectively. Out of more than two hundred participants, nine unique and
top-performing solutions have appeared, with novel designs ranging from the
following aspects: spatial- and frequency-domain augmentations, masked image
modeling, image restoration and super-resolution, adversarial training,
diffusion-based noise suppression, vision-language pre-training, learned model
ensembling, and hierarchical feature enhancement. Extensive experimental
analyses along with insightful observations are drawn to better understand the
rationale behind each design. We hope this challenge could lay a solid
foundation for future research on robust and reliable depth estimation and
beyond. The datasets, competition toolkit, workshop recordings, and source code
from the winning teams are publicly available on the challenge website.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report; 65 pages, 34 figures, 24 tables; Code at
  https://github.com/ldkong1205/RoboDepth</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SurGen: Text-Guided Diffusion Model for Surgical Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.14028v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.14028v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joseph Cho, Samuel Schmidgall, Cyril Zakka, Mrudang Mathur, Dhamanpreet Kaur, Rohan Shad, William Hiesinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion-based video generation models have made significant strides,
producing outputs with improved visual fidelity, temporal coherence, and user
control. These advancements hold great promise for improving surgical education
by enabling more realistic, diverse, and interactive simulation environments.
In this study, we introduce SurGen, a text-guided diffusion model tailored for
surgical video synthesis. SurGen produces videos with the highest resolution
and longest duration among existing surgical video generation models. We
validate the visual and temporal quality of the outputs using standard image
and video generation metrics. Additionally, we assess their alignment to the
corresponding text prompts through a deep learning classifier trained on
surgical data. Our results demonstrate the potential of diffusion models to
serve as valuable educational tools for surgical trainees.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion Models For Multi-Modal Generative Modeling <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.17571v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.17571v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changyou Chen, Han Ding, Bunyamin Sisman, Yi Xu, Ouye Xie, Benjamin Z. Yao, Son Dinh Tran, Belinda Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion-based generative modeling has been achieving state-of-the-art
results on various generation tasks. Most diffusion models, however, are
limited to a single-generation modeling. Can we generalize diffusion models
with the ability of multi-modal generative training for more generalizable
modeling? In this paper, we propose a principled way to define a diffusion
model by constructing a unified multi-modal diffusion model in a common
diffusion space. We define the forward diffusion process to be driven by an
information aggregation from multiple types of task-data, e.g., images for a
generation task and labels for a classification task. In the reverse process,
we enforce information sharing by parameterizing a shared backbone denoising
network with additional modality-specific decoder heads. Such a structure can
simultaneously learn to generate different types of multi-modal data with a
multi-task loss, which is derived from a new multi-modal variational lower
bound that generalizes the standard diffusion model. We propose several
multimodal generation settings to verify our framework, including image
transition, masked-image training, joint image-label and joint
image-representation generative modeling. Extensive experimental results on
ImageNet indicate the effectiveness of our framework for various multi-modal
generative modeling, which we believe is an important research direction worthy
of more future explorations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Scoping <span class="highlight-title">Review</span> of Earth Observation and Machine Learning for Causal
  Inference: Implications for the Geography of Poverty 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02584v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02584v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kazuki Sakamoto, Connor T. Jerzak, Adel Daoud
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Earth observation (EO) data such as satellite imagery can have far-reaching
impacts on our understanding of the geography of poverty, especially when
coupled with machine learning (ML) and computer vision. Early research in
computer vision used predictive models to estimate living conditions,
especially in contexts where data availability on poverty was scarce. Recent
work has progressed beyond using EO data to predict such outcomes -- now also
using it to conduct causal inference. However, how such EO-ML models are used
for causality remains incompletely mapped. To address this gap, we conduct a
scoping review where we first document the growth of interest in using
satellite images and other sources of EO data in causal analysis. We then trace
the methodological relationship between spatial statistics and ML methods
before discussing five ways in which EO data has been used in scientific
workflows -- (1) outcome imputation for downstream causal analysis, (2) EO
image deconfounding, (3) EO-based treatment effect heterogeneity, (4) EO-based
transportability analysis, and (5) image-informed causal discovery. We
consolidate these observations by providing a detailed workflow for how
researchers can incorporate EO data in causal analysis going forward -- from
data requirements to choice of computer vision model and evaluation metrics.
While our discussion focuses on health and living conditions outcomes, our
workflow applies to other measures of sustainable development where EO data are
informative.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear as: Sakamoto, Kazuki, Connor T. Jerzak, and Adel Daoud. "A
  Scoping Review of Earth Observation and Machine Learning for Causal
  Inference: Implications for the Geography of Poverty." In Geography of
  Poverty, edited by Ola Hall and Ibrahim Wahab. Edward Elgar Publishing
  (Cheltenham, UK), 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">21</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Algorithmic Drift: A Simulation Framework to Study the Effects of
  Recommender Systems on User Preferences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16478v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16478v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erica Coppolillo, Simone Mungari, Ettore Ritacco, Francesco Fabbri, Marco Minici, Francesco Bonchi, Giuseppe Manco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Digital platforms such as social media and e-commerce websites adopt
Recommender Systems to provide value to the user. However, the social
consequences deriving from their adoption are still unclear. Many scholars
argue that recommenders may lead to detrimental effects, such as
bias-amplification deriving from the feedback loop between algorithmic
suggestions and users' choices. Nonetheless, the extent to which recommenders
influence changes in users leaning remains uncertain. In this context, it is
important to provide a controlled environment for evaluating the recommendation
algorithm before deployment. To address this, we propose a stochastic
simulation framework that mimics user-recommender system interactions in a
long-term scenario. In particular, we simulate the user choices by formalizing
a user model, which comprises behavioral aspects, such as the user resistance
towards the recommendation algorithm and their inertia in relying on the
received suggestions. Additionally, we introduce two novel metrics for
quantifying the algorithm's impact on user preferences, specifically in terms
of drift over time. We conduct an extensive evaluation on multiple synthetic
datasets, aiming at testing the robustness of our framework when considering
different scenarios and hyper-parameters setting. The experimental results
prove that the proposed methodology is effective in detecting and quantifying
the drift over the users preferences by means of the simulation. All the code
and data used to perform the experiments are publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modern Hopfield Networks meet Encoded Neural Representations --
  Addressing Practical Considerations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16408v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16408v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Satyananda Kashyap, Niharika S. D'Souza, Luyao Shi, Ken C. L. Wong, Hongzhi Wang, Tanveer Syeda-Mahmood
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Content-addressable memories such as Modern Hopfield Networks (MHN) have been
studied as mathematical models of auto-association and storage/retrieval in the
human declarative memory, yet their practical use for large-scale content
storage faces challenges. Chief among them is the occurrence of meta-stable
states, particularly when handling large amounts of high dimensional content.
This paper introduces Hopfield Encoding Networks (HEN), a framework that
integrates encoded neural representations into MHNs to improve pattern
separability and reduce meta-stable states. We show that HEN can also be used
for retrieval in the context of hetero association of images with natural
language queries, thus removing the limitation of requiring access to partial
content in the same domain. Experimental results demonstrate substantial
reduction in meta-stable states and increased storage capacity while still
enabling perfect recall of a significantly larger number of inputs advancing
the practical utility of associative memory networks for real-world tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 8 figures, workshop submission to Neurips</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Enhancing Linked Data Retrieval in Conversational UIs using
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16220v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16220v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omar Mussa, Omer Rana, Benoît Goossens, Pablo Orozco-Terwengel, Charith Perera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the recent broad adoption of Large Language Models (LLMs) across
various domains, their potential for enriching information systems in
extracting and exploring Linked Data (LD) and Resource Description Framework
(RDF) triplestores has not been extensively explored. This paper examines the
integration of LLMs within existing systems, emphasising the enhancement of
conversational user interfaces (UIs) and their capabilities for data extraction
by producing more accurate SPARQL queries without the requirement for model
retraining. Typically, conversational UI models necessitate retraining with the
introduction of new datasets or updates, limiting their functionality as
general-purpose extraction tools. Our approach addresses this limitation by
incorporating LLMs into the conversational UI workflow, significantly enhancing
their ability to comprehend and process user queries effectively. By leveraging
the advanced natural language understanding capabilities of LLMs, our method
improves RDF entity extraction within web systems employing conventional
chatbots. This integration facilitates a more nuanced and context-aware
interaction model, critical for handling the complex query patterns often
encountered in RDF datasets and Linked Open Data (LOD) endpoints. The
evaluation of this methodology shows a marked enhancement in system
expressivity and the accuracy of responses to user queries, indicating a
promising direction for future research in this area. This investigation not
only underscores the versatility of LLMs in enhancing existing information
systems but also sets the stage for further explorations into their potential
applications within more specialised domains of web information systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted at the 25th International Web
  Information Systems Engineering Conference (WISE 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TiM4Rec: An Efficient Sequential Recommendation Model Based on
  Time-Aware Structured State Space Duality Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Fan, Mengyi Zhu, Yanrong Hu, Hailin Feng, Zhijie He, Hongjiu Liu, Qingyang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommendation represents a pivotal branch of recommendation
systems, centered around dynamically analyzing the sequential dependencies
between user preferences and their interactive behaviors. Despite the
Transformer architecture-based models achieving commendable performance within
this domain, their quadratic computational complexity relative to the sequence
dimension impedes efficient modeling. In response, the innovative Mamba
architecture, characterized by linear computational complexity, has emerged.
Mamba4Rec further pioneers the application of Mamba in sequential
recommendation. Nonetheless, Mamba 1's hardware-aware algorithm struggles to
efficiently leverage modern matrix computational units, which lead to the
proposal of the improved State Space Duality (SSD), also known as Mamba 2.
While the SSD4Rec successfully adapts the SSD architecture for sequential
recommendation, showing promising results in high-dimensional contexts, it
suffers significant performance drops in low-dimensional scenarios crucial for
pure ID sequential recommendation tasks. Addressing this challenge, we propose
a novel sequential recommendation backbone model, TiM4Rec, which ameliorates
the low-dimensional performance loss of the SSD architecture while preserving
its computational efficiency. Drawing inspiration from TiSASRec, we develop a
time-aware enhancement method tailored for the linear computation demands of
the SSD architecture, thereby enhancing its adaptability and achieving
state-of-the-art (SOTA) performance in both low and high-dimensional modeling.
The code for our model is publicly accessible at
https://github.com/AlwaysFHao/TiM4Rec.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Seeing Faces in Things: A Model and <span class="highlight-title">Dataset</span> for Pareidolia 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16143v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16143v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mark Hamilton, Simon Stent, Vasha DuTell, Anne Harrington, Jennifer Corbett, Ruth Rosenholtz, William T. Freeman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The human visual system is well-tuned to detect faces of all shapes and
sizes. While this brings obvious survival advantages, such as a better chance
of spotting unknown predators in the bush, it also leads to spurious face
detections. ``Face pareidolia'' describes the perception of face-like structure
among otherwise random stimuli: seeing faces in coffee stains or clouds in the
sky. In this paper, we study face pareidolia from a computer vision
perspective. We present an image dataset of ``Faces in Things'', consisting of
five thousand web images with human-annotated pareidolic faces. Using this
dataset, we examine the extent to which a state-of-the-art human face detector
exhibits pareidolia, and find a significant behavioral gap between humans and
machines. We find that the evolutionary need for humans to detect animal faces,
as well as human faces, may explain some of this gap. Finally, we propose a
simple statistical model of pareidolia in images. Through studies on human
subjects and our pareidolic face detectors we confirm a key prediction of our
model regarding what image conditions are most likely to induce pareidolia.
Dataset and Website: https://aka.ms/faces-in-things
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Hint Generation Approaches in Open-Domain Question Answering <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16096v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16096v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jamshid Mozafari, Abdelrahman Abdallah, Bhawna Piryani, Adam Jatowt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic Question Answering (QA) systems rely on contextual information to
provide accurate answers. Commonly, contexts are prepared through either
retrieval-based or generation-based methods. The former involves retrieving
relevant documents from a corpus like Wikipedia, whereas the latter uses
generative models such as Large Language Models (LLMs) to generate the context.
In this paper, we introduce a novel context preparation approach called HINTQA,
which employs Automatic Hint Generation (HG) techniques. Unlike traditional
methods, HINTQA prompts LLMs to produce hints about potential answers for the
question rather than generating relevant context. We evaluate our approach
across three QA datasets including TriviaQA, NaturalQuestions, and Web
Questions, examining how the number and order of hints impact performance. Our
findings show that the HINTQA surpasses both retrieval-based and
generation-based approaches. We demonstrate that hints enhance the accuracy of
answers more than retrieved and generated contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SLIMER-IT: Zero-Shot NER on Italian Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15933v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15933v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Zamai, Leonardo Rigutini, Marco Maggini, Andrea Zugarini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional approaches to Named Entity Recognition (NER) frame the task into
a BIO sequence labeling problem. Although these systems often excel in the
downstream task at hand, they require extensive annotated data and struggle to
generalize to out-of-distribution input domains and unseen entity types. On the
contrary, Large Language Models (LLMs) have demonstrated strong zero-shot
capabilities. While several works address Zero-Shot NER in English, little has
been done in other languages. In this paper, we define an evaluation framework
for Zero-Shot NER, applying it to the Italian language. Furthermore, we
introduce SLIMER-IT, the Italian version of SLIMER, an instruction-tuning
approach for zero-shot NER leveraging prompts enriched with definition and
guidelines. Comparisons with other state-of-the-art models, demonstrate the
superiority of SLIMER-IT on never-seen-before entity tags.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ducho meets Elliot: Large-scale Benchmarks for Multimodal Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15857v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15857v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Attimonelli, Danilo Danese, Angela Di Fazio, Daniele Malitesta, Claudio Pomo, Tommaso Di Noia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In specific domains like fashion, music, and movie recommendation, the
multi-faceted features characterizing products and services may influence each
customer on online selling platforms differently, paving the way to novel
multimodal recommendation models that can learn from such multimodal content.
According to the literature, the common multimodal recommendation pipeline
involves (i) extracting multimodal features, (ii) refining their high-level
representations to suit the recommendation task, (iii) optionally fusing all
multimodal features, and (iv) predicting the user-item score. While great
effort has been put into designing optimal solutions for (ii-iv), to the best
of our knowledge, very little attention has been devoted to exploring
procedures for (i). In this respect, the existing literature outlines the large
availability of multimodal datasets and the ever-growing number of large models
accounting for multimodal-aware tasks, but (at the same time) an unjustified
adoption of limited standardized solutions. This motivates us to explore more
extensive techniques for the (i) stage of the pipeline. To this end, this paper
settles as the first attempt to offer a large-scale benchmarking for multimodal
recommender systems, with a specific focus on multimodal extractors.
Specifically, we take advantage of two popular and recent frameworks for
multimodal feature extraction and reproducibility in recommendation, Ducho and
Elliot, to offer a unified and ready-to-use experimental environment able to
run extensive benchmarking analyses leveraging novel multimodal feature
extractors. Results, largely validated under different hyper-parameter settings
for the chosen extractors, provide important insights on how to train and tune
the next generation of multimodal recommendation algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Digital Discrimination in Dating Apps -- The Dutch Breeze
  case 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15828v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15828v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim de Jonge, Frederik Zuiderveen Borgesius
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In September 2023, the Netherlands Institute for Human Rights, the Dutch
non-discrimination authority, decided that Breeze, a Dutch dating app, was
justified in suspecting that their algorithm discriminated against non-white.
Consequently, the Institute decided that Breeze must prevent this
discrimination based on ethnicity. This paper explores two questions. (i) Is
the discrimination based on ethnicity in Breeze's matching algorithm illegal?
(ii) How can dating apps mitigate or stop discrimination in their matching
algorithms? We illustrate the legal and technical difficulties dating apps face
in tackling discrimination and illustrate promising solutions. We analyse the
Breeze decision in-depth, combining insights from computer science and law. We
discuss the implications of this judgment for scholarship and practice in the
field of fair and non-discriminatory machine learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IRSC: A Zero-shot Evaluation Benchmark for Information Retrieval through
  Semantic Comprehension in Retrieval-Augmented Generation Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15763v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15763v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hai Lin, Shaoxiong Zhan, Junyou Su, Haitao Zheng, Hui Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Retrieval-Augmented Generation (RAG) tasks using Large Language Models
(LLMs), the quality of retrieved information is critical to the final output.
This paper introduces the IRSC benchmark for evaluating the performance of
embedding models in multilingual RAG tasks. The benchmark encompasses five
retrieval tasks: query retrieval, title retrieval, part-of-paragraph retrieval,
keyword retrieval, and summary retrieval. Our research addresses the current
lack of comprehensive testing and effective comparison methods for embedding
models in RAG scenarios. We introduced new metrics: the Similarity of Semantic
Comprehension Index (SSCI) and the Retrieval Capability Contest Index (RCCI),
and evaluated models such as Snowflake-Arctic, BGE, GTE, and M3E. Our
contributions include: 1) the IRSC benchmark, 2) the SSCI and RCCI metrics, and
3) insights into the cross-lingual limitations of embedding models. The IRSC
benchmark aims to enhance the understanding and development of accurate
retrieval systems in RAG tasks. All code and datasets are available at:
https://github.com/Jasaxion/IRSC\_Benchmark
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM-Cure: LLM-based Competitor User <span class="highlight-title">Review</span> Analysis for Feature
  Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15724v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15724v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maram Assi, Safwat Hassan, Ying Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The exponential growth of the mobile app market underscores the importance of
constant innovation and rapid response to user demands. As user satisfaction is
paramount to the success of a mobile application (app), developers typically
rely on user reviews, which represent user feedback that includes ratings and
comments to identify areas for improvement. However, the sheer volume of user
reviews poses challenges in manual analysis, necessitating automated
approaches. Existing automated approaches either analyze only the target apps
reviews, neglecting the comparison of similar features to competitors or fail
to provide suggestions for feature enhancement. To address these gaps, we
propose a Large Language Model (LLM)-based Competitive User Review Analysis for
Feature Enhancement) (LLM-Cure), an approach powered by LLMs to automatically
generate suggestion s for mobile app feature improvements. More specifically,
LLM-Cure identifies and categorizes features within reviews by applying LLMs.
When provided with a complaint in a user review, LLM-Cure curates highly rated
(4 and 5 stars) reviews in competing apps related to the complaint and proposes
potential improvements tailored to the target application. We evaluate LLM-Cure
on 1,056,739 reviews of 70 popular Android apps. Our evaluation demonstrates
that LLM-Cure significantly outperforms the state-of-the-art approaches in
assigning features to reviews by up to 13% in F1-score, up to 16% in recall and
up to 11% in precision. Additionally, LLM-Cure demonstrates its capability to
provide suggestions for resolving user complaints. We verify the suggestions
using the release notes that reflect the changes of features in the target
mobile app. LLM-Cure achieves a promising average of 73% of the implementation
of the provided suggestions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Making Text Embedders Few-Shot Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15700v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15700v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaofan Li, MingHao Qin, Shitao Xiao, Jianlyu Chen, Kun Luo, Yingxia Shao, Defu Lian, Zheng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) with decoder-only architectures demonstrate
remarkable in-context learning (ICL) capabilities. This feature enables them to
effectively handle both familiar and novel tasks by utilizing examples provided
within their input context. Recognizing the potential of this capability, we
propose leveraging the ICL feature in LLMs to enhance the process of text
embedding generation. To this end, we introduce a novel model bge-en-icl, which
employs few-shot examples to produce high-quality text embeddings. Our approach
integrates task-related examples directly into the query side, resulting in
significant improvements across various tasks. Additionally, we have
investigated how to effectively utilize LLMs as embedding models, including
various attention mechanisms, pooling methods, etc. Our findings suggest that
retaining the original framework often yields the best results, underscoring
that simplicity is best. Experimental results on the MTEB and AIR-Bench
benchmarks demonstrate that our approach sets new state-of-the-art (SOTA)
performance. Our model, code and dataset are freely available at
https://github.com/FlagOpen/FlagEmbedding .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> of Stance Detection on Social Media: New Directions and
  Perspectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15690v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15690v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Zhang, Genan Dai, Fuqiang Niu, Nan Yin, Xiaomao Fan, Hu Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In modern digital environments, users frequently express opinions on
contentious topics, providing a wealth of information on prevailing attitudes.
The systematic analysis of these opinions offers valuable insights for
decision-making in various sectors, including marketing and politics. As a
result, stance detection has emerged as a crucial subfield within affective
computing, enabling the automatic detection of user stances in social media
conversations and providing a nuanced understanding of public sentiment on
complex issues. Recent years have seen a surge of research interest in
developing effective stance detection methods, with contributions from multiple
communities, including natural language processing, web science, and social
computing. This paper provides a comprehensive survey of stance detection
techniques on social media, covering task definitions, datasets, approaches,
and future works. We review traditional stance detection models, as well as
state-of-the-art methods based on large language models, and discuss their
strengths and limitations. Our survey highlights the importance of stance
detection in understanding public opinion and sentiment, and identifies gaps in
current research. We conclude by outlining potential future directions for
stance detection on social media, including the need for more robust and
generalizable models, and the importance of addressing emerging challenges such
as multi-modal stance detection and stance detection in low-resource languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Qualitative Insights Tool (QualIT): LLM Enhanced Topic Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15626v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15626v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Satya Kapoor, Alex Gil, Sreyoshi Bhaduri, Anshul Mittal, Rutu Mulkar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topic modeling is a widely used technique for uncovering thematic structures
from large text corpora. However, most topic modeling approaches e.g. Latent
Dirichlet Allocation (LDA) struggle to capture nuanced semantics and contextual
understanding required to accurately model complex narratives. Recent
advancements in this area include methods like BERTopic, which have
demonstrated significantly improved topic coherence and thus established a new
standard for benchmarking. In this paper, we present a novel approach, the
Qualitative Insights Tool (QualIT) that integrates large language models (LLMs)
with existing clustering-based topic modeling approaches. Our method leverages
the deep contextual understanding and powerful language generation capabilities
of LLMs to enrich the topic modeling process using clustering. We evaluate our
approach on a large corpus of news articles and demonstrate substantial
improvements in topic coherence and topic diversity compared to baseline topic
modeling techniques. On the 20 ground-truth topics, our method shows 70% topic
coherence (vs 65% & 57% benchmarks) and 95.5% topic diversity (vs 85% & 72%
benchmarks). Our findings suggest that the integration of LLMs can unlock new
opportunities for topic modeling of dynamic and complex text data, as is common
in talent management research contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 tables, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WebQuest: A Benchmark for Multimodal QA on Web Page Sequences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13711v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13711v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maria Wang, Srinivas Sunkara, Gilles Baechler, Jason Lin, Yun Zhu, Fedir Zubach, Lei Shu, Jindong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of powerful multimodal LLMs has enhanced the viability of building
web agents which can, with increasing levels of autonomy, assist users to
retrieve information and complete tasks on various human-computer interfaces.
It is hence necessary to build challenging benchmarks that span a wide-variety
of use cases reflecting real-world usage. In this work, we present WebQuest, a
multi-page question-answering dataset that requires reasoning across multiple
related web pages. In contrast to existing UI benchmarks that focus on
multi-step web navigation and task completion, our dataset evaluates
information extraction, multimodal retrieval and composition of information
from many web pages. WebQuest includes three question categories: single-screen
QA, multi-screen QA, and QA based on navigation traces. We evaluate leading
proprietary multimodal models like GPT-4V, Gemini Flash, Claude 3, and open
source models like InstructBLIP, PaliGemma on our dataset, revealing a
significant gap between single-screen and multi-screen reasoning. Finally, we
investigate inference time techniques like Chain-of-Thought prompting to
improve model capabilities on multi-screen reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GLoCIM: Global-view Long Chain Interest Modeling for news recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00859v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00859v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Yang, Wenhui Wang, Tao Qi, Peng Zhang, Tianyun Zhang, Ru Zhang, Jianyi Liu, Yongfeng Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately recommending candidate news articles to users has always been the
core challenge of news recommendation system. News recommendations often
require modeling of user interest to match candidate news. Recent efforts have
primarily focused on extracting local subgraph information in a global click
graph constructed by the clicked news sequence of all users. Howerer, the
computational complexity of extracting global click graph information has
hindered the ability to utilize far-reaching linkage which is hidden between
two distant nodes in global click graph collaboratively among similar users. To
overcome the problem above, we propose a Global-view Long Chain Interests
Modeling for news recommendation (GLoCIM), which combines neighbor interest
with long chain interest distilled from a global click graph, leveraging the
collaboration among similar users to enhance news recommendation. We therefore
design a long chain selection algorithm and long chain interest encoder to
obtain global-view long chain interest from the global click graph. We design a
gated network to integrate long chain interest with neighbor interest to
achieve the collaborative interest among similar users. Subsequently we
aggregate it with local news category-enhanced representation to generate final
user representation. Then candidate news representation can be formed to match
user representation to achieve news recommendation. Experimental results on
real-world datasets validate the effectiveness of our method to improve the
performance of news recommendation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fashion Image-to-Image Translation for Complementary Item Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.09847v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.09847v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Attimonelli, Claudio Pomo, Dietmar Jannach, Tommaso Di Noia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing demand for online fashion retail has boosted research in
fashion compatibility modeling and item retrieval, focusing on matching user
queries (textual descriptions or reference images) with compatible fashion
items. A key challenge is top-bottom retrieval, where precise compatibility
modeling is essential. Traditional methods, often based on Bayesian
Personalized Ranking (BPR), have shown limited performance. Recent efforts have
explored using generative models in compatibility modeling and item retrieval,
where generated images serve as additional inputs. However, these approaches
often overlook the quality of generated images, which could be crucial for
model performance. Additionally, generative models typically require large
datasets, posing challenges when such data is scarce.
  To address these issues, we introduce the Generative Compatibility Model
(GeCo), a two-stage approach that improves fashion image retrieval through
paired image-to-image translation. First, the Complementary Item Generation
Model (CIGM), built on Conditional Generative Adversarial Networks (GANs),
generates target item images (e.g., bottoms) from seed items (e.g., tops),
offering conditioning signals for retrieval. These generated samples are then
integrated into GeCo, enhancing compatibility modeling and retrieval accuracy.
Evaluations on three datasets show that GeCo outperforms state-of-the-art
baselines. Key contributions include: (i) the GeCo model utilizing paired
image-to-image translation within the Composed Image Retrieval framework, (ii)
comprehensive evaluations on benchmark datasets, and (iii) the release of a new
Fashion Taobao dataset designed for top-bottom retrieval, promoting further
research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Projected Gradient Descent for Spectral Compressed Sensing via Symmetric
  Hankel Factorization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09031v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09031v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinsheng Li, Wei Cui, Xu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current spectral compressed sensing methods via Hankel matrix completion
employ symmetric factorization to demonstrate the low-rank property of the
Hankel matrix. However, previous non-convex gradient methods only utilize
asymmetric factorization to achieve spectral compressed sensing. In this paper,
we propose a novel nonconvex projected gradient descent method for spectral
compressed sensing via symmetric factorization named Symmetric Hankel Projected
Gradient Descent (SHGD), which updates only one matrix and avoids a balancing
regularization term. SHGD reduces about half of the computation and storage
costs compared to the prior gradient method based on asymmetric factorization.
{Besides, the symmetric factorization employed in our work is completely novel
to the prior low-rank factorization model, introducing a new factorization
ambiguity under complex orthogonal transformation}. Novel distance metrics are
designed for our factorization method and a linear convergence guarantee to the
desired signal is established with $O(r^2\log(n))$ observations. Numerical
simulations demonstrate the superior performance of the proposed SHGD method in
phase transitions and computation efficiency compared to state-of-the-art
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted in IEEE Transactions on Signal Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recommendation Unlearning via Influence Function 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02147v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02147v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Zhang, Zhiyu Hu, Yimeng Bai, Jiancan Wu, Qifan Wang, Fuli Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommendation unlearning is an emerging task to serve users for erasing
unusable data (e.g., some historical behaviors) from a well-trained recommender
model. Existing methods process unlearning requests by fully or partially
retraining the model after removing the unusable data. However, these methods
are impractical due to the high computation cost of full retraining and the
highly possible performance damage of partial training. In this light, a
desired recommendation unlearning method should obtain a similar model as full
retraining in a more efficient manner, i.e., achieving complete, efficient and
harmless unlearning.
  In this work, we propose a new Influence Function-based Recommendation
Unlearning (IFRU) framework, which efficiently updates the model without
retraining by estimating the influence of the unusable data on the model via
the influence function. In the light that recent recommender models use
historical data for both the constructions of the optimization loss and the
computational graph (e.g., neighborhood aggregation), IFRU jointly estimates
the direct influence of unusable data on optimization loss and the spillover
influence on the computational graph to pursue complete unlearning.
Furthermore, we propose an importance-based pruning algorithm to reduce the
cost of the influence function. IFRU is harmless and applicable to mainstream
differentiable models. Extensive experiments demonstrate that IFRU achieves
more than 250 times acceleration compared to retraining-based methods with
recommendation performance comparable to full retraining. Codes are avaiable at
https://github.com/baiyimeng/IFRU.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM TORS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ C-Pack: Packed Resources For General Chinese Embeddings <span class="chip">SIGIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.07597v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.07597v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muennighoff, Defu Lian, Jian-Yun Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce C-Pack, a package of resources that significantly advance the
field of general Chinese embeddings. C-Pack includes three critical resources.
1) C-MTEB is a comprehensive benchmark for Chinese text embeddings covering 6
tasks and 35 datasets. 2) C-MTP is a massive text embedding dataset curated
from labeled and unlabeled Chinese corpora for training embedding models. 3)
C-TEM is a family of embedding models covering multiple sizes. Our models
outperform all prior Chinese text embeddings on C-MTEB by up to +10% upon the
time of the release. We also integrate and optimize the entire suite of
training methods for C-TEM. Along with our resources on general Chinese
embedding, we release our data and models for English text embeddings. The
English models achieve state-of-the-art performance on MTEB benchmark;
meanwhile, our released English data is 2 times larger than the Chinese data.
All these resources are made publicly available at
https://github.com/FlagOpen/FlagEmbedding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGIR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adversarial Attacks to Multi-Modal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06793v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06793v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihao Dou, Xin Hu, Haibo Yang, Zhuqing Liu, Minghong Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal models have gained significant attention due to their powerful
capabilities. These models effectively align embeddings across diverse data
modalities, showcasing superior performance in downstream tasks compared to
their unimodal counterparts. Recent study showed that the attacker can
manipulate an image or audio file by altering it in such a way that its
embedding matches that of an attacker-chosen targeted input, thereby deceiving
downstream models. However, this method often underperforms due to inherent
disparities in data from different modalities. In this paper, we introduce
CrossFire, an innovative approach to attack multi-modal models. CrossFire
begins by transforming the targeted input chosen by the attacker into a format
that matches the modality of the original image or audio file. We then
formulate our attack as an optimization problem, aiming to minimize the angular
deviation between the embeddings of the transformed input and the modified
image or audio file. Solving this problem determines the perturbations to be
added to the original media. Our extensive experiments on six real-world
benchmark datasets reveal that CrossFire can significantly manipulate
downstream tasks, surpassing existing attacks. Additionally, we evaluate six
defensive strategies against CrossFire, finding that current defenses are
insufficient to counteract our CrossFire.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the ACM Workshop on Large AI Systems and Models with
  Privacy and Safety Analysis 2024 (LAMPS '24)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GSplatLoc: Grounding Keypoint Descriptors into 3D Gaussian Splatting for
  Improved Visual Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gennady Sidorov, Malik Mohrat, Ksenia Lebedeva, Ruslan Rakhimov, Sergey Kolyubin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although various visual localization approaches exist, such as scene
coordinate and pose regression, these methods often struggle with high memory
consumption or extensive optimization requirements. To address these
challenges, we utilize recent advancements in novel view synthesis,
particularly 3D Gaussian Splatting (3DGS), to enhance localization. 3DGS allows
for the compact encoding of both 3D geometry and scene appearance with its
spatial features. Our method leverages the dense description maps produced by
XFeat's lightweight keypoint detection and description model. We propose
distilling these dense keypoint descriptors into 3DGS to improve the model's
spatial understanding, leading to more accurate camera pose predictions through
2D-3D correspondences. After estimating an initial pose, we refine it using a
photometric warping loss. Benchmarking on popular indoor and outdoor datasets
shows that our approach surpasses state-of-the-art Neural Render Pose (NRP)
methods, including NeRFMatch and PNeRFLoc.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website at https://gsplatloc.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Linear Dynamics from Bilinear Observations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16499v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16499v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yahya Sattar, Yassir Jedra, Sarah Dean
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of learning a realization of a partially observed
dynamical system with linear state transitions and bilinear observations. Under
very mild assumptions on the process and measurement noises, we provide a
finite time analysis for learning the unknown dynamics matrices (up to a
similarity transform). Our analysis involves a regression problem with
heavy-tailed and dependent data. Moreover, each row of our design matrix
contains a Kronecker product of current input with a history of inputs, making
it difficult to guarantee persistence of excitation. We overcome these
challenges, first providing a data-dependent high probability error bound for
arbitrary but fixed inputs. Then, we derive a data-independent error bound for
inputs chosen according to a simple random design. Our main results provide an
upper bound on the statistical error rates and sample complexity of learning
the unknown dynamics matrices from a single finite trajectory of bilinear
observations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Flight: A FaaS-Based Framework for Complex and Hierarchical Federated
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16495v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16495v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathaniel Hudson, Valerie Hayot-Sasson, Yadu Babuji, Matt Baughman, J. Gregory Pauloski, Ryan Chard, Ian Foster, Kyle Chard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) is a decentralized machine learning paradigm where
models are trained on distributed devices and are aggregated at a central
server. Existing FL frameworks assume simple two-tier network topologies where
end devices are directly connected to the aggregation server. While this is a
practical mental model, it does not exploit the inherent topology of real-world
distributed systems like the Internet-of-Things. We present Flight, a novel FL
framework that supports complex hierarchical multi-tier topologies,
asynchronous aggregation, and decouples the control plane from the data plane.
We compare the performance of Flight against Flower, a state-of-the-art FL
framework. Our results show that Flight scales beyond Flower, supporting up to
2048 simultaneous devices, and reduces FL makespan across several models.
Finally, we show that Flight's hierarchical FL model can reduce communication
overheads by more than 60%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Knowledge Tracing in Tutor-Student Dialogues 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16490v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16490v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Scarlatos, Andrew Lan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in large language models (LLMs) have led to the development
of artificial intelligence (AI)-powered tutoring chatbots, showing promise in
providing broad access to high-quality personalized education. Existing works
have primarily studied how to make LLMs follow tutoring principles but not how
to model student behavior in dialogues. However, analyzing student dialogue
turns can serve as a formative assessment, since open-ended student discourse
may indicate their knowledge levels and reveal specific misconceptions. In this
work, we present a first attempt at performing knowledge tracing (KT) in
tutor-student dialogues. We propose LLM prompting methods to identify the
knowledge components/skills involved in each dialogue turn and diagnose whether
the student responds correctly to the tutor, and verify the LLM's effectiveness
via an expert human evaluation. We then apply a range of KT methods on the
resulting labeled data to track student knowledge levels over an entire
dialogue. We conduct experiments on two tutoring dialogue datasets, and show
that a novel yet simple LLM-based method, LLMKT, significantly outperforms
existing KT methods in predicting student response correctness in dialogues. We
perform extensive qualitative analyses to highlight the challenges in dialogue
KT and outline multiple avenues for future work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SurGen: Text-Guided Diffusion Model for Surgical Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.14028v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.14028v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joseph Cho, Samuel Schmidgall, Cyril Zakka, Mrudang Mathur, Dhamanpreet Kaur, Rohan Shad, William Hiesinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion-based video generation models have made significant strides,
producing outputs with improved visual fidelity, temporal coherence, and user
control. These advancements hold great promise for improving surgical education
by enabling more realistic, diverse, and interactive simulation environments.
In this study, we introduce SurGen, a text-guided diffusion model tailored for
surgical video synthesis. SurGen produces videos with the highest resolution
and longest duration among existing surgical video generation models. We
validate the visual and temporal quality of the outputs using standard image
and video generation metrics. Additionally, we assess their alignment to the
corresponding text prompts through a deep learning classifier trained on
surgical data. Our results demonstrate the potential of diffusion models to
serve as valuable educational tools for surgical trainees.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Introducing CausalBench: A Flexible Benchmark Framework for Causal
  Analysis and Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08419v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08419v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmet Kapkiç, Pratanu Mandal, Shu Wan, Paras Sheth, Abhinav Gorantla, Yoonhyuk Choi, Huan Liu, K. Selçuk Candan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While witnessing the exceptional success of machine learning (ML)
technologies in many applications, users are starting to notice a critical
shortcoming of ML: correlation is a poor substitute for causation. The
conventional way to discover causal relationships is to use randomized
controlled experiments (RCT); in many situations, however, these are
impractical or sometimes unethical. Causal learning from observational data
offers a promising alternative. While being relatively recent, causal learning
aims to go far beyond conventional machine learning, yet several major
challenges remain. Unfortunately, advances are hampered due to the lack of
unified benchmark datasets, algorithms, metrics, and evaluation service
interfaces for causal learning. In this paper, we introduce {\em CausalBench},
a transparent, fair, and easy-to-use evaluation platform, aiming to (a) enable
the advancement of research in causal learning by facilitating scientific
collaboration in novel algorithms, datasets, and metrics and (b) promote
scientific objectivity, reproducibility, fairness, and awareness of bias in
causal learning research. CausalBench provides services for benchmarking data,
algorithms, models, and metrics, impacting the needs of a broad of scientific
and engineering disciplines.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FastTalker: Jointly Generating Speech and Conversational Gestures from
  Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16404v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16404v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixin Guo, Jian Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating 3D human gestures and speech from a text script is critical for
creating realistic talking avatars. One solution is to leverage separate
pipelines for text-to-speech (TTS) and speech-to-gesture (STG), but this
approach suffers from poor alignment of speech and gestures and slow inference
times. In this paper, we introduce FastTalker, an efficient and effective
framework that simultaneously generates high-quality speech audio and 3D human
gestures at high inference speeds. Our key insight is reusing the intermediate
features from speech synthesis for gesture generation, as these features
contain more precise rhythmic information than features re-extracted from
generated speech. Specifically, 1) we propose an end-to-end framework that
concurrently generates speech waveforms and full-body gestures, using
intermediate speech features such as pitch, onset, energy, and duration
directly for gesture decoding; 2) we redesign the causal network architecture
to eliminate dependencies on future inputs for real applications; 3) we employ
Reinforcement Learning-based Neural Architecture Search (NAS) to enhance both
performance and inference speed by optimizing our network architecture.
Experimental results on the BEAT2 dataset demonstrate that FastTalker achieves
state-of-the-art performance in both speech synthesis and gesture generation,
processing speech and gestures in 0.17 seconds per second on an NVIDIA 3090.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>European Conference on Computer Vision Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HA-FGOVD: Highlighting Fine-grained Attributes via Explicit Linear
  Composition for Open-Vocabulary Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16136v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16136v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqi Ma, Mengyin Liu, Chao Zhu, Xu-Cheng Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-vocabulary object detection (OVD) models are considered to be Large
Multi-modal Models (LMM), due to their extensive training data and a large
number of parameters. Mainstream OVD models prioritize object coarse-grained
category rather than focus on their fine-grained attributes, e.g., colors or
materials, thus failed to identify objects specified with certain attributes.
However, OVD models are pretrained on large-scale image-text pairs with rich
attribute words, whose latent feature space can represent the global text
feature as a linear composition of fine-grained attribute tokens without
highlighting them. Therefore, we propose in this paper a universal and explicit
approach for frozen mainstream OVD models that boosts their attribute-level
detection capabilities by highlighting fine-grained attributes in explicit
linear space. Firstly, a LLM is leveraged to highlight attribute words within
the input text as a zero-shot prompted task. Secondly, by strategically
adjusting the token masks, the text encoders of OVD models extract both global
text and attribute-specific features, which are then explicitly composited as
two vectors in linear space to form the new attribute-highlighted feature for
detection tasks, where corresponding scalars are hand-crafted or learned to
reweight both two vectors. Notably, these scalars can be seamlessly transferred
among different OVD models, which proves that such an explicit linear
composition is universal. Empirical evaluation on the FG-OVD dataset
demonstrates that our proposed method uniformly improves fine-grained
attribute-level OVD of various mainstream models and achieves new
state-of-the-art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Layer-wise Model Merging for Unsupervised Domain Adaptation in
  Segmentation Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15813v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15813v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roberto Alcover-Couso, Juan C. SanMiguel, Marcos Escudero-Viñolo, Jose M Martínez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Merging parameters of multiple models has resurfaced as an effective strategy
to enhance task performance and robustness, but prior work is limited by the
high costs of ensemble creation and inference. In this paper, we leverage the
abundance of freely accessible trained models to introduce a cost-free approach
to model merging. It focuses on a layer-wise integration of merged models,
aiming to maintain the distinctiveness of the task-specific final layers while
unifying the initial layers, which are primarily associated with feature
extraction. This approach ensures parameter consistency across all layers,
essential for boosting performance. Moreover, it facilitates seamless
integration of knowledge, enabling effective merging of models from different
datasets and tasks. Specifically, we investigate its applicability in
Unsupervised Domain Adaptation (UDA), an unexplored area for model merging, for
Semantic and Panoptic Segmentation. Experimental results demonstrate
substantial UDA improvements without additional costs for merging
same-architecture models from distinct datasets ($\uparrow 2.6\%$ mIoU) and
different-architecture models with a shared backbone ($\uparrow 6.8\%$ mIoU).
Furthermore, merging Semantic and Panoptic Segmentation models increases mPQ by
$\uparrow 7\%$. These findings are validated across a wide variety of UDA
strategies, architectures, and datasets.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-09-23T00:00:00Z">2024-09-23</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">15</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing News Text Classification with Bi-LSTM and Attention Mechanism
  for Efficient Data Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15576v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15576v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingyao Liu, Jiajing Chen, Rui Wang, Junming Huang, Yuanshuai Luo, Jianjun Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of Internet technology has led to a rapid increase in news
information. Filtering out valuable content from complex information has become
an urgentproblem that needs to be solved. In view of the shortcomings of
traditional manual classification methods that are time-consuming and
inefficient, this paper proposes an automaticclassification scheme for news
texts based on deep learning. This solution achieves efficient classification
and management of news texts by introducing advanced machine learning
algorithms, especially an optimization model that combines Bi-directional Long
Short-Term Memory Network (Bi-LSTM) and Attention Mechanism. Experimental
results show that this solution can not only significantly improve the accuracy
and timeliness of classification, but also significantly reduce the need for
manual intervention. It has important practical significance for improving the
information processing capabilities of the news industry and accelerating the
speed of information flow. Through comparative analysis of multiple common
models, the effectiveness and advancement of the proposed method are proved,
laying a solid foundation for future news text classification research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-Domain Latent Factors Sharing via Implicit Matrix Factorization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15568v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15568v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdulaziz Samra, Evgeney Frolov, Alexey Vasilev, Alexander Grigorievskiy, Anton Vakhrushev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data sparsity has been one of the long-standing problems for recommender
systems. One of the solutions to mitigate this issue is to exploit knowledge
available in other source domains. However, many cross-domain recommender
systems introduce a complex architecture that makes them less scalable in
practice. On the other hand, matrix factorization methods are still considered
to be strong baselines for single-domain recommendations. In this paper, we
introduce the CDIMF, a model that extends the standard implicit matrix
factorization with ALS to cross-domain scenarios. We apply the Alternating
Direction Method of Multipliers to learn shared latent factors for overlapped
users while factorizing the interaction matrix. In a dual-domain setting,
experiments on industrial datasets demonstrate a competing performance of CDIMF
for both cold-start and warm-start. The proposed model can outperform most
other recent cross-domain and single-domain models. We also provide the code to
reproduce experiments on GitHub.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stalactite: Toolbox for Fast Prototyping of Vertical Federated Learning
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15558v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15558v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anastasiia Zakharova, Dmitriy Alexandrov, Maria Khodorchenko, Nikolay Butakov, Alexey Vasilev, Maxim Savchenko, Alexander Grigorievskiy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning (ML) models trained on datasets owned by different
organizations and physically located in remote databases offer benefits in many
real-world use cases. State regulations or business requirements often prevent
data transfer to a central location, making it difficult to utilize standard
machine learning algorithms. Federated Learning (FL) is a technique that
enables models to learn from distributed datasets without revealing the
original data. Vertical Federated learning (VFL) is a type of FL where data
samples are divided by features across several data owners. For instance, in a
recommendation task, a user can interact with various sets of items, and the
logs of these interactions are stored by different organizations. In this demo
paper, we present \emph{Stalactite} - an open-source framework for VFL that
provides the necessary functionality for building prototypes of VFL systems. It
has several advantages over the existing frameworks. In particular, it allows
researchers to focus on the algorithmic side rather than engineering and to
easily deploy learning in a distributed environment. It implements several VFL
algorithms and has a built-in homomorphic encryption layer. We demonstrate its
use on a real-world recommendation datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative AI Is Not Ready for Clinical Use in Patient Education for
  Lower Back Pain Patients, Even With Retrieval-Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15260v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15260v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi-Fei Zhao, Allyn Bove, David Thompson, James Hill, Yi Xu, Yufan Ren, Andrea Hassman, Leming Zhou, Yanshan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low back pain (LBP) is a leading cause of disability globally. Following the
onset of LBP and subsequent treatment, adequate patient education is crucial
for improving functionality and long-term outcomes. Despite advancements in
patient education strategies, significant gaps persist in delivering
personalized, evidence-based information to patients with LBP. Recent
advancements in large language models (LLMs) and generative artificial
intelligence (GenAI) have demonstrated the potential to enhance patient
education. However, their application and efficacy in delivering educational
content to patients with LBP remain underexplored and warrant further
investigation. In this study, we introduce a novel approach utilizing LLMs with
Retrieval-Augmented Generation (RAG) and few-shot learning to generate tailored
educational materials for patients with LBP. Physical therapists manually
evaluated our model responses for redundancy, accuracy, and completeness using
a Likert scale. In addition, the readability of the generated education
materials is assessed using the Flesch Reading Ease score. The findings
demonstrate that RAG-based LLMs outperform traditional LLMs, providing more
accurate, complete, and readable patient education materials with less
redundancy. Having said that, our analysis reveals that the generated materials
are not yet ready for use in clinical practice. This study underscores the
potential of AI-driven models utilizing RAG to improve patient education for
LBP; however, significant challenges remain in ensuring the clinical relevance
and granularity of content generated by these models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lessons Learned on Information Retrieval in Electronic Health Records: A
  Comparison of Embedding Models and Pooling Strategies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15163v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15163v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Skatje Myers, Timothy A. Miller, Yanjun Gao, Matthew M. Churpek, Anoop Mayampurath, Dmitriy Dligach, Majid Afshar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objective: Applying large language models (LLMs) to the clinical domain is
challenging due to the context-heavy nature of processing medical records.
Retrieval-augmented generation (RAG) offers a solution by facilitating
reasoning over large text sources. However, there are many parameters to
optimize in just the retrieval system alone. This paper presents an ablation
study exploring how different embedding models and pooling methods affect
information retrieval for the clinical domain.
  Methods: Evaluating on three retrieval tasks on two electronic health record
(EHR) data sources, we compared seven models, including medical- and
general-domain models, specialized encoder embedding models, and off-the-shelf
decoder LLMs. We also examine the choice of embedding pooling strategy for each
model, independently on the query and the text to retrieve.
  Results: We found that the choice of embedding model significantly impacts
retrieval performance, with BGE, a comparatively small general-domain model,
consistently outperforming all others, including medical-specific models.
However, our findings also revealed substantial variability across datasets and
query text phrasings. We also determined the best pooling methods for each of
these models to guide future design of retrieval systems.
  Discussion: The choice of embedding model, pooling strategy, and query
formulation can significantly impact retrieval performance and the performance
of these models on other public benchmarks does not necessarily transfer to new
domains. Further studies such as this one are vital for guiding
empirically-grounded development of retrieval frameworks, such as in the
context of RAG, for the clinical domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Don't Use LLMs to Make Relevance Judgments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15133v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15133v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ian Soboroff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Making the relevance judgments for a TREC-style test collection can be
complex and expensive. A typical TREC track usually involves a team of six
contractors working for 2-4 weeks. Those contractors need to be trained and
monitored. Software has to be written to support recording relevance judgments
correctly and efficiently. The recent advent of large language models that
produce astoundingly human-like flowing text output in response to a natural
language prompt has inspired IR researchers to wonder how those models might be
used in the relevance judgment collection process. At the ACM SIGIR 2024
conference, a workshop ``LLM4Eval'' provided a venue for this work, and
featured a data challenge activity where participants reproduced TREC deep
learning track judgments, as was done by Thomas et al (arXiv:2408.08896,
arXiv:2309.10621). I was asked to give a keynote at the workshop, and this
paper presents that keynote in article form. The bottom-line-up-front message
is, don't use LLMs to create relevance judgments for TREC-style evaluations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EMERS: Energy Meter for Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15060v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15060v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Wegmeth, Tobias Vente, Alan Said, Joeran Beel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to recent advancements in machine learning, recommender systems use
increasingly more energy for training, evaluation, and deployment. However, the
recommender systems community often does not report the energy consumption of
their experiments. In today's research landscape, no tools exist to easily
measure the energy consumption of recommender systems experiments. To bridge
this gap, we introduce EMERS, the first software library that simplifies
measuring, monitoring, recording, and sharing the energy consumption of
recommender systems experiments. EMERS measures energy consumption with smart
power plugs and offers a user interface to monitor and compare the energy
consumption of recommender systems experiments. Thereby, EMERS improves
sustainability awareness and simplifies self-reporting energy consumption for
recommender systems practitioners and researchers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the RecSoGood 2024 Workshop co-located with the 18th ACM
  Conference on Recommender Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vi<span class="highlight-title">BERT</span>grid BiLSTM-CRF: Multimodal Key Information Extraction from
  Unstructured Financial Documents <span class="chip">ECML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15004v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15004v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Furkan Pala, Mehmet Yasin Akpınar, Onur Deniz, Gülşen Eryiğit
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal key information extraction (KIE) models have been studied
extensively on semi-structured documents. However, their investigation on
unstructured documents is an emerging research topic. The paper presents an
approach to adapt a multimodal transformer (i.e., ViBERTgrid previously
explored on semi-structured documents) for unstructured financial documents, by
incorporating a BiLSTM-CRF layer. The proposed ViBERTgrid BiLSTM-CRF model
demonstrates a significant improvement in performance (up to 2 percentage
points) on named entity recognition from unstructured documents in financial
domain, while maintaining its KIE performance on semi-structured documents. As
an additional contribution, we publicly released token-level annotations for
the SROIE dataset in order to pave the way for its use in multimodal sequence
labeling models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in MIDAS (The 8th Workshop on MIning DAta for financial
  applicationS) workshop of ECML PKDD 2023 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Learning on User Segmentation: Universal to Specific
  Representation via Bipartite Neural Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14945v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14945v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Tan, Yongxin Deng, Chao Qu, Siqiao Xue, Xiaoming Shi, James Zhang, Xihe Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, models for user representation learning have been widely applied in
click-through-rate (CTR) and conversion-rate (CVR) prediction. Usually, the
model learns a universal user representation as the input for subsequent
scenario-specific models. However, in numerous industrial applications (e.g.,
recommendation and marketing), the business always operates such applications
as various online activities among different user segmentation. These
segmentation are always created by domain experts. Due to the difference in
user distribution (i.e., user segmentation) and business objectives in
subsequent tasks, learning solely on universal representation may lead to
detrimental effects on both model performance and robustness. In this paper, we
propose a novel learning framework that can first learn general universal user
representation through information bottleneck. Then, merge and learn a
segmentation-specific or a task-specific representation through neural
interaction. We design the interactive learning process by leveraging a
bipartite graph architecture to model the representation learning and merging
between contextual clusters and each user segmentation. Our proposed method is
evaluated in two open-source benchmarks, two offline business datasets, and
deployed on two online marketing applications to predict users' CVR. The
results demonstrate that our method can achieve superior performance and
surpass the baseline methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedSlate:A Federated Deep Reinforcement Learning Recommender System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14872v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14872v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongxin Deng, Xiaoyu Tan, Xihe Qiu, Yaochu Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning methods have been used to optimize long-term user
engagement in recommendation systems. However, existing reinforcement
learning-based recommendation systems do not fully exploit the relevance of
individual user behavior across different platforms. One potential solution is
to aggregate data from various platforms in a centralized location and use the
aggregated data for training. However, this approach raises economic and legal
concerns, including increased communication costs and potential threats to user
privacy. To address these challenges, we propose \textbf{FedSlate}, a federated
reinforcement learning recommendation algorithm that effectively utilizes
information that is prohibited from being shared at a legal level. We employ
the SlateQ algorithm to assist FedSlate in learning users' long-term behavior
and evaluating the value of recommended content. We extend the existing
application scope of recommendation systems from single-user single-platform to
single-user multi-platform and address cross-platform learning challenges by
introducing federated learning. We use RecSim to construct a simulation
environment for evaluating FedSlate and compare its performance with
state-of-the-art benchmark recommendation models. Experimental results
demonstrate the superior effects of FedSlate over baseline methods in various
environmental settings, and FedSlate facilitates the learning of recommendation
strategies in scenarios where baseline methods are completely inapplicable.
Code is available at \textit{https://github.com/TianYaDY/FedSlate}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Pre-train</span>ed Language Model and Knowledge Distillation for Lightweight
  Sequential Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14810v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14810v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Li, Mingyue Cheng, Zhiding Liu, Hao Zhang, Qi Liu, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommendation models user interests based on historical behaviors
to provide personalized recommendation. Previous sequential recommendation
algorithms primarily employ neural networks to extract features of user
interests, achieving good performance. However, due to the recommendation
system datasets sparsity, these algorithms often employ small-scale network
frameworks, resulting in weaker generalization capability. Recently, a series
of sequential recommendation algorithms based on large pre-trained language
models have been proposed. Nonetheless, given the real-time demands of
recommendation systems, the challenge remains in applying pre-trained language
models for rapid recommendations in real scenarios. To address this, we propose
a sequential recommendation algorithm based on a pre-trained language model and
knowledge distillation. The key of proposed algorithm is to transfer
pre-trained knowledge across domains and achieve lightweight inference by
knowledge distillation. The algorithm operates in two stages: in the first
stage, we fine-tune the pre-trained language model on the recommendation
dataset to transfer the pre-trained knowledge to the recommendation task; in
the second stage, we distill the trained language model to transfer the learned
knowledge to a lightweight model. Extensive experiments on multiple public
recommendation datasets show that the proposed algorithm enhances
recommendation accuracy and provide timely recommendation services.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>in Chinese language</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EDGE-Rec: Efficient and Data-Guided Edge Diffusion For Recommender
  Systems Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14689v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14689v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Utkarsh Priyam, Hemit Shah, Edoardo Botta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most recommender systems research focuses on binary historical user-item
interaction encodings to predict future interactions. User features, item
features, and interaction strengths remain largely under-utilized in this space
or only indirectly utilized, despite proving largely effective in large-scale
production recommendation systems. We propose a new attention mechanism,
loosely based on the principles of collaborative filtering, called Row-Column
Separable Attention RCSA to take advantage of real-valued interaction weights
as well as user and item features directly. Building on this mechanism, we
additionally propose a novel Graph Diffusion Transformer GDiT architecture
which is trained to iteratively denoise the weighted interaction matrix of the
user-item interaction graph directly. The weighted interaction matrix is built
from the bipartite structure of the user-item interaction graph and
corresponding edge weights derived from user-item rating interactions. Inspired
by the recent progress in text-conditioned image generation, our method
directly produces user-item rating predictions on the same scale as the
original ratings by conditioning the denoising process on user and item
features with a principled approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reducing the Footprint of Multi-Vector Retrieval with Minimal
  Performance Impact via Token Pooling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14683v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14683v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Clavié, Antoine Chaffin, Griffin Adams
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the last few years, multi-vector retrieval methods, spearheaded by
ColBERT, have become an increasingly popular approach to Neural IR. By storing
representations at the token level rather than at the document level, these
methods have demonstrated very strong retrieval performance, especially in
out-of-domain settings. However, the storage and memory requirements necessary
to store the large number of associated vectors remain an important drawback,
hindering practical adoption. In this paper, we introduce a simple
clustering-based token pooling approach to aggressively reduce the number of
vectors that need to be stored. This method can reduce the space & memory
footprint of ColBERT indexes by 50% with virtually no retrieval performance
degradation. This method also allows for further reductions, reducing the
vector count by 66%-to-75% , with degradation remaining below 5% on a vast
majority of datasets. Importantly, this approach requires no architectural
change nor query-time processing, and can be used as a simple drop-in during
indexation with any ColBERT-like model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Training Objectives Improve Embedding-based Retrieval in
  Industrial Recommendation Systems <span class="chip">RecSys</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14682v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14682v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Kolodner, Mingxuan Ju, Zihao Fan, Tong Zhao, Elham Ghazizadeh, Yan Wu, Neil Shah, Yozen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Improving recommendation systems (RS) can greatly enhance the user experience
across many domains, such as social media. Many RS utilize embedding-based
retrieval (EBR) approaches to retrieve candidates for recommendation. In an EBR
system, the embedding quality is key. According to recent literature,
self-supervised multitask learning (SSMTL) has showed strong performance on
academic benchmarks in embedding learning and resulted in an overall
improvement in multiple downstream tasks, demonstrating a larger resilience to
the adverse conditions between each downstream task and thereby increased
robustness and task generalization ability through the training objective.
However, whether or not the success of SSMTL in academia as a robust training
objectives translates to large-scale (i.e., over hundreds of million users and
interactions in-between) industrial RS still requires verification. Simply
adopting academic setups in industrial RS might entail two issues. Firstly,
many self-supervised objectives require data augmentations (e.g., embedding
masking/corruption) over a large portion of users and items, which is
prohibitively expensive in industrial RS. Furthermore, some self-supervised
objectives might not align with the recommendation task, which might lead to
redundant computational overheads or negative transfer. In light of these two
challenges, we evaluate using a robust training objective, specifically SSMTL,
through a large-scale friend recommendation system on a social media platform
in the tech sector, identifying whether this increase in robustness can work at
scale in enhancing retrieval in the production setting. Through online A/B
testing with SSMTL-based EBR, we observe statistically significant increases in
key metrics in the friend recommendations, with up to 5.45% improvements in new
friends made and 1.91% improvements in new friends made with cold-start users.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>RobustRecSys workshop @ RecSys 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GNNAnatomy: Systematic Generation and Evaluation of Multi-Level
  Explanations for Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04548v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04548v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hsiao-Ying Lu, Yiran Li, Ujwal Pratap Krishna Kaluvakolanu Thyagarajan, Kwan-Liu Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) excel in machine learning tasks involving
graphs, such as node classification, graph classification, and link prediction.
However, explaining their decision-making process is challenging due to the
complex transformations GNNs perform by aggregating relational information from
graph topology. Existing methods for explaining GNNs face key limitations: (1)
lack of flexibility in generating explanations at varying levels, (2)
difficulty in identifying unique substructures relevant to class
differentiation, and (3) little support to ensure the trustworthiness of
explanations. To address these challenges, we introduce GNNAnatomy, a visual
analytics system designed to generate and evaluate multi-level GNN explanations
for graph classification tasks. GNNAnatomy uses graphlets, primitive graph
substructures, to identify the most critical substructures in a graph class by
analyzing the correlation between GNN predictions and graphlet frequencies.
These correlations are presented interactively for user-selected group of
graphs through our visual analytics system. To further validate top-ranked
graphlets, we measure the change in classification confidence after removing
each graphlet from the original graph. We demonstrate the effectiveness of
GNNAnatomy through case studies on synthetic and real-world graph datasets from
sociology and biology domains. Additionally, we compare GNNAnatomy with
state-of-the-art explainable GNN methods to showcase its utility and
versatility.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">10</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revise, Reason, and Recognize: LLM-Based Emotion Recognition via
  Emotion-Specific <span class="highlight-title">Prompt</span>s and ASR Error Correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15551v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15551v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanchao Li, Yuan Gong, Chao-Han Huck Yang, Peter Bell, Catherine Lai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Annotating and recognizing speech emotion using prompt engineering has
recently emerged with the advancement of Large Language Models (LLMs), yet its
efficacy and reliability remain questionable. In this paper, we conduct a
systematic study on this topic, beginning with the proposal of novel prompts
that incorporate emotion-specific knowledge from acoustics, linguistics, and
psychology. Subsequently, we examine the effectiveness of LLM-based prompting
on Automatic Speech Recognition (ASR) transcription, contrasting it with
ground-truth transcription. Furthermore, we propose a Revise-Reason-Recognize
prompting pipeline for robust LLM-based emotion recognition from spoken
language with ASR errors. Additionally, experiments on context-aware learning,
in-context learning, and instruction tuning are performed to examine the
usefulness of LLM training schemes in this direction. Finally, we investigate
the sensitivity of LLMs to minor prompt variations. Experimental results
demonstrate the efficacy of the emotion-specific prompts, ASR error correction,
and LLM training schemes for LLM-based emotion recognition. Our study aims to
refine the use of LLMs in emotion recognition and related domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Emotion Bias in Music via Frechet Audio Distance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanchao Li, Azalea Gui, Dimitra Emmanouilidou, Hannes Gamper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The subjective nature of music emotion introduces inherent bias in both
recognition and generation, especially when relying on a single audio encoder,
emotion classifier, or evaluation metric. In this work, we conduct a study on
Music Emotion Recognition (MER) and Emotional Music Generation (EMG), employing
diverse audio encoders alongside the Frechet Audio Distance (FAD), a
reference-free evaluation metric. Our study begins with a benchmark evaluation
of MER, highlighting the limitations associated with using a single audio
encoder and the disparities observed across different measurements. We then
propose assessing MER performance using FAD from multiple encoders to provide a
more objective measure of music emotion. Furthermore, we introduce an enhanced
EMG approach designed to improve both the variation and prominence of generated
music emotion, thus enhancing realism. Additionally, we investigate the realism
disparities between the emotions conveyed in real and synthetic music,
comparing our EMG model against two baseline models. Experimental results
underscore the emotion bias problem in both MER and EMG and demonstrate the
potential of using FAD and diverse audio encoders to evaluate music emotion
objectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LoVA: Long-form Video-to-Audio Generation <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15157v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15157v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Cheng, Xihua Wang, Yihan Wu, Yuyue Wang, Ruihua Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video-to-audio (V2A) generation is important for video editing and
post-processing, enabling the creation of semantics-aligned audio for silent
video. However, most existing methods focus on generating short-form audio for
short video segment (less than 10 seconds), while giving little attention to
the scenario of long-form video inputs. For current UNet-based diffusion V2A
models, an inevitable problem when handling long-form audio generation is the
inconsistencies within the final concatenated audio. In this paper, we first
highlight the importance of long-form V2A problem. Besides, we propose LoVA, a
novel model for Long-form Video-to-Audio generation. Based on the Diffusion
Transformer (DiT) architecture, LoVA proves to be more effective at generating
long-form audio compared to existing autoregressive models and UNet-based
diffusion models. Extensive objective and subjective experiments demonstrate
that LoVA achieves comparable performance on 10-second V2A benchmark and
outperforms all other baselines on a benchmark with long-form video input.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DanceCamAnimator: Keyframe-Based Controllable 3D Dance Camera Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14925v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14925v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixuan Wang, Jiayi Li, Xiaoyu Qin, Shikun Sun, Songtao Zhou, Jia Jia, Jiebo Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthesizing camera movements from music and dance is highly challenging due
to the contradicting requirements and complexities of dance cinematography.
Unlike human movements, which are always continuous, dance camera movements
involve both continuous sequences of variable lengths and sudden drastic
changes to simulate the switching of multiple cameras. However, in previous
works, every camera frame is equally treated and this causes jittering and
unavoidable smoothing in post-processing. To solve these problems, we propose
to integrate animator dance cinematography knowledge by formulating this task
as a three-stage process: keyframe detection, keyframe synthesis, and tween
function prediction. Following this formulation, we design a novel end-to-end
dance camera synthesis framework \textbf{DanceCamAnimator}, which imitates
human animation procedures and shows powerful keyframe-based controllability
with variable lengths. Extensive experiments on the DCM dataset demonstrate
that our method surpasses previous baselines quantitatively and qualitatively.
Code will be available at
\url{https://github.com/Carmenw1203/DanceCamAnimator-Official}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM Multimedia 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RoWSFormer: A Robust Watermarking Framework with Swin <span class="highlight-title">Transformer</span> for
  Enhanced Geometric Attack Resilience 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14829v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14829v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weitong Chen, Yuheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, digital watermarking techniques based on deep learning have
been widely studied. To achieve both imperceptibility and robustness of image
watermarks, most current methods employ convolutional neural networks to build
robust watermarking frameworks. However, despite the success of CNN-based
watermarking models, they struggle to achieve robustness against geometric
attacks due to the limitations of convolutional neural networks in capturing
global and long-range relationships. To address this limitation, we propose a
robust watermarking framework based on the Swin Transformer, named RoWSFormer.
Specifically, we design the Locally-Channel Enhanced Swin Transformer Block as
the core of both the encoder and decoder. This block utilizes the
self-attention mechanism to capture global and long-range information, thereby
significantly improving adaptation to geometric distortions. Additionally, we
construct the Frequency-Enhanced Transformer Block to extract frequency domain
information, which further strengthens the robustness of the watermarking
framework. Experimental results demonstrate that our RoWSFormer surpasses
existing state-of-the-art watermarking methods. For most non-geometric attacks,
RoWSFormer improves the PSNR by 3 dB while maintaining the same extraction
accuracy. In the case of geometric attacks (such as rotation, scaling, and
affine transformations), RoWSFormer achieves over a 6 dB improvement in PSNR,
with extraction accuracy exceeding 97\%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AIM 2024 Challenge on Video Saliency Prediction: Methods and Results <span class="chip">ECCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrey Moskalenko, Alexey Bryncev, Dmitry Vatolin, Radu Timofte, Gen Zhan, Li Yang, Yunlong Tang, Yiting Liao, Jiongzhi Lin, Baitao Huang, Morteza Moradi, Mohammad Moradi, Francesco Rundo, Concetto Spampinato, Ali Borji, Simone Palazzo, Yuxin Zhu, Yinan Sun, Huiyu Duan, Yuqin Cao, Ziheng Jia, Qiang Hu, Xiongkuo Min, Guangtao Zhai, Hao Fang, Runmin Cong, Xiankai Lu, Xiaofei Zhou, Wei Zhang, Chunyu Zhao, Wentao Mu, Tao Deng, Hamed R. Tavakoli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper reviews the Challenge on Video Saliency Prediction at AIM 2024.
The goal of the participants was to develop a method for predicting accurate
saliency maps for the provided set of video sequences. Saliency maps are widely
exploited in various applications, including video compression, quality
assessment, visual perception studies, the advertising industry, etc. For this
competition, a previously unused large-scale audio-visual mouse saliency
(AViMoS) dataset of 1500 videos with more than 70 observers per video was
collected using crowdsourced mouse tracking. The dataset collection methodology
has been validated using conventional eye-tracking data and has shown high
consistency. Over 30 teams registered in the challenge, and there are 7 teams
that submitted the results in the final phase. The final phase solutions were
tested and ranked by commonly used quality metrics on a private test subset.
The results of this evaluation and the descriptions of the solutions are
presented in this report. All data, including the private test subset, is made
publicly available on the challenge homepage -
https://challenges.videoprocessing.ai/challenges/video-saliency-prediction.html.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCVW 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multimedia Framework for Continuum Robots: Systematic, Computational,
  and Control Perspectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14708v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14708v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Po-Yu Hsieh, June-Hao Hou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continuum robots, which often rely on interdisciplinary and multimedia
collaborations, have been increasingly recognized for their potential to
revolutionize the field of human-robot interaction (HRI) in varied applications
due to their adaptive, responsive, and flexible characteristics. Despite their
promises, the lack of an integrated framework poses significant challenges for
both users and developers, resulting in inefficiency and complexity during
preliminary developments. Thus, this paper introduces a unified framework for
bionic robotics that addresses these challenges by integrating system
architecture, dynamics computation, and control strategy. The proposed method
allows for efficient modeling and quick preview of the results in both digital
and physical environments, which can enhance the quality of robot developments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MemeCLIP: Leveraging CLIP Representations for Multimodal Meme
  Classification <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14703v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14703v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddhant Bikram Shah, Shuvam Shiwakoti, Maheep Chaudhary, Haohan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The complexity of text-embedded images presents a formidable challenge in
machine learning given the need for multimodal understanding of the multiple
aspects of expression conveyed in them. While previous research in multimodal
analysis has primarily focused on singular aspects such as hate speech and its
subclasses, our study expands the focus to encompass multiple aspects of
linguistics: hate, target, stance, and humor detection. We introduce a novel
dataset PrideMM comprising text-embedded images associated with the LGBTQ+
Pride movement, thereby addressing a serious gap in existing resources. We
conduct extensive experimentation on PrideMM by using unimodal and multimodal
baseline methods to establish benchmarks for each task. Additionally, we
propose a novel framework MemeCLIP for efficient downstream learning while
preserving the knowledge of the pre-trained CLIP model. The results of our
experiments show that MemeCLIP achieves superior performance compared to
previously proposed frameworks on two real-world datasets. We further compare
the performance of MemeCLIP and zero-shot GPT-4 on the hate classification
task. Finally, we discuss the shortcomings of our model by qualitatively
analyzing misclassified samples. Our code and dataset are publicly available
at: https://github.com/SiddhantBikram/MemeCLIP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 (Main)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Feeding the Crave: How People with Eating Disorders Get Trapped in the
  Perpetual Cycle of Digital Food Content 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.05920v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.05920v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryuhaerang Choi, Subin Park, Sujin Han, Sung-Ju Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have examined how digital food content impacts viewers'
dietary health. A few have found that individuals with eating disorders are
particularly sensitive to digital food content, such as eating and cooking
videos, which contribute to disordered eating behaviors. However, there is a
lack of comprehensive studies that investigate how these individuals interact
with various digital food content. To fill this gap, we conducted two rounds of
studies (N=23 and 22, respectively) with individuals with eating disorders to
understand their motivations and practices of consuming digital food content.
Our study reveals that participants anticipate positive effects from food media
to overcome their condition, but in practice, it often exacerbates their
disorder. We also discovered that many participants experienced a cycle of
quitting and returning to digital food content consumption. Based on these
findings, we articulate design implications for digital food content and
multimedia platforms to support vulnerable individuals.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SVDD 2024: The Inaugural Singing Voice Deepfake Detection Challenge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16132v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16132v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        You Zhang, Yongyi Zang, Jiatong Shi, Ryuichi Yamamoto, Tomoki Toda, Zhiyao Duan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advancements in singing voice generation and the growing presence of
AI singers on media platforms, the inaugural Singing Voice Deepfake Detection
(SVDD) Challenge aims to advance research in identifying AI-generated singing
voices from authentic singers. This challenge features two tracks: a controlled
setting track (CtrSVDD) and an in-the-wild scenario track (WildSVDD). The
CtrSVDD track utilizes publicly available singing vocal data to generate
deepfakes using state-of-the-art singing voice synthesis and conversion
systems. Meanwhile, the WildSVDD track expands upon the existing SingFake
dataset, which includes data sourced from popular user-generated content
websites. For the CtrSVDD track, we received submissions from 47 teams, with 37
surpassing our baselines and the top team achieving a 1.65% equal error rate.
For the WildSVDD track, we benchmarked the baselines. This paper reviews these
results, discusses key findings, and outlines future directions for SVDD
research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, Accepted by 2024 IEEE Spoken Language Technology Workshop
  (SLT 2024)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-09-22T00:00:00Z">2024-09-22</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Nirjas: An open source framework for extracting metadata from the source
  code 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14609v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14609v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayush Bhardwaj,  Sahil, Kaushlendra Pratap, Gaurav Mishra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Metadata and comments are critical elements of any software development
process. In this paper, we explain how metadata and comments in source code can
play an essential role in comprehending software. We introduce a Python-based
open-source framework, Nirjas, which helps in extracting this metadata in a
structured manner. Various syntaxes, types, and widely accepted conventions
exist for adding comments in source files of different programming languages.
Edge cases can create noise in extraction, for which we use Regex to accurately
retrieve metadata. Non-Regex methods can give results but often miss accuracy
and noise separation. Nirjas also separates different types of comments, source
code, and provides details about those comments, such as line number, file
name, language used, total SLOC, etc. Nirjas is a standalone Python
framework/library and can be easily installed via source or pip (the Python
package installer). Nirjas was initially created as part of a Google Summer of
Code project and is currently developed and maintained under the FOSSology
organization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2022 12th International Conference on Cloud Computing, Data Science &
  Engineering (Confluence)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Words: Evaluating Large Language Models in Transportation
  Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14516v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14516v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaowei Ying, Zhenlong Li, Manzhu Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The resurgence and rapid advancement of Generative Artificial Intelligence
(GenAI) in 2023 has catalyzed transformative shifts across numerous industry
sectors, including urban transportation and logistics. This study investigates
the evaluation of Large Language Models (LLMs), specifically GPT-4 and
Phi-3-mini, to enhance transportation planning. The study assesses the
performance and spatial comprehension of these models through a
transportation-informed evaluation framework that includes general geospatial
skills, general transportation domain skills, and real-world transportation
problem-solving. Utilizing a mixed-methods approach, the research encompasses
an evaluation of the LLMs' general Geographic Information System (GIS) skills,
general transportation domain knowledge as well as abilities to support human
decision-making in the real-world transportation planning scenarios of
congestion pricing. Results indicate that GPT-4 demonstrates superior accuracy
and reliability across various GIS and transportation-specific tasks compared
to Phi-3-mini, highlighting its potential as a robust tool for transportation
planners. Nonetheless, Phi-3-mini exhibits competence in specific analytical
scenarios, suggesting its utility in resource-constrained environments. The
findings underscore the transformative potential of GenAI technologies in urban
transportation planning. Future work could explore the application of newer
LLMs and the impact of Retrieval-Augmented Generation (RAG) techniques, on a
broader set of real-world transportation planning and operations challenges, to
deepen the integration of advanced AI models in transportation management
practices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ XRec: Large Language Models for Explainable Recommendation <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02377v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02377v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiyao Ma, Xubin Ren, Chao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems help users navigate information overload by providing
personalized recommendations aligned with their preferences. Collaborative
Filtering (CF) is a widely adopted approach, but while advanced techniques like
graph neural networks (GNNs) and self-supervised learning (SSL) have enhanced
CF models for better user representations, they often lack the ability to
provide explanations for the recommended items. Explainable recommendations aim
to address this gap by offering transparency and insights into the
recommendation decision-making process, enhancing users' understanding. This
work leverages the language capabilities of Large Language Models (LLMs) to
push the boundaries of explainable recommender systems. We introduce a
model-agnostic framework called XRec, which enables LLMs to provide
comprehensive explanations for user behaviors in recommender systems. By
integrating collaborative signals and designing a lightweight collaborative
adaptor, the framework empowers LLMs to understand complex patterns in
user-item interactions and gain a deeper understanding of user preferences. Our
extensive experiments demonstrate the effectiveness of XRec, showcasing its
ability to generate comprehensive and meaningful explanations that outperform
baseline approaches in explainable recommender systems. We open-source our
model implementation at https://github.com/HKUDS/XRec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Analyzing the Effectiveness of Listwise Reranking with Positional
  Invariance on Temporal Generalizability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.06716v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.06716v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soyoung Yoon, Jongyoon Kim, Seung-won Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This working note outlines our participation in the retrieval task at CLEF
2024. We highlight the considerable gap between studying retrieval performance
on static knowledge documents and understanding performance in real-world
environments. Therefore, Addressing these discrepancies and measuring the
temporal persistence of IR systems is crucial. By investigating the LongEval
benchmark, specifically designed for such dynamic environments, our findings
demonstrate the effectiveness of a listwise reranking approach, which
proficiently handles inaccuracies induced by temporal distribution shifts.
Among listwise rerankers, our findings show that ListT5, which effectively
mitigates the positional bias problem by adopting the Fusion-in-Decoder
architecture, is especially effective, and more so, as temporal drift
increases, on the test-long subset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CLEF 2024 LongEval track</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Self-Supervised</span> Audio-Visual Soundscape Stylization <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14340v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14340v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tingle Li, Renhao Wang, Po-Yao Huang, Andrew Owens, Gopala Anumanchipalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech sounds convey a great deal of information about the scenes, resulting
in a variety of effects ranging from reverberation to additional ambient
sounds. In this paper, we manipulate input speech to sound as though it was
recorded within a different scene, given an audio-visual conditional example
recorded from that scene. Our model learns through self-supervision, taking
advantage of the fact that natural video contains recurring sound events and
textures. We extract an audio clip from a video and apply speech enhancement.
We then train a latent diffusion model to recover the original speech, using
another audio-visual clip taken from elsewhere in the video as a conditional
hint. Through this process, the model learns to transfer the conditional
example's sound properties to the input speech. We show that our model can be
successfully trained using unlabeled, in-the-wild videos, and that an
additional visual signal can improve its sound prediction abilities. Please see
our project webpage for video results:
https://tinglok.netlify.app/files/avsoundscape/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scene-Text Grounding for Text-Based Video Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14319v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14319v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheng Zhou, Junbin Xiao, Xun Yang, Peipei Song, Dan Guo, Angela Yao, Meng Wang, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing efforts in text-based video question answering (TextVideoQA) are
criticized for their opaque decisionmaking and heavy reliance on scene-text
recognition. In this paper, we propose to study Grounded TextVideoQA by forcing
models to answer questions and spatio-temporally localize the relevant
scene-text regions, thus decoupling QA from scenetext recognition and promoting
research towards interpretable QA. The task has three-fold significance. First,
it encourages scene-text evidence versus other short-cuts for answer
predictions. Second, it directly accepts scene-text regions as visual answers,
thus circumventing the problem of ineffective answer evaluation by stringent
string matching. Third, it isolates the challenges inherited in VideoQA and
scene-text recognition. This enables the diagnosis of the root causes for
failure predictions, e.g., wrong QA or wrong scene-text recognition? To achieve
Grounded TextVideoQA, we propose the T2S-QA model that highlights a
disentangled temporal-to-spatial contrastive learning strategy for
weakly-supervised scene-text grounding and grounded TextVideoQA. To facilitate
evaluation, we construct a new dataset ViTXT-GQA which features 52K scene-text
bounding boxes within 2.2K temporal segments related to 2K questions and 729
videos. With ViTXT-GQA, we perform extensive experiments and demonstrate the
severe limitations of existing techniques in Grounded TextVideoQA. While T2S-QA
achieves superior results, the large performance gap with human leaves ample
space for improvement. Our further analysis of oracle scene-text inputs posits
that the major challenge is scene-text recognition. To advance the research of
Grounded TextVideoQA, our dataset and code are at
\url{https://github.com/zhousheng97/ViTXT-GQA.git}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-similarity Prior Distillation for Unsupervised Remote Physiological
  Measurement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.05100v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.05100v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Zhang, Weiyu Sun, Hao Lu, Ying Chen, Yun Ge, Xiaolin Huang, Jie Yuan, Yingcong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Remote photoplethysmography (rPPG) is a noninvasive technique that aims to
capture subtle variations in facial pixels caused by changes in blood volume
resulting from cardiac activities. Most existing unsupervised methods for rPPG
tasks focus on the contrastive learning between samples while neglecting the
inherent self-similar prior in physiological signals. In this paper, we propose
a Self-Similarity Prior Distillation (SSPD) framework for unsupervised rPPG
estimation, which capitalizes on the intrinsic self-similarity of cardiac
activities. Specifically, we first introduce a physical-prior embedded
augmentation technique to mitigate the effect of various types of noise. Then,
we tailor a self-similarity-aware network to extract more reliable self-similar
physiological features. Finally, we develop a hierarchical self-distillation
paradigm to assist the network in disentangling self-similar physiological
patterns from facial videos. Comprehensive experiments demonstrate that the
unsupervised SSPD framework achieves comparable or even superior performance
compared to the state-of-the-art supervised methods. Meanwhile, SSPD maintains
the lowest inference time and computation cost among end-to-end models.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-09-21T00:00:00Z">2024-09-21</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting BPR: A Replicability Study of a Common Recommender System
  Baseline <span class="chip">RecSys
  '24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14217v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14217v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksandr Milogradskii, Oleg Lashinin, Alexander P, Marina Ananyeva, Sergey Kolesnikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian Personalized Ranking (BPR), a collaborative filtering approach based
on matrix factorization, frequently serves as a benchmark for recommender
systems research. However, numerous studies often overlook the nuances of BPR
implementation, claiming that it performs worse than newly proposed methods
across various tasks. In this paper, we thoroughly examine the features of the
BPR model, indicating their impact on its performance, and investigate
open-source BPR implementations. Our analysis reveals inconsistencies between
these implementations and the original BPR paper, leading to a significant
decrease in performance of up to 50% for specific implementations. Furthermore,
through extensive experiments on real-world datasets under modern evaluation
settings, we demonstrate that with proper tuning of its hyperparameters, the
BPR model can achieve performance levels close to state-of-the-art methods on
the top-n recommendation tasks and even outperform them on specific datasets.
Specifically, on the Million Song Dataset, the BPR model with hyperparameters
tuning statistically significantly outperforms Mult-VAE by 10% in NDCG@100 with
binary relevance function.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted at the Reproducibility track of the ACM RecSys
  '24 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge in Triples for LLMs: Enhancing Table QA Accuracy with Semantic
  Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14192v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14192v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hossein Sholehrasa, Sanaz Saki Norouzi, Pascal Hitzler, Majid Jaberi-Douraki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrating structured knowledge from tabular formats poses significant
challenges within natural language processing (NLP), mainly when dealing with
complex, semi-structured tables like those found in the FeTaQA dataset. These
tables require advanced methods to interpret and generate meaningful responses
accurately. Traditional approaches, such as SQL and SPARQL, often fail to fully
capture the semantics of such data, especially in the presence of irregular
table structures like web tables. This paper addresses these challenges by
proposing a novel approach that extracts triples straightforward from tabular
data and integrates it with a retrieval-augmented generation (RAG) model to
enhance the accuracy, coherence, and contextual richness of responses generated
by a fine-tuned GPT-3.5-turbo-0125 model. Our approach significantly
outperforms existing baselines on the FeTaQA dataset, particularly excelling in
Sacre-BLEU and ROUGE metrics. It effectively generates contextually accurate
and detailed long-form answers from tables, showcasing its strength in complex
data interpretation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data Generation via Latent Factor Simulation for Fairness-aware
  Re-ranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14078v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14078v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elena Stefancova, Cassidy All, Joshua Paup, Martin Homola, Nicholas Mattei, Robin Burke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthetic data is a useful resource for algorithmic research. It allows for
the evaluation of systems under a range of conditions that might be difficult
to achieve in real world settings. In recommender systems, the use of synthetic
data is somewhat limited; some work has concentrated on building user-item
interaction data at large scale. We believe that fairness-aware recommendation
research can benefit from simulated data as it allows the study of protected
groups and their interactions without depending on sensitive data that needs
privacy protection. In this paper, we propose a novel type of data for
fairness-aware recommendation: synthetic recommender system outputs that can be
used to study re-ranking algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OAEI-LLM: A Benchmark <span class="highlight-title">Dataset</span> for Understanding Large Language Model
  Hallucinations in Ontology Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14038v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14038v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhangcheng Qiang, Kerry Taylor, Weiqing Wang, Jing Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hallucinations of large language models (LLMs) commonly occur in
domain-specific downstream tasks, with no exception in ontology matching (OM).
The prevalence of using LLMs for OM raises the need for benchmarks to better
understand LLM hallucinations. The OAEI-LLM dataset is an extended version of
the Ontology Alignment Evaluation Initiative (OAEI) datasets that evaluate
LLM-specific hallucinations in OM tasks. We outline the methodology used in
dataset construction and schema extension, and provide examples of potential
use cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cost-Effective Community-Hierarchy-Based Mutual Voting Approach for
  Influence Maximization in Complex Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14034v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14034v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Liu, Xiaoan Tang, Witold Pedrycz, Qiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Various types of promising techniques have come into being for influence
maximization whose aim is to identify influential nodes in complex networks. In
essence, real-world applications usually have high requirements on the balance
between time complexity and accuracy of influential nodes identification. To
address the challenges of imperfect node influence measurement and inefficient
seed nodes selection mechanism in such class of foregoing techniques, this
article proposes a novel approach called Cost-Effective
Community-Hierarchy-Based Mutual Voting for influence maximization in complex
networks. First, we develop a method for measuring the importance of different
nodes in networks based on an original concept of Dual-Scale
Community-Hierarchy Information that synthesizes both hierarchy structural
information and community structural information of nodes. The community
structural information contained in the nodes is measured by a new notion of
Hierarchical-Community Entropy. Second, we develop a method named
Cost-Effective Mutual-Influence-based Voting for seed nodes selection.
Hereinto, a low-computational-cost mutual voting mechanism and an updating
strategy called Lazy Score Updating Strategy are newly constructed for
optimizing the selecting of seed nodes. Third, we develop a balance index to
evaluate the performance of different methods in striking the tradeoff between
time complexity and the accuracy of influential nodes identification. Finally,
we demonstrate the approach performance over ten public datasets. The extensive
experiments show that the proposed approach outperforms 16 state-of-the-art
techniques on the balance between time complexity and accuracy of influential
nodes identification. Compared with the method with the second highest value of
the balance index, our approach can be improved by at most 9.29%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DRAGIN: Dynamic Retrieval Augmented Generation based on the Information
  Needs of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10081v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10081v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weihang Su, Yichen Tang, Qingyao Ai, Zhijing Wu, Yiqun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic retrieval augmented generation (RAG) paradigm actively decides when
and what to retrieve during the text generation process of Large Language
Models (LLMs). There are two key elements of this paradigm: identifying the
optimal moment to activate the retrieval module (deciding when to retrieve) and
crafting the appropriate query once retrieval is triggered (determining what to
retrieve). However, current dynamic RAG methods fall short in both aspects.
Firstly, the strategies for deciding when to retrieve often rely on static
rules. Moreover, the strategies for deciding what to retrieve typically limit
themselves to the LLM's most recent sentence or the last few tokens, while the
LLM's real-time information needs may span across the entire context. To
overcome these limitations, we introduce a new framework, DRAGIN, i.e., Dynamic
Retrieval Augmented Generation based on the real-time Information Needs of
LLMs. Our framework is specifically designed to make decisions on when and what
to retrieve based on the LLM's real-time information needs during the text
generation process. We evaluate DRAGIN along with existing methods
comprehensively over 4 knowledge-intensive generation datasets. Experimental
results show that DRAGIN achieves superior performance on all tasks,
demonstrating the effectiveness of our method. We have open-sourced all the
code, data, and models in GitHub: https://github.com/oneal2000/DRAGIN/tree/main
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BRep Boundary and Junction Detection for CAD Reverse Engineering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14087v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14087v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sk Aziz Ali, Mohammad Sadil Khan, Didier Stricker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In machining process, 3D reverse engineering of the mechanical system is an
integral, highly important, and yet time consuming step to obtain parametric
CAD models from 3D scans. Therefore, deep learning-based Scan-to-CAD modeling
can offer designers enormous editability to quickly modify CAD model, being
able to parse all its structural compositions and design steps. In this paper,
we propose a supervised boundary representation (BRep) detection network
BRepDetNet from 3D scans of CC3D and ABC dataset. We have carefully annotated
the 50K and 45K scans of both the datasets with appropriate topological
relations (e.g., next, mate, previous) between the geometrical primitives
(i.e., boundaries, junctions, loops, faces) of their BRep data structures. The
proposed solution decomposes the Scan-to-CAD problem in Scan-to-BRep ensuring
the right step towards feature-based modeling, and therefore, leveraging other
existing BRep-to-CAD modeling methods. Our proposed Scan-to-BRep neural network
learns to detect BRep boundaries and junctions by minimizing focal-loss and
non-maximal suppression (NMS) during training time. Experimental results show
that our BRepDetNet with NMS-Loss achieves impressive results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CUS3D :CLIP-based Unsupervised 3D Segmentation via Object-level Denoise 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13982v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13982v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuyang Yu, Runze Tian, Zhen Wang, Xiaochuan Wang, Xiaohui Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To ease the difficulty of acquiring annotation labels in 3D data, a common
method is using unsupervised and open-vocabulary semantic segmentation, which
leverage 2D CLIP semantic knowledge. In this paper, unlike previous research
that ignores the ``noise'' raised during feature projection from 2D to 3D, we
propose a novel distillation learning framework named CUS3D. In our approach,
an object-level denosing projection module is designed to screen out the
``noise'' and ensure more accurate 3D feature. Based on the obtained features,
a multimodal distillation learning module is designed to align the 3D feature
with CLIP semantic feature space with object-centered constrains to achieve
advanced unsupervised semantic segmentation. We conduct comprehensive
experiments in both unsupervised and open-vocabulary segmentation, and the
results consistently showcase the superiority of our model in achieving
advanced unsupervised segmentation results and its effectiveness in
open-vocabulary segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages,3 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-09-20T00:00:00Z">2024-09-20</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">17</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causal Feature Selection Method for Contextual Multi-Armed Bandits in
  Recommender System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13888v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13888v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyu Zhao, Yexi Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Features (a.k.a. context) are critical for contextual multi-armed bandits
(MAB) performance. In practice of large scale online system, it is important to
select and implement important features for the model: missing important
features can led to sub-optimal reward outcome, and including irrelevant
features can cause overfitting, poor model interpretability, and implementation
cost. However, feature selection methods for conventional machine learning
models fail short for contextual MAB use cases, as conventional methods select
features correlated with the outcome variable, but not necessarily causing
heterogeneuous treatment effect among arms which are truely important for
contextual MAB. In this paper, we introduce model-free feature selection
methods designed for contexutal MAB problem, based on heterogeneous causal
effect contributed by the feature to the reward distribution. Empirical
evaluation is conducted based on synthetic data as well as real data from an
online experiment for optimizing content cover image in a recommender system.
The results show this feature selection method effectively selects the
important features that lead to higher contextual MAB reward than unimportant
features. Compared with model embedded method, this model-free method has
advantage of fast computation speed, ease of implementation, and prune of model
mis-specification issues.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Segment Discovery: Enhancing E-commerce Targeting <span class="chip">RecSys'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13847v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13847v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiqi Li, Roopali Singh, Charin Polpanumas, Tanner Fiez, Namita Kumar, Shreya Chakrabarti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern e-commerce services frequently target customers with incentives or
interventions to engage them in their products such as games, shopping, video
streaming, etc. This customer engagement increases acquisition of more
customers and retention of existing ones, leading to more business for the
company while improving customer experience. Often, customers are either
randomly targeted or targeted based on the propensity of desirable behavior.
However, such policies can be suboptimal as they do not target the set of
customers who would benefit the most from the intervention and they may also
not take account of any constraints. In this paper, we propose a policy
framework based on uplift modeling and constrained optimization that identifies
customers to target for a use-case specific intervention so as to maximize the
value to the business, while taking account of any given constraints. We
demonstrate improvement over state-of-the-art targeting approaches using two
large-scale experimental studies and a production implementation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the CONSEQUENCES'24 workshop, co-located with ACM
  RecSys'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beauty Beyond Words: Explainable Beauty Product Recommendations Using
  Ingredient-Based Product Attributes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13628v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13628v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siliang Liu, Rahul Suresh, Amin Banitalebi-Dehkordi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate attribute extraction is critical for beauty product recommendations
and building trust with customers. This remains an open problem, as existing
solutions are often unreliable and incomplete. We present a system to extract
beauty-specific attributes using end-to-end supervised learning based on beauty
product ingredients. A key insight to our system is a novel energy-based
implicit model architecture. We show that this implicit model architecture
offers significant benefits in terms of accuracy, explainability, robustness,
and flexibility. Furthermore, our implicit model can be easily fine-tuned to
incorporate additional attributes as they become available, making it more
useful in real-world applications. We validate our model on a major e-commerce
skincare product catalog dataset and demonstrate its effectiveness. Finally, we
showcase how ingredient-based attribute extraction contributes to enhancing the
explainability of beauty recommendations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18th ACM Conference on Recommender Systems, Workshop on Strategic and
  Utility-aware REcommendation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing Event Causality Identification via Heuristic Semantic
  Dependency Inquiry Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13621v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13621v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Li, Qiang Gao, Hongmei Wu, Li Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event Causality Identification (ECI) focuses on extracting causal relations
between events in texts. Existing methods for ECI primarily rely on causal
features and external knowledge. However, these approaches fall short in two
dimensions: (1) causal features between events in a text often lack explicit
clues, and (2) external knowledge may introduce bias, while specific problems
require tailored analyses. To address these issues, we propose SemDI - a simple
and effective Semantic Dependency Inquiry Network for ECI. SemDI captures
semantic dependencies within the context using a unified encoder. Then, it
utilizes a Cloze Analyzer to generate a fill-in token based on comprehensive
context understanding. Finally, this fill-in token is used to inquire about the
causal relation between two events. Extensive experiments demonstrate the
effectiveness of SemDI, surpassing state-of-the-art methods on three widely
used benchmarks. Code is available at https://github.com/hrlics/SemDI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data Augmentation for Sequential Recommendation: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhou Dang, Enneng Yang, Yuting Liu, Guibing Guo, Linying Jiang, Jianzhe Zhao, Xingwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As an essential branch of recommender systems, sequential recommendation (SR)
has received much attention due to its well-consistency with real-world
situations. However, the widespread data sparsity issue limits the SR model's
performance. Therefore, researchers have proposed many data augmentation (DA)
methods to mitigate this phenomenon and have achieved impressive progress. In
this survey, we provide a comprehensive review of DA methods for SR. We start
by introducing the research background and motivation. Then, we categorize
existing methodologies regarding their augmentation principles, objects, and
purposes. Next, we present a comparative discussion of their advantages and
disadvantages, followed by the exhibition and analysis of representative
experimental results. Finally, we outline directions for future research and
summarize this survey. We also maintain a repository with a paper list at
\url{https://github.com/KingGugu/DA-CL-4Rec}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multimodal Dense Retrieval Approach for Speech-Based Open-Domain
  Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13483v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13483v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgios Sidiropoulos, Evangelos Kanoulas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech-based open-domain question answering (QA over a large corpus of text
passages with spoken questions) has emerged as an important task due to the
increasing number of users interacting with QA systems via speech interfaces.
Passage retrieval is a key task in speech-based open-domain QA. So far,
previous works adopted pipelines consisting of an automatic speech recognition
(ASR) model that transcribes the spoken question before feeding it to a dense
text retriever. Such pipelines have several limitations. The need for an ASR
model limits the applicability to low-resource languages and specialized
domains with no annotated speech data. Furthermore, the ASR model propagates
its errors to the retriever. In this work, we try to alleviate these
limitations by proposing an ASR-free, end-to-end trained multimodal dense
retriever that can work directly on spoken questions. Our experimental results
showed that, on shorter questions, our retriever is a promising alternative to
the \textit{ASR and Retriever} pipeline, achieving better retrieval performance
in cases where ASR would have mistranscribed important words in the question or
have produced a transcription with a high word error rate.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Procedure Model for Building Knowledge Graphs for Industry Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sascha Meckler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Enterprise knowledge graphs combine business data and organizational
knowledge by means of a semantic network of concepts, properties, individuals
and relationships. The graph-based integration of previously unconnected
information with domain knowledge provides new insights and enables intelligent
business applications. However, knowledge graph construction is a large
investment which requires a joint effort of domain and technical experts. This
paper presents a practical step-by-step procedure model for building an RDF
knowledge graph that interconnects heterogeneous data and expert knowledge for
an industry use case. The self-contained process adapts the "Cross Industry
Standard Process for Data Mining" and uses competency questions throughout the
entire development cycle. The procedure model starts with business and data
understanding, describes tasks for ontology modeling and the graph setup, and
ends with process steps for evaluation and deployment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contextual Compression in Retrieval-Augmented Generation for Large
  Language Models: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13385v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13385v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sourav Verma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) showcase remarkable abilities, yet they struggle
with limitations such as hallucinations, outdated knowledge, opacity, and
inexplicable reasoning. To address these challenges, Retrieval-Augmented
Generation (RAG) has proven to be a viable solution, leveraging external
databases to improve the consistency and coherence of generated content,
especially valuable for complex, knowledge-rich tasks, and facilitates
continuous improvement by leveraging domain-specific insights. By combining the
intrinsic knowledge of LLMs with the vast, dynamic repositories of external
databases, RAG achieves a synergistic effect. However, RAG is not without its
limitations, including a limited context window, irrelevant information, and
the high processing overhead for extensive contextual data. In this
comprehensive work, we explore the evolution of Contextual Compression
paradigms, providing an in-depth examination of the field. Finally, we outline
the current challenges and suggest potential research and development
directions, paving the way for future advancements in this area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ongoing Work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ More Clustering Quality Metrics for ABCDE 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13376v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13376v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stephan van Staden
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ABCDE is a technique for evaluating clusterings of very large populations of
items. Given two clusterings, namely a Baseline clustering and an Experiment
clustering, ABCDE can characterize their differences with impact and quality
metrics, and thus help to determine which clustering to prefer. We previously
described the basic quality metrics of ABCDE, namely the GoodSplitRate,
BadSplitRate, GoodMergeRate, BadMergeRate and DeltaPrecision, and how to
estimate them on the basis of human judgements. This paper extends that
treatment with more quality metrics. It describes a technique that aims to
characterize the DeltaRecall of the clustering change. It introduces a new
metric, called IQ, to characterize the degree to which the clustering diff
translates into an improvement in the quality. Ideally, a large diff would
improve the quality by a large amount. Finally, this paper mentions ways to
characterize the absolute Precision and Recall of a single clustering with
ABCDE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Unified Causal Framework for Auditing Recommender Systems for Ethical
  Concerns 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13210v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13210v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vibhhu Sharma, Shantanu Gupta, Nil-Jana Akpinar, Zachary C. Lipton, Liu Leqi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As recommender systems become widely deployed in different domains, they
increasingly influence their users' beliefs and preferences. Auditing
recommender systems is crucial as it not only ensures the continuous
improvement of recommendation algorithms but also safeguards against potential
issues like biases and ethical concerns. In this paper, we view recommender
system auditing from a causal lens and provide a general recipe for defining
auditing metrics. Under this general causal auditing framework, we categorize
existing auditing metrics and identify gaps in them -- notably, the lack of
metrics for auditing user agency while accounting for the multi-step dynamics
of the recommendation process. We leverage our framework and propose two
classes of such metrics:future- and past-reacheability and stability, that
measure the ability of a user to influence their own and other users'
recommendations, respectively. We provide both a gradient-based and a black-box
approach for computing these metrics, allowing the auditor to compute them
under different levels of access to the recommender system. In our experiments,
we demonstrate the efficacy of methods for computing the proposed metrics and
inspect the design of recommender systems through these proposed metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RPAF: A Reinforcement Prediction-Allocation Framework for Cache
  Allocation in Large-Scale Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13175v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13175v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuo Su, Xiaoshuang Chen, Yao Wang, Yulin Wu, Ziqiang Zhang, Kaiqiao Zhan, Ben Wang, Kun Gai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern recommender systems are built upon computation-intensive
infrastructure, and it is challenging to perform real-time computation for each
request, especially in peak periods, due to the limited computational
resources. Recommending by user-wise result caches is widely used when the
system cannot afford a real-time recommendation. However, it is challenging to
allocate real-time and cached recommendations to maximize the users' overall
engagement. This paper shows two key challenges to cache allocation, i.e., the
value-strategy dependency and the streaming allocation. Then, we propose a
reinforcement prediction-allocation framework (RPAF) to address these issues.
RPAF is a reinforcement-learning-based two-stage framework containing
prediction and allocation stages. The prediction stage estimates the values of
the cache choices considering the value-strategy dependency, and the allocation
stage determines the cache choices for each individual request while satisfying
the global budget constraint. We show that the challenge of training RPAF
includes globality and the strictness of budget constraints, and a relaxed
local allocator (RLA) is proposed to address this issue. Moreover, a PoolRank
algorithm is used in the allocation stage to deal with the streaming allocation
problem. Experiments show that RPAF significantly improves users' engagement
under computational budget constraints.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large language models in biomedical natural language processing:
  benchmarks, baselines, and recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.16326v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.16326v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyu Chen, Jingcheng Du, Yan Hu, Vipina Kuttichi Keloth, Xueqing Peng, Kalpana Raja, Rui Zhang, Zhiyong Lu, Hua Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Biomedical literature is growing rapidly, making it challenging to curate and
extract knowledge manually. Biomedical natural language processing (BioNLP)
techniques that can automatically extract information from biomedical
literature help alleviate this burden. Recently, large Language Models (LLMs),
such as GPT-3 and GPT-4, have gained significant attention for their impressive
performance. However, their effectiveness in BioNLP tasks and impact on method
development and downstream users remain understudied. This pilot study (1)
establishes the baseline performance of GPT-3 and GPT-4 at both zero-shot and
one-shot settings in eight BioNLP datasets across four applications: named
entity recognition, relation extraction, multi-label document classification,
and semantic similarity and reasoning, (2) examines the errors produced by the
LLMs and categorized the errors into three types: missingness, inconsistencies,
and unwanted artificial content, and (3) provides suggestions for using LLMs in
BioNLP applications. We make the datasets, baselines, and results publicly
available to the community via
https://github.com/qingyu-qc/gpt_bionlp_benchmark.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Train Once, Use Flexibly: A Modular Framework for Multi-Aspect Neural
  News Recommendation <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.16089v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.16089v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreea Iana, Goran Glavaš, Heiko Paulheim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent neural news recommenders (NNRs) extend content-based recommendation
(1) by aligning additional aspects (e.g., topic, sentiment) between candidate
news and user history or (2) by diversifying recommendations w.r.t. these
aspects. This customization is achieved by ``hardcoding`` additional
constraints into the NNR's architecture and/or training objectives: any change
in the desired recommendation behavior thus requires retraining the model with
a modified objective. This impedes widespread adoption of multi-aspect news
recommenders. In this work, we introduce MANNeR, a modular framework for
multi-aspect neural news recommendation that supports on-the-fly customization
over individual aspects at inference time. With metric-based learning as its
backbone, MANNeR learns aspect-specialized news encoders and then flexibly and
linearly combines the resulting aspect-specific similarity scores into
different ranking functions, alleviating the need for ranking function-specific
retraining of the model. Extensive experimental results show that MANNeR
consistently outperforms state-of-the-art NNRs on both standard content-based
recommendation and single- and multi-aspect customization. Lastly, we validate
that MANNeR's aspect-customization module is robust to language and domain
transfer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 2024 Conference on Empirical Methods in Natural
  Language Processing (EMNLP 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TISIS : Trajectory Indexing for SImilarity Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11301v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11301v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sara Jarrad, Hubert Naacke, Stephane Gancarski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social media platforms enable users to share diverse types of information,
including geolocation data that captures their movement patterns. Such
geolocation data can be leveraged to reconstruct the trajectory of a user's
visited Points of Interest (POIs). A key requirement in numerous applications
is the ability to measure the similarity between such trajectories, as this
facilitates the retrieval of trajectories that are similar to a given reference
trajectory. This is the main focus of our work. Existing methods predominantly
rely on applying a similarity function to each candidate trajectory to identify
those that are sufficiently similar. However, this approach becomes
computationally expensive when dealing with large-scale datasets. To mitigate
this challenge, we propose TISIS, an efficient method that uses trajectory
indexing to quickly find similar trajectories that share common POIs in the
same order. Furthermore, to account for scenarios where POIs in trajectories
may not exactly match but are contextually similar, we introduce TISIS*, a
variant of TISIS that incorporates POI embeddings. This extension allows for
more comprehensive retrieval of similar trajectories by considering semantic
similarities between POIs, beyond mere exact matches. Extensive experimental
evaluations demonstrate that the proposed approach significantly outperforms a
baseline method based on the well-known Longest Common SubSequence (LCSS)
algorithm, yielding substantial performance improvements across various
real-world datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visualising Personal Data Flows: Insights from a Case Study of
  Booking.com 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.09603v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.09603v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiyue Yuan, Matthew Boakes, Xiao Ma, Dongmei Cao, Shujun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Commercial organisations are holding and processing an ever-increasing amount
of personal data. Policies and laws are continually changing to require these
companies to be more transparent regarding the collection, storage, processing
and sharing of this data. This paper reports our work of taking Booking.com as
a case study to visualise personal data flows extracted from their privacy
policy. By showcasing how the company shares its consumers' personal data, we
raise questions and extend discussions on the challenges and limitations of
using privacy policies to inform online users about the true scale and the
landscape of personal data flows. This case study can inform us about future
research on more data flow-oriented privacy policy analysis and on the
construction of a more comprehensive ontology on personal data flows in
complicated business ecosystems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is the full edition of a paper published in Intelligent
  Information Systems: CAiSE Forum 2023, Zaragoza, Spain, June 12-16, 2023,
  Proceedings, Lecture Notes in Business Information Processing (LNBIP), Volume
  477, pp. 52-60, 2023, Springer Nature,
  https://link.springer.com/book/10.1007/978-3-031-34674-3_7</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Relevance of Item-Co-Exposure For Exposure Bias Mitigation <span class="chip">RecSys
  '24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12912v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12912v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thorsten Krause, Alina Deriyeva, Jan Heinrich Beinke, Gerrit York Bartels, Oliver Thomas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Through exposing items to users, implicit feedback recommender systems
influence the logged interactions, and, ultimately, their own recommendations.
This effect is called exposure bias and it can lead to issues such as filter
bubbles and echo chambers. Previous research employed the multinomial logit
model (MNL) with exposure information to reduce exposure bias on synthetic
data.
  This extended abstract summarizes our previous study in which we investigated
whether (i) these findings hold for human-generated choices, (ii) other
discrete choice models mitigate bias better, and (iii) an item's estimated
relevance can depend on the relevances of the other items that were presented
with it. We collected a data set of biased and unbiased choices in a controlled
online user study and measured the effects of overexposure and competition.
  We found that (i) the discrete choice models effectively mitigated exposure
bias on human-generated choice data, (ii) there were no significant differences
in robustness among the different discrete choice models, and (iii) only
multivariate discrete choice models were robust to competition between items.
We conclude that discrete choice models mitigate exposure bias effectively
because they consider item-co-exposure. Moreover, exposing items alongside more
or less popular items can bias future recommendations significantly and item
exposure must be tracked for overcoming exposure bias. We consider our work
vital for understanding what exposure bias it, how it forms, and how it can be
mitigated.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the CONSEQUENCES '24 workshop, co-located with ACM RecSys
  '24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Information Retrieval Landscapes: An Investigation of a Novel
  Evaluation Techniques and Comparative Document Splitting Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08479v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08479v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Esmaeil Narimissa, David Raithel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The performance of Retrieval-Augmented Generation (RAG) systems in
information retrieval is significantly influenced by the characteristics of the
documents being processed. In this study, the structured nature of textbooks,
the conciseness of articles, and the narrative complexity of novels are shown
to require distinct retrieval strategies. A comparative evaluation of multiple
document-splitting methods reveals that the Recursive Character Splitter
outperforms the Token-based Splitter in preserving contextual integrity. A
novel evaluation technique is introduced, utilizing an open-source model to
generate a comprehensive dataset of question-and-answer pairs, simulating
realistic retrieval scenarios to enhance testing efficiency and metric
reliability. The evaluation employs weighted scoring metrics, including
SequenceMatcher, BLEU, METEOR, and BERT Score, to assess the system's accuracy
and relevance. This approach establishes a refined standard for evaluating the
precision of RAG systems, with future research focusing on optimizing chunk and
overlap sizes to improve retrieval accuracy and efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This article is 16 pages long and includes detailed comparisons of
  RAG systems and document splitting techniques</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Temporally Aligned Audio for Video with Autoregression <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13689v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13689v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilpo Viertola, Vladimir Iashin, Esa Rahtu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce V-AURA, the first autoregressive model to achieve high temporal
alignment and relevance in video-to-audio generation. V-AURA uses a
high-framerate visual feature extractor and a cross-modal audio-visual feature
fusion strategy to capture fine-grained visual motion events and ensure precise
temporal alignment. Additionally, we propose VisualSound, a benchmark dataset
with high audio-visual relevance. VisualSound is based on VGGSound, a video
dataset consisting of in-the-wild samples extracted from YouTube. During the
curation, we remove samples where auditory events are not aligned with the
visual ones. V-AURA outperforms current state-of-the-art models in temporal
alignment and semantic relevance while maintaining comparable audio quality.
Code, samples, VisualSound and models are available at
https://v-aura.notion.site
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICASSP 2025. Project page https://v-aura.notion.site</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ChemDFM-X: Towards Large Multimodal Model for Chemistry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13194v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13194v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Zhao, Bo Chen, Jingpiao Li, Lu Chen, Liyang Wen, Pengyu Wang, Zichen Zhu, Danyang Zhang, Ziping Wan, Yansi Li, Zhongyang Dai, Xin Chen, Kai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rapid developments of AI tools are expected to offer unprecedented assistance
to the research of natural science including chemistry. However, neither
existing unimodal task-specific specialist models nor emerging general large
multimodal models (LMM) can cover the wide range of chemical data modality and
task categories. To address the real demands of chemists, a cross-modal
Chemical General Intelligence (CGI) system, which serves as a truly practical
and useful research assistant utilizing the great potential of LMMs, is in
great need. In this work, we introduce the first Cross-modal Dialogue
Foundation Model for Chemistry (ChemDFM-X). Diverse multimodal data are
generated from an initial modality by approximate calculations and
task-specific model predictions. This strategy creates sufficient chemical
training corpora, while significantly reducing excessive expense, resulting in
an instruction-tuning dataset containing 7.6M data. After instruction
finetuning, ChemDFM-X is evaluated on extensive experiments of different
chemical tasks with various data modalities. The results demonstrate the
capacity of ChemDFM-X for multimodal and inter-modal knowledge comprehension.
ChemDFM-X marks a significant milestone toward aligning all modalities in
chemistry, a step closer to CGI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 7 figures, 11 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ High Perceptual Quality Wireless Image Delivery with Denoising Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.15889v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.15889v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Selim F. Yilmaz, Xueyan Niu, Bo Bai, Wei Han, Lei Deng, Deniz Gunduz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the image transmission problem over a noisy wireless channel via
deep learning-based joint source-channel coding (DeepJSCC) along with a
denoising diffusion probabilistic model (DDPM) at the receiver. Specifically,
we are interested in the perception-distortion trade-off in the practical
finite block length regime, in which separate source and channel coding can be
highly suboptimal. We introduce a novel scheme, where the conventional DeepJSCC
encoder targets transmitting a lower resolution version of the image, which
later can be refined thanks to the generative model available at the receiver.
In particular, we utilize the range-null space decomposition of the target
image; DeepJSCC transmits the range-space of the image, while DDPM
progressively refines its null space contents. Through extensive experiments,
we demonstrate significant improvements in distortion and perceptual quality of
reconstructed images compared to standard DeepJSCC and the state-of-the-art
generative learning-based method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures. Published at INFOCOM 2024 Workshops</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Efficient SDRTV-to-HDRTV by Learning from Image Formation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.04084v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.04084v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangyu Chen, Zheyuan Li, Zhengwen Zhang, Jimmy S. Ren, Yihao Liu, Jingwen He, Yu Qiao, Jiantao Zhou, Chao Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern displays can render video content with high dynamic range (HDR) and
wide color gamut (WCG). However, most resources are still in standard dynamic
range (SDR). Therefore, transforming existing SDR content into the HDRTV
standard holds significant value. This paper defines and analyzes the
SDRTV-to-HDRTV task by modeling the formation of SDRTV/HDRTV content. Our
findings reveal that a naive endto-end supervised training approach suffers
from severe gamut transition errors. To address this, we propose a new
three-step solution called HDRTVNet++, which includes adaptive global color
mapping, local enhancement, and highlight refinement. The adaptive global color
mapping step utilizes global statistics for image-adaptive color adjustments. A
local enhancement network further enhances details, and the two sub-networks
are combined as a generator to achieve highlight consistency through GANbased
joint training. Designed for ultra-high-definition TV content, our method is
both effective and lightweight for processing 4K resolution images. We also
constructed a dataset using HDR videos in the HDR10 standard, named HDRTV1K,
containing 1235 training and 117 testing images, all in 4K resolution.
Additionally, we employ five metrics to evaluate SDRTV-to-HDRTV performance.
Our results demonstrate state-of-the-art performance both quantitatively and
visually. The codes and models are available at
https://github.com/xiaom233/HDRTVNet-plus.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of HDRTVNet</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-09-19T00:00:00Z">2024-09-19</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">14</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Performance and Scalability of Large-Scale Recommendation
  Systems with Jagged Flash Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15373v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15373v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rengan Xu, Junjie Yang, Yifan Xu, Hong Li, Xing Liu, Devashish Shankar, Haoci Zhang, Meng Liu, Boyang Li, Yuxi Hu, Mingwei Tang, Zehua Zhang, Tunhou Zhang, Dai Li, Sijia Chen, Gian-Paolo Musumeci, Jiaqi Zhai, Bill Zhu, Hong Yan, Srihari Reddy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of hardware accelerators has significantly advanced the
capabilities of modern recommendation systems, enabling the exploration of
complex ranking paradigms previously deemed impractical. However, the GPU-based
computational costs present substantial challenges. In this paper, we
demonstrate our development of an efficiency-driven approach to explore these
paradigms, moving beyond traditional reliance on native PyTorch modules. We
address the specific challenges posed by ranking models' dependence on
categorical features, which vary in length and complicate GPU utilization. We
introduce Jagged Feature Interaction Kernels, a novel method designed to
extract fine-grained insights from long categorical features through efficient
handling of dynamically sized tensors. We further enhance the performance of
attention mechanisms by integrating Jagged tensors with Flash Attention. Our
novel Jagged Flash Attention achieves up to 9x speedup and 22x memory reduction
compared to dense attention. Notably, it also outperforms dense flash
attention, with up to 3x speedup and 53% more memory efficiency. In production
models, we observe 10% QPS improvement and 18% memory savings, enabling us to
scale our recommendation systems with longer features and more complex
architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MMSearch: Benchmarking the Potential of Large Models as Multi-modal
  Search Engines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12959v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12959v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanmin Wu, Jiayi Lei, Pengshuo Qiu, Pan Lu, Zehui Chen, Guanglu Song, Peng Gao, Yu Liu, Chunyuan Li, Hongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of Large Language Models (LLMs) has paved the way for AI search
engines, e.g., SearchGPT, showcasing a new paradigm in human-internet
interaction. However, most current AI search engines are limited to text-only
settings, neglecting the multimodal user queries and the text-image interleaved
nature of website information. Recently, Large Multimodal Models (LMMs) have
made impressive strides. Yet, whether they can function as AI search engines
remains under-explored, leaving the potential of LMMs in multimodal search an
open question. To this end, we first design a delicate pipeline,
MMSearch-Engine, to empower any LMMs with multimodal search capabilities. On
top of this, we introduce MMSearch, a comprehensive evaluation benchmark to
assess the multimodal search performance of LMMs. The curated dataset contains
300 manually collected instances spanning 14 subfields, which involves no
overlap with the current LMMs' training data, ensuring the correct answer can
only be obtained within searching. By using MMSearch-Engine, the LMMs are
evaluated by performing three individual tasks (requery, rerank, and
summarization), and one challenging end-to-end task with a complete searching
process. We conduct extensive experiments on closed-source and open-source
LMMs. Among all tested models, GPT-4o with MMSearch-Engine achieves the best
results, which surpasses the commercial product, Perplexity Pro, in the
end-to-end task, demonstrating the effectiveness of our proposed pipeline. We
further present error analysis to unveil current LMMs still struggle to fully
grasp the multimodal search tasks, and conduct ablation study to indicate the
potential of scaling test-time computation for AI search engine. We hope
MMSearch may provide unique insights to guide the future development of
multimodal AI search engine. Project Page: https://mmsearch.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://mmsearch.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HLLM: Enhancing Sequential Recommendations via Hierarchical Large
  Language Models for Item and User Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12740v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12740v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyi Chen, Lu Chi, Bingyue Peng, Zehuan Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have achieved remarkable success in various
fields, prompting several studies to explore their potential in recommendation
systems. However, these attempts have so far resulted in only modest
improvements over traditional recommendation models. Moreover, three critical
questions remain under-explored: firstly, the real value of LLMs' pre-trained
weights, often considered to encapsulate world knowledge; secondly, the
necessity of fine-tuning for recommendation tasks; lastly, whether LLMs can
exhibit the same scalability benefits in recommendation systems as they do in
other domains. In this paper, we propose a novel Hierarchical Large Language
Model (HLLM) architecture designed to enhance sequential recommendation
systems. Our approach employs a two-tier model: the first Item LLM extracts
rich content features from the detailed text description of the item, while the
second User LLM utilizes these features to predict users' future interests
based on their interaction history. Extensive experiments demonstrate that our
method effectively leverages the pre-trained capabilities of open-source LLMs,
and further fine-tuning leads to significant performance boosts. Additionally,
HLLM achieves excellent scalability, with the largest configuration utilizing
7B parameters for both item feature extraction and user interest modeling.
Moreover, HLLM offers excellent training and serving efficiency, making it
practical in real-world applications. Evaluations on two large-scale datasets,
PixelRec and Amazon Reviews, show that HLLM achieves state-of-the-art results,
outperforming traditional ID-based models by a wide margin. In online A/B
testing, HLLM showcases notable gains, validating its practical impact in
real-world recommendation scenarios. Codes are available at
https://github.com/bytedance/HLLM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When SparseMoE Meets Noisy Interactions: An Ensemble View on Denoising
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12730v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12730v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weipu Chen, Zhuangzhuang He, Fei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning user preferences from implicit feedback is one of the core
challenges in recommendation. The difficulty lies in the potential noise within
implicit feedback. Therefore, various denoising recommendation methods have
been proposed recently. However, most of them overly rely on the hyperparameter
configurations, inevitably leading to inadequacies in model adaptability and
generalization performance. In this study, we propose a novel Adaptive Ensemble
Learning (AEL) for denoising recommendation, which employs a sparse gating
network as a brain, selecting suitable experts to synthesize appropriate
denoising capacities for different data samples. To address the ensemble
learning shortcoming of model complexity and ensure sub-recommender diversity,
we also proposed a novel method that stacks components to create
sub-recommenders instead of directly constructing them. Extensive experiments
across various datasets demonstrate that AEL outperforms others in kinds of
popular metrics, even in the presence of substantial and dynamic noise. Our
code is available at https://github.com/cpu9xx/AEL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Large Language Models for Product Attribute Value
  Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kassem Sabeh, Mouna Kacimi, Johann Gamper, Robert Litschko, Barbara Plank
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Product attribute value identification (PAVI) involves automatically
identifying attributes and their values from product information, enabling
features like product search, recommendation, and comparison. Existing methods
primarily rely on fine-tuning pre-trained language models, such as BART and T5,
which require extensive task-specific training data and struggle to generalize
to new attributes. This paper explores large language models (LLMs), such as
LLaMA and Mistral, as data-efficient and robust alternatives for PAVI. We
propose various strategies: comparing one-step and two-step prompt-based
approaches in zero-shot settings and utilizing parametric and non-parametric
knowledge through in-context learning examples. We also introduce a dense
demonstration retriever based on a pre-trained T5 model and perform instruction
fine-tuning to explicitly train LLMs on task-specific instructions. Extensive
experiments on two product benchmarks show that our two-step approach
significantly improves performance in zero-shot settings, and instruction
fine-tuning further boosts performance when using training data, demonstrating
the practical benefits of using LLMs for PAVI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Deep Dive into Fairness, Bias, Threats, and Privacy in Recommender
  Systems: Insights and Future Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12651v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12651v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Falguni Roy, Xiaofeng Ding, K. -K. R. Choo, Pan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems are essential for personalizing digital experiences on
e-commerce sites, streaming services, and social media platforms. While these
systems are necessary for modern digital interactions, they face fairness,
bias, threats, and privacy challenges. Bias in recommender systems can result
in unfair treatment of specific users and item groups, and fairness concerns
demand that recommendations be equitable for all users and items. These systems
are also vulnerable to various threats that compromise reliability and
security. Furthermore, privacy issues arise from the extensive use of personal
data, making it crucial to have robust protection mechanisms to safeguard user
information. This study explores fairness, bias, threats, and privacy in
recommender systems. It examines how algorithmic decisions can unintentionally
reinforce biases or marginalize specific user and item groups, emphasizing the
need for fair recommendation strategies. The study also looks at the range of
threats in the form of attacks that can undermine system integrity and
discusses advanced privacy-preserving techniques. By addressing these critical
areas, the study highlights current limitations and suggests future research
directions to improve recommender systems' robustness, fairness, and privacy.
Ultimately, this research aims to help develop more trustworthy and ethical
recommender systems that better serve diverse user populations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-View Adaptive Contrastive Learning for Information Retrieval Based
  Fault Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12519v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12519v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunying Zhou, Xiaoyuan Xie, Gong Chen, Peng He, Bing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most studies focused on information retrieval-based techniques for fault
localization, which built representations for bug reports and source code files
and matched their semantic vectors through similarity measurement. However,
such approaches often ignore some useful information that might help improve
localization performance, such as 1) the interaction relationship between bug
reports and source code files; 2) the similarity relationship between bug
reports; and 3) the co-citation relationship between source code files. In this
paper, we propose a novel approach named Multi-View Adaptive Contrastive
Learning for Information Retrieval Fault Localization (MACL-IRFL) to learn the
above-mentioned relationships for software fault localization. Specifically, we
first generate data augmentations from report-code interaction view,
report-report similarity view and code-code co-citation view separately, and
adopt graph neural network to aggregate the information of bug reports or
source code files from the three views in the embedding process. Moreover, we
perform contrastive learning across these views. Our design of contrastive
learning task will force the bug report representations to encode information
shared by report-report and report-code views,and the source code file
representations shared by code-code and report-code views, thereby alleviating
the noise from auxiliary information. Finally, to evaluate the performance of
our approach, we conduct extensive experiments on five open-source Java
projects. The results show that our model can improve over the best baseline up
to 28.93%, 25.57% and 20.35% on Accuracy@1, MAP and MRR, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Familiarity-aware Evidence Compression for Retrieval Augmented
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12468v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12468v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongwon Jung, Qin Liu, Tenghao Huang, Ben Zhou, Muhao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval Augmented Generation (RAG) improves large language models (LMs) by
incorporating non-parametric knowledge through evidence retrieval from external
sources. However, it often struggles to filter out inconsistent and irrelevant
information that can distract the LM from its tasks. While compressing the
retrieved evidence with a compression model aims to address this issue, the
compressed evidence may still be unfamiliar to the target model used for
downstream task, potentially failing to utilize the evidence effectively. We
propose FaviComp (Familiarity-aware Evidence Compression), a novel
training-free evidence compression technique that makes retrieved evidence more
familiar to the target model, while seamlessly integrating parametric knowledge
from the model. Specifically, FaviComp proactively lowers the perplexity of the
compressed evidence with regard to the target model by combining token
probabilities from both the compression model and the target model to generate
context that is more familiar to the target model. This approach balances the
integration of parametric and non-parametric knowledge, which is especially
helpful in complex tasks where the retrieved evidence set may not contain all
the necessary information. Experimental results demonstrate that FaviComp
consistently outperforms existing baselines in multiple open-domain QA
datasets, achieving high compression rates and showcasing the effective
integration of both parametric and non-parametric knowledge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bundle Fragments into a Whole: Mining More Complete Clusters via
  Submodular Selection of Interesting webpages for Web Topic Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12380v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12380v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junbiao Pang, Anjing Hu, Qingming Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Organizing interesting webpages into hot topics is one of key steps to
understand the trends of multimodal web data. A state-of-the-art solution is
firstly to organize webpages into a large volume of multi-granularity topic
candidates; hot topics are further identified by estimating their
interestingness. However, these topic candidates contain a large number of
fragments of hot topics due to both the inefficient feature representations and
the unsupervised topic generation. This paper proposes a bundling-refining
approach to mine more complete hot topics from fragments. Concretely, the
bundling step organizes the fragment topics into coarse topics; next, the
refining step proposes a submodular-based method to refine coarse topics in a
scalable approach. The propose unconventional method is simple, yet powerful by
leveraging submodular optimization, our approach outperforms the traditional
ranking methods which involve the careful design and complex steps. Extensive
experiments demonstrate that the proposed approach surpasses the
state-of-the-art method (i.e., latent Poisson deconvolution Pang et al. (2016))
20% accuracy and 10% one on two public data sets, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Skill matching at scale: freelancer-project alignment for efficient
  multilingual candidate retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12097v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12097v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Warren Jouanneau, Marc Palyart, Emma Jouffroy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Finding the perfect match between a job proposal and a set of freelancers is
not an easy task to perform at scale, especially in multiple languages. In this
paper, we propose a novel neural retriever architecture that tackles this
problem in a multilingual setting. Our method encodes project descriptions and
freelancer profiles by leveraging pre-trained multilingual language models. The
latter are used as backbone for a custom transformer architecture that aims to
keep the structure of the profiles and project. This model is trained with a
contrastive loss on historical data. Thanks to several experiments, we show
that this approach effectively captures skill matching similarity and
facilitates efficient matching, outperforming traditional methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ jina-embeddings-v3: Multilingual Embeddings With Task LoRA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10173v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10173v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saba Sturua, Isabelle Mohr, Mohammad Kalim Akram, Michael Günther, Bo Wang, Markus Krimmel, Feng Wang, Georgios Mastrapas, Andreas Koukounas, Nan Wang, Han Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce jina-embeddings-v3, a novel text embedding model with 570
million parameters, achieves state-of-the-art performance on multilingual data
and long-context retrieval tasks, supporting context lengths of up to 8192
tokens. The model includes a set of task-specific Low-Rank Adaptation (LoRA)
adapters to generate high-quality embeddings for query-document retrieval,
clustering, classification, and text matching. Evaluation on the MTEB benchmark
shows that jina-embeddings-v3 outperforms the latest proprietary embeddings
from OpenAI and Cohere on English tasks, while achieving superior performance
compared to multilingual-e5-large-instruct across all multilingual tasks. With
a default output dimension of 1024, users can flexibly reduce the embedding
dimensions to as low as 32 without compromising performance, enabled by
Matryoshka Representation Learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, pp11-13 references, pp14-20 appendix and experiment tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Empowering Sequential Recommendation from Collaborative Signals and
  Semantic Relatedness <span class="chip">DASFAA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07623v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07623v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyue Cheng, Hao Zhang, Qi Liu, Fajie Yuan, Zhi Li, Zhenya Huang, Enhong Chen, Jun Zhou, Longfei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommender systems (SRS) could capture dynamic user preferences
by modeling historical behaviors ordered in time. Despite effectiveness,
focusing only on the \textit{collaborative signals} from behaviors does not
fully grasp user interests. It is also significant to model the
\textit{semantic relatedness} reflected in content features, e.g., images and
text. Towards that end, in this paper, we aim to enhance the SRS tasks by
effectively unifying collaborative signals and semantic relatedness together.
Notably, we empirically point out that it is nontrivial to achieve such a goal
due to semantic gap issues. Thus, we propose an end-to-end two-stream
architecture for sequential recommendation, named TSSR, to learn user
preferences from ID-based and content-based sequence. Specifically, we first
present novel hierarchical contrasting module, including coarse user-grained
and fine item-grained terms, to align the representations of inter-modality.
Furthermore, we also design a two-stream architecture to learn the dependence
of intra-modality sequence and the complex interactions of inter-modality
sequence, which can yield more expressive capacity in understanding user
interests. We conduct extensive experiments on five public datasets. The
experimental results show that the TSSR could yield superior performance than
competitive baselines. We also make our experimental codes publicly available
at https://github.com/Mingyue-Cheng/TSSR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted By DASFAA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Incremental Concept Formation over Visual Images Without Catastrophic
  Forgetting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16933v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16933v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicki Barari, Xin Lian, Christopher J. MacLellan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks have excelled in machine learning, particularly in
vision tasks, however, they often suffer from catastrophic forgetting when
learning new tasks sequentially. In this work, we introduce Cobweb4V, an
alternative to traditional neural network approaches. Cobweb4V is a novel
visual classification method that builds on Cobweb, a human like learning
system that is inspired by the way humans incrementally learn new concepts over
time. In this research, we conduct a comprehensive evaluation, showcasing
Cobweb4Vs proficiency in learning visual concepts, requiring less data to
achieve effective learning outcomes compared to traditional methods,
maintaining stable performance over time, and achieving commendable asymptotic
behavior, without catastrophic forgetting effects. These characteristics align
with learning strategies in human cognition, positioning Cobweb4V as a
promising alternative to neural network approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by The Eleventh Annual Conference on Advances in Cognitive
  Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM-Powered Text Simulation Attack Against ID-Free Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11690v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11690v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongwei Wang, Min Gao, Junliang Yu, Xinyi Gao, Quoc Viet Hung Nguyen, Shazia Sadiq, Hongzhi Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ID-free recommendation paradigm has been proposed to address the
limitation that traditional recommender systems struggle to model cold-start
users or items with new IDs. Despite its effectiveness, this study uncovers
that ID-free recommender systems are vulnerable to the proposed Text Simulation
attack (TextSimu) which aims to promote specific target items. As a novel type
of text poisoning attack, TextSimu exploits large language models (LLM) to
alter the textual information of target items by simulating the characteristics
of popular items. It operates effectively in both black-box and white-box
settings, utilizing two key components: a unified popularity extraction module,
which captures the essential characteristics of popular items, and an N-persona
consistency simulation strategy, which creates multiple personas to
collaboratively synthesize refined promotional textual descriptions for target
items by simulating the popular items. To withstand TextSimu-like attacks, we
further explore the detection approach for identifying LLM-generated
promotional text. Extensive experiments conducted on three datasets demonstrate
that TextSimu poses a more significant threat than existing poisoning attacks,
while our defense method can detect malicious text of target items generated by
TextSimu. By identifying the vulnerability, we aim to advance the development
of more robust ID-free recommender systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">7</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffSSD: A Diffusion-Based <span class="highlight-title">Dataset</span> For Speech Forensics <span class="chip">ICASSP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13049v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13049v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kratika Bhagtani, Amit Kumar Singh Yadav, Paolo Bestagini, Edward J. Delp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion-based speech generators are ubiquitous. These methods can generate
very high quality synthetic speech and several recent incidents report their
malicious use. To counter such misuse, synthetic speech detectors have been
developed. Many of these detectors are trained on datasets which do not include
diffusion-based synthesizers. In this paper, we demonstrate that existing
detectors trained on one such dataset, ASVspoof2019, do not perform well in
detecting synthetic speech from recent diffusion-based synthesizers. We propose
the Diffusion-Based Synthetic Speech Dataset (DiffSSD), a dataset consisting of
about 200 hours of labeled speech, including synthetic speech generated by 8
diffusion-based open-source and 2 commercial generators. We also examine the
performance of existing synthetic speech detectors on DiffSSD in both
closed-set and open-set scenarios. The results highlight the importance of this
dataset in detecting synthetic speech generated from recent open-source and
commercial speech generators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE International Conference on Acoustics, Speech, and
  Signal Processing (ICASSP) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Across-Game Engagement Modelling via Few-Shot Learning <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13002v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13002v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kosmas Pinitas, Konstantinos Makantasis, Georgios N. Yannakakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain generalisation involves learning artificial intelligence (AI) models
that can maintain high performance across diverse domains within a specific
task. In video games, for instance, such AI models can supposedly learn to
detect player actions across different games. Despite recent advancements in
AI, domain generalisation for modelling the users' experience remains largely
unexplored. While video games present unique challenges and opportunities for
the analysis of user experience -- due to their dynamic and rich contextual
nature -- modelling such experiences is limited by generally small datasets. As
a result, conventional modelling methods often struggle to bridge the domain
gap between users and games due to their reliance on large labelled training
data and assumptions of common distributions of user experience. In this paper,
we tackle this challenge by introducing a framework that decomposes the general
domain-agnostic modelling of user experience into several domain-specific and
game-dependent tasks that can be solved via few-shot learning. We test our
framework on a variation of the publicly available GameVibe corpus, designed
specifically to test a model's ability to predict user engagement across
different first-person shooter games. Our findings demonstrate the superior
performance of few-shot learners over traditional modelling methods and thus
showcase the potential of few-shot learning for robust experience modelling in
video games and beyond.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, accepted for publication at ECCV 2024 CV2 Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InfiMM-WebMath-40B: Advancing Multimodal <span class="highlight-title">Pre-Train</span>ing for Enhanced
  Mathematical Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12568v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12568v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaotian Han, Yiren Jian, Xuefeng Hu, Haogeng Liu, Yiqi Wang, Qihang Fan, Yuang Ai, Huaibo Huang, Ran He, Zhenheng Yang, Quanzeng You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-training on large-scale, high-quality datasets is crucial for enhancing
the reasoning capabilities of Large Language Models (LLMs), especially in
specialized domains such as mathematics. Despite the recognized importance, the
Multimodal LLMs (MLLMs) field currently lacks a comprehensive open-source
pre-training dataset specifically designed for mathematical reasoning. To
address this gap, we introduce InfiMM-WebMath-40B, a high-quality dataset of
interleaved image-text documents. It comprises 24 million web pages, 85 million
associated image URLs, and 40 billion text tokens, all meticulously extracted
and filtered from CommonCrawl. We provide a detailed overview of our data
collection and processing pipeline. To demonstrate the robustness of
InfiMM-WebMath-40B, we conducted evaluations in both text-only and multimodal
settings. Our evaluations on text-only benchmarks show that, despite utilizing
only 40 billion tokens, our dataset significantly enhances the performance of
our 1.3B model, delivering results comparable to DeepSeekMath-1.3B, which uses
120 billion tokens for the same model size. Nevertheless, with the introduction
of our multi-modal math pre-training dataset, our models set a new
state-of-the-art among open-source models on multi-modal math benchmarks such
as MathVerse and We-Math. We release our data at
https://huggingface.co/datasets/Infi-MM/InfiMM-WebMath-40B.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mutual Information-based Representations Disentanglement for Unaligned
  Multimodal Language Sequences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12408v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12408v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Qian, Jiqing Han, Jianchen Li, Yongjun He, Tieran Zheng, Guibin Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The key challenge in unaligned multimodal language sequences lies in
effectively integrating information from various modalities to obtain a refined
multimodal joint representation. Recently, the disentangle and fuse methods
have achieved the promising performance by explicitly learning
modality-agnostic and modality-specific representations and then fusing them
into a multimodal joint representation. However, these methods often
independently learn modality-agnostic representations for each modality and
utilize orthogonal constraints to reduce linear correlations between
modality-agnostic and modality-specific representations, neglecting to
eliminate their nonlinear correlations. As a result, the obtained multimodal
joint representation usually suffers from information redundancy, leading to
overfitting and poor generalization of the models. In this paper, we propose a
Mutual Information-based Representations Disentanglement (MIRD) method for
unaligned multimodal language sequences, in which a novel disentanglement
framework is designed to jointly learn a single modality-agnostic
representation. In addition, the mutual information minimization constraint is
employed to ensure superior disentanglement of representations, thereby
eliminating information redundancy within the multimodal joint representation.
Furthermore, the challenge of estimating mutual information caused by the
limited labeled data is mitigated by introducing unlabeled data. Meanwhile, the
unlabeled data also help to characterize the underlying structure of multimodal
data, consequently further preventing overfitting and enhancing the performance
of the models. Experimental results on several widely used benchmark datasets
validate the effectiveness of our proposed approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Multi-label Recognition using Class Co-Occurrence
  Probabilities <span class="chip">ICPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.16193v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.16193v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samyak Rawlekar, Shubhang Bhatnagar, Vishnuvardhan Pogunulu Srinivasulu, Narendra Ahuja
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-label Recognition (MLR) involves the identification of multiple objects
within an image. To address the additional complexity of this problem, recent
works have leveraged information from vision-language models (VLMs) trained on
large text-images datasets for the task. These methods learn an independent
classifier for each object (class), overlooking correlations in their
occurrences. Such co-occurrences can be captured from the training data as
conditional probabilities between a pair of classes. We propose a framework to
extend the independent classifiers by incorporating the co-occurrence
information for object pairs to improve the performance of independent
classifiers. We use a Graph Convolutional Network (GCN) to enforce the
conditional probabilities between classes, by refining the initial estimates
derived from image and text sources obtained using VLMs. We validate our method
on four MLR datasets, where our approach outperforms all state-of-the-art
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICPR 2024, CVPR workshops 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-Attention Fusion of Visual and Geometric Features for Large
  Vocabulary Arabic Lipreading 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11520v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11520v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samar Daou, Achraf Ben-Hamadou, Ahmed Rekik, Abdelaziz Kallel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lipreading involves using visual data to recognize spoken words by analyzing
the movements of the lips and surrounding area. It is a hot research topic with
many potential applications, such as human-machine interaction and enhancing
audio speech recognition. Recent deep-learning based works aim to integrate
visual features extracted from the mouth region with landmark points on the lip
contours. However, employing a simple combination method such as concatenation
may not be the most effective approach to get the optimal feature vector. To
address this challenge, firstly, we propose a cross-attention fusion-based
approach for large lexicon Arabic vocabulary to predict spoken words in videos.
Our method leverages the power of cross-attention networks to efficiently
integrate visual and geometric features computed on the mouth region. Secondly,
we introduce the first large-scale Lipreading in the Wild for Arabic (LRW-AR)
dataset containing 20,000 videos for 100-word classes, uttered by 36 speakers.
The experimental results obtained on LRW-AR and ArabicVisual databases showed
the effectiveness and robustness of the proposed approach in recognizing Arabic
words. Our work provides insights into the feasibility and effectiveness of
applying lipreading techniques to the Arabic language, opening doors for
further research in this field. Link to the project page:
https://crns-smartvision.github.io/lrwar
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted for review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Simple Baseline with Single-encoder for Referring Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.15521v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.15521v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seonghoon Yu, Ilchae Jung, Byeongju Han, Taeoh Kim, Yunho Kim, Dongyoon Wee, Jeany Son
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Referring image segmentation (RIS) requires dense vision-language
interactions between visual pixels and textual words to segment objects based
on a given description. However, commonly adapted dual-encoders in RIS, e.g.,
Swin transformer and BERT (uni-modal encoders) or CLIP (a multi-modal
dual-encoder), lack dense multi-modal interactions during pre-training, leading
to a gap with a pixel-level RIS task. To bridge this gap, existing RIS methods
often rely on multi-modal fusion modules that interact two encoders, but this
approach leads to high computational costs. In this paper, we present a novel
RIS method with a single-encoder, i.e., BEiT-3, maximizing the potential of
shared self-attention across all framework components. This enables seamless
interactions of two modalities from input to final prediction, producing
granularly aligned multi-modal features. Furthermore, we propose lightweight
yet effective decoder modules, a Shared FPN and a Shared Mask Decoder, which
contribute to the high efficiency of our model. Our simple baseline with a
single encoder achieves outstanding performances on the RIS benchmark datasets
while maintaining computational efficiency, compared to the most recent SoTA
methods based on dual-encoders.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv pre-print</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-09-18T00:00:00Z">2024-09-18</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">17</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MedCodER: A Generative AI Assistant for Medical Coding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15368v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15368v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Krishanu Das Baksi, Elijah Soba, John J. Higgins, Ravi Saini, Jaden Wood, Jane Cook, Jack Scott, Nirmala Pudota, Tim Weninger, Edward Bowen, Sanmitra Bhattacharya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical coding is essential for standardizing clinical data and communication
but is often time-consuming and prone to errors. Traditional Natural Language
Processing (NLP) methods struggle with automating coding due to the large label
space, lengthy text inputs, and the absence of supporting evidence annotations
that justify code selection. Recent advancements in Generative Artificial
Intelligence (AI) offer promising solutions to these challenges. In this work,
we introduce MedCodER, a Generative AI framework for automatic medical coding
that leverages extraction, retrieval, and re-ranking techniques as core
components. MedCodER achieves a micro-F1 score of 0.60 on International
Classification of Diseases (ICD) code prediction, significantly outperforming
state-of-the-art methods. Additionally, we present a new dataset containing
medical records annotated with disease diagnoses, ICD codes, and supporting
evidence texts (https://doi.org/10.5281/zenodo.13308316). Ablation tests
confirm that MedCodER's performance depends on the integration of each of its
aforementioned components, as performance declines when these components are
evaluated in isolation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recommendation with Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15173v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15173v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yashar Deldjoo, Zhankui He, Julian McAuley, Anton Korikov, Scott Sanner, Arnau Ramisa, Rene Vidal, Maheswaran Sathiamoorthy, Atoosa Kasrizadeh, Silvia Milano, Francesco Ricci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models are a class of AI models capable of creating new instances
of data by learning and sampling from their statistical distributions. In
recent years, these models have gained prominence in machine learning due to
the development of approaches such as generative adversarial networks (GANs),
variational autoencoders (VAEs), and transformer-based architectures such as
GPT. These models have applications across various domains, such as image
generation, text synthesis, and music composition. In recommender systems,
generative models, referred to as Gen-RecSys, improve the accuracy and
diversity of recommendations by generating structured outputs, text-based
interactions, and multimedia content. By leveraging these capabilities,
Gen-RecSys can produce more personalized, engaging, and dynamic user
experiences, expanding the role of AI in eCommerce, media, and beyond.
  Our book goes beyond existing literature by offering a comprehensive
understanding of generative models and their applications, with a special focus
on deep generative models (DGMs) and their classification. We introduce a
taxonomy that categorizes DGMs into three types: ID-driven models, large
language models (LLMs), and multimodal models. Each category addresses unique
technical and architectural advancements within its respective research area.
This taxonomy allows researchers to easily navigate developments in Gen-RecSys
across domains such as conversational AI and multimodal content generation.
Additionally, we examine the impact and potential risks of generative models,
emphasizing the importance of robust evaluation frameworks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This submission is a full-length book, expanding significantly on two
  chapters previously submitted (arXiv:2409.10993v1, arXiv:2408.10946v1). It
  includes additional chapters, context, analysis, and content, providing a
  comprehensive presentation of the subject. We have ensured it is
  appropriately presented as a new, distinct work. arXiv admin note:
  substantial text overlap with arXiv:2409.10993</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalized compression and compressive search of large <span class="highlight-title">dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12161v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12161v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Morgan E. Prior, Thomas Howard III, Emily Light, Najib Ishaq, Noah M. Daniels
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Big Data explosion has necessitated the development of search algorithms
that scale sub-linearly in time and memory.
  While compression algorithms and search algorithms do exist independently,
few algorithms offer both, and those which do are domain-specific.
  We present panCAKES, a novel approach to compressive search, i.e., a way to
perform $k$-NN and $\rho$-NN search on compressed data while only decompressing
a small, relevant, portion of the data.
  panCAKES assumes the manifold hypothesis and leverages the low-dimensional
structure of the data to compress and search it efficiently.
  panCAKES is generic over any distance function for which the distance between
two points is proportional to the memory cost of storing an encoding of one in
terms of the other.
  This property holds for many widely-used distance functions, e.g. string edit
distances (Levenshtein, Needleman-Wunsch, etc.) and set dissimilarity measures
(Jaccard, Dice, etc.).
  We benchmark panCAKES on a variety of datasets, including genomic, proteomic,
and set data.
  We compare compression ratios to gzip, and search performance between the
compressed and uncompressed versions of the same dataset.
  panCAKES achieves compression ratios close to those of gzip, while offering
sub-linear time performance for $k$-NN and $\rho$-NN search.
  We conclude that panCAKES is an efficient, general-purpose algorithm for
exact compressive search on large datasets that obey the manifold hypothesis.
  We provide an open-source implementation of panCAKES in the Rust programming
language.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decoding Style: Efficient Fine-Tuning of LLMs for Image-Guided Outfit
  Recommendation with Preference <span class="chip">CIKM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12150v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12150v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Najmeh Forouzandehmehr, Nima Farrokhsiar, Ramin Giahi, Evren Korpeoglu, Kannan Achan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalized outfit recommendation remains a complex challenge, demanding
both fashion compatibility understanding and trend awareness. This paper
presents a novel framework that harnesses the expressive power of large
language models (LLMs) for this task, mitigating their "black box" and static
nature through fine-tuning and direct feedback integration. We bridge the item
visual-textual gap in items descriptions by employing image captioning with a
Multimodal Large Language Model (MLLM). This enables the LLM to extract style
and color characteristics from human-curated fashion images, forming the basis
for personalized recommendations. The LLM is efficiently fine-tuned on the
open-source Polyvore dataset of curated fashion images, optimizing its ability
to recommend stylish outfits. A direct preference mechanism using negative
examples is employed to enhance the LLM's decision-making process. This creates
a self-enhancing AI feedback loop that continuously refines recommendations in
line with seasonal fashion trends. Our framework is evaluated on the Polyvore
dataset, demonstrating its effectiveness in two key tasks: fill-in-the-blank,
and complementary item retrieval. These evaluations underline the framework's
ability to generate stylish, trend-aligned outfit suggestions, continuously
improving through direct feedback. The evaluation results demonstrated that our
proposed framework significantly outperforms the base LLM, creating more
cohesive outfits. The improved performance in these tasks underscores the
proposed framework's potential to enhance the shopping experience with accurate
suggestions, proving its effectiveness over the vanilla LLM based outfit
generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CIKM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VERA: Validation and Enhancement for Retrieval Augmented systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15364v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15364v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nitin Aravind Birur, Tanay Baswa, Divyanshu Kumar, Jatan Loya, Sahil Agarwal, Prashanth Harshangi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) exhibit remarkable capabilities but often
produce inaccurate responses, as they rely solely on their embedded knowledge.
Retrieval-Augmented Generation (RAG) enhances LLMs by incorporating an external
information retrieval system, supplying additional context along with the query
to mitigate inaccuracies for a particular context. However, accuracy issues
still remain, as the model may rely on irrelevant documents or extrapolate
incorrectly from its training knowledge. To assess and improve the performance
of both the retrieval system and the LLM in a RAG framework, we propose
\textbf{VERA} (\textbf{V}alidation and \textbf{E}nhancement for
\textbf{R}etrieval \textbf{A}ugmented systems), a system designed to: 1)
Evaluate and enhance the retrieved context before response generation, and 2)
Evaluate and refine the LLM-generated response to ensure precision and minimize
errors. VERA employs an evaluator-cum-enhancer LLM that first checks if
external retrieval is necessary, evaluates the relevance and redundancy of the
retrieved context, and refines it to eliminate non-essential information.
Post-response generation, VERA splits the response into atomic statements,
assesses their relevance to the query, and ensures adherence to the context.
Our experiments demonstrate VERA's remarkable efficacy not only in improving
the performance of smaller open-source models, but also larger state-of-the art
models. These enhancements underscore VERA's potential to produce accurate and
relevant responses, advancing the state-of-the-art in retrieval-augmented
language modeling. VERA's robust methodology, combining multiple evaluation and
refinement steps, effectively mitigates hallucinations and improves retrieval
and response processes, making it a valuable tool for applications demanding
high accuracy and reliability in information generation. .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding the Effects of the Baidu-ULTR Logging Policy on Two-Tower
  Models <span class="chip">RecSys
  '24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12043v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12043v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Morris de Haan, Philipp Hager
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the popularity of the two-tower model for unbiased learning to rank
(ULTR) tasks, recent work suggests that it suffers from a major limitation that
could lead to its collapse in industry applications: the problem of logging
policy confounding. Several potential solutions have even been proposed;
however, the evaluation of these methods was mostly conducted using
semi-synthetic simulation experiments. This paper bridges the gap between
theory and practice by investigating the confounding problem on the largest
real-world dataset, Baidu-ULTR. Our main contributions are threefold: 1) we
show that the conditions for the confounding problem are given on Baidu-ULTR,
2) the confounding problem bears no significant effect on the two-tower model,
and 3) we point to a potential mismatch between expert annotations, the golden
standard in ULTR, and user click behavior.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the CONSEQUENCES '24 workshop, co-located with ACM RecSys
  '24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AlignBot: Aligning VLM-powered Customized Task Planning with User
  Reminders Through Fine-Tuning for Household Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11905v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11905v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Zhaxizhuoma, Pengan Chen, Ziniu Wu, Jiawei Sun, Dong Wang, Peng Zhou, Nieqing Cao, Yan Ding, Bin Zhao, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents AlignBot, a novel framework designed to optimize
VLM-powered customized task planning for household robots by effectively
aligning with user reminders. In domestic settings, aligning task planning with
user reminders poses significant challenges due to the limited quantity,
diversity, and multimodal nature of the reminders. To address these challenges,
AlignBot employs a fine-tuned LLaVA-7B model, functioning as an adapter for
GPT-4o. This adapter model internalizes diverse forms of user reminders-such as
personalized preferences, corrective guidance, and contextual assistance-into
structured instruction-formatted cues that prompt GPT-4o in generating
customized task plans. Additionally, AlignBot integrates a dynamic retrieval
mechanism that selects task-relevant historical successes as prompts for
GPT-4o, further enhancing task planning accuracy. To validate the effectiveness
of AlignBot, experiments are conducted in real-world household environments,
which are constructed within the laboratory to replicate typical household
settings. A multimodal dataset with over 1,500 entries derived from volunteer
reminders is used for training and evaluation. The results demonstrate that
AlignBot significantly improves customized task planning, outperforming
existing LLM- and VLM-powered planners by interpreting and aligning with user
reminders, achieving 86.8% success rate compared to the vanilla GPT-4o baseline
at 21.6%, reflecting a 65% improvement and over four times greater
effectiveness. Supplementary materials are available at:
https://yding25.com/AlignBot/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retrieve, Annotate, Evaluate, Repeat: Leveraging Multimodal LLMs for
  Large-Scale Product Retrieval Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11860v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11860v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kasra Hosseini, Thomas Kober, Josip Krapac, Roland Vollgraf, Weiwei Cheng, Ana Peleteiro Ramallo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating production-level retrieval systems at scale is a crucial yet
challenging task due to the limited availability of a large pool of
well-trained human annotators. Large Language Models (LLMs) have the potential
to address this scaling issue and offer a viable alternative to humans for the
bulk of annotation tasks. In this paper, we propose a framework for assessing
the product search engines in a large-scale e-commerce setting, leveraging
Multimodal LLMs for (i) generating tailored annotation guidelines for
individual queries, and (ii) conducting the subsequent annotation task. Our
method, validated through deployment on a large e-commerce platform,
demonstrates comparable quality to human annotations, significantly reduces
time and cost, facilitates rapid problem discovery, and provides an effective
solution for production-level quality control at scale.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 5 figures, 4 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Factuality of Large Language Models in the Legal Domain <span class="chip">CIKM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11798v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11798v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rajaa El Hamdani, Thomas Bonald, Fragkiskos Malliaros, Nils Holzenberger, Fabian Suchanek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the factuality of large language models (LLMs) as
knowledge bases in the legal domain, in a realistic usage scenario: we allow
for acceptable variations in the answer, and let the model abstain from
answering when uncertain. First, we design a dataset of diverse factual
questions about case law and legislation. We then use the dataset to evaluate
several LLMs under different evaluation methods, including exact, alias, and
fuzzy matching. Our results show that the performance improves significantly
under the alias and fuzzy matching methods. Further, we explore the impact of
abstaining and in-context examples, finding that both strategies enhance
precision. Finally, we demonstrate that additional pre-training on legal
documents, as seen with SaulLM, further improves factual precision from 63% to
81%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CIKM 2024, short paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Active Reconfigurable Intelligent Surface Empowered Synthetic Aperture
  Radar Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11728v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11728v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Sun, Rang Liu, Zhiping Lu, Honghao Luo, Ming Li, Qian Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthetic Aperture Radar (SAR) utilizes the movement of the radar antenna
over a specific area of interest to achieve higher spatial resolution imaging.
In this paper, we aim to investigate the realization of SAR imaging for a
stationary radar system with the assistance of active reconfigurable
intelligent surface (ARIS) mounted on an unmanned aerial vehicle (UAV). As the
UAV moves along the stationary trajectory, the ARIS can not only build a
high-quality virtual line-of-sight (LoS) propagation path, but its mobility can
also effectively create a much larger virtual aperture, which can be utilized
to realize a SAR system. In this paper, we first present a range-Doppler (RD)
imaging algorithm to obtain imaging results for the proposed ARIS-empowered SAR
system. Then, to further improve the SAR imaging performance, we attempt to
optimize the reflection coefficients of ARIS to maximize the signal-to-noise
ratio (SNR) at the stationary radar receiver under the constraints of ARIS
maximum power and amplification factor. An effective algorithm based on
fractional programming (FP) and majorization minimization (MM) methods is
developed to solve the resulting non-convex problem. Simulation results
validate the effectiveness of ARIS-assisted SAR imaging and our proposed RD
imaging and ARIS optimization algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FLARE: Fusing Language Models and Collaborative Architectures for
  Recommender Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liam Hebert, Marialena Kyriakidi, Hubert Pham, Krishna Sayana, James Pine, Sukhdeep Sodhi, Ambarish Jash
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hybrid recommender systems, combining item IDs and textual descriptions,
offer potential for improved accuracy. However, previous work has largely
focused on smaller datasets and model architectures. This paper introduces
Flare (Fusing Language models and collaborative Architectures for Recommender
Enhancement), a novel hybrid recommender that integrates a language model (mT5)
with a collaborative filtering model (Bert4Rec) using a Perceiver network. This
architecture allows Flare to effectively combine collaborative and content
information for enhanced recommendations.
  We conduct a two-stage evaluation, first assessing Flare's performance
against established baselines on smaller datasets, where it demonstrates
competitive accuracy. Subsequently, we evaluate Flare on a larger, more
realistic dataset with a significantly larger item vocabulary, introducing new
baselines for this setting. Finally, we showcase Flare's inherent ability to
support critiquing, enabling users to provide feedback and refine
recommendations. We further leverage critiquing as an evaluation method to
assess the model's language understanding and its transferability to the
recommendation task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Basket-Enhanced Heterogenous Hypergraph for Price-Sensitive Next Basket
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuening Zhou, Yulin Wang, Qian Cui, Xinyu Guan, Francisco Cisternas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Next Basket Recommendation (NBR) is a new type of recommender system that
predicts combinations of items users are likely to purchase together. Existing
NBR models often overlook a crucial factor, which is price, and do not fully
capture item-basket-user interactions. To address these limitations, we propose
a novel method called Basket-augmented Dynamic Heterogeneous Hypergraph (BDHH).
BDHH utilizes a heterogeneous multi-relational graph to capture the intricate
relationships among item features, with price as a critical factor. Moreover,
our approach includes a basket-guided dynamic augmentation network that could
dynamically enhances item-basket-user interactions. Experiments on real-world
datasets demonstrate that BDHH significantly improves recommendation accuracy,
providing a more comprehensive understanding of user behavior.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Enhanced-State Reinforcement Learning Algorithm for Multi-Task Fusion
  in Large-Scale Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Liu, Jiawei Zhu, Cong Xu, Ming Zhao, Bin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the last key stage of Recommender Systems (RSs), Multi-Task Fusion (MTF)
is in charge of combining multiple scores predicted by Multi-Task Learning
(MTL) into a final score to maximize user satisfaction, which decides the
ultimate recommendation results. In recent years, to maximize long-term user
satisfaction within a recommendation session, Reinforcement Learning (RL) is
widely used for MTF in large-scale RSs. However, limited by their modeling
pattern, all the current RL-MTF methods can only utilize user features as the
state to generate actions for each user, but unable to make use of item
features and other valuable features, which leads to suboptimal results.
Addressing this problem is a challenge that requires breaking through the
current modeling pattern of RL-MTF. To solve this problem, we propose a novel
method called Enhanced-State RL for MTF in RSs. Unlike the existing methods
mentioned above, our method first defines user features, item features, and
other valuable features collectively as the enhanced state; then proposes a
novel actor and critic learning process to utilize the enhanced state to make
much better action for each user-item pair. To the best of our knowledge, this
novel modeling pattern is being proposed for the first time in the field of
RL-MTF. We conduct extensive offline and online experiments in a large-scale
RS. The results demonstrate that our model outperforms other models
significantly. Enhanced-State RL has been fully deployed in our RS more than
half a year, improving +3.84% user valid consumption and +0.58% user duration
time compared to baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2404.17589</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Designing Interfaces for Multimodal Vector Search Applications <span class="chip">CIKM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11629v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11629v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Owen Pendrigh Elliott, Tom Hamer, Jesse Clark
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal vector search offers a new paradigm for information retrieval by
exposing numerous pieces of functionality which are not possible in traditional
lexical search engines. While multimodal vector search can be treated as a drop
in replacement for these traditional systems, the experience can be
significantly enhanced by leveraging the unique capabilities of multimodal
search. Central to any information retrieval system is a user who expresses an
information need, traditional user interfaces with a single search bar allow
users to interact with lexical search systems effectively however are not
necessarily optimal for multimodal vector search. In this paper we explore
novel capabilities of multimodal vector search applications utilising CLIP
models and present implementations and design patterns which better allow users
to express their information needs and effectively interact with these systems
in an information retrieval context.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures, CIKM 2024 MMSR Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language Models and Retrieval Augmented Generation for Automated
  Structured Data Extraction from Diagnostic Reports 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10576v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10576v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Sobhi Jabal, Pranav Warman, Jikai Zhang, Kartikeye Gupta, Ayush Jain, Maciej Mazurowski, Walter Wiggins, Kirti Magudia, Evan Calabrese
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Purpose: To develop and evaluate an automated system for extracting
structured clinical information from unstructured radiology and pathology
reports using open-weights large language models (LMs) and retrieval augmented
generation (RAG), and to assess the effects of model configuration variables on
extraction performance. Methods and Materials: The study utilized two datasets:
7,294 radiology reports annotated for Brain Tumor Reporting and Data System
(BT-RADS) scores and 2,154 pathology reports annotated for isocitrate
dehydrogenase (IDH) mutation status. An automated pipeline was developed to
benchmark the performance of various LMs and RAG configurations. The impact of
model size, quantization, prompting strategies, output formatting, and
inference parameters was systematically evaluated. Results: The best performing
models achieved over 98% accuracy in extracting BT-RADS scores from radiology
reports and over 90% for IDH mutation status extraction from pathology reports.
The top model being medical fine-tuned llama3. Larger, newer, and domain
fine-tuned models consistently outperformed older and smaller models. Model
quantization had minimal impact on performance. Few-shot prompting
significantly improved accuracy. RAG improved performance for complex pathology
reports but not for shorter radiology reports. Conclusions: Open LMs
demonstrate significant potential for automated extraction of structured
clinical data from unstructured clinical reports with local privacy-preserving
application. Careful model selection, prompt engineering, and semi-automated
optimization using annotated data are critical for optimal performance. These
approaches could be reliable enough for practical use in research workflows,
highlighting the potential for human-machine collaboration in healthcare data
extraction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AugTriever: Unsupervised Dense Retrieval by Scalable Data Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08841v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08841v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Meng, Ye Liu, Semih Yavuz, Divyansh Agarwal, Lifu Tu, Ning Yu, Jianguo Zhang, Meghana Bhat, Yingbo Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dense retrievers have made significant strides in text retrieval and
open-domain question answering. However, most of these achievements have relied
heavily on extensive human-annotated supervision. In this study, we aim to
develop unsupervised methods for improving dense retrieval models. We propose
two approaches that enable annotation-free and scalable training by creating
pseudo querydocument pairs: query extraction and transferred query generation.
The query extraction method involves selecting salient spans from the original
document to generate pseudo queries. On the other hand, the transferred query
generation method utilizes generation models trained for other NLP tasks, such
as summarization, to produce pseudo queries. Through extensive experimentation,
we demonstrate that models trained using these augmentation methods can achieve
comparable, if not better, performance than multiple strong dense baselines.
Moreover, combining these strategies leads to further improvements, resulting
in superior performance of unsupervised dense retrieval, unsupervised domain
adaptation and supervised finetuning, benchmarked on both BEIR and ODQA
datasets. Code and datasets are publicly available at
https://github.com/salesforce/AugTriever.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>DCAI24, October 25, 2024, Boise, ID</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Best-of-Both Approach to Improve Match Predictions and Reciprocal
  Recommendations for Job Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10992v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10992v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuhei Goda, Yudai Hayashi, Yuta Saito
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Matching users with mutual preferences is a critical aspect of services
driven by reciprocal recommendations, such as job search. To produce
recommendations in such scenarios, one can predict match probabilities and
construct rankings based on these predictions. However, this direct match
prediction approach often underperforms due to the extreme sparsity of match
labels. Therefore, most existing methods predict preferences separately for
each direction (e.g., job seeker to employer and employer to job seeker) and
then aggregate the predictions to generate overall matching scores and produce
recommendations. However, this typical approach often leads to practical
issues, such as biased error propagation between the two models. This paper
introduces and demonstrates a novel and practical solution to improve
reciprocal recommendations in production by leveraging pseudo-match scores.
Specifically, our approach generates dense and more directly relevant
pseudo-match scores by combining the true match labels, which are accurate but
sparse, with relatively inaccurate but dense match predictions. We then train a
meta-model to output the final match predictions by minimizing the prediction
loss against the pseudo-match scores. Our method can be seen as a best-of-both
(BoB) approach, as it combines the high-level ideas of both direct match
prediction and the two separate models approach. It also allows for
user-specific weights to construct personalized pseudo-match scores, achieving
even better matching performance through appropriate tuning of the weights.
Offline experiments on real-world job search data demonstrate the superior
performance of our BoB method, particularly with personalized pseudo-match
scores, compared to existing approaches in terms of finding potential matches.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">9</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models Are Strong Audio-Visual Speech Recognition
  Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12319v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12319v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Umberto Cappellazzo, Minsu Kim, Honglie Chen, Pingchuan Ma, Stavros Petridis, Daniele Falavigna, Alessio Brutti, Maja Pantic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) have recently become a focal point
of research due to their formidable multimodal understanding capabilities. For
example, in the audio and speech domains, an LLM can be equipped with
(automatic) speech recognition (ASR) abilities by just concatenating the audio
tokens, computed with an audio encoder, and the text tokens to achieve
state-of-the-art results. On the contrary, tasks like visual and audio-visual
speech recognition (VSR/AVSR), which also exploit noise-invariant lip movement
information, have received little or no attention. To bridge this gap, we
propose Llama-AVSR, a new MLLM with strong audio-visual speech recognition
capabilities. It leverages pre-trained audio and video encoders to produce
modality-specific tokens which, together with the text tokens, are processed by
a pre-trained LLM (e.g., Llama3.1-8B) to yield the resulting response in an
auto-regressive fashion. Llama-AVSR requires a small number of trainable
parameters as only modality-specific projectors and LoRA modules are trained
whereas the multi-modal encoders and LLM are kept frozen. We evaluate our
proposed approach on LRS3, the largest public AVSR benchmark, and we achieve
new state-of-the-art results for the tasks of ASR and AVSR with a WER of 0.81%
and 0.77%, respectively. To bolster our results, we investigate the key factors
that underpin the effectiveness of Llama-AVSR: the choice of the pre-trained
encoders and LLM, the efficient integration of LoRA modules, and the optimal
performance-efficiency trade-off obtained via modality-aware compression rates.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The code will be made available at this link:
  https://github.com/umbertocappellazzo/AVSR-LLMs</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vista3D: Unravel the 3D Darkside of a Single Image <span class="chip">ECCV'2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12193v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12193v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiuhong Shen, Xingyi Yang, Michael Bi Mi, Xinchao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We embark on the age-old quest: unveiling the hidden dimensions of objects
from mere glimpses of their visible parts. To address this, we present Vista3D,
a framework that realizes swift and consistent 3D generation within a mere 5
minutes. At the heart of Vista3D lies a two-phase approach: the coarse phase
and the fine phase. In the coarse phase, we rapidly generate initial geometry
with Gaussian Splatting from a single image. In the fine phase, we extract a
Signed Distance Function (SDF) directly from learned Gaussian Splatting,
optimizing it with a differentiable isosurface representation. Furthermore, it
elevates the quality of generation by using a disentangled representation with
two independent implicit functions to capture both visible and obscured aspects
of objects. Additionally, it harmonizes gradients from 2D diffusion prior with
3D-aware diffusion priors by angular diffusion prior composition. Through
extensive evaluation, we demonstrate that Vista3D effectively sustains a
balance between the consistency and diversity of the generated 3D objects.
Demos and code will be available at https://github.com/florinshen/Vista3D.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV'2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MoRAG -- Multi-Fusion Retrieval Augmented Generation for Human Motion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12140v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12140v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kalakonda Sai Shashank, Shubh Maheshwari, Ravi Kiran Sarvadevabhatla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce MoRAG, a novel multi-part fusion based retrieval-augmented
generation strategy for text-based human motion generation. The method enhances
motion diffusion models by leveraging additional knowledge obtained through an
improved motion retrieval process. By effectively prompting large language
models (LLMs), we address spelling errors and rephrasing issues in motion
retrieval. Our approach utilizes a multi-part retrieval strategy to improve the
generalizability of motion retrieval across the language space. We create
diverse samples through the spatial composition of the retrieved motions.
Furthermore, by utilizing low-level, part-specific motion information, we can
construct motion samples for unseen text descriptions. Our experiments
demonstrate that our framework can serve as a plug-and-play module, improving
the performance of motion diffusion models. Code, pretrained models and sample
videos will be made available at: https://motion-rag.github.io/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Low-Resolution Face Recognition via Bridge Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11786v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11786v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiming Ge, Shengwei Zhao, Chenyu Li, Yu Zhang, Jia Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Face recognition in the wild is now advancing towards light-weight models,
fast inference speed and resolution-adapted capability. In this paper, we
propose a bridge distillation approach to turn a complex face model pretrained
on private high-resolution faces into a light-weight one for low-resolution
face recognition. In our approach, such a cross-dataset resolution-adapted
knowledge transfer problem is solved via two-step distillation. In the first
step, we conduct cross-dataset distillation to transfer the prior knowledge
from private high-resolution faces to public high-resolution faces and generate
compact and discriminative features. In the second step, the resolution-adapted
distillation is conducted to further transfer the prior knowledge to synthetic
low-resolution faces via multi-task learning. By learning low-resolution face
representations and mimicking the adapted high-resolution knowledge, a
light-weight student model can be constructed with high efficiency and
promising accuracy in recognizing low-resolution faces. Experimental results
show that the student model performs impressively in recognizing low-resolution
faces with only 0.21M parameters and 0.057MB memory. Meanwhile, its speed
reaches up to 14,705, ~934 and 763 faces per second on GPU, CPU and mobile
phone, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is published in IEEE TIP 2020</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DETECLAP: Enhancing Audio-Visual Representation Learning with Object
  Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shota Nakada, Taichi Nishimura, Hokuto Munakata, Masayoshi Kondo, Tatsuya Komatsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current audio-visual representation learning can capture rough object
categories (e.g., ``animals'' and ``instruments''), but it lacks the ability to
recognize fine-grained details, such as specific categories like ``dogs'' and
``flutes'' within animals and instruments. To address this issue, we introduce
DETECLAP, a method to enhance audio-visual representation learning with object
information. Our key idea is to introduce an audio-visual label prediction loss
to the existing Contrastive Audio-Visual Masked AutoEncoder to enhance its
object awareness. To avoid costly manual annotations, we prepare object labels
from both audio and visual inputs using state-of-the-art language-audio models
and object detectors. We evaluate the method of audio-visual retrieval and
classification using the VGGSound and AudioSet20K datasets. Our method achieves
improvements in recall@10 of +1.5% and +1.2% for audio-to-visual and
visual-to-audio retrieval, respectively, and an improvement in accuracy of
+0.6% for audio-visual classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init
  Attention <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.16199v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.16199v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Yu Qiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present LLaMA-Adapter, a lightweight adaption method to efficiently
fine-tune LLaMA into an instruction-following model. Using 52K self-instruct
demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon
the frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8
A100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and
prepend them to the word tokens at higher transformer layers. Then, a
zero-initialized attention mechanism with zero gating is proposed, which
adaptively injects the new instructional cues into LLaMA, while effectively
preserves its pre-trained knowledge. With our efficient training, LLaMA-Adapter
can generate high-quality responses, comparable to Alpaca with fully fine-tuned
7B parameters. Besides language commands, our approach can be simply extended
to multi-modal instructions for learning image-conditioned LLaMA model, which
achieves superior reasoning performance on ScienceQA and COCO Caption
benchmarks. Furthermore, we also evaluate the zero-initialized attention
mechanism for fine-tuning other pre-trained models (ViT, RoBERTa) on
traditional vision and language tasks, demonstrating the superior
generalization capacity of our approach. Code is released at
https://github.com/OpenGVLab/LLaMA-Adapter.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2024. Code is available at
  https://github.com/OpenGVLab/LLaMA-Adapter</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Performance Evaluation of Associative Watermarking Using Statistical
  Neurodynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.05508v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.05508v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryoto Kanegae, Masaki Kawamura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We theoretically evaluated the performance of our proposed associative
watermarking method in which the watermark is not embedded directly into the
image. We previously proposed a watermarking method that extends the
zero-watermarking model by applying associative memory models. In this model,
the hetero-associative memory model is introduced to the mapping process
between image features and watermarks, and the auto-associative memory model is
applied to correct watermark errors. We herein show that the associative
watermarking model outperforms the zero-watermarking model through computer
simulations using actual images. In this paper, we describe how we derive the
macroscopic state equation for the associative watermarking model using the
Okada theory. The theoretical results obtained by the fourth-order theory were
in good agreement with those obtained by computer simulations. Furthermore, the
performance of the associative watermarking model was evaluated using the bit
error rate of the watermark, both theoretically and using computer simulations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-modal Misinformation Detection: Approaches, Challenges and
  Opportunities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13883v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13883v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sara Abdali, Sina shaham, Bhaskar Krishnamachari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As social media platforms are evolving from text-based forums into
multi-modal environments, the nature of misinformation in social media is also
transforming accordingly. Taking advantage of the fact that visual modalities
such as images and videos are more favorable and attractive to the users and
textual contents are sometimes skimmed carelessly, misinformation spreaders
have recently targeted contextual connections between the modalities e.g., text
and image. Hence many researchers have developed automatic techniques for
detecting possible cross-modal discordance in web-based content. We analyze,
categorize and identify existing approaches in addition to challenges and
shortcomings they face in order to unearth new research opportunities in the
field of multi-modal misinformation detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Official-NV: An LLM-Generated News Video <span class="highlight-title">Dataset</span> for Multimodal Fake
  News Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19493v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19493v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihao Wang, Lizhi Chen, Zhong Qian, Peifeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  News media, especially video news media, have penetrated into every aspect of
daily life, which also brings the risk of fake news. Therefore, multimodal fake
news detection has recently garnered increased attention. However, the existing
datasets are comprised of user-uploaded videos and contain an excess amounts of
superfluous data, which introduces noise into the model training process. To
address this issue, we construct a dataset named Official-NV, comprising
officially published news videos. The crawl officially published videos are
augmented through the use of LLMs-based generation and manual verification,
thereby expanding the dataset. Furthermore, the proposed dataset is benchmarked
against several baselines to demonstrate its effectiveness in multimodal news
detection.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2024-09-26T05:27:08.683245643Z">
            2024-09-26 05:27:08 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
