{"2024-09-25T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2409.17146v1","updated":"2024-09-25T17:59:51Z","published":"2024-09-25T17:59:51Z","title":"Molmo and PixMo: Open Weights and Open Data for State-of-the-Art\n  Multimodal Models","summary":"  Today's most advanced multimodal models remain proprietary. The strongest\nopen-weight models rely heavily on synthetic data from proprietary VLMs to\nachieve good performance, effectively distilling these closed models into open\nones. As a result, the community is still missing foundational knowledge about\nhow to build performant VLMs from scratch. We present Molmo, a new family of\nVLMs that are state-of-the-art in their class of openness. Our key innovation\nis a novel, highly detailed image caption dataset collected entirely from human\nannotators using speech-based descriptions. To enable a wide array of user\ninteractions, we also introduce a diverse dataset mixture for fine-tuning that\nincludes in-the-wild Q&A and innovative 2D pointing data. The success of our\napproach relies on careful choices for the model architecture details, a\nwell-tuned training pipeline, and, most critically, the quality of our newly\ncollected datasets, all of which will be released. The best-in-class 72B model\nwithin the Molmo family not only outperforms others in the class of open weight\nand data models but also compares favorably against proprietary systems like\nGPT-4o, Claude 3.5, and Gemini 1.5 on both academic benchmarks and human\nevaluation.\n  We will be releasing all of our model weights, captioning and fine-tuning\ndata, and source code in the near future. Select model weights, inference code,\nand demo are available at https://molmo.allenai.org.\n","authors":["Matt Deitke","Christopher Clark","Sangho Lee","Rohun Tripathi","Yue Yang","Jae Sung Park","Mohammadreza Salehi","Niklas Muennighoff","Kyle Lo","Luca Soldaini","Jiasen Lu","Taira Anderson","Erin Bransom","Kiana Ehsani","Huong Ngo","YenSung Chen","Ajay Patel","Mark Yatskar","Chris Callison-Burch","Andrew Head","Rose Hendrix","Favyen Bastani","Eli VanderBilt","Nathan Lambert","Yvonne Chou","Arnavi Chheda","Jenna Sparks","Sam Skjonsberg","Michael Schmitz","Aaron Sarnat","Byron Bischoff","Pete Walsh","Chris Newell","Piper Wolters","Tanmay Gupta","Kuo-Hao Zeng","Jon Borchardt","Dirk Groeneveld","Jen Dumas","Crystal Nam","Sophie Lebrecht","Caitlin Wittlif","Carissa Schoenick","Oscar Michel","Ranjay Krishna","Luca Weihs","Noah A. Smith","Hannaneh Hajishirzi","Ross Girshick","Ali Farhadi","Aniruddha Kembhavi"],"pdf_url":"https://arxiv.org/pdf/2409.17146v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17141v1","updated":"2024-09-25T17:58:35Z","published":"2024-09-25T17:58:35Z","title":"FineZip : Pushing the Limits of Large Language Models for Practical\n  Lossless Text Compression","summary":"  While the language modeling objective has been shown to be deeply connected\nwith compression, it is surprising that modern LLMs are not employed in\npractical text compression systems. In this paper, we provide an in-depth\nanalysis of neural network and transformer-based compression techniques to\nanswer this question. We compare traditional text compression systems with\nneural network and LLM-based text compression methods. Although LLM-based\nsystems significantly outperform conventional compression methods, they are\nhighly impractical. Specifically, LLMZip, a recent text compression system\nusing Llama3-8B requires 9.5 days to compress just 10 MB of text, although with\nhuge improvements in compression ratios. To overcome this, we present FineZip -\na novel LLM-based text compression system that combines ideas of online\nmemorization and dynamic context to reduce the compression time immensely.\nFineZip can compress the above corpus in approximately 4 hours compared to 9.5\ndays, a 54 times improvement over LLMZip and comparable performance. FineZip\noutperforms traditional algorithmic compression methods with a large margin,\nimproving compression ratios by approximately 50\\%. With this work, we take the\nfirst step towards making lossless text compression with LLMs a reality. While\nFineZip presents a significant step in that direction, LLMs are still not a\nviable solution for large-scale text compression. We hope our work paves the\nway for future research and innovation to solve this problem.\n","authors":["Fazal Mittu","Yihuan Bu","Akshat Gupta","Ashok Devireddy","Alp Eren Ozdarendeli","Anant Singh","Gopala Anumanchipalli"],"pdf_url":"https://arxiv.org/pdf/2409.17141v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.16201v2","updated":"2024-09-25T17:58:21Z","published":"2023-11-27T07:19:26Z","title":"Pre-trained Language Models Do Not Help Auto-regressive Text-to-Image\n  Generation","summary":"  Recent advances in image tokenizers, such as VQ-VAE, have enabled\ntext-to-image generation using auto-regressive methods, similar to language\nmodeling. However, these methods have yet to leverage pre-trained language\nmodels, despite their adaptability to various downstream tasks. In this work,\nwe explore this gap by adapting a pre-trained language model for\nauto-regressive text-to-image generation, and find that pre-trained language\nmodels offer limited help. We provide a two-fold explanation by analyzing\ntokens from each modality. First, we demonstrate that image tokens possess\nsignificantly different semantics compared to text tokens, rendering\npre-trained language models no more effective in modeling them than randomly\ninitialized ones. Second, the text tokens in the image-text datasets are too\nsimple compared to normal language model pre-training data, which causes the\ncatastrophic degradation of language models' capability.\n","authors":["Yuhui Zhang","Brandon McKinzie","Zhe Gan","Vaishaal Shankar","Alexander Toshev"],"pdf_url":"https://arxiv.org/pdf/2311.16201v2.pdf","comment":"Published at EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2409.17130v1","updated":"2024-09-25T17:48:59Z","published":"2024-09-25T17:48:59Z","title":"Assessing the Level of Toxicity Against Distinct Groups in Bangla Social\n  Media Comments: A Comprehensive Investigation","summary":"  Social media platforms have a vital role in the modern world, serving as\nconduits for communication, the exchange of ideas, and the establishment of\nnetworks. However, the misuse of these platforms through toxic comments, which\ncan range from offensive remarks to hate speech, is a concerning issue. This\nstudy focuses on identifying toxic comments in the Bengali language targeting\nthree specific groups: transgender people, indigenous people, and migrant\npeople, from multiple social media sources. The study delves into the intricate\nprocess of identifying and categorizing toxic language while considering the\nvarying degrees of toxicity: high, medium, and low. The methodology involves\ncreating a dataset, manual annotation, and employing pre-trained transformer\nmodels like Bangla-BERT, bangla-bert-base, distil-BERT, and\nBert-base-multilingual-cased for classification. Diverse assessment metrics\nsuch as accuracy, recall, precision, and F1-score are employed to evaluate the\nmodel's effectiveness. The experimental findings reveal that Bangla-BERT\nsurpasses alternative models, achieving an F1-score of 0.8903. This research\nexposes the complexity of toxicity in Bangla social media dialogues, revealing\nits differing impacts on diverse demographic groups.\n","authors":["Mukaffi Bin Moin","Pronay Debnath","Usafa Akther Rifa","Rijeet Bin Anis"],"pdf_url":"https://arxiv.org/pdf/2409.17130v1.pdf","comment":"Accepted for publication in \"18th International Conference on\n  Information Technology and Applications (ICITA 2024)\""},{"id":"http://arxiv.org/abs/2409.17120v1","updated":"2024-09-25T17:31:45Z","published":"2024-09-25T17:31:45Z","title":"Deep Learning and Machine Learning, Advancing Big Data Analytics and\n  Management: Handy Appetizer","summary":"  This book explores the role of Artificial Intelligence (AI), Machine Learning\n(ML), and Deep Learning (DL) in driving the progress of big data analytics and\nmanagement. The book focuses on simplifying the complex mathematical concepts\nbehind deep learning, offering intuitive visualizations and practical case\nstudies to help readers understand how neural networks and technologies like\nConvolutional Neural Networks (CNNs) work. It introduces several classic models\nand technologies such as Transformers, GPT, ResNet, BERT, and YOLO,\nhighlighting their applications in fields like natural language processing,\nimage recognition, and autonomous driving. The book also emphasizes the\nimportance of pre-trained models and how they can enhance model performance and\naccuracy, with instructions on how to apply these models in various real-world\nscenarios. Additionally, it provides an overview of key big data management\ntechnologies like SQL and NoSQL databases, as well as distributed computing\nframeworks such as Apache Hadoop and Spark, explaining their importance in\nmanaging and processing vast amounts of data. Ultimately, the book underscores\nthe value of mastering deep learning and big data management skills as critical\ntools for the future workforce, making it an essential resource for both\nbeginners and experienced professionals.\n","authors":["Benji Peng","Xuanhe Pan","Yizhu Wen","Ziqian Bi","Keyu Chen","Ming Li","Ming Liu","Qian Niu","Junyu Liu","Jinlang Wang","Sen Zhang","Jiawei Xu","Pohsun Feng"],"pdf_url":"https://arxiv.org/pdf/2409.17120v1.pdf","comment":"This book contains 93 pages and 60 figures"},{"id":"http://arxiv.org/abs/2409.17115v1","updated":"2024-09-25T17:28:13Z","published":"2024-09-25T17:28:13Z","title":"Programming Every Example: Lifting Pre-training Data Quality like\n  Experts at Scale","summary":"  Large language model pre-training has traditionally relied on human experts\nto craft heuristics for improving the corpora quality, resulting in numerous\nrules developed to date. However, these rules lack the flexibility to address\nthe unique characteristics of individual example effectively. Meanwhile,\napplying tailored rules to every example is impractical for human experts. In\nthis paper, we demonstrate that even small language models, with as few as 0.3B\nparameters, can exhibit substantial data refining capabilities comparable to\nthose of human experts. We introduce Programming Every Example (ProX), a novel\nframework that treats data refinement as a programming task, enabling models to\nrefine corpora by generating and executing fine-grained operations, such as\nstring normalization, for each individual example at scale. Experimental\nresults show that models pre-trained on ProX-curated data outperform either\noriginal data or data filtered by other selection methods by more than 2%\nacross various downstream benchmarks. Its effectiveness spans various model\nsizes and pre-training corpora, including C4, RedPajama-V2, and FineWeb.\nFurthermore, ProX exhibits significant potential in domain-specific continual\npre-training: without domain specific design, models trained on OpenWebMath\nrefined by ProX outperform human-crafted rule-based methods, improving average\naccuracy by 7.6% over Mistral-7B, with 14.6% for Llama-2-7B and 20.3% for\nCodeLlama-7B, all within 10B tokens to be comparable to models like Llemma-7B\ntrained on 200B tokens. Further analysis highlights that ProX significantly\nsaves training FLOPs, offering a promising path for efficient LLM\npre-training.We are open-sourcing ProX with >100B corpus, models, and sharing\nall training and implementation details for reproducible research and future\ninnovation. Code: https://github.com/GAIR-NLP/ProX\n","authors":["Fan Zhou","Zengzhi Wang","Qian Liu","Junlong Li","Pengfei Liu"],"pdf_url":"https://arxiv.org/pdf/2409.17115v1.pdf","comment":"45 pages, 13 figures, 34 tables"},{"id":"http://arxiv.org/abs/2309.17012v3","updated":"2024-09-25T16:57:20Z","published":"2023-09-29T06:53:10Z","title":"Benchmarking Cognitive Biases in Large Language Models as Evaluators","summary":"  Large Language Models are cognitively biased judges. Large Language Models\n(LLMs) have recently been shown to be effective as automatic evaluators with\nsimple prompting and in-context learning. In this work, we assemble 15 LLMs of\nfour different size ranges and evaluate their output responses by preference\nranking from the other LLMs as evaluators, such as System Star is better than\nSystem Square. We then evaluate the quality of ranking outputs introducing the\nCognitive Bias Benchmark for LLMs as Evaluators (CoBBLEr), a benchmark to\nmeasure six different cognitive biases in LLM evaluation outputs, such as the\nEgocentric bias where a model prefers to rank its own outputs highly in\nevaluation. We find that LLMs are biased text quality evaluators, exhibiting\nstrong indications on our bias benchmark (average of 40% of comparisons across\nall models) within each of their evaluations that question their robustness as\nevaluators. Furthermore, we examine the correlation between human and machine\npreferences and calculate the average Rank-Biased Overlap (RBO) score to be\n49.6%, indicating that machine preferences are misaligned with humans.\nAccording to our findings, LLMs may still be unable to be utilized for\nautomatic annotation aligned with human preferences. Our project page is at:\nhttps://minnesotanlp.github.io/cobbler.\n","authors":["Ryan Koo","Minhwa Lee","Vipul Raheja","Jong Inn Park","Zae Myung Kim","Dongyeop Kang"],"pdf_url":"https://arxiv.org/pdf/2309.17012v3.pdf","comment":"Publishsed at ACL 2024. 29 pages, 9 figures, 14 tables"},{"id":"http://arxiv.org/abs/2409.17080v1","updated":"2024-09-25T16:45:02Z","published":"2024-09-25T16:45:02Z","title":"Can Vision Language Models Learn from Visual Demonstrations of Ambiguous\n  Spatial Reasoning?","summary":"  Large vision-language models (VLMs) have become state-of-the-art for many\ncomputer vision tasks, with in-context learning (ICL) as a popular adaptation\nstrategy for new ones. But can VLMs learn novel concepts purely from visual\ndemonstrations, or are they limited to adapting to the output format of ICL\nexamples? We propose a new benchmark we call Spatial Visual Ambiguity Tasks\n(SVAT) that challenges state-of-the-art VLMs to learn new visuospatial tasks\nin-context. We find that VLMs fail to do this zero-shot, and sometimes continue\nto fail after finetuning. However, adding simpler data to the training by\ncurriculum learning leads to improved ICL performance.\n","authors":["Bowen Zhao","Leo Parker Dirac","Paulina Varshavskaya"],"pdf_url":"https://arxiv.org/pdf/2409.17080v1.pdf","comment":"13 pages, 4 figures. Code released at\n  https://github.com/groundlight/vlm-visual-demonstrations"},{"id":"http://arxiv.org/abs/2409.17073v1","updated":"2024-09-25T16:32:35Z","published":"2024-09-25T16:32:35Z","title":"Enhancing Post-Hoc Attributions in Long Document Comprehension via\n  Coarse Grained Answer Decomposition","summary":"  Accurately attributing answer text to its source document is crucial for\ndeveloping a reliable question-answering system. However, attribution for long\ndocuments remains largely unexplored. Post-hoc attribution systems are designed\nto map answer text back to the source document, yet the granularity of this\nmapping has not been addressed. Furthermore, a critical question arises: What\nprecisely should be attributed, with an emphasis on identifying the information\nunits within an answer that necessitate grounding? In this paper, we propose\nand investigate a novel approach to the factual decomposition of generated\nanswers for attribution, employing template-based in-context learning. To\naccomplish this, we utilize the question and integrate negative sampling during\nfew-shot in-context learning for decomposition. This approach enhances the\nsemantic understanding of both abstractive and extractive answers. We examine\nthe impact of answer decomposition by providing a thorough examination of\nvarious attribution approaches, ranging from retrieval-based techniques to\nLLM-based attributors.\n","authors":["Pritika Ramu","Koustava Goswami","Apoorv Saxena","Balaji Vasan Srinivavsan"],"pdf_url":"https://arxiv.org/pdf/2409.17073v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14829v2","updated":"2024-09-25T16:27:50Z","published":"2024-06-21T02:18:03Z","title":"Is This a Bad Table? A Closer Look at the Evaluation of Table Generation\n  from Text","summary":"  Understanding whether a generated table is of good quality is important to be\nable to use it in creating or editing documents using automatic methods. In\nthis work, we underline that existing measures for table quality evaluation\nfail to capture the overall semantics of the tables, and sometimes unfairly\npenalize good tables and reward bad ones. We propose TabEval, a novel table\nevaluation strategy that captures table semantics by first breaking down a\ntable into a list of natural language atomic statements and then compares them\nwith ground truth statements using entailment-based measures. To validate our\napproach, we curate a dataset comprising of text descriptions for 1,250 diverse\nWikipedia tables, covering a range of topics and structures, in contrast to the\nlimited scope of existing datasets. We compare TabEval with existing metrics\nusing unsupervised and supervised text-to-table generation methods,\ndemonstrating its stronger correlation with human judgments of table quality\nacross four datasets.\n","authors":["Pritika Ramu","Aparna Garimella","Sambaran Bandyopadhyay"],"pdf_url":"https://arxiv.org/pdf/2406.14829v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17054v1","updated":"2024-09-25T16:13:42Z","published":"2024-09-25T16:13:42Z","title":"Using LLM for Real-Time Transcription and Summarization of\n  Doctor-Patient Interactions into ePuskesmas in Indonesia","summary":"  One of the key issues contributing to inefficiency in Puskesmas is the\ntime-consuming nature of doctor-patient interactions. Doctors need to conduct\nthorough consultations, which include diagnosing the patient's condition,\nproviding treatment advice, and transcribing detailed notes into medical\nrecords. In regions with diverse linguistic backgrounds, doctors often have to\nask clarifying questions, further prolonging the process. While diagnosing is\nessential, transcription and summarization can often be automated using AI to\nimprove time efficiency and help doctors enhance care quality and enable early\ndiagnosis and intervention. This paper proposes a solution using a localized\nlarge language model (LLM) to transcribe, translate, and summarize\ndoctor-patient conversations. We utilize the Whisper model for transcription\nand GPT-3 to summarize them into the ePuskemas medical records format. This\nsystem is implemented as an add-on to an existing web browser extension,\nallowing doctors to fill out patient forms while talking. By leveraging this\nsolution for real-time transcription, translation, and summarization, doctors\ncan improve the turnaround time for patient care while enhancing the quality of\nrecords, which become more detailed and insightful for future visits. This\ninnovation addresses challenges like overcrowded facilities and the\nadministrative burden on healthcare providers in Indonesia. We believe this\nsolution will help doctors save time, provide better care, and produce more\naccurate medical records, representing a significant step toward modernizing\nhealthcare and ensuring patients receive timely, high-quality care, even in\nresource-constrained settings.\n","authors":["Azmul Asmar Irfan","Nur Ahmad Khatim","Mansur M. Arief"],"pdf_url":"https://arxiv.org/pdf/2409.17054v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17046v1","updated":"2024-09-25T15:59:58Z","published":"2024-09-25T15:59:58Z","title":"Detecting Temporal Ambiguity in Questions","summary":"  Detecting and answering ambiguous questions has been a challenging task in\nopen-domain question answering. Ambiguous questions have different answers\ndepending on their interpretation and can take diverse forms. Temporally\nambiguous questions are one of the most common types of such questions. In this\npaper, we introduce TEMPAMBIQA, a manually annotated temporally ambiguous QA\ndataset consisting of 8,162 open-domain questions derived from existing\ndatasets. Our annotations focus on capturing temporal ambiguity to study the\ntask of detecting temporally ambiguous questions. We propose a novel approach\nby using diverse search strategies based on disambiguated versions of the\nquestions. We also introduce and test non-search, competitive baselines for\ndetecting temporal ambiguity using zero-shot and few-shot approaches.\n","authors":["Bhawna Piryani","Abdelrahman Abdallah","Jamshid Mozafari","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2409.17046v1.pdf","comment":"Accepted at EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2409.17044v1","updated":"2024-09-25T15:54:29Z","published":"2024-09-25T15:54:29Z","title":"How to Connect Speech Foundation Models and Large Language Models? What\n  Matters and What Does Not","summary":"  The remarkable performance achieved by Large Language Models (LLM) has driven\nresearch efforts to leverage them for a wide range of tasks and input\nmodalities. In speech-to-text (S2T) tasks, the emerging solution consists of\nprojecting the output of the encoder of a Speech Foundational Model (SFM) into\nthe LLM embedding space through an adapter module. However, no work has yet\ninvestigated how much the downstream-task performance depends on each component\n(SFM, adapter, LLM) nor whether the best design of the adapter depends on the\nchosen SFM and LLM. To fill this gap, we evaluate the combination of 5 adapter\nmodules, 2 LLMs (Mistral and Llama), and 2 SFMs (Whisper and SeamlessM4T) on\ntwo widespread S2T tasks, namely Automatic Speech Recognition and Speech\nTranslation. Our results demonstrate that the SFM plays a pivotal role in\ndownstream performance, while the adapter choice has moderate impact and\ndepends on the SFM and LLM.\n","authors":["Francesco Verdini","Pierfrancesco Melucci","Stefano Perna","Francesco Cariaggi","Marco Gaido","Sara Papi","Szymon Mazurek","Marek Kasztelnik","Luisa Bentivogli","Sébastien Bratières","Paolo Merialdo","Simone Scardapane"],"pdf_url":"https://arxiv.org/pdf/2409.17044v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14507v3","updated":"2024-09-25T15:50:51Z","published":"2024-09-22T16:11:02Z","title":"A is for Absorption: Studying Feature Splitting and Absorption in Sparse\n  Autoencoders","summary":"  Sparse Autoencoders (SAEs) have emerged as a promising approach to decompose\nthe activations of Large Language Models (LLMs) into human-interpretable\nlatents. In this paper, we pose two questions. First, to what extent do SAEs\nextract monosemantic and interpretable latents? Second, to what extent does\nvarying the sparsity or the size of the SAE affect monosemanticity /\ninterpretability? By investigating these questions in the context of a simple\nfirst-letter identification task where we have complete access to ground truth\nlabels for all tokens in the vocabulary, we are able to provide more detail\nthan prior investigations. Critically, we identify a problematic form of\nfeature-splitting we call feature absorption where seemingly monosemantic\nlatents fail to fire in cases where they clearly should. Our investigation\nsuggests that varying SAE size or sparsity is insufficient to solve this issue,\nand that there are deeper conceptual issues in need of resolution.\n","authors":["David Chanin","James Wilken-Smith","Tomáš Dulka","Hardik Bhatnagar","Joseph Bloom"],"pdf_url":"https://arxiv.org/pdf/2409.14507v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17027v1","updated":"2024-09-25T15:30:24Z","published":"2024-09-25T15:30:24Z","title":"Counterfactual Token Generation in Large Language Models","summary":"  \"Sure, I am happy to generate a story for you: Captain Lyra stood at the helm\nof her trusty ship, the Maelstrom's Fury, gazing out at the endless sea. [...]\nLyra's eyes welled up with tears as she realized the bitter truth - she had\nsacrificed everything for fleeting riches, and lost the love of her crew, her\nfamily, and herself.\" Although this story, generated by a large language model,\nis captivating, one may wonder -- how would the story have unfolded if the\nmodel had chosen \"Captain Maeve\" as the protagonist instead? We cannot know.\nState-of-the-art large language models are stateless -- they maintain no\ninternal memory or state. Given a prompt, they generate a sequence of tokens as\nan output using an autoregressive process. As a consequence, they cannot reason\nabout counterfactual alternatives to tokens they have generated in the past. In\nthis work, our goal is to enhance them with this functionality. To this end, we\ndevelop a causal model of token generation that builds upon the Gumbel-Max\nstructural causal model. Our model allows any large language model to perform\ncounterfactual token generation at almost no cost in comparison with vanilla\ntoken generation, it is embarrassingly simple to implement, and it does not\nrequire any fine-tuning nor prompt engineering. We implement our model on Llama\n3 8B-instruct and conduct both qualitative and quantitative analyses of\ncounterfactually generated text. We conclude with a demonstrative application\nof counterfactual token generation for bias detection, unveiling interesting\ninsights about the model of the world constructed by large language models.\n","authors":["Ivi Chatzi","Nina Corvelo Benz","Eleni Straitouri","Stratis Tsirtsis","Manuel Gomez-Rodriguez"],"pdf_url":"https://arxiv.org/pdf/2409.17027v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17011v1","updated":"2024-09-25T15:15:57Z","published":"2024-09-25T15:15:57Z","title":"LLM-CARD: Towards a Description and Landscape of Large Language Models","summary":"  With the rapid growth of the Natural Language Processing (NLP) field, a vast\nvariety of Large Language Models (LLMs) continue to emerge for diverse NLP\ntasks. As an increasing number of papers are presented, researchers and\ndevelopers face the challenge of information overload. Thus, it is particularly\nimportant to develop a system that can automatically extract and organise key\ninformation about LLMs from academic papers (\\textbf{LLM model card}). This\nwork is to develop such a pioneer system by using Named Entity Recognition\n(\\textbf{NER}) and Relation Extraction (\\textbf{RE}) methods that automatically\nextract key information about large language models from the papers, helping\nresearchers to efficiently access information about LLMs. These features\ninclude model \\textit{licence}, model \\textit{name}, and model\n\\textit{application}. With these features, we can form a model card for each\npaper. \\textbf{Data-contribution} wise, 106 academic papers were processed by\ndefining three dictionaries - LLMs name, licence, and application. 11,051\nsentences were extracted through dictionary lookup, and the dataset was\nconstructed through manual review of the final selection of 129 sentences that\nhave a link between the name and the licence, and 106 sentences that have a\nlink between the model name and the application.\n","authors":["Shengwei Tian","Lifeng Han","Erick Mendez Guzman","Goran Nenadic"],"pdf_url":"https://arxiv.org/pdf/2409.17011v1.pdf","comment":"ongoing work, 16 pages"},{"id":"http://arxiv.org/abs/2409.17005v1","updated":"2024-09-25T15:08:08Z","published":"2024-09-25T15:08:08Z","title":"Models Can and Should Embrace the Communicative Nature of\n  Human-Generated Math","summary":"  Math is constructed by people for people: just as natural language corpora\nreflect not just propositions but the communicative goals of language users,\nthe math data that models are trained on reflects not just idealized\nmathematical entities but rich communicative intentions. While there are\nimportant advantages to treating math in a purely symbolic manner, we here\nhypothesize that there are benefits to treating math as situated linguistic\ncommunication and that language models are well suited for this goal, in ways\nthat are not fully appreciated. We illustrate these points with two case\nstudies. First, we ran an experiment in which we found that language models\ninterpret the equals sign in a humanlike way -- generating systematically\ndifferent word problems for the same underlying equation arranged in different\nways. Second, we found that language models prefer proofs to be ordered in\nnaturalistic ways, even though other orders would be logically equivalent. We\nadvocate for AI systems that learn from and represent the communicative\nintentions latent in human-generated math.\n","authors":["Sasha Boguraev","Ben Lipkin","Leonie Weissweiler","Kyle Mahowald"],"pdf_url":"https://arxiv.org/pdf/2409.17005v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03589v3","updated":"2024-09-25T14:59:24Z","published":"2024-06-05T19:14:21Z","title":"Ranking Manipulation for Conversational Search Engines","summary":"  Major search engine providers are rapidly incorporating Large Language Model\n(LLM)-generated content in response to user queries. These conversational\nsearch engines operate by loading retrieved website text into the LLM context\nfor summarization and interpretation. Recent research demonstrates that LLMs\nare highly vulnerable to jailbreaking and prompt injection attacks, which\ndisrupt the safety and quality goals of LLMs using adversarial strings. This\nwork investigates the impact of prompt injections on the ranking order of\nsources referenced by conversational search engines. To this end, we introduce\na focused dataset of real-world consumer product websites and formalize\nconversational search ranking as an adversarial problem. Experimentally, we\nanalyze conversational search rankings in the absence of adversarial injections\nand show that different LLMs vary significantly in prioritizing product name,\ndocument content, and context position. We then present a tree-of-attacks-based\njailbreaking technique which reliably promotes low-ranked products.\nImportantly, these attacks transfer effectively to state-of-the-art\nconversational search engines such as perplexity$.$ai. Given the strong\nfinancial incentive for website owners to boost their search ranking, we argue\nthat our problem formulation is of critical importance for future robustness\nwork.\n","authors":["Samuel Pfrommer","Yatong Bai","Tanmay Gautam","Somayeh Sojoudi"],"pdf_url":"https://arxiv.org/pdf/2406.03589v3.pdf","comment":"2024 Conference on Empirical Methods in Natural Language Processing\n  (Main)"},{"id":"http://arxiv.org/abs/2409.16984v1","updated":"2024-09-25T14:45:52Z","published":"2024-09-25T14:45:52Z","title":"AXCEL: Automated eXplainable Consistency Evaluation using LLMs","summary":"  Large Language Models (LLMs) are widely used in both industry and academia\nfor various tasks, yet evaluating the consistency of generated text responses\ncontinues to be a challenge. Traditional metrics like ROUGE and BLEU show a\nweak correlation with human judgment. More sophisticated metrics using Natural\nLanguage Inference (NLI) have shown improved correlations but are complex to\nimplement, require domain-specific training due to poor cross-domain\ngeneralization, and lack explainability. More recently, prompt-based metrics\nusing LLMs as evaluators have emerged; while they are easier to implement, they\nstill lack explainability and depend on task-specific prompts, which limits\ntheir generalizability. This work introduces Automated eXplainable Consistency\nEvaluation using LLMs (AXCEL), a prompt-based consistency metric which offers\nexplanations for the consistency scores by providing detailed reasoning and\npinpointing inconsistent text spans. AXCEL is also a generalizable metric which\ncan be adopted to multiple tasks without changing the prompt. AXCEL outperforms\nboth non-prompt and prompt-based state-of-the-art (SOTA) metrics in detecting\ninconsistencies across summarization by 8.7%, free text generation by 6.2%, and\ndata-to-text conversion tasks by 29.4%. We also evaluate the influence of\nunderlying LLMs on prompt based metric performance and recalibrate the SOTA\nprompt-based metrics with the latest LLMs for fair comparison. Further, we show\nthat AXCEL demonstrates strong performance using open source LLMs.\n","authors":["P Aditya Sreekar","Sahil Verma","Suransh Chopra","Sarik Ghazarian","Abhishek Persad","Narayanan Sadagopan"],"pdf_url":"https://arxiv.org/pdf/2409.16984v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12997v5","updated":"2024-09-25T14:37:39Z","published":"2024-02-20T13:25:16Z","title":"Towards Trustworthy Reranking: A Simple yet Effective Abstention\n  Mechanism","summary":"  Neural Information Retrieval (NIR) has significantly improved upon\nheuristic-based Information Retrieval (IR) systems. Yet, failures remain\nfrequent, the models used often being unable to retrieve documents relevant to\nthe user's query. We address this challenge by proposing a lightweight\nabstention mechanism tailored for real-world constraints, with particular\nemphasis placed on the reranking phase. We introduce a protocol for evaluating\nabstention strategies in black-box scenarios (typically encountered when\nrelying on API services), demonstrating their efficacy, and propose a simple\nyet effective data-driven mechanism. We provide open-source code for experiment\nreplication and abstention implementation, fostering wider adoption and\napplication in diverse contexts.\n","authors":["Hippolyte Gisserot-Boukhlef","Manuel Faysse","Emmanuel Malherbe","Céline Hudelot","Pierre Colombo"],"pdf_url":"https://arxiv.org/pdf/2402.12997v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16974v1","updated":"2024-09-25T14:36:30Z","published":"2024-09-25T14:36:30Z","title":"Decoding Large-Language Models: A Systematic Overview of Socio-Technical\n  Impacts, Constraints, and Emerging Questions","summary":"  There have been rapid advancements in the capabilities of large language\nmodels (LLMs) in recent years, greatly revolutionizing the field of natural\nlanguage processing (NLP) and artificial intelligence (AI) to understand and\ninteract with human language. Therefore, in this work, we conduct a systematic\ninvestigation of the literature to identify the prominent themes and directions\nof LLM developments, impacts, and limitations. Our findings illustrate the\naims, methodologies, limitations, and future directions of LLM research. It\nincludes responsible development considerations, algorithmic improvements,\nethical challenges, and societal implications of LLM development. Overall, this\npaper provides a rigorous and comprehensive overview of current research in LLM\nand identifies potential directions for future development. The article\nhighlights the application areas that could have a positive impact on society\nalong with the ethical considerations.\n","authors":["Zeyneb N. Kaya","Souvick Ghosh"],"pdf_url":"https://arxiv.org/pdf/2409.16974v1.pdf","comment":"28 pages, 5 figures, preprint submitted to journal"},{"id":"http://arxiv.org/abs/2409.16973v1","updated":"2024-09-25T14:35:06Z","published":"2024-09-25T14:35:06Z","title":"Adaptive Self-Supervised Learning Strategies for Dynamic On-Device LLM\n  Personalization","summary":"  Large language models (LLMs) have revolutionized how we interact with\ntechnology, but their personalization to individual user preferences remains a\nsignificant challenge, particularly in on-device applications. Traditional\nmethods often depend heavily on labeled datasets and can be resource-intensive.\nTo address these issues, we present Adaptive Self-Supervised Learning\nStrategies (ASLS), which utilizes self-supervised learning techniques to\npersonalize LLMs dynamically. The framework comprises a user profiling layer\nfor collecting interaction data and a neural adaptation layer for real-time\nmodel fine-tuning. This innovative approach enables continuous learning from\nuser feedback, allowing the model to generate responses that align closely with\nuser-specific contexts. The adaptive mechanisms of ASLS minimize computational\ndemands and enhance personalization efficiency. Experimental results across\nvarious user scenarios illustrate the superior performance of ASLS in boosting\nuser engagement and satisfaction, highlighting its potential to redefine LLMs\nas highly responsive and context-aware systems on-device.\n","authors":["Rafael Mendoza","Isabella Cruz","Richard Liu","Aarav Deshmukh","David Williams","Jesscia Peng","Rohan Iyer"],"pdf_url":"https://arxiv.org/pdf/2409.16973v1.pdf","comment":"First ASLS"},{"id":"http://arxiv.org/abs/2409.16954v1","updated":"2024-09-25T14:09:09Z","published":"2024-09-25T14:09:09Z","title":"Weighted Cross-entropy for Low-Resource Languages in Multilingual Speech\n  Recognition","summary":"  This paper addresses the challenge of integrating low-resource languages into\nmultilingual automatic speech recognition (ASR) systems. We introduce a novel\napplication of weighted cross-entropy, typically used for unbalanced datasets,\nto facilitate the integration of low-resource languages into pre-trained\nmultilingual ASR models within the context of continual multilingual learning.\nWe fine-tune the Whisper multilingual ASR model on five high-resource languages\nand one low-resource language, employing language-weighted dynamic\ncross-entropy and data augmentation. The results show a remarkable 6.69% word\nerror rate (WER) reduction for the low-resource language compared to the\nfine-tuned model without applying our approach, and a 48.86% WER reduction\ncompared to the original Whisper model. In addition, our approach yields an\naverage WER reduction of 3.29% across the six languages, showing no degradation\nfor the high-resource languages.\n","authors":["Andrés Piñeiro-Martín","Carmen García-Mateo","Laura Docío-Fernández","María del Carmen López-Pérez","Georg Rehm"],"pdf_url":"https://arxiv.org/pdf/2409.16954v1.pdf","comment":"5 pages, 1 figure. Presented at Interspeech 2024"},{"id":"http://arxiv.org/abs/2305.12620v2","updated":"2024-09-25T14:06:31Z","published":"2023-05-22T01:02:45Z","title":"Keeping Up with the Language Models: Systematic Benchmark Extension for\n  Bias Auditing","summary":"  Bias auditing of language models (LMs) has received considerable attention as\nLMs are becoming widespread. As such, several benchmarks for bias auditing have\nbeen proposed. At the same time, the rapid evolution of LMs can make these\nbenchmarks irrelevant in no time. Bias auditing is further complicated by LM\nbrittleness: when a presumably biased outcome is observed, is it due to model\nbias or model brittleness? We propose enlisting the models themselves to help\nconstruct bias auditing datasets that remain challenging, and introduce bias\nmeasures that distinguish between different types of model errors. First, we\nextend an existing bias benchmark for NLI (BBNLI) using a combination of\nLM-generated lexical variations, adversarial filtering, and human validation.\nWe demonstrate that the newly created dataset BBNLI-next is more challenging\nthan BBNLI: on average, BBNLI-next reduces the accuracy of state-of-the-art NLI\nmodels from 95.3%, as observed by BBNLI, to a strikingly low 57.5%. Second, we\nemploy BBNLI-next to showcase the interplay between robustness and bias: we\npoint out shortcomings in current bias scores and propose bias measures that\ntake into account both bias and model brittleness. Third, despite the fact that\nBBNLI-next was designed with non-generative models in mind, we show that the\nnew dataset is also able to uncover bias in state-of-the-art open-source\ngenerative LMs.\n  Note: All datasets included in this work are in English and they address\nUS-centered social biases. In the spirit of efficient NLP research, no model\ntraining or fine-tuning was performed to conduct this research.\n  Warning: This paper contains offensive text examples.\n","authors":["Ioana Baldini","Chhavi Yadav","Manish Nagireddy","Payel Das","Kush R. Varshney"],"pdf_url":"https://arxiv.org/pdf/2305.12620v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.02135v2","updated":"2024-09-25T14:00:18Z","published":"2024-06-04T09:24:04Z","title":"Robust Interaction-Based Relevance Modeling for Online e-Commerce Search","summary":"  Semantic relevance calculation is crucial for e-commerce search engines, as\nit ensures that the items selected closely align with customer intent.\nInadequate attention to this aspect can detrimentally affect user experience\nand engagement. Traditional text-matching techniques are prevalent but often\nfail to capture the nuances of search intent accurately, so neural networks now\nhave become a preferred solution to processing such complex text matching.\nExisting methods predominantly employ representation-based architectures, which\nstrike a balance between high traffic capacity and low latency. However, they\nexhibit significant shortcomings in generalization and robustness when compared\nto interaction-based architectures. In this work, we introduce a robust\ninteraction-based modeling paradigm to address these shortcomings. It\nencompasses 1) a dynamic length representation scheme for expedited inference,\n2) a professional terms recognition method to identify subjects and core\nattributes from complex sentence structures, and 3) a contrastive adversarial\ntraining protocol to bolster the model's robustness and matching capabilities.\nExtensive offline evaluations demonstrate the superior robustness and\neffectiveness of our approach, and online A/B testing confirms its ability to\nimprove relevance in the same exposure position, resulting in more clicks and\nconversions. To the best of our knowledge, this method is the first\ninteraction-based approach for large e-commerce search relevance calculation.\nNotably, we have deployed it for the entire search traffic on alibaba.com, the\nlargest B2B e-commerce platform in the world.\n","authors":["Ben Chen","Huangyu Dai","Xiang Ma","Wen Jiang","Wei Ning"],"pdf_url":"https://arxiv.org/pdf/2406.02135v2.pdf","comment":"Accepted by ECML-PKDD'24 as Outstanding Paper. 8 pages, 2 figures, 7\n  tables"},{"id":"http://arxiv.org/abs/2409.16937v1","updated":"2024-09-25T13:51:19Z","published":"2024-09-25T13:51:19Z","title":"Semi-Supervised Cognitive State Classification from Speech with\n  Multi-View Pseudo-Labeling","summary":"  The lack of labeled data is a common challenge in speech classification\ntasks, particularly those requiring extensive subjective assessment, such as\ncognitive state classification. In this work, we propose a Semi-Supervised\nLearning (SSL) framework, introducing a novel multi-view pseudo-labeling method\nthat leverages both acoustic and linguistic characteristics to select the most\nconfident data for training the classification model. Acoustically, unlabeled\ndata are compared to labeled data using the Frechet audio distance, calculated\nfrom embeddings generated by multiple audio encoders. Linguistically, large\nlanguage models are prompted to revise automatic speech recognition\ntranscriptions and predict labels based on our proposed task-specific\nknowledge. High-confidence data are identified when pseudo-labels from both\nsources align, while mismatches are treated as low-confidence data. A bimodal\nclassifier is then trained to iteratively label the low-confidence data until a\npredefined criterion is met. We evaluate our SSL framework on emotion\nrecognition and dementia detection tasks. Experimental results demonstrate that\nour method achieves competitive performance compared to fully supervised\nlearning using only 30% of the labeled data and significantly outperforms two\nselected baselines.\n","authors":["Yuanchao Li","Zixing Zhang","Jing Han","Peter Bell","Catherine Lai"],"pdf_url":"https://arxiv.org/pdf/2409.16937v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16934v1","updated":"2024-09-25T13:45:23Z","published":"2024-09-25T13:45:23Z","title":"Investigating OCR-Sensitive Neurons to Improve Entity Recognition in\n  Historical Documents","summary":"  This paper investigates the presence of OCR-sensitive neurons within the\nTransformer architecture and their influence on named entity recognition (NER)\nperformance on historical documents. By analysing neuron activation patterns in\nresponse to clean and noisy text inputs, we identify and then neutralise\nOCR-sensitive neurons to improve model performance. Based on two open access\nlarge language models (Llama2 and Mistral), experiments demonstrate the\nexistence of OCR-sensitive regions and show improvements in NER performance on\nhistorical newspapers and classical commentaries, highlighting the potential of\ntargeted neuron modulation to improve models' performance on noisy text.\n","authors":["Emanuela Boros","Maud Ehrmann"],"pdf_url":"https://arxiv.org/pdf/2409.16934v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19280v3","updated":"2024-09-25T13:36:27Z","published":"2024-06-27T15:50:41Z","title":"HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into\n  Multimodal LLMs at Scale","summary":"  The rapid development of multimodal large language models (MLLMs), such as\nGPT-4V, has led to significant advancements. However, these models still face\nchallenges in medical multimodal capabilities due to limitations in the\nquantity and quality of medical vision-text data, stemming from data privacy\nconcerns and high annotation costs. While pioneering approaches utilize\nPubMed's large-scale, de-identified medical image-text pairs to address these\nlimitations, they still fall short due to inherent data noise. To tackle this,\nwe refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in\nan 'unblinded' capacity to denoise and reformat the data, resulting in the\ncreation of the PubMedVision dataset with 1.3 million medical VQA samples. Our\nvalidation demonstrates that: (1) PubMedVision can significantly enhance the\nmedical multimodal capabilities of current MLLMs, showing significant\nimprovement in benchmarks including the MMMU Health & Medicine track; (2)\nmanual checks by medical experts and empirical results validate the superior\ndata quality of our dataset compared to other data construction methods. Using\nPubMedVision, we train a 34B medical MLLM HuatuoGPT-Vision, which shows\nsuperior performance in medical multimodal scenarios among open-source MLLMs.\n","authors":["Junying Chen","Chi Gui","Ruyi Ouyang","Anningzhe Gao","Shunian Chen","Guiming Hardy Chen","Xidong Wang","Ruifei Zhang","Zhenyang Cai","Ke Ji","Guangjun Yu","Xiang Wan","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2406.19280v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16920v1","updated":"2024-09-25T13:27:17Z","published":"2024-09-25T13:27:17Z","title":"Cross-lingual Speech Emotion Recognition: Humans vs. Self-Supervised\n  Models","summary":"  Utilizing Self-Supervised Learning (SSL) models for Speech Emotion\nRecognition (SER) has proven effective, yet limited research has explored\ncross-lingual scenarios. This study presents a comparative analysis between\nhuman performance and SSL models, beginning with a layer-wise analysis and an\nexploration of parameter-efficient fine-tuning strategies in monolingual,\ncross-lingual, and transfer learning contexts. We further compare the SER\nability of models and humans at both utterance- and segment-levels.\nAdditionally, we investigate the impact of dialect on cross-lingual SER through\nhuman evaluation. Our findings reveal that models, with appropriate knowledge\ntransfer, can adapt to the target language and achieve performance comparable\nto native speakers. We also demonstrate the significant effect of dialect on\nSER for individuals without prior linguistic and paralinguistic background.\nMoreover, both humans and models exhibit distinct behaviors across different\nemotions. These results offer new insights into the cross-lingual SER\ncapabilities of SSL models, underscoring both their similarities to and\ndifferences from human emotion perception.\n","authors":["Zhichen Han","Tianqi Geng","Hui Feng","Jiahong Yuan","Korin Richmond","Yuanchao Li"],"pdf_url":"https://arxiv.org/pdf/2409.16920v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16914v1","updated":"2024-09-25T13:18:57Z","published":"2024-09-25T13:18:57Z","title":"Zero-Shot Detection of LLM-Generated Text using Token Cohesiveness","summary":"  The increasing capability and widespread usage of large language models\n(LLMs) highlight the desirability of automatic detection of LLM-generated text.\nZero-shot detectors, due to their training-free nature, have received\nconsiderable attention and notable success. In this paper, we identify a new\nfeature, token cohesiveness, that is useful for zero-shot detection, and we\ndemonstrate that LLM-generated text tends to exhibit higher token cohesiveness\nthan human-written text. Based on this observation, we devise TOCSIN, a generic\ndual-channel detection paradigm that uses token cohesiveness as a plug-and-play\nmodule to improve existing zero-shot detectors. To calculate token\ncohesiveness, TOCSIN only requires a few rounds of random token deletion and\nsemantic difference measurement, making it particularly suitable for a\npractical black-box setting where the source model used for generation is not\naccessible. Extensive experiments with four state-of-the-art base detectors on\nvarious datasets, source models, and evaluation settings demonstrate the\neffectiveness and generality of the proposed approach. Code available at:\n\\url{https://github.com/Shixuan-Ma/TOCSIN}.\n","authors":["Shixuan Ma","Quan Wang"],"pdf_url":"https://arxiv.org/pdf/2409.16914v1.pdf","comment":"To appear at the main conference of EMNLP 2024"},{"id":"http://arxiv.org/abs/2409.16911v1","updated":"2024-09-25T13:15:50Z","published":"2024-09-25T13:15:50Z","title":"Pruning Multilingual Large Language Models for Multilingual Inference","summary":"  Multilingual large language models (MLLMs), trained on multilingual balanced\ndata, demonstrate better zero-shot learning performance in non-English\nlanguages compared to large language models trained on English-dominant data.\nHowever, the disparity in performance between English and non-English languages\nremains a challenge yet to be fully addressed. A distinctive characteristic of\nMLLMs is their high-quality translation capabilities, indicating an acquired\nproficiency in aligning between languages. This study explores how to enhance\nthe zero-shot performance of MLLMs in non-English languages by leveraging their\nalignment capability between English and non-English languages. To achieve\nthis, we first analyze the behavior of MLLMs when performing translation and\nreveal that there are large magnitude features that play a critical role in the\ntranslation process. Inspired by these findings, we retain the weights\nassociated with operations involving the large magnitude features and prune\nother weights to force MLLMs to rely on these features for tasks beyond\ntranslation. We empirically demonstrate that this pruning strategy can enhance\nthe MLLMs' performance in non-English language.\n","authors":["Hwichan Kim","Jun Suzuki","Tosho Hirasawa","Mamoru Komachi"],"pdf_url":"https://arxiv.org/pdf/2409.16911v1.pdf","comment":"Accepted at EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2409.16909v1","updated":"2024-09-25T13:13:21Z","published":"2024-09-25T13:13:21Z","title":"Enhancing Temporal Sensitivity and Reasoning for Time-Sensitive Question\n  Answering","summary":"  Time-Sensitive Question Answering (TSQA) demands the effective utilization of\nspecific temporal contexts, encompassing multiple time-evolving facts, to\naddress time-sensitive questions. This necessitates not only the parsing of\ntemporal information within questions but also the identification and\nunderstanding of time-evolving facts to generate accurate answers. However,\ncurrent large language models still have limited sensitivity to temporal\ninformation and their inadequate temporal reasoning capabilities.In this paper,\nwe propose a novel framework that enhances temporal awareness and reasoning\nthrough Temporal Information-Aware Embedding and Granular Contrastive\nReinforcement Learning. Experimental results on four TSQA datasets demonstrate\nthat our framework significantly outperforms existing LLMs in TSQA tasks,\nmarking a step forward in bridging the performance gap between machine and\nhuman temporal understanding and reasoning.\n","authors":["Wanqi Yang","Yanda Li","Meng Fang","Ling Chen"],"pdf_url":"https://arxiv.org/pdf/2409.16909v1.pdf","comment":"Accepted by EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2409.16900v1","updated":"2024-09-25T13:09:23Z","published":"2024-09-25T13:09:23Z","title":"A Roadmap for Embodied and Social Grounding in LLMs","summary":"  The fusion of Large Language Models (LLMs) and robotic systems has led to a\ntransformative paradigm in the robotic field, offering unparalleled\ncapabilities not only in the communication domain but also in skills like\nmultimodal input handling, high-level reasoning, and plan generation. The\ngrounding of LLMs knowledge into the empirical world has been considered a\ncrucial pathway to exploit the efficiency of LLMs in robotics. Nevertheless,\nconnecting LLMs' representations to the external world with multimodal\napproaches or with robots' bodies is not enough to let them understand the\nmeaning of the language they are manipulating. Taking inspiration from humans,\nthis work draws attention to three necessary elements for an agent to grasp and\nexperience the world. The roadmap for LLMs grounding is envisaged in an active\nbodily system as the reference point for experiencing the environment, a\ntemporally structured experience for a coherent, self-related interaction with\nthe external world, and social skills to acquire a common-grounded shared\nexperience.\n","authors":["Sara Incao","Carlo Mazzola","Giulia Belgiovine","Alessandra Sciutti"],"pdf_url":"https://arxiv.org/pdf/2409.16900v1.pdf","comment":"Accepted Version of a conference paper presented at Robophilosophy\n  Conference 2024"},{"id":"http://arxiv.org/abs/2409.16899v1","updated":"2024-09-25T13:08:43Z","published":"2024-09-25T13:08:43Z","title":"Robotic Backchanneling in Online Conversation Facilitation: A\n  Cross-Generational Study","summary":"  Japan faces many challenges related to its aging society, including\nincreasing rates of cognitive decline in the population and a shortage of\ncaregivers. Efforts have begun to explore solutions using artificial\nintelligence (AI), especially socially embodied intelligent agents and robots\nthat can communicate with people. Yet, there has been little research on the\ncompatibility of these agents with older adults in various everyday situations.\nTo this end, we conducted a user study to evaluate a robot that functions as a\nfacilitator for a group conversation protocol designed to prevent cognitive\ndecline. We modified the robot to use backchannelling, a natural human way of\nspeaking, to increase receptiveness of the robot and enjoyment of the group\nconversation experience. We conducted a cross-generational study with young\nadults and older adults. Qualitative analyses indicated that younger adults\nperceived the backchannelling version of the robot as kinder, more trustworthy,\nand more acceptable than the non-backchannelling robot. Finally, we found that\nthe robot's backchannelling elicited nonverbal backchanneling in older\nparticipants.\n","authors":["Sota Kobuki","Katie Seaborn","Seiki Tokunaga","Kosuke Fukumori","Shun Hidaka","Kazuhiro Tamura","Koji Inoue","Tatsuya Kawahara","Mihoko Otake-Mastuura"],"pdf_url":"https://arxiv.org/pdf/2409.16899v1.pdf","comment":"Published at Proceedings of the 2023 32nd IEEE International\n  Conference on Robot and Human Interactive Communication (RO-MAN 2023)"},{"id":"http://arxiv.org/abs/2402.04854v7","updated":"2024-09-25T12:57:03Z","published":"2024-02-07T13:54:06Z","title":"Hierarchical Tree-structured Knowledge Graph For Academic Insight Survey","summary":"  Research surveys have always posed a challenge for beginner researchers who\nlack of research training. These researchers struggle to understand the\ndirections within their research topic, and the discovery of new research\nfindings within a short time. One way to provide intuitive assistance to\nbeginner researchers is by offering relevant knowledge graphs(KG) and\nrecommending related academic papers. However, existing navigation knowledge\ngraphs primarily rely on keywords in the research field and often fail to\npresent the logical hierarchy among multiple related papers clearly. Moreover,\nmost recommendation systems for academic papers simply rely on high text\nsimilarity, which can leave researchers confused as to why a particular article\nis being recommended. They may lack of grasp important information about the\ninsight connection between \"Issue resolved\" and \"Issue finding\" that they hope\nto obtain. To address these issues, this study aims to support research insight\nsurveys for beginner researchers by establishing a hierarchical tree-structured\nknowledge graph that reflects the inheritance insight of research topics and\nthe relevance insight among the academic papers.\n","authors":["Jinghong Li","Huy Phan","Wen Gu","Koichi Ota","Shinobu Hasegawa"],"pdf_url":"https://arxiv.org/pdf/2402.04854v7.pdf","comment":"This paper has been published by 'The 18TH International Conference\n  on INnovations in Intelligent SysTems and Applications (INISTA 2024)'"},{"id":"http://arxiv.org/abs/2409.16884v1","updated":"2024-09-25T12:52:21Z","published":"2024-09-25T12:52:21Z","title":"Shifting from endangerment to rebirth in the Artificial Intelligence\n  Age: An Ensemble Machine Learning Approach for Hawrami Text Classification","summary":"  Hawrami, a dialect of Kurdish, is classified as an endangered language as it\nsuffers from the scarcity of data and the gradual loss of its speakers. Natural\nLanguage Processing projects can be used to partially compensate for data\navailability for endangered languages/dialects through a variety of approaches,\nsuch as machine translation, language model building, and corpora development.\nSimilarly, NLP projects such as text classification are in language\ndocumentation. Several text classification studies have been conducted for\nKurdish, but they were mainly dedicated to two particular dialects: Sorani\n(Central Kurdish) and Kurmanji (Northern Kurdish). In this paper, we introduce\nvarious text classification models using a dataset of 6,854 articles in Hawrami\nlabeled into 15 categories by two native speakers. We use K-nearest Neighbor\n(KNN), Linear Support Vector Machine (Linear SVM), Logistic Regression (LR),\nand Decision Tree (DT) to evaluate how well those methods perform the\nclassification task. The results indicate that the Linear SVM achieves a 96% of\naccuracy and outperforms the other approaches.\n","authors":["Aram Khaksar","Hossein Hassani"],"pdf_url":"https://arxiv.org/pdf/2409.16884v1.pdf","comment":"19 pages, 7 tables, 14 figures"},{"id":"http://arxiv.org/abs/2409.11022v3","updated":"2024-09-25T12:33:27Z","published":"2024-09-17T09:32:12Z","title":"GEIC: Universal and Multilingual Named Entity Recognition with Large\n  Language Models","summary":"  Large Language Models (LLMs) have supplanted traditional methods in numerous\nnatural language processing tasks. Nonetheless, in Named Entity Recognition\n(NER), existing LLM-based methods underperform compared to baselines and\nrequire significantly more computational resources, limiting their application.\nIn this paper, we introduce the task of generation-based extraction and\nin-context classification (GEIC), designed to leverage LLMs' prior knowledge\nand self-attention mechanisms for NER tasks. We then propose CascadeNER, a\nuniversal and multilingual GEIC framework for few-shot and zero-shot NER.\nCascadeNER employs model cascading to utilize two small-parameter LLMs to\nextract and classify independently, reducing resource consumption while\nenhancing accuracy. We also introduce AnythingNER, the first NER dataset\nspecifically designed for LLMs, including 8 languages, 155 entity types and a\nnovel dynamic categorization system. Experiments show that CascadeNER achieves\nstate-of-the-art performance on low-resource and fine-grained scenarios,\nincluding CrossNER and FewNERD. Our work is openly accessible.\n","authors":["Hanjun Luo","Yingbin Jin","Xuecheng Liu","Tong Shang","Ruizhe Chen","Zuozhu Liu"],"pdf_url":"https://arxiv.org/pdf/2409.11022v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.09848v2","updated":"2024-09-25T12:18:37Z","published":"2023-10-15T14:40:57Z","title":"Enhancing Stance Classification on Social Media Using Quantified Moral\n  Foundations","summary":"  This study enhances stance detection on social media by incorporating deeper\npsychological attributes, specifically individuals' moral foundations. These\ntheoretically-derived dimensions aim to provide a comprehensive profile of an\nindividual's moral concerns which, in recent work, has been linked to behaviour\nin a range of domains, including society, politics, health, and the\nenvironment. In this paper, we investigate how moral foundation dimensions can\ncontribute to predicting an individual's stance on a given target. Specifically\nwe incorporate moral foundation features extracted from text, along with\nmessage semantic features, to classify stances at both message- and user-levels\nusing both traditional machine learning models and large language models. Our\npreliminary results suggest that encoding moral foundations can enhance the\nperformance of stance detection tasks and help illuminate the associations\nbetween specific moral foundations and online stances on target topics. The\nresults highlight the importance of considering deeper psychological attributes\nin stance analysis and underscores the role of moral foundations in guiding\nonline social behavior.\n","authors":["Hong Zhang","Prasanta Bhattacharya","Wei Gao","Liang Ze Wong","Brandon Siyuan Loh","Joseph J. P. Simons","Jisun An"],"pdf_url":"https://arxiv.org/pdf/2310.09848v2.pdf","comment":"8 pages, 3 figures"},{"id":"http://arxiv.org/abs/2409.16860v1","updated":"2024-09-25T12:15:15Z","published":"2024-09-25T12:15:15Z","title":"The Role of Language Models in Modern Healthcare: A Comprehensive Review","summary":"  The application of large language models (LLMs) in healthcare has gained\nsignificant attention due to their ability to process complex medical data and\nprovide insights for clinical decision-making. These models have demonstrated\nsubstantial capabilities in understanding and generating natural language,\nwhich is crucial for medical documentation, diagnostics, and patient\ninteraction. This review examines the trajectory of language models from their\nearly stages to the current state-of-the-art LLMs, highlighting their strengths\nin healthcare applications and discussing challenges such as data privacy,\nbias, and ethical considerations. The potential of LLMs to enhance healthcare\ndelivery is explored, alongside the necessary steps to ensure their ethical and\neffective integration into medical practice.\n","authors":["Amna Khalid","Ayma Khalid","Umar Khalid"],"pdf_url":"https://arxiv.org/pdf/2409.16860v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16849v1","updated":"2024-09-25T11:55:02Z","published":"2024-09-25T11:55:02Z","title":"Exposing Assumptions in AI Benchmarks through Cognitive Modelling","summary":"  Cultural AI benchmarks often rely on implicit assumptions about measured\nconstructs, leading to vague formulations with poor validity and unclear\ninterrelations. We propose exposing these assumptions using explicit cognitive\nmodels formulated as Structural Equation Models. Using cross-lingual alignment\ntransfer as an example, we show how this approach can answer key research\nquestions and identify missing datasets. This framework grounds benchmark\nconstruction theoretically and guides dataset development to improve construct\nmeasurement. By embracing transparency, we move towards more rigorous,\ncumulative AI evaluation science, challenging researchers to critically examine\ntheir assessment foundations.\n","authors":["Jonathan H. Rystrøm","Kenneth C. Enevoldsen"],"pdf_url":"https://arxiv.org/pdf/2409.16849v1.pdf","comment":"11 pages, 2 figures"},{"id":"http://arxiv.org/abs/2402.05200v2","updated":"2024-09-25T11:43:59Z","published":"2024-02-07T19:10:36Z","title":"Are LLMs Ready for Real-World Materials Discovery?","summary":"  Large Language Models (LLMs) create exciting possibilities for powerful\nlanguage processing tools to accelerate research in materials science. While\nLLMs have great potential to accelerate materials understanding and discovery,\nthey currently fall short in being practical materials science tools. In this\nposition paper, we show relevant failure cases of LLMs in materials science\nthat reveal current limitations of LLMs related to comprehending and reasoning\nover complex, interconnected materials science knowledge. Given those\nshortcomings, we outline a framework for developing Materials Science LLMs\n(MatSci-LLMs) that are grounded in materials science knowledge and hypothesis\ngeneration followed by hypothesis testing. The path to attaining performant\nMatSci-LLMs rests in large part on building high-quality, multi-modal datasets\nsourced from scientific literature where various information extraction\nchallenges persist. As such, we describe key materials science information\nextraction challenges which need to be overcome in order to build large-scale,\nmulti-modal datasets that capture valuable materials science knowledge.\nFinally, we outline a roadmap for applying future MatSci-LLMs for real-world\nmaterials discovery via: 1. Automated Knowledge Base Generation; 2. Automated\nIn-Silico Material Design; and 3. MatSci-LLM Integrated Self-Driving Materials\nLaboratories.\n","authors":["Santiago Miret","N M Anoop Krishnan"],"pdf_url":"https://arxiv.org/pdf/2402.05200v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.20440v2","updated":"2024-09-25T11:22:20Z","published":"2023-10-31T13:22:38Z","title":"Integrating curation into scientific publishing to train AI models","summary":"  High throughput extraction and structured labeling of data from academic\narticles is critical to enable downstream machine learning applications and\nsecondary analyses. We have embedded multimodal data curation into the academic\npublishing process to annotate segmented figure panels and captions. Natural\nlanguage processing (NLP) was combined with human-in-the-loop feedback from the\noriginal authors to increase annotation accuracy. Annotation included eight\nclasses of bioentities (small molecules, gene products, subcellular components,\ncell lines, cell types, tissues, organisms, and diseases) plus additional\nclasses delineating the entities' roles in experiment designs and\nmethodologies. The resultant dataset, SourceData-NLP, contains more than\n620,000 annotated biomedical entities, curated from 18,689 figures in 3,223\narticles in molecular and cell biology. We evaluate the utility of the dataset\nto train AI models using named-entity recognition, segmentation of figure\ncaptions into their constituent panels, and a novel context-dependent semantic\ntask assessing whether an entity is a controlled intervention target or a\nmeasurement object. We also illustrate the use of our dataset in performing a\nmulti-modal task for segmenting figures into panel images and their\ncorresponding captions.\n","authors":["Jorge Abreu-Vicente","Hannah Sonntag","Thomas Eidens","Cassie S. Mitchell","Thomas Lemberger"],"pdf_url":"https://arxiv.org/pdf/2310.20440v2.pdf","comment":"Submitted to Journal for revision"},{"id":"http://arxiv.org/abs/2409.16819v1","updated":"2024-09-25T11:18:52Z","published":"2024-09-25T11:18:52Z","title":"CodeInsight: A Curated Dataset of Practical Coding Solutions from Stack\n  Overflow","summary":"  We introduce a novel dataset tailored for code generation, aimed at aiding\ndevelopers in common tasks. Our dataset provides examples that include a\nclarified intent, code snippets associated, and an average of three related\nunit tests. It encompasses a range of libraries such as \\texttt{Pandas},\n\\texttt{Numpy}, and \\texttt{Regex}, along with more than 70 standard libraries\nin Python code derived from Stack Overflow. Comprising 3,409 crafted examples\nby Python experts, our dataset is designed for both model finetuning and\nstandalone evaluation. To complete unit tests evaluation, we categorize\nexamples in order to get more fine grained analysis, enhancing the\nunderstanding of models' strengths and weaknesses in specific coding tasks. The\nexamples have been refined to reduce data contamination, a process confirmed by\nthe performance of three leading models: Mistral 7B, CodeLLaMa 13B, and\nStarcoder 15B. We further investigate data-contamination testing GPT-4\nperformance on a part of our dataset. The benchmark can be accessed at\n\\url{https://github.com/NathanaelBeau/CodeInsight}.\n","authors":["Nathanaël Beau","Benoît Crabbé"],"pdf_url":"https://arxiv.org/pdf/2409.16819v1.pdf","comment":"Accepted to ACL 2024 Findings"},{"id":"http://arxiv.org/abs/2409.16807v1","updated":"2024-09-25T10:56:28Z","published":"2024-09-25T10:56:28Z","title":"A Few Hypocrites: Few-Shot Learning and Subtype Definitions for\n  Detecting Hypocrisy Accusations in Online Climate Change Debates","summary":"  The climate crisis is a salient issue in online discussions, and hypocrisy\naccusations are a central rhetorical element in these debates. However, for\nlarge-scale text analysis, hypocrisy accusation detection is an understudied\ntool, most often defined as a smaller subtask of fallacious argument detection.\nIn this paper, we define hypocrisy accusation detection as an independent task\nin NLP, and identify different relevant subtypes of hypocrisy accusations. Our\nClimate Hypocrisy Accusation Corpus (CHAC) consists of 420 Reddit climate\ndebate comments, expert-annotated into two different types of hypocrisy\naccusations: personal versus political hypocrisy. We evaluate few-shot\nin-context learning with 6 shots and 3 instruction-tuned Large Language Models\n(LLMs) for detecting hypocrisy accusations in this dataset. Results indicate\nthat the GPT-4o and Llama-3 models in particular show promise in detecting\nhypocrisy accusations (F1 reaching 0.68, while previous work shows F1 of 0.44).\nHowever, context matters for a complex semantic concept such as hypocrisy\naccusations, and we find models struggle especially at identifying political\nhypocrisy accusations compared to personal moral hypocrisy. Our study\ncontributes new insights in hypocrisy detection and climate change discourse,\nand is a stepping stone for large-scale analysis of hypocrisy accusation in\nonline climate debates.\n","authors":["Paulina Garcia Corral","Avishai Green","Hendrik Meyer","Anke Stoll","Xiaoyue Yan","Myrthe Reuver"],"pdf_url":"https://arxiv.org/pdf/2409.16807v1.pdf","comment":"cite the public version, published at CPSS 2024 @ KONVENS"},{"id":"http://arxiv.org/abs/2408.04872v2","updated":"2024-09-25T10:00:03Z","published":"2024-08-09T05:25:17Z","title":"SCOI: Syntax-augmented Coverage-based In-context Example Selection for\n  Machine Translation","summary":"  In-context learning (ICL) greatly improves the performance of large language\nmodels (LLMs) on various down-stream tasks, where the improvement highly\ndepends on the quality of demonstrations. In this work, we introduce syntactic\nknowledge to select better in-context examples for machine translation (MT). We\npropose a new strategy, namely Syntax-augmented COverage-based In-context\nexample selection (SCOI), leveraging the deep syntactic structure beyond\nconventional word matching. Specifically, we measure the set-level syntactic\ncoverage by computing the coverage of polynomial terms with the help of a\nsimplified tree-to-polynomial algorithm, and lexical coverage using word\noverlap. Furthermore, we devise an alternate selection approach to combine both\ncoverage measures, taking advantage of syntactic and lexical information. We\nconduct experiments with two multi-lingual LLMs on six translation directions.\nEmpirical results show that our proposed SCOI obtains the highest average COMET\nscore among all learning-free methods, indicating that combining syntactic and\nlexical coverage successfully helps to select better in-context examples for\nMT. Our code is available at https://github.com/JamyDon/SCOI.\n","authors":["Chenming Tang","Zhixiang Wang","Yunfang Wu"],"pdf_url":"https://arxiv.org/pdf/2408.04872v2.pdf","comment":"EMNLP 2024 main conference long paper. 16 pages, 2 figures, 14 tables"},{"id":"http://arxiv.org/abs/2408.08640v2","updated":"2024-09-25T09:53:13Z","published":"2024-08-16T10:11:05Z","title":"Math-PUMA: Progressive Upward Multimodal Alignment to Enhance\n  Mathematical Reasoning","summary":"  Multimodal Large Language Models (MLLMs) excel in solving text-based\nmathematical problems, but they struggle with mathematical diagrams since they\nare primarily trained on natural scene images. For humans, visual aids\ngenerally enhance problem-solving, but MLLMs perform worse as information\nshifts from textual to visual modality. This decline is mainly due to their\nshortcomings in aligning images and text. To tackle aforementioned challenges,\nwe propose Math-PUMA, a methodology focused on Progressive Upward Multimodal\nAlignment. This approach is designed to improve the mathematical reasoning\nskills of MLLMs through a three-stage training process, with the second stage\nbeing the critical alignment stage. We first enhance the language model's\nmathematical reasoning capabilities with extensive set of textual mathematical\nproblems. We then construct a multimodal dataset with varying degrees of\ntextual and visual information, creating data pairs by presenting each problem\nin at least two forms. By leveraging the Kullback-Leibler (KL) divergence of\nnext-token prediction distributions to align visual and textual modalities,\nconsistent problem-solving abilities are ensured. Finally, we utilize\nmultimodal instruction tuning for MLLMs with high-quality multimodal data.\nExperimental results on multiple mathematical reasoning benchmarks demonstrate\nthat the MLLMs trained with Math-PUMA surpass most open-source MLLMs. Our\napproach effectively narrows the performance gap for problems presented in\ndifferent modalities. The code and data are available at:\n\\url{https://github.com/wwzhuang01/Math-PUMA}.\n","authors":["Wenwen Zhuang","Xin Huang","Xiantao Zhang","Jin Zeng"],"pdf_url":"https://arxiv.org/pdf/2408.08640v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16788v1","updated":"2024-09-25T09:52:44Z","published":"2024-09-25T09:52:44Z","title":"Mitigating the Bias of Large Language Model Evaluation","summary":"  Recently, there has been a trend of evaluating the Large Language Model (LLM)\nquality in the flavor of LLM-as-a-Judge, namely leveraging another LLM to\nevaluate the current output quality. However, existing judges are proven to be\nbiased, namely they would favor answers which present better superficial\nquality (such as verbosity, fluency) while ignoring the instruction following\nability. In this work, we propose systematic research about the bias of\nLLM-as-a-Judge. Specifically, for closed-source judge models, we apply\ncalibration to mitigate the significance of superficial quality, both on\nprobability level and prompt level. For open-source judge models, we propose to\nmitigate the bias by contrastive training, with curated negative samples that\ndeviate from instruction but present better superficial quality. We apply our\nmethods on the bias evaluation benchmark, and experiment results show our\nmethods mitigate the bias by a large margin while maintaining a satisfactory\nevaluation accuracy.\n","authors":["Hongli Zhou","Hui Huang","Yunfei Long","Bing Xu","Conghui Zhu","Hailong Cao","Muyun Yang","Tiejun Zhao"],"pdf_url":"https://arxiv.org/pdf/2409.16788v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16783v1","updated":"2024-09-25T09:44:48Z","published":"2024-09-25T09:44:48Z","title":"Holistic Automated Red Teaming for Large Language Models through\n  Top-Down Test Case Generation and Multi-turn Interaction","summary":"  Automated red teaming is an effective method for identifying misaligned\nbehaviors in large language models (LLMs). Existing approaches, however, often\nfocus primarily on improving attack success rates while overlooking the need\nfor comprehensive test case coverage. Additionally, most of these methods are\nlimited to single-turn red teaming, failing to capture the multi-turn dynamics\nof real-world human-machine interactions. To overcome these limitations, we\npropose HARM (Holistic Automated Red teaMing), which scales up the diversity of\ntest cases using a top-down approach based on an extensible, fine-grained risk\ntaxonomy. Our method also leverages a novel fine-tuning strategy and\nreinforcement learning techniques to facilitate multi-turn adversarial probing\nin a human-like manner. Experimental results demonstrate that our framework\nenables a more systematic understanding of model vulnerabilities and offers\nmore targeted guidance for the alignment process.\n","authors":["Jinchuan Zhang","Yan Zhou","Yaxin Liu","Ziming Li","Songlin Hu"],"pdf_url":"https://arxiv.org/pdf/2409.16783v1.pdf","comment":"EMNLP 2024 camera ready version"},{"id":"http://arxiv.org/abs/2406.16332v2","updated":"2024-09-25T09:36:49Z","published":"2024-06-24T06:10:13Z","title":"DemoRank: Selecting Effective Demonstrations for Large Language Models\n  in Ranking Task","summary":"  Recently, there has been increasing interest in applying large language\nmodels (LLMs) as zero-shot passage rankers. However, few studies have explored\nhow to select appropriate in-context demonstrations for the passage ranking\ntask, which is the focus of this paper. Previous studies mainly use LLM's\nfeedback to train a retriever for demonstration selection. These studies apply\nthe LLM to score each demonstration independently, which ignores the\ndependencies between demonstrations (especially important in ranking task),\nleading to inferior performance of top-$k$ retrieved demonstrations. To\nmitigate this issue, we introduce a demonstration reranker to rerank the\nretrieved demonstrations so that top-$k$ ranked ones are more suitable for ICL.\nHowever, generating training data for such reranker is quite challenging. On\nthe one hand, different from demonstration retriever, the training samples of\nreranker need to incorporate demonstration dependencies. On the other hand,\nobtaining the gold ranking from the retrieved demonstrations is an NP-hard\nproblem, which is hard to implement. To overcome these challenges, we propose a\nmethod to approximate the optimal demonstration list iteratively and utilize\nLLM to score demonstration lists of varying lengths. By doing so, the search\nspace is greatly reduced and demonstration dependencies are considered. Based\non these scored demonstration lists, we further design a list-pairwise training\napproach which compares a pair of lists that only differ in the last\ndemonstration, to teach the reranker how to select the next demonstration given\na previous sequence. In this paper, we propose a demonstration selection\nframework DemoRank for ranking task and conduct extensive experiments to prove\nits strong ability.\n","authors":["Wenhan Liu","Yutao Zhu","Zhicheng Dou"],"pdf_url":"https://arxiv.org/pdf/2406.16332v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16751v1","updated":"2024-09-25T09:02:48Z","published":"2024-09-25T09:02:48Z","title":"E-SQL: Direct Schema Linking via Question Enrichment in Text-to-SQL","summary":"  Translating Natural Language Queries into Structured Query Language\n(Text-to-SQL or NLQ-to-SQL) is a critical task extensively studied by both the\nnatural language processing and database communities, aimed at providing a\nnatural language interface to databases (NLIDB) and lowering the barrier for\nnon-experts. Despite recent advancements made through the use of Large Language\nModels (LLMs), significant challenges remain. These include handling complex\ndatabase schemas, resolving ambiguity in user queries, and generating SQL\nqueries with intricate structures that accurately reflect the user's intent. In\nthis work, we introduce E-SQL, a novel pipeline specifically designed to\naddress these challenges through direct schema linking and candidate predicate\naugmentation. E-SQL enhances the natural language query by incorporating\nrelevant database items (i.e., tables, columns, and values) and conditions\ndirectly into the question, bridging the gap between the query and the database\nstructure. The pipeline leverages candidate predicate augmentation to mitigate\nerroneous or incomplete predicates in generated SQLs. We further investigate\nthe impact of schema filtering, a technique widely explored in previous work,\nand demonstrate its diminishing returns when applied alongside advanced large\nlanguage models. Comprehensive evaluations on the BIRD benchmark illustrate\nthat E-SQL achieves competitive performance, particularly excelling in complex\nqueries with a 66.29% execution accuracy on the test set. All code required to\nreproduce the reported results is publicly available on our GitHub repository.\n","authors":["Hasan Alp Caferoğlu","Özgür Ulusoy"],"pdf_url":"https://arxiv.org/pdf/2409.16751v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14913v2","updated":"2024-09-25T08:52:49Z","published":"2024-09-23T11:08:04Z","title":"Towards a Realistic Long-Term Benchmark for Open-Web Research Agents","summary":"  We present initial results of a forthcoming benchmark for evaluating LLM\nagents on white-collar tasks of economic value. We evaluate agents on\nreal-world \"messy\" open-web research tasks of the type that are routine in\nfinance and consulting. In doing so, we lay the groundwork for an LLM agent\nevaluation suite where good performance directly corresponds to a large\neconomic and societal impact. We built and tested several agent architectures\nwith o1-preview, GPT-4o, Claude-3.5 Sonnet, Llama 3.1 (405b), and GPT-4o-mini.\nOn average, LLM agents powered by Claude-3.5 Sonnet and o1-preview\nsubstantially outperformed agents using GPT-4o, with agents based on Llama 3.1\n(405b) and GPT-4o-mini lagging noticeably behind. Across LLMs, a ReAct\narchitecture with the ability to delegate subtasks to subagents performed best.\nIn addition to quantitative evaluations, we qualitatively assessed the\nperformance of the LLM agents by inspecting their traces and reflecting on\ntheir observations. Our evaluation represents the first in-depth assessment of\nagents' abilities to conduct challenging, economically valuable analyst-style\nresearch on the real open web.\n","authors":["Peter Mühlbacher","Nikos I. Bosse","Lawrence Phillips"],"pdf_url":"https://arxiv.org/pdf/2409.14913v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16727v1","updated":"2024-09-25T08:23:46Z","published":"2024-09-25T08:23:46Z","title":"RoleBreak: Character Hallucination as a Jailbreak Attack in Role-Playing\n  Systems","summary":"  Role-playing systems powered by large language models (LLMs) have become\nincreasingly influential in emotional communication applications. However,\nthese systems are susceptible to character hallucinations, where the model\ndeviates from predefined character roles and generates responses that are\ninconsistent with the intended persona. This paper presents the first\nsystematic analysis of character hallucination from an attack perspective,\nintroducing the RoleBreak framework. Our framework identifies two core\nmechanisms-query sparsity and role-query conflict-as key factors driving\ncharacter hallucination. Leveraging these insights, we construct a novel\ndataset, RoleBreakEval, to evaluate existing hallucination mitigation\ntechniques. Our experiments reveal that even enhanced models trained to\nminimize hallucination remain vulnerable to attacks. To address these\nvulnerabilities, we propose a novel defence strategy, the Narrator Mode, which\ngenerates supplemental context through narration to mitigate role-query\nconflicts and improve query generalization. Experimental results demonstrate\nthat Narrator Mode significantly outperforms traditional refusal-based\nstrategies by reducing hallucinations, enhancing fidelity to character roles\nand queries, and improving overall narrative coherence.\n","authors":["Yihong Tang","Bo Wang","Xu Wang","Dongming Zhao","Jing Liu","Jijun Zhang","Ruifang He","Yuexian Hou"],"pdf_url":"https://arxiv.org/pdf/2409.16727v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16722v1","updated":"2024-09-25T08:20:24Z","published":"2024-09-25T08:20:24Z","title":"PMSS: Pretrained Matrices Skeleton Selection for LLM Fine-tuning","summary":"  Low-rank adaptation (LoRA) and its variants have recently gained much\ninterest due to their ability to avoid excessive inference costs. However, LoRA\nstill encounters the following challenges: (1) Limitation of low-rank\nassumption; and (2) Its initialization method may be suboptimal. To this end,\nwe propose PMSS(Pre-trained Matrices Skeleton Selection), which enables\nhigh-rank updates with low costs while leveraging semantic and linguistic\ninformation inherent in pre-trained weight. It achieves this by selecting\nskeletons from the pre-trained weight matrix and only learning a small matrix\ninstead. Experiments demonstrate that PMSS outperforms LoRA and other\nfine-tuning methods across tasks with much less trainable parameters. We\ndemonstrate its effectiveness, especially in handling complex tasks such as\nDROP benchmark(+3.4%/+5.9% on LLaMA2-7B/13B) and math\nreasoning(+12.89%/+5.61%/+3.11% on LLaMA2-7B, Mistral-7B and Gemma-7B of\nGSM8K). The code and model will be released soon.\n","authors":["Qibin Wang","Xiaolin Hu","Weikai Xu","Wei Liu","Jian Luan","Bin Wang"],"pdf_url":"https://arxiv.org/pdf/2409.16722v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12246v3","updated":"2024-09-25T08:17:21Z","published":"2024-06-18T03:42:00Z","title":"TroL: Traversal of Layers for Large Language and Vision Models","summary":"  Large language and vision models (LLVMs) have been driven by the\ngeneralization power of large language models (LLMs) and the advent of visual\ninstruction tuning. Along with scaling them up directly, these models enable\nLLVMs to showcase powerful vision language (VL) performances by covering\ndiverse tasks via natural language instructions. However, existing open-source\nLLVMs that perform comparably to closed-source LLVMs such as GPT-4V are often\nconsidered too large (e.g., 26B, 34B, and 110B parameters), having a larger\nnumber of layers. These large models demand costly, high-end resources for both\ntraining and inference. To address this issue, we present a new efficient LLVM\nfamily with 1.8B, 3.8B, and 7B LLM model sizes, Traversal of Layers (TroL),\nwhich enables the reuse of layers in a token-wise manner. This layer traversing\ntechnique simulates the effect of looking back and retracing the answering\nstream while increasing the number of forward propagation layers without\nphysically adding more layers. We demonstrate that TroL employs a simple layer\ntraversing approach yet efficiently outperforms the open-source LLVMs with\nlarger model sizes and rivals the performances of the closed-source LLVMs with\nsubstantial sizes.\n","authors":["Byung-Kwan Lee","Sangyun Chung","Chae Won Kim","Beomchan Park","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2406.12246v3.pdf","comment":"EMNLP 2024. Code is available in https://github.com/ByungKwanLee/TroL"},{"id":"http://arxiv.org/abs/2409.16718v1","updated":"2024-09-25T08:07:18Z","published":"2024-09-25T08:07:18Z","title":"Vision-Language Model Fine-Tuning via Simple Parameter-Efficient\n  Modification","summary":"  Recent advances in fine-tuning Vision-Language Models (VLMs) have witnessed\nthe success of prompt tuning and adapter tuning, while the classic model\nfine-tuning on inherent parameters seems to be overlooked. It is believed that\nfine-tuning the parameters of VLMs with few-shot samples corrupts the\npre-trained knowledge since fine-tuning the CLIP model even degrades\nperformance. In this paper, we revisit this viewpoint, and propose a new\nperspective: fine-tuning the specific parameters instead of all will uncover\nthe power of classic model fine-tuning on VLMs. Through our meticulous study,\nwe propose ClipFit, a simple yet effective method to fine-tune CLIP without\nintroducing any overhead of extra parameters. We demonstrate that by only\nfine-tuning the specific bias terms and normalization layers, ClipFit can\nimprove the performance of zero-shot CLIP by 7.27\\% average harmonic mean\naccuracy. Lastly, to understand how fine-tuning in CLIPFit affects the\npre-trained models, we conducted extensive experimental analyses w.r.t. changes\nin internal parameters and representations. We found that low-level text bias\nlayers and the first layer normalization layer change much more than other\nlayers. The code is available at \\url{https://github.com/minglllli/CLIPFit}.\n","authors":["Ming Li","Jike Zhong","Chenxin Li","Liuzhuozheng Li","Nie Lin","Masashi Sugiyama"],"pdf_url":"https://arxiv.org/pdf/2409.16718v1.pdf","comment":"EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2404.01204v2","updated":"2024-09-25T07:59:49Z","published":"2024-04-01T16:00:01Z","title":"The Fine Line: Navigating Large Language Model Pretraining with\n  Down-streaming Capability Analysis","summary":"  Uncovering early-stage metrics that reflect final model performance is one\ncore principle for large-scale pretraining. The existing scaling law\ndemonstrates the power-law correlation between pretraining loss and training\nflops, which serves as an important indicator of the current training state for\nlarge language models. However, this principle only focuses on the model's\ncompression properties on the training data, resulting in an inconsistency with\nthe ability improvements on the downstream tasks. Some follow-up works\nattempted to extend the scaling-law to more complex metrics (such as\nhyperparameters), but still lacked a comprehensive analysis of the dynamic\ndifferences among various capabilities during pretraining. To address the\naforementioned limitations, this paper undertakes a comprehensive comparison of\nmodel capabilities at various pretraining intermediate checkpoints. Through\nthis analysis, we confirm that specific downstream metrics exhibit similar\ntraining dynamics across models of different sizes, up to 67 billion\nparameters. In addition to our core findings, we've reproduced Amber and\nOpenLLaMA, releasing their intermediate checkpoints. This initiative offers\nvaluable resources to the research community and facilitates the verification\nand exploration of LLM pretraining by open-source researchers. Besides, we\nprovide empirical summaries, including performance comparisons of different\nmodels and capabilities, and tuition of key metrics for different training\nphases. Based on these findings, we provide a more user-friendly strategy for\nevaluating the optimization state, offering guidance for establishing a stable\npretraining process.\n","authors":["Chen Yang","Junzhuo Li","Xinyao Niu","Xinrun Du","Songyang Gao","Haoran Zhang","Zhaoliang Chen","Xingwei Qu","Ruibin Yuan","Yizhi Li","Jiaheng Liu","Stephen W. Huang","Shawn Yue","Jie Fu","Ge Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.01204v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16710v1","updated":"2024-09-25T07:55:36Z","published":"2024-09-25T07:55:36Z","title":"Beyond Turing Test: Can GPT-4 Sway Experts' Decisions?","summary":"  In the post-Turing era, evaluating large language models (LLMs) involves\nassessing generated text based on readers' reactions rather than merely its\nindistinguishability from human-produced content. This paper explores how\nLLM-generated text impacts readers' decisions, focusing on both amateur and\nexpert audiences. Our findings indicate that GPT-4 can generate persuasive\nanalyses affecting the decisions of both amateurs and professionals.\nFurthermore, we evaluate the generated text from the aspects of grammar,\nconvincingness, logical coherence, and usefulness. The results highlight a high\ncorrelation between real-world evaluation through audience reactions and the\ncurrent multi-dimensional evaluators commonly used for generative models.\nOverall, this paper shows the potential and risk of using generated text to\nsway human decisions and also points out a new direction for evaluating\ngenerated text, i.e., leveraging the reactions and decisions of readers. We\nrelease our dataset to assist future research.\n","authors":["Takehiro Takayanagi","Hiroya Takamura","Kiyoshi Izumi","Chung-Chi Chen"],"pdf_url":"https://arxiv.org/pdf/2409.16710v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16707v1","updated":"2024-09-25T07:54:16Z","published":"2024-09-25T07:54:16Z","title":"Probing Omissions and Distortions in Transformer-based RDF-to-Text\n  Models","summary":"  In Natural Language Generation (NLG), important information is sometimes\nomitted in the output text. To better understand and analyse how this type of\nmistake arises, we focus on RDF-to-Text generation and explore two methods of\nprobing omissions in the encoder output of BART (Lewis et al, 2020) and of T5\n(Raffel et al, 2019): (i) a novel parameter-free probing method based on the\ncomputation of cosine similarity between embeddings of RDF graphs and of RDF\ngraphs in which we removed some entities and (ii) a parametric probe which\nperforms binary classification on the encoder embeddings to detect omitted\nentities. We also extend our analysis to distorted entities, i.e. entities that\nare not fully correctly mentioned in the generated text (e.g. misspelling of\nentity, wrong units of measurement). We found that both omitted and distorted\nentities can be probed in the encoder's output embeddings. This suggests that\nthe encoder emits a weaker signal for these entities and therefore is\nresponsible for some loss of information. This also shows that probing methods\ncan be used to detect mistakes in the output of NLG models.\n","authors":["Juliette Faille","Albert Gatt","Claire Gardent"],"pdf_url":"https://arxiv.org/pdf/2409.16707v1.pdf","comment":"Accepted for publication in Transactions of the ACL (TACL)"},{"id":"http://arxiv.org/abs/2309.00223v3","updated":"2024-09-25T07:39:08Z","published":"2023-09-01T02:56:20Z","title":"The FruitShell French synthesis system at the Blizzard 2023 Challenge","summary":"  This paper presents a French text-to-speech synthesis system for the Blizzard\nChallenge 2023. The challenge consists of two tasks: generating high-quality\nspeech from female speakers and generating speech that closely resembles\nspecific individuals. Regarding the competition data, we conducted a screening\nprocess to remove missing or erroneous text data. We organized all symbols\nexcept for phonemes and eliminated symbols that had no pronunciation or zero\nduration. Additionally, we added word boundary and start/end symbols to the\ntext, which we have found to improve speech quality based on our previous\nexperience. For the Spoke task, we performed data augmentation according to the\ncompetition rules. We used an open-source G2P model to transcribe the French\ntexts into phonemes. As the G2P model uses the International Phonetic Alphabet\n(IPA), we applied the same transcription process to the provided competition\ndata for standardization. However, due to compiler limitations in recognizing\nspecial symbols from the IPA chart, we followed the rules to convert all\nphonemes into the phonetic scheme used in the competition data. Finally, we\nresampled all competition audio to a uniform sampling rate of 16 kHz. We\nemployed a VITS-based acoustic model with the hifigan vocoder. For the Spoke\ntask, we trained a multi-speaker model and incorporated speaker information\ninto the duration predictor, vocoder, and flow layers of the model. The\nevaluation results of our system showed a quality MOS score of 3.6 for the Hub\ntask and 3.4 for the Spoke task, placing our system at an average level among\nall participating teams.\n","authors":["Xin Qi","Xiaopeng Wang","Zhiyong Wang","Wang Liu","Mingming Ding","Shuchen Shi"],"pdf_url":"https://arxiv.org/pdf/2309.00223v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11375v2","updated":"2024-09-25T07:38:37Z","published":"2024-06-17T09:51:38Z","title":"Boosting Scientific Concepts Understanding: Can Analogy from Teacher\n  Models Empower Student Models?","summary":"  Analogical reasoning plays a critical role in human cognition, enabling us to\nunderstand new concepts by associating them with familiar ones. Previous\nresearch in the AI community has mainly focused on identifying and generating\nanalogies and then examining their quality under human evaluation, which\noverlooks the practical application of these analogies in real-world settings.\nInspired by the human education process, in this paper, we propose to\ninvestigate how analogies created by teacher language models (LMs) can assist\nstudent LMs in understanding scientific concepts, thereby aligning more closely\nwith practical scenarios. Our results suggest that free-form analogies can\nindeed aid LMs in understanding concepts. Additionally, analogies generated by\nstudent LMs can improve their own performance on scientific question answering,\ndemonstrating their capability to use analogies for self-learning new\nknowledge. Resources are available at https://github.com/siyuyuan/SCUA.\n","authors":["Siyu Yuan","Cheng Jiayang","Lin Qiu","Deqing Yang"],"pdf_url":"https://arxiv.org/pdf/2406.11375v2.pdf","comment":"Accepted to EMNLP 2024"},{"id":"http://arxiv.org/abs/2409.16694v1","updated":"2024-09-25T07:38:02Z","published":"2024-09-25T07:38:02Z","title":"A Survey of Low-bit Large Language Models: Basics, Systems, and\n  Algorithms","summary":"  Large language models (LLMs) have achieved remarkable advancements in natural\nlanguage processing, showcasing exceptional performance across various tasks.\nHowever, the expensive memory and computational requirements present\nsignificant challenges for their practical deployment. Low-bit quantization has\nemerged as a critical approach to mitigate these challenges by reducing the\nbit-width of model parameters, activations, and gradients, thus decreasing\nmemory usage and computational demands. This paper presents a comprehensive\nsurvey of low-bit quantization methods tailored for LLMs, covering the\nfundamental principles, system implementations, and algorithmic strategies. An\noverview of basic concepts and new data formats specific to low-bit LLMs is\nfirst introduced, followed by a review of frameworks and systems that\nfacilitate low-bit LLMs across various hardware platforms. Then, we categorize\nand analyze techniques and toolkits for efficient low-bit training and\ninference of LLMs. Finally, we conclude with a discussion of future trends and\npotential advancements of low-bit LLMs. Our systematic overview from basic,\nsystem, and algorithm perspectives can offer valuable insights and guidelines\nfor future works to enhance the efficiency and applicability of LLMs through\nlow-bit quantization.\n","authors":["Ruihao Gong","Yifu Ding","Zining Wang","Chengtao Lv","Xingyu Zheng","Jinyang Du","Haotong Qin","Jinyang Guo","Michele Magno","Xianglong Liu"],"pdf_url":"https://arxiv.org/pdf/2409.16694v1.pdf","comment":"Ruihao Gong leads the overall organization of the survey, with Yifu\n  Ding and Jinyang Du contributing to Sections 2 and 3. Xingyu Zheng is\n  responsible for authoring Section 4, while Chengtao Lv and Zining Wang\n  collaborate on Section 5. Haotong Qin, Jinyang Guo, Michele Magno, and\n  Xianglong Liu provide guidance during the whole process and assist in\n  refining the final manuscript"},{"id":"http://arxiv.org/abs/2409.16686v1","updated":"2024-09-25T07:21:51Z","published":"2024-09-25T07:21:51Z","title":"MSI-Agent: Incorporating Multi-Scale Insight into Embodied Agents for\n  Superior Planning and Decision-Making","summary":"  Long-term memory is significant for agents, in which insights play a crucial\nrole. However, the emergence of irrelevant insight and the lack of general\ninsight can greatly undermine the effectiveness of insight. To solve this\nproblem, in this paper, we introduce Multi-Scale Insight Agent (MSI-Agent), an\nembodied agent designed to improve LLMs' planning and decision-making ability\nby summarizing and utilizing insight effectively across different scales. MSI\nachieves this through the experience selector, insight generator, and insight\nselector. Leveraging a three-part pipeline, MSI can generate task-specific and\nhigh-level insight, store it in a database, and then use relevant insight from\nit to aid in decision-making. Our experiments show that MSI outperforms another\ninsight strategy when planning by GPT3.5. Moreover, We delve into the\nstrategies for selecting seed experience and insight, aiming to provide LLM\nwith more useful and relevant insight for better decision-making. Our\nobservations also indicate that MSI exhibits better robustness when facing\ndomain-shifting scenarios.\n","authors":["Dayuan Fu","Biqing Qi","Yihuai Gao","Che Jiang","Guanting Dong","Bowen Zhou"],"pdf_url":"https://arxiv.org/pdf/2409.16686v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16682v1","updated":"2024-09-25T07:18:45Z","published":"2024-09-25T07:18:45Z","title":"SynTQA: Synergistic Table-based Question Answering via Mixture of\n  Text-to-SQL and E2E TQA","summary":"  Text-to-SQL parsing and end-to-end question answering (E2E TQA) are two main\napproaches for Table-based Question Answering task. Despite success on multiple\nbenchmarks, they have yet to be compared and their synergy remains unexplored.\nIn this paper, we identify different strengths and weaknesses through\nevaluating state-of-the-art models on benchmark datasets: Text-to-SQL\ndemonstrates superiority in handling questions involving arithmetic operations\nand long tables; E2E TQA excels in addressing ambiguous questions, non-standard\ntable schema, and complex table contents. To combine both strengths, we propose\na Synergistic Table-based Question Answering approach that integrate different\nmodels via answer selection, which is agnostic to any model types. Further\nexperiments validate that ensembling models by either feature-based or\nLLM-based answer selector significantly improves the performance over\nindividual models.\n","authors":["Siyue Zhang","Anh Tuan Luu","Chen Zhao"],"pdf_url":"https://arxiv.org/pdf/2409.16682v1.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2409.16681v1","updated":"2024-09-25T07:16:16Z","published":"2024-09-25T07:16:16Z","title":"Emotional Dimension Control in Language Model-Based Text-to-Speech:\n  Spanning a Broad Spectrum of Human Emotions","summary":"  Current emotional text-to-speech (TTS) systems face challenges in mimicking a\nbroad spectrum of human emotions due to the inherent complexity of emotions and\nlimitations in emotional speech datasets and models. This paper proposes a TTS\nframework that facilitates control over pleasure, arousal, and dominance, and\ncan synthesize a diversity of emotional styles without requiring any emotional\nspeech data during TTS training. We train an emotional attribute predictor\nusing only categorical labels from speech data, aligning with psychological\nresearch and incorporating anchored dimensionality reduction on self-supervised\nlearning (SSL) features. The TTS framework converts text inputs into phonetic\ntokens via an autoregressive language model and uses pseudo-emotional\ndimensions to guide the parallel prediction of fine-grained acoustic details.\nExperiments conducted on the LibriTTS dataset demonstrate that our framework\ncan synthesize speech with enhanced naturalness and a variety of emotional\nstyles by effectively controlling emotional dimensions, even without the\ninclusion of any emotional speech during TTS training.\n","authors":["Kun Zhou","You Zhang","Shengkui Zhao","Hao Wang","Zexu Pan","Dianwen Ng","Chong Zhang","Chongjia Ni","Yukun Ma","Trung Hieu Nguyen","Jia Qi Yip","Bin Ma"],"pdf_url":"https://arxiv.org/pdf/2409.16681v1.pdf","comment":"submitted to ICASSP 2025"},{"id":"http://arxiv.org/abs/2409.16673v1","updated":"2024-09-25T07:05:44Z","published":"2024-09-25T07:05:44Z","title":"SWE2: SubWord Enriched and Significant Word Emphasized Framework for\n  Hate Speech Detection","summary":"  Hate speech detection on online social networks has become one of the\nemerging hot topics in recent years. With the broad spread and fast propagation\nspeed across online social networks, hate speech makes significant impacts on\nsociety by increasing prejudice and hurting people. Therefore, there are\naroused attention and concern from both industry and academia. In this paper,\nwe address the hate speech problem and propose a novel hate speech detection\nframework called SWE2, which only relies on the content of messages and\nautomatically identifies hate speech. In particular, our framework exploits\nboth word-level semantic information and sub-word knowledge. It is intuitively\npersuasive and also practically performs well under a situation with/without\ncharacter-level adversarial attack. Experimental results show that our proposed\nmodel achieves 0.975 accuracy and 0.953 macro F1, outperforming 7\nstate-of-the-art baselines under no adversarial attack. Our model robustly and\nsignificantly performed well under extreme adversarial attack (manipulation of\n50% messages), achieving 0.967 accuracy and 0.934 macro F1.\n","authors":["Guanyi Mou","Pengyi Ye","Kyumin Lee"],"pdf_url":"https://arxiv.org/pdf/2409.16673v1.pdf","comment":"Published in CIKM 2020"},{"id":"http://arxiv.org/abs/2407.03536v2","updated":"2024-09-25T07:05:16Z","published":"2024-07-03T22:45:36Z","title":"Social Bias in Large Language Models For Bangla: An Empirical Study on\n  Gender and Religious Bias","summary":"  The rapid growth of Large Language Models (LLMs) has put forward the study of\nbiases as a crucial field. It is important to assess the influence of different\ntypes of biases embedded in LLMs to ensure fair use in sensitive fields.\nAlthough there have been extensive works on bias assessment in English, such\nefforts are rare and scarce for a major language like Bangla. In this work, we\nexamine two types of social biases in LLM generated outputs for Bangla\nlanguage. Our main contributions in this work are: (1) bias studies on two\ndifferent social biases for Bangla (2) a curated dataset for bias measurement\nbenchmarking (3) testing two different probing techniques for bias detection in\nthe context of Bangla. This is the first work of such kind involving bias\nassessment of LLMs for Bangla to the best of our knowledge. All our code and\nresources are publicly available for the progress of bias related research in\nBangla NLP.\n","authors":["Jayanta Sadhu","Maneesha Rani Saha","Rifat Shahriyar"],"pdf_url":"https://arxiv.org/pdf/2407.03536v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15868v2","updated":"2024-09-25T07:03:05Z","published":"2024-09-24T08:41:26Z","title":"Privacy Evaluation Benchmarks for NLP Models","summary":"  By inducing privacy attacks on NLP models, attackers can obtain sensitive\ninformation such as training data and model parameters, etc. Although\nresearchers have studied, in-depth, several kinds of attacks in NLP models,\nthey are non-systematic analyses. It lacks a comprehensive understanding of the\nimpact caused by the attacks. For example, we must consider which scenarios can\napply to which attacks, what the common factors are that affect the performance\nof different attacks, the nature of the relationships between different\nattacks, and the influence of various datasets and models on the effectiveness\nof the attacks, etc. Therefore, we need a benchmark to holistically assess the\nprivacy risks faced by NLP models. In this paper, we present a privacy attack\nand defense evaluation benchmark in the field of NLP, which includes the\nconventional/small models and large language models (LLMs). This benchmark\nsupports a variety of models, datasets, and protocols, along with standardized\nmodules for comprehensive evaluation of attacks and defense strategies. Based\non the above framework, we present a study on the association between auxiliary\ndata from different domains and the strength of privacy attacks. And we provide\nan improved attack method in this scenario with the help of Knowledge\nDistillation (KD). Furthermore, we propose a chained framework for privacy\nattacks. Allowing a practitioner to chain multiple attacks to achieve a\nhigher-level attack objective. Based on this, we provide some defense and\nenhanced attack strategies. The code for reproducing the results can be found\nat https://github.com/user2311717757/nlp_doctor.\n","authors":["Wei Huang","Yinggui Wang","Cen Chen"],"pdf_url":"https://arxiv.org/pdf/2409.15868v2.pdf","comment":"Needs further optimization"},{"id":"http://arxiv.org/abs/2409.16668v1","updated":"2024-09-25T06:55:33Z","published":"2024-09-25T06:55:33Z","title":"Topic-aware Causal Intervention for Counterfactual Detection","summary":"  Counterfactual statements, which describe events that did not or cannot take\nplace, are beneficial to numerous NLP applications. Hence, we consider the\nproblem of counterfactual detection (CFD) and seek to enhance the CFD models.\nPrevious models are reliant on clue phrases to predict counterfactuality, so\nthey suffer from significant performance drop when clue phrase hints do not\nexist during testing. Moreover, these models tend to predict\nnon-counterfactuals over counterfactuals. To address these issues, we propose\nto integrate neural topic model into the CFD model to capture the global\nsemantics of the input statement. We continue to causally intervene the hidden\nrepresentations of the CFD model to balance the effect of the class labels.\nExtensive experiments show that our approach outperforms previous\nstate-of-the-art CFD and bias-resolving methods in both the CFD and other\nbias-sensitive tasks.\n","authors":["Thong Nguyen","Truc-My Nguyen"],"pdf_url":"https://arxiv.org/pdf/2409.16668v1.pdf","comment":"Accepted to the 4th EMNLP-NLP4DH 2024 workshop"},{"id":"http://arxiv.org/abs/2409.16667v1","updated":"2024-09-25T06:54:29Z","published":"2024-09-25T06:54:29Z","title":"A Character-Centric Creative Story Generation via Imagination","summary":"  Creative story generation with diverse and detailed story elements is a\nlong-standing goal for large language models. While existing methodologies\ngenerate long and coherent stories, they fall significantly short of human\ncapabilities in terms of diversity and character detail. To address this, we\nintroduce a novel story generation framework called CCI (Character-centric\nCreative story generation via Imagination). CCI features two innovative modules\nfor creative story generation: IG (Image-Guided Imagination) and MW\n(Multi-Writer model). In the IG module, we utilize DALL-E 3 to create visual\nrepresentations of key story elements. The IG generates more novel and concrete\ncharacters, backgrounds, and main plots than text-only methods. The MW module\nuses these story elements created by IG to generate multiple description\ncandidates for the protagonist and select the best one. This method\nincorporates vivid and rich character descriptions into the story. We compared\nthe stories generated by CCI and baseline models through human evaluation and\nstatistical analysis. The results showed significant improvements in the\ncreativity. Furthermore, by enabling interactive multi-modal story generation\nwith users, we have opened up possibilities for human-LLM integration in\ncultural development.\n","authors":["Kyeongman Park","Minbeom Kim","Kyomin Jung"],"pdf_url":"https://arxiv.org/pdf/2409.16667v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15355v2","updated":"2024-09-25T06:46:42Z","published":"2024-09-14T02:34:26Z","title":"Block-Attention for Efficient RAG","summary":"  We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Unlike existing works that encodes the whole context, its main idea\nlies in dividing the retrieved documents into blocks, where each block\ncalculates key-value (KV) states independently except for the final block. In\nRAG scenarios, by defining each passage as a block, Block-Attention enables us\nto pre-compute the KV states for all passages and cache them in memory,\nsignificantly reducing the latency and the computation cost during inference.\nThe implementation involves block segmentation, positional encoding\ncalculation, and fine-tuning the LLM to adapt to the Block-Attention mechanism.\nExperiments on four RAG benchmarks demonstrate that after block fine-tuning,\nthe Block Attention model can achieve performance comparable to (68.4\\% vs\n67.9\\% on Llama3) or even better (62.8\\% vs 59.6\\% on Mistral) than\nself-attention models. Notably, Block-Attention reduces the TTFT (the time to\nfirst token) and FLOPs (floating point operations) to a very low level. It only\ntakes 45 ms to output the first token for an input sequence with a total length\nof 32K. Compared with the self-attention model, the time consumption and\ncorresponding FLOPs are reduced by 98.7\\% and 99.8\\%, respectively.\n","authors":["East Sun","Yan Wang","Lan Tian"],"pdf_url":"https://arxiv.org/pdf/2409.15355v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16658v1","updated":"2024-09-25T06:22:30Z","published":"2024-09-25T06:22:30Z","title":"Pre-trained Language Models Return Distinguishable Probability\n  Distributions to Unfaithfully Hallucinated Texts","summary":"  In this work, we show the pre-trained language models return distinguishable\ngeneration probability and uncertainty distribution to unfaithfully\nhallucinated texts, regardless of their size and structure. By examining 24\nmodels on 6 data sets, we find out that 88-98% of cases return statistically\nsignificantly distinguishable generation probability and uncertainty\ndistributions. Using this general phenomenon, we showcase a\nhallucination-reducing training algorithm. Our algorithm outperforms other\nbaselines by achieving higher faithfulness metrics while maintaining sound\ngeneral text quality measures.\n","authors":["Taehun Cha","Donghun Lee"],"pdf_url":"https://arxiv.org/pdf/2409.16658v1.pdf","comment":"10 pages, EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2408.11871v2","updated":"2024-09-25T06:21:26Z","published":"2024-08-19T13:27:07Z","title":"MegaFake: A Theory-Driven Dataset of Fake News Generated by Large\n  Language Models","summary":"  The advent of large language models (LLMs) has revolutionized online content\ncreation, making it much easier to generate high-quality fake news. This misuse\nthreatens the integrity of our digital environment and ethical standards.\nTherefore, understanding the motivations and mechanisms behind LLM-generated\nfake news is crucial. In this study, we analyze the creation of fake news from\na social psychology perspective and develop a comprehensive LLM-based\ntheoretical framework, LLM-Fake Theory. We introduce a novel pipeline that\nautomates the generation of fake news using LLMs, thereby eliminating the need\nfor manual annotation. Utilizing this pipeline, we create a theoretically\ninformed Machine-generated Fake news dataset, MegaFake, derived from the\nGossipCop dataset. We conduct comprehensive analyses to evaluate our MegaFake\ndataset. We believe that our dataset and insights will provide valuable\ncontributions to future research focused on the detection and governance of\nfake news in the era of LLMs.\n","authors":["Lionel Z. Wang","Yiming Ma","Renfei Gao","Beichen Guo","Han Zhu","Wenqi Fan","Zexin Lu","Ka Chung Ng"],"pdf_url":"https://arxiv.org/pdf/2408.11871v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16654v1","updated":"2024-09-25T06:17:23Z","published":"2024-09-25T06:17:23Z","title":"Speech Recognition Rescoring with Large Speech-Text Foundation Models","summary":"  Large language models (LLM) have demonstrated the ability to understand human\nlanguage by leveraging large amount of text data. Automatic speech recognition\n(ASR) systems are often limited by available transcribed speech data and\nbenefit from a second pass rescoring using LLM. Recently multi-modal large\nlanguage models, particularly speech and text foundational models have\ndemonstrated strong spoken language understanding. Speech-Text foundational\nmodels leverage large amounts of unlabelled and labelled data both in speech\nand text modalities to model human language. In this work, we propose novel\ntechniques to use multi-modal LLM for ASR rescoring. We also explore\ndiscriminative training to further improve the foundational model rescoring\nperformance. We demonstrate cross-modal knowledge transfer in speech-text LLM\ncan benefit rescoring. Our experiments demonstrate up-to 20% relative\nimprovements over Whisper large ASR and up-to 15% relative improvements over\ntext-only LLM.\n","authors":["Prashanth Gurunath Shivakumar","Jari Kolehmainen","Aditya Gourav","Yi Gu","Ankur Gandhe","Ariya Rastrow","Ivan Bulyko"],"pdf_url":"https://arxiv.org/pdf/2409.16654v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.21439v2","updated":"2024-09-25T06:14:03Z","published":"2024-07-31T08:43:17Z","title":"MLLM Is a Strong Reranker: Advancing Multimodal Retrieval-augmented\n  Generation via Knowledge-enhanced Reranking and Noise-injected Training","summary":"  Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in processing and generating content across multiple data\nmodalities. However, a significant drawback of MLLMs is their reliance on\nstatic training data, leading to outdated information and limited contextual\nawareness. This static nature hampers their ability to provide accurate and\nup-to-date responses, particularly in dynamic or rapidly evolving contexts.\nThough integrating Multimodal Retrieval-augmented Generation (Multimodal RAG)\noffers a promising solution, the system would inevitably encounter the\nmulti-granularity noisy correspondence (MNC) problem, which hinders accurate\nretrieval and generation. In this work, we propose RagVL, a novel framework\nwith knowledge-enhanced reranking and noise-injected training, to address these\nlimitations. We instruction-tune the MLLM with a simple yet effective\ninstruction template to induce its ranking ability and serve it as a reranker\nto precisely filter the top-k retrieved images. For generation, we inject\nvisual noise during training at the data and token levels to enhance the\ngenerator's robustness. Extensive experiments on the subsets of two datasets\nthat require retrieving and reasoning over images to answer a given query\nverify the effectiveness of our method. Code and models are available at\nhttps://github.com/IDEA-FinAI/RagVL.\n","authors":["Zhanpeng Chen","Chengjin Xu","Yiyan Qi","Jian Guo"],"pdf_url":"https://arxiv.org/pdf/2407.21439v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16647v1","updated":"2024-09-25T06:04:03Z","published":"2024-09-25T06:04:03Z","title":"Domain-Independent Automatic Generation of Descriptive Texts for\n  Time-Series Data","summary":"  Due to scarcity of time-series data annotated with descriptive texts,\ntraining a model to generate descriptive texts for time-series data is\nchallenging. In this study, we propose a method to systematically generate\ndomain-independent descriptive texts from time-series data. We identify two\ndistinct approaches for creating pairs of time-series data and descriptive\ntexts: the forward approach and the backward approach. By implementing the\nnovel backward approach, we create the Temporal Automated Captions for\nObservations (TACO) dataset. Experimental results demonstrate that a\ncontrastive learning based model trained using the TACO dataset is capable of\ngenerating descriptive texts for time-series data in novel domains.\n","authors":["Kota Dohi","Aoi Ito","Harsh Purohit","Tomoya Nishida","Takashi Endo","Yohei Kawaguchi"],"pdf_url":"https://arxiv.org/pdf/2409.16647v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16646v1","updated":"2024-09-25T05:57:09Z","published":"2024-09-25T05:57:09Z","title":"Cross-Lingual and Cross-Cultural Variation in Image Descriptions","summary":"  Do speakers of different languages talk differently about what they see?\nBehavioural and cognitive studies report cultural effects on perception;\nhowever, these are mostly limited in scope and hard to replicate. In this work,\nwe conduct the first large-scale empirical study of cross-lingual variation in\nimage descriptions. Using a multimodal dataset with 31 languages and images\nfrom diverse locations, we develop a method to accurately identify entities\nmentioned in captions and present in the images, then measure how they vary\nacross languages. Our analysis reveals that pairs of languages that are\ngeographically or genetically closer tend to mention the same entities more\nfrequently. We also identify entity categories whose saliency is universally\nhigh (such as animate beings), low (clothing accessories) or displaying high\nvariance across languages (landscape). In a case study, we measure the\ndifferences in a specific language pair (e.g., Japanese mentions clothing far\nmore frequently than English). Furthermore, our method corroborates previous\nsmall-scale studies, including 1) Rosch et al. (1976)'s theory of basic-level\ncategories, demonstrating a preference for entities that are neither too\ngeneric nor too specific, and 2) Miyamoto et al. (2006)'s hypothesis that\nenvironments afford patterns of perception, such as entity counts. Overall, our\nwork reveals the presence of both universal and culture-specific patterns in\nentity mentions.\n","authors":["Uri Berger","Edoardo M. Ponti"],"pdf_url":"https://arxiv.org/pdf/2409.16646v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16644v1","updated":"2024-09-25T05:44:44Z","published":"2024-09-25T05:44:44Z","title":"Enabling Auditory Large Language Models for Automatic Speech Quality\n  Evaluation","summary":"  Speech quality assessment typically requires evaluating audio from multiple\naspects, such as mean opinion score (MOS) and speaker similarity (SIM) etc.,\nwhich can be challenging to cover using one small model designed for a single\ntask. In this paper, we propose leveraging recently introduced auditory large\nlanguage models (LLMs) for automatic speech quality assessment. By employing\ntask-specific prompts, auditory LLMs are finetuned to predict MOS, SIM and A/B\ntesting results, which are commonly used for evaluating text-to-speech systems.\nAdditionally, the finetuned auditory LLM is able to generate natural language\ndescriptions assessing aspects like noisiness, distortion, discontinuity, and\noverall quality, providing more interpretable outputs. Extensive experiments\nhave been performed on the NISQA, BVCC, SOMOS and VoxSim speech quality\ndatasets, using open-source auditory LLMs such as SALMONN, Qwen-Audio, and\nQwen2-Audio. For the natural language descriptions task, a commercial model\nGoogle Gemini 1.5 Pro is also evaluated. The results demonstrate that auditory\nLLMs achieve competitive performance compared to state-of-the-art task-specific\nsmall models in predicting MOS and SIM, while also delivering promising results\nin A/B testing and natural language descriptions. Our data processing scripts\nand finetuned model checkpoints will be released upon acceptance.\n","authors":["Siyin Wang","Wenyi Yu","Yudong Yang","Changli Tang","Yixuan Li","Jimin Zhuang","Xianzhao Chen","Xiaohai Tian","Jun Zhang","Guangzhi Sun","Lu Lu","Chao Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.16644v1.pdf","comment":"submitted to ICASSP 2025"},{"id":"http://arxiv.org/abs/2409.16636v1","updated":"2024-09-25T05:28:33Z","published":"2024-09-25T05:28:33Z","title":"Training Language Models to Win Debates with Self-Play Improves Judge\n  Accuracy","summary":"  We test the robustness of debate as a method of scalable oversight by\ntraining models to debate with data generated via self-play. In a long-context\nreading comprehension task, we find that language model based evaluators answer\nquestions more accurately when judging models optimized to win debates. By\ncontrast, we find no such relationship for consultancy models trained to\npersuade a judge without an opposing debater present. In quantitative and\nqualitative comparisons between our debate models and novel consultancy\nbaselines, we find evidence that debate training encourages stronger and more\ninformative arguments, showing promise that it can help provide high-quality\nsupervision for tasks that are difficult to directly evaluate.\n","authors":["Samuel Arnesen","David Rein","Julian Michael"],"pdf_url":"https://arxiv.org/pdf/2409.16636v1.pdf","comment":"48 pages, 12 figures; code at\n  https://github.com/samuelarnesen/nyu-debate-modeling"},{"id":"http://arxiv.org/abs/2409.14509v2","updated":"2024-09-25T04:58:57Z","published":"2024-09-22T16:13:00Z","title":"Can AI writing be salvaged? Mitigating Idiosyncrasies and Improving\n  Human-AI Alignment in the Writing Process through Edits","summary":"  LLM-based applications are helping people write, and LLM-generated text is\nmaking its way into social media, journalism, and our classrooms. However, the\ndifferences between LLM-generated and human-written text remain unclear. To\nexplore this, we hired professional writers to edit paragraphs in several\ncreative domains. We first found these writers agree on undesirable\nidiosyncrasies in LLM-generated text, formalizing it into a seven-category\ntaxonomy (e.g. cliches, unnecessary exposition). Second, we curated the LAMP\ncorpus: 1,057 LLM-generated paragraphs edited by professional writers according\nto our taxonomy. Analysis of LAMP reveals that none of the LLMs used in our\nstudy (GPT4o, Claude-3.5-Sonnet, Llama-3.1-70b) outperform each other in terms\nof writing quality, revealing common limitations across model families. Third,\nwe explored automatic editing methods to improve LLM-generated text. A\nlarge-scale preference annotation confirms that although experts largely prefer\ntext edited by other experts, automatic editing methods show promise in\nimproving alignment between LLM-generated and human-written text.\n","authors":["Tuhin Chakrabarty","Philippe Laban","Chien-Sheng Wu"],"pdf_url":"https://arxiv.org/pdf/2409.14509v2.pdf","comment":"NLP+HCI, Behavioral Science"},{"id":"http://arxiv.org/abs/2409.16618v1","updated":"2024-09-25T04:53:27Z","published":"2024-09-25T04:53:27Z","title":"Claim-Guided Textual Backdoor Attack for Practical Applications","summary":"  Recent advances in natural language processing and the increased use of large\nlanguage models have exposed new security vulnerabilities, such as backdoor\nattacks. Previous backdoor attacks require input manipulation after model\ndistribution to activate the backdoor, posing limitations in real-world\napplicability. Addressing this gap, we introduce a novel Claim-Guided Backdoor\nAttack (CGBA), which eliminates the need for such manipulations by utilizing\ninherent textual claims as triggers. CGBA leverages claim extraction,\nclustering, and targeted training to trick models to misbehave on targeted\nclaims without affecting their performance on clean data. CGBA demonstrates its\neffectiveness and stealthiness across various datasets and models,\nsignificantly enhancing the feasibility of practical backdoor attacks. Our code\nand data will be available at https://github.com/PaperCGBA/CGBA.\n","authors":["Minkyoo Song","Hanna Kim","Jaehan Kim","Youngjin Jin","Seungwon Shin"],"pdf_url":"https://arxiv.org/pdf/2409.16618v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2310.02374v5","updated":"2024-09-25T04:50:38Z","published":"2023-10-03T18:54:10Z","title":"Conversational Health Agents: A Personalized LLM-Powered Agent Framework","summary":"  Conversational Health Agents (CHAs) are interactive systems that provide\nhealthcare services, such as assistance and diagnosis. Current CHAs, especially\nthose utilizing Large Language Models (LLMs), primarily focus on conversation\naspects. However, they offer limited agent capabilities, specifically lacking\nmulti-step problem-solving, personalized conversations, and multimodal data\nanalysis. Our aim is to overcome these limitations. We propose openCHA, an\nopen-source LLM-powered framework, to empower conversational agents to generate\na personalized response for users' healthcare queries. This framework enables\ndevelopers to integrate external sources including data sources, knowledge\nbases, and analysis models, into their LLM-based solutions. openCHA includes an\norchestrator to plan and execute actions for gathering information from\nexternal sources, essential for formulating responses to user inquiries. It\nfacilitates knowledge acquisition, problem-solving capabilities, multilingual\nand multimodal conversations, and fosters interaction with various AI\nplatforms. We illustrate the framework's proficiency in handling complex\nhealthcare tasks via two demonstrations and four use cases. Moreover, we\nrelease openCHA as open source available to the community via GitHub.\n","authors":["Mahyar Abbasian","Iman Azimi","Amir M. Rahmani","Ramesh Jain"],"pdf_url":"https://arxiv.org/pdf/2310.02374v5.pdf","comment":"23 pages, 6 figures, 2 tables, 4 appendices, journal paper"},{"id":"http://arxiv.org/abs/2409.16605v1","updated":"2024-09-25T04:12:38Z","published":"2024-09-25T04:12:38Z","title":"Evaluating and Enhancing Large Language Models for Novelty Assessment in\n  Scholarly Publications","summary":"  Recent studies have evaluated the creativity/novelty of large language models\n(LLMs) primarily from a semantic perspective, using benchmarks from cognitive\nscience. However, accessing the novelty in scholarly publications is a largely\nunexplored area in evaluating LLMs. In this paper, we introduce a scholarly\nnovelty benchmark (SchNovel) to evaluate LLMs' ability to assess novelty in\nscholarly papers. SchNovel consists of 15000 pairs of papers across six fields\nsampled from the arXiv dataset with publication dates spanning 2 to 10 years\napart. In each pair, the more recently published paper is assumed to be more\nnovel. Additionally, we propose RAG-Novelty, which simulates the review process\ntaken by human reviewers by leveraging the retrieval of similar papers to\nassess novelty. Extensive experiments provide insights into the capabilities of\ndifferent LLMs to assess novelty and demonstrate that RAG-Novelty outperforms\nrecent baseline models.\n","authors":["Ethan Lin","Zhiyuan Peng","Yi Fang"],"pdf_url":"https://arxiv.org/pdf/2409.16605v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2409.16603v1","updated":"2024-09-25T04:02:54Z","published":"2024-09-25T04:02:54Z","title":"Overview of the First Shared Task on Clinical Text Generation: RRG24 and\n  \"Discharge Me!\"","summary":"  Recent developments in natural language generation have tremendous\nimplications for healthcare. For instance, state-of-the-art systems could\nautomate the generation of sections in clinical reports to alleviate physician\nworkload and streamline hospital documentation. To explore these applications,\nwe present a shared task consisting of two subtasks: (1) Radiology Report\nGeneration (RRG24) and (2) Discharge Summary Generation (\"Discharge Me!\").\nRRG24 involves generating the 'Findings' and 'Impression' sections of radiology\nreports given chest X-rays. \"Discharge Me!\" involves generating the 'Brief\nHospital Course' and 'Discharge Instructions' sections of discharge summaries\nfor patients admitted through the emergency department. \"Discharge Me!\"\nsubmissions were subsequently reviewed by a team of clinicians. Both tasks\nemphasize the goal of reducing clinician burnout and repetitive workloads by\ngenerating documentation. We received 201 submissions from across 8 teams for\nRRG24, and 211 submissions from across 16 teams for \"Discharge Me!\".\n","authors":["Justin Xu","Zhihong Chen","Andrew Johnston","Louis Blankemeier","Maya Varma","Jason Hom","William J. Collins","Ankit Modi","Robert Lloyd","Benjamin Hopkins","Curtis Langlotz","Jean-Benoit Delbrouck"],"pdf_url":"https://arxiv.org/pdf/2409.16603v1.pdf","comment":"ACL Proceedings. BioNLP workshop"},{"id":"http://arxiv.org/abs/2409.15657v2","updated":"2024-09-25T03:24:39Z","published":"2024-09-24T01:40:24Z","title":"M^2PT: Multimodal Prompt Tuning for Zero-shot Instruction Learning","summary":"  Multimodal Large Language Models (MLLMs) demonstrate remarkable performance\nacross a wide range of domains, with increasing emphasis on enhancing their\nzero-shot generalization capabilities for unseen tasks across various\nmodalities. Instruction tuning has emerged as an effective strategy for\nachieving zero-shot generalization by finetuning pretrained models on diverse\nmultimodal tasks. As the scale of MLLMs continues to grow, parameter-efficient\nfinetuning becomes increasingly critical. However, most existing\nparameter-efficient approaches focus only on single modalities and often\noverlook the multimodal characteristics during finetuning. In this work, we\nintroduce a novel Multimodal Prompt Tuning (M$^2$PT) approach for efficient\ninstruction tuning of MLLMs. M$^2$PT effectively integrates visual and textual\nprompts into the vision encoder and language processor respectively during\nfinetuning, facilitating the extraction and alignment of features across\nmodalities. Empirical results on various multimodal evaluation datasets\ndemonstrate the superior performance of our approach compared to several\nstate-of-the-art baselines. A comprehensive set of ablation studies validates\nthe effectiveness of our prompt design and the efficiency of our approach.\n","authors":["Taowen Wang","Yiyang Liu","James Chenhao Liang","junhan zhao","Yiming Cui","Yuning Mao","Shaoliang Nie","Jiahao Liu","Fuli Feng","Zenglin Xu","Cheng Han","Lifu Huang","Qifan Wang","Dongfang Liu"],"pdf_url":"https://arxiv.org/pdf/2409.15657v2.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2409.16570v1","updated":"2024-09-25T02:53:27Z","published":"2024-09-25T02:53:27Z","title":"Disentangling Questions from Query Generation for Task-Adaptive\n  Retrieval","summary":"  This paper studies the problem of information retrieval, to adapt to unseen\ntasks. Existing work generates synthetic queries from domain-specific documents\nto jointly train the retriever. However, the conventional query generator\nassumes the query as a question, thus failing to accommodate general search\nintents. A more lenient approach incorporates task-adaptive elements, such as\nfew-shot learning with an 137B LLM. In this paper, we challenge a trend\nequating query and question, and instead conceptualize query generation task as\na \"compilation\" of high-level intent into task-adaptive query. Specifically, we\npropose EGG, a query generator that better adapts to wide search intents\nexpressed in the BeIR benchmark. Our method outperforms baselines and existing\nmodels on four tasks with underexplored intents, while utilizing a query\ngenerator 47 times smaller than the previous state-of-the-art. Our findings\nreveal that instructing the LM with explicit search intent is a key aspect of\nmodeling an effective query generator.\n","authors":["Yoonsang Lee","Minsoo Kim","Seung-won Hwang"],"pdf_url":"https://arxiv.org/pdf/2409.16570v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.16257v2","updated":"2024-09-25T02:15:32Z","published":"2024-04-25T00:05:19Z","title":"Translation of Multifaceted Data without Re-Training of Machine\n  Translation Systems","summary":"  Translating major language resources to build minor language resources\nbecomes a widely-used approach. Particularly in translating complex data points\ncomposed of multiple components, it is common to translate each component\nseparately. However, we argue that this practice often overlooks the\ninterrelation between components within the same data point. To address this\nlimitation, we propose a novel MT pipeline that considers the intra-data\nrelation in implementing MT for training data. In our MT pipeline, all the\ncomponents in a data point are concatenated to form a single translation\nsequence and subsequently reconstructed to the data components after\ntranslation. We introduce a Catalyst Statement (CS) to enhance the intra-data\nrelation, and Indicator Token (IT) to assist the decomposition of a translated\nsequence into its respective data components. Through our approach, we have\nachieved a considerable improvement in translation quality itself, along with\nits effectiveness as training data. Compared with the conventional approach\nthat translates each data component separately, our method yields better\ntraining data that enhances the performance of the trained model by 2.690\npoints for the web page ranking (WPR) task, and 0.845 for the question\ngeneration (QG) task in the XGLUE benchmark.\n","authors":["Hyeonseok Moon","Seungyoon Lee","Seongtae Hong","Seungjun Lee","Chanjun Park","Heuiseok Lim"],"pdf_url":"https://arxiv.org/pdf/2404.16257v2.pdf","comment":"Accepted to EMNLP2024 findings"},{"id":"http://arxiv.org/abs/2409.11283v3","updated":"2024-09-25T01:55:29Z","published":"2024-09-17T15:38:36Z","title":"Zero-resource Hallucination Detection for Text Generation via\n  Graph-based Contextual Knowledge Triples Modeling","summary":"  LLMs obtain remarkable performance but suffer from hallucinations. Most\nresearch on detecting hallucination focuses on the questions with short and\nconcrete correct answers that are easy to check the faithfulness. Hallucination\ndetections for text generation with open-ended answers are more challenging.\nSome researchers use external knowledge to detect hallucinations in generated\ntexts, but external resources for specific scenarios are hard to access. Recent\nstudies on detecting hallucinations in long text without external resources\nconduct consistency comparison among multiple sampled outputs. To handle long\ntexts, researchers split long texts into multiple facts and individually\ncompare the consistency of each pairs of facts. However, these methods (1)\nhardly achieve alignment among multiple facts; (2) overlook dependencies\nbetween multiple contextual facts. In this paper, we propose a graph-based\ncontext-aware (GCA) hallucination detection for text generations, which aligns\nknowledge facts and considers the dependencies between contextual knowledge\ntriples in consistency comparison. Particularly, to align multiple facts, we\nconduct a triple-oriented response segmentation to extract multiple knowledge\ntriples. To model dependencies among contextual knowledge triple (facts), we\nconstruct contextual triple into a graph and enhance triples' interactions via\nmessage passing and aggregating via RGCN. To avoid the omission of knowledge\ntriples in long text, we conduct a LLM-based reverse verification via\nreconstructing the knowledge triples. Experiments show that our model enhances\nhallucination detection and excels all baselines.\n","authors":["Xinyue Fang","Zhen Huang","Zhiliang Tian","Minghui Fang","Ziyi Pan","Quntian Fang","Zhihua Wen","Hengyue Pan","Dongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2409.11283v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.12822v2","updated":"2024-09-25T00:32:31Z","published":"2024-09-19T14:50:34Z","title":"Language Models Learn to Mislead Humans via RLHF","summary":"  Language models (LMs) can produce errors that are hard to detect for humans,\nespecially when the task is complex. RLHF, the most popular post-training\nmethod, may exacerbate this problem: to achieve higher rewards, LMs might get\nbetter at convincing humans that they are right even when they are wrong. We\nstudy this phenomenon under a standard RLHF pipeline, calling it \"U-SOPHISTRY\"\nsince it is Unintended by model developers. Specifically, we ask\ntime-constrained (e.g., 3-10 minutes) human subjects to evaluate the\ncorrectness of model outputs and calculate humans' accuracy against gold\nlabels. On a question-answering task (QuALITY) and programming task (APPS),\nRLHF makes LMs better at convincing our subjects but not at completing the task\ncorrectly. RLHF also makes the model harder to evaluate: our subjects' false\npositive rate increases by 24.1% on QuALITY and 18.3% on APPS. Finally, we show\nthat probing, a state-of-the-art approach for detecting Intended Sophistry\n(e.g. backdoored LMs), does not generalize to U-SOPHISTRY. Our results\nhighlight an important failure mode of RLHF and call for more research in\nassisting humans to align them.\n","authors":["Jiaxin Wen","Ruiqi Zhong","Akbir Khan","Ethan Perez","Jacob Steinhardt","Minlie Huang","Samuel R. Bowman","He He","Shi Feng"],"pdf_url":"https://arxiv.org/pdf/2409.12822v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.01556v2","updated":"2024-09-25T00:31:18Z","published":"2024-09-03T02:50:04Z","title":"Benchmarking Cognitive Domains for LLMs: Insights from Taiwanese Hakka\n  Culture","summary":"  This study introduces a comprehensive benchmark designed to evaluate the\nperformance of large language models (LLMs) in understanding and processing\ncultural knowledge, with a specific focus on Hakka culture as a case study.\nLeveraging Bloom's Taxonomy, the study develops a multi-dimensional framework\nthat systematically assesses LLMs across six cognitive domains: Remembering,\nUnderstanding, Applying, Analyzing, Evaluating, and Creating. This benchmark\nextends beyond traditional single-dimensional evaluations by providing a deeper\nanalysis of LLMs' abilities to handle culturally specific content, ranging from\nbasic recall of facts to higher-order cognitive tasks such as creative\nsynthesis. Additionally, the study integrates Retrieval-Augmented Generation\n(RAG) technology to address the challenges of minority cultural knowledge\nrepresentation in LLMs, demonstrating how RAG enhances the models' performance\nby dynamically incorporating relevant external information. The results\nhighlight the effectiveness of RAG in improving accuracy across all cognitive\ndomains, particularly in tasks requiring precise retrieval and application of\ncultural knowledge. However, the findings also reveal the limitations of RAG in\ncreative tasks, underscoring the need for further optimization. This benchmark\nprovides a robust tool for evaluating and comparing LLMs in culturally diverse\ncontexts, offering valuable insights for future research and development in\nAI-driven cultural knowledge preservation and dissemination.\n","authors":["Chen-Chi Chang","Ching-Yuan Chen","Hung-Shin Lee","Chih-Cheng Lee"],"pdf_url":"https://arxiv.org/pdf/2409.01556v2.pdf","comment":"Accepted to O-COCOSDA 2024"},{"id":"http://arxiv.org/abs/2403.07937v2","updated":"2024-09-25T00:28:55Z","published":"2024-03-08T08:10:29Z","title":"Speech Robust Bench: A Robustness Benchmark For Speech Recognition","summary":"  As Automatic Speech Recognition (ASR) models become ever more pervasive, it\nis important to ensure that they make reliable predictions under corruptions\npresent in the physical and digital world. We propose Speech Robust Bench\n(SRB), a comprehensive benchmark for evaluating the robustness of ASR models to\ndiverse corruptions. SRB is composed of 114 input perturbations which simulate\nan heterogeneous range of corruptions that ASR models may encounter when\ndeployed in the wild. We use SRB to evaluate the robustness of several\nstate-of-the-art ASR models and observe that model size and certain modeling\nchoices such as the use of discrete representations, or self-training appear to\nbe conducive to robustness. We extend this analysis to measure the robustness\nof ASR models on data from various demographic subgroups, namely English and\nSpanish speakers, and males and females. Our results revealed noticeable\ndisparities in the model's robustness across subgroups. We believe that SRB\nwill significantly facilitate future research towards robust ASR models, by\nmaking it easier to conduct comprehensive and comparable robustness\nevaluations.\n","authors":["Muhammad A. Shah","David Solans Noguero","Mikko A. Heikkila","Bhiksha Raj","Nicolas Kourtellis"],"pdf_url":"https://arxiv.org/pdf/2403.07937v2.pdf","comment":"submitted to NeurIPS datasets and benchmark track 2025"},{"id":"http://arxiv.org/abs/2409.16521v1","updated":"2024-09-25T00:26:11Z","published":"2024-09-25T00:26:11Z","title":"Understanding the Cognitive Complexity in Language Elicited by Product\n  Images","summary":"  Product images (e.g., a phone) can be used to elicit a diverse set of\nconsumer-reported features expressed through language, including surface-level\nperceptual attributes (e.g., \"white\") and more complex ones, like perceived\nutility (e.g., \"battery\"). The cognitive complexity of elicited language\nreveals the nature of cognitive processes and the context required to\nunderstand them; cognitive complexity also predicts consumers' subsequent\nchoices. This work offers an approach for measuring and validating the\ncognitive complexity of human language elicited by product images, providing a\ntool for understanding the cognitive processes of human as well as virtual\nrespondents simulated by Large Language Models (LLMs). We also introduce a\nlarge dataset that includes diverse descriptive labels for product images,\nincluding human-rated complexity. We demonstrate that human-rated cognitive\ncomplexity can be approximated using a set of natural language models that,\ncombined, roughly capture the complexity construct. Moreover, this approach is\nminimally supervised and scalable, even in use cases with limited human\nassessment of complexity.\n","authors":["Yan-Ying Chen","Shabnam Hakimi","Monica Van","Francine Chen","Matthew Hong","Matt Klenk","Charlene Wu"],"pdf_url":"https://arxiv.org/pdf/2409.16521v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.05892v3","updated":"2024-09-25T00:02:03Z","published":"2024-04-08T22:20:59Z","title":"Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence","summary":"  We present Eagle (RWKV-5) and Finch (RWKV-6), sequence models improving upon\nthe RWKV (RWKV-4) architecture. Our architectural design advancements include\nmulti-headed matrix-valued states and a dynamic recurrence mechanism that\nimprove expressivity while maintaining the inference efficiency characteristics\nof RNNs. We introduce a new multilingual corpus with 1.12 trillion tokens and a\nfast tokenizer based on greedy matching for enhanced multilinguality. We\ntrained four Eagle models, ranging from 0.46 to 7.5 billion parameters, and two\nFinch models with 1.6 and 3.1 billion parameters and find that they achieve\ncompetitive performance across a wide variety of benchmarks. We release all our\nmodels on HuggingFace under the Apache 2.0 license. Models at:\nhttps://huggingface.co/RWKV Training code at: https://github.com/RWKV/RWKV-LM\nInference code at: https://github.com/RWKV/ChatRWKV Time-parallel training code\nat: https://github.com/RWKV/RWKV-infctx-trainer\n","authors":["Bo Peng","Daniel Goldstein","Quentin Anthony","Alon Albalak","Eric Alcaide","Stella Biderman","Eugene Cheah","Xingjian Du","Teddy Ferdinan","Haowen Hou","Przemysław Kazienko","Kranthi Kiran GV","Jan Kocoń","Bartłomiej Koptyra","Satyapriya Krishna","Ronald McClelland Jr.","Niklas Muennighoff","Fares Obeid","Atsushi Saito","Guangyu Song","Haoqin Tu","Stanisław Woźniak","Ruichong Zhang","Bingchen Zhao","Qihang Zhao","Peng Zhou","Jian Zhu","Rui-Jie Zhu"],"pdf_url":"https://arxiv.org/pdf/2404.05892v3.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2409.17146v1","updated":"2024-09-25T17:59:51Z","published":"2024-09-25T17:59:51Z","title":"Molmo and PixMo: Open Weights and Open Data for State-of-the-Art\n  Multimodal Models","summary":"  Today's most advanced multimodal models remain proprietary. The strongest\nopen-weight models rely heavily on synthetic data from proprietary VLMs to\nachieve good performance, effectively distilling these closed models into open\nones. As a result, the community is still missing foundational knowledge about\nhow to build performant VLMs from scratch. We present Molmo, a new family of\nVLMs that are state-of-the-art in their class of openness. Our key innovation\nis a novel, highly detailed image caption dataset collected entirely from human\nannotators using speech-based descriptions. To enable a wide array of user\ninteractions, we also introduce a diverse dataset mixture for fine-tuning that\nincludes in-the-wild Q&A and innovative 2D pointing data. The success of our\napproach relies on careful choices for the model architecture details, a\nwell-tuned training pipeline, and, most critically, the quality of our newly\ncollected datasets, all of which will be released. The best-in-class 72B model\nwithin the Molmo family not only outperforms others in the class of open weight\nand data models but also compares favorably against proprietary systems like\nGPT-4o, Claude 3.5, and Gemini 1.5 on both academic benchmarks and human\nevaluation.\n  We will be releasing all of our model weights, captioning and fine-tuning\ndata, and source code in the near future. Select model weights, inference code,\nand demo are available at https://molmo.allenai.org.\n","authors":["Matt Deitke","Christopher Clark","Sangho Lee","Rohun Tripathi","Yue Yang","Jae Sung Park","Mohammadreza Salehi","Niklas Muennighoff","Kyle Lo","Luca Soldaini","Jiasen Lu","Taira Anderson","Erin Bransom","Kiana Ehsani","Huong Ngo","YenSung Chen","Ajay Patel","Mark Yatskar","Chris Callison-Burch","Andrew Head","Rose Hendrix","Favyen Bastani","Eli VanderBilt","Nathan Lambert","Yvonne Chou","Arnavi Chheda","Jenna Sparks","Sam Skjonsberg","Michael Schmitz","Aaron Sarnat","Byron Bischoff","Pete Walsh","Chris Newell","Piper Wolters","Tanmay Gupta","Kuo-Hao Zeng","Jon Borchardt","Dirk Groeneveld","Jen Dumas","Crystal Nam","Sophie Lebrecht","Caitlin Wittlif","Carissa Schoenick","Oscar Michel","Ranjay Krishna","Luca Weihs","Noah A. Smith","Hannaneh Hajishirzi","Ross Girshick","Ali Farhadi","Aniruddha Kembhavi"],"pdf_url":"https://arxiv.org/pdf/2409.17146v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17145v1","updated":"2024-09-25T17:59:45Z","published":"2024-09-25T17:59:45Z","title":"DreamWaltz-G: Expressive 3D Gaussian Avatars from Skeleton-Guided 2D\n  Diffusion","summary":"  Leveraging pretrained 2D diffusion models and score distillation sampling\n(SDS), recent methods have shown promising results for text-to-3D avatar\ngeneration. However, generating high-quality 3D avatars capable of expressive\nanimation remains challenging. In this work, we present DreamWaltz-G, a novel\nlearning framework for animatable 3D avatar generation from text. The core of\nthis framework lies in Skeleton-guided Score Distillation and Hybrid 3D\nGaussian Avatar representation. Specifically, the proposed skeleton-guided\nscore distillation integrates skeleton controls from 3D human templates into 2D\ndiffusion models, enhancing the consistency of SDS supervision in terms of view\nand human pose. This facilitates the generation of high-quality avatars,\nmitigating issues such as multiple faces, extra limbs, and blurring. The\nproposed hybrid 3D Gaussian avatar representation builds on the efficient 3D\nGaussians, combining neural implicit fields and parameterized 3D meshes to\nenable real-time rendering, stable SDS optimization, and expressive animation.\nExtensive experiments demonstrate that DreamWaltz-G is highly effective in\ngenerating and animating 3D avatars, outperforming existing methods in both\nvisual quality and animation expressiveness. Our framework further supports\ndiverse applications, including human video reenactment and multi-subject scene\ncomposition.\n","authors":["Yukun Huang","Jianan Wang","Ailing Zeng","Zheng-Jun Zha","Lei Zhang","Xihui Liu"],"pdf_url":"https://arxiv.org/pdf/2409.17145v1.pdf","comment":"Project page: https://yukun-huang.github.io/DreamWaltz-G/"},{"id":"http://arxiv.org/abs/2409.17143v1","updated":"2024-09-25T17:59:13Z","published":"2024-09-25T17:59:13Z","title":"Attention Prompting on Image for Large Vision-Language Models","summary":"  Compared with Large Language Models (LLMs), Large Vision-Language Models\n(LVLMs) can also accept images as input, thus showcasing more interesting\nemergent capabilities and demonstrating impressive performance on various\nvision-language tasks. Motivated by text prompting in LLMs, visual prompting\nhas been explored to enhance LVLMs' capabilities of perceiving visual\ninformation. However, previous visual prompting techniques solely process\nvisual inputs without considering text queries, limiting the models' ability to\nfollow text instructions to complete tasks. To fill this gap, in this work, we\npropose a new prompting technique named Attention Prompting on Image, which\njust simply overlays a text-query-guided attention heatmap on the original\ninput image and effectively enhances LVLM on various tasks. Specifically, we\ngenerate an attention heatmap for the input image dependent on the text query\nwith an auxiliary model like CLIP. Then the heatmap simply multiplies the pixel\nvalues of the original image to obtain the actual input image for the LVLM.\nExtensive experiments on various vison-language benchmarks verify the\neffectiveness of our technique. For example, Attention Prompting on Image\nimproves LLaVA-1.5 by 3.8% and 2.9% on MM-Vet and LLaVA-Wild benchmarks,\nrespectively.\n","authors":["Runpeng Yu","Weihao Yu","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2409.17143v1.pdf","comment":"Website, see https://yu-rp.github.io/api-prompting"},{"id":"http://arxiv.org/abs/2311.16201v2","updated":"2024-09-25T17:58:21Z","published":"2023-11-27T07:19:26Z","title":"Pre-trained Language Models Do Not Help Auto-regressive Text-to-Image\n  Generation","summary":"  Recent advances in image tokenizers, such as VQ-VAE, have enabled\ntext-to-image generation using auto-regressive methods, similar to language\nmodeling. However, these methods have yet to leverage pre-trained language\nmodels, despite their adaptability to various downstream tasks. In this work,\nwe explore this gap by adapting a pre-trained language model for\nauto-regressive text-to-image generation, and find that pre-trained language\nmodels offer limited help. We provide a two-fold explanation by analyzing\ntokens from each modality. First, we demonstrate that image tokens possess\nsignificantly different semantics compared to text tokens, rendering\npre-trained language models no more effective in modeling them than randomly\ninitialized ones. Second, the text tokens in the image-text datasets are too\nsimple compared to normal language model pre-training data, which causes the\ncatastrophic degradation of language models' capability.\n","authors":["Yuhui Zhang","Brandon McKinzie","Zhe Gan","Vaishaal Shankar","Alexander Toshev"],"pdf_url":"https://arxiv.org/pdf/2311.16201v2.pdf","comment":"Published at EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2409.17137v1","updated":"2024-09-25T17:56:00Z","published":"2024-09-25T17:56:00Z","title":"PACE: marrying generalization in PArameter-efficient fine-tuning with\n  Consistency rEgularization","summary":"  Parameter-Efficient Fine-Tuning (PEFT) effectively adapts pre-trained vision\ntransformers to downstream tasks. However, the optimization for tasks\nperformance often comes at the cost of generalizability in fine-tuned models.\nTo address this issue, we theoretically connect smaller weight gradient norms\nduring training and larger datasets to the improved model generalization.\nMotivated by this connection, we propose reducing gradient norms for enhanced\ngeneralization and aligning fine-tuned model with the pre-trained counterpart\nto retain knowledge from large-scale pre-training data. Yet, naive alignment\ndoes not guarantee gradient reduction and can potentially cause gradient\nexplosion, complicating efforts to manage gradients. To address such issues, we\npropose PACE, marrying generalization of PArameter-efficient fine-tuning with\nConsistency rEgularization. We perturb features learned from the adapter with\nthe multiplicative noise and ensure the fine-tuned model remains consistent for\nsame sample under different perturbations. Theoretical analysis shows that PACE\nnot only implicitly regularizes gradients for enhanced generalization, but also\nimplicitly aligns the fine-tuned and pre-trained models to retain knowledge.\nExperimental evidence supports our theories. PACE outperforms existing PEFT\nmethods in four visual adaptation tasks: VTAB-1k, FGVC, few-shot learning and\ndomain adaptation. Code will be available at\nhttps://github.com/MaxwellYaoNi/PACE\n","authors":["Yao Ni","Shan Zhang","Piotr Koniusz"],"pdf_url":"https://arxiv.org/pdf/2409.17137v1.pdf","comment":"Accepted by NeurIPS 2024 as a spotlight. This preliminary version\n  will soon be extended with the experiments and analyses from the rebuttal"},{"id":"http://arxiv.org/abs/2404.11569v2","updated":"2024-09-25T17:53:48Z","published":"2024-04-17T17:11:47Z","title":"Simple Image Signal Processing using Global Context Guidance","summary":"  In modern smartphone cameras, the Image Signal Processor (ISP) is the core\nelement that converts the RAW readings from the sensor into perceptually\npleasant RGB images for the end users. The ISP is typically proprietary and\nhandcrafted and consists of several blocks such as white balance, color\ncorrection, and tone mapping. Deep learning-based ISPs aim to transform RAW\nimages into DSLR-like RGB images using deep neural networks. However, most\nlearned ISPs are trained using patches (small regions) due to computational\nlimitations. Such methods lack global context, which limits their efficacy on\nfull-resolution images and harms their ability to capture global properties\nsuch as color constancy or illumination. First, we propose a novel module that\ncan be integrated into any neural ISP to capture the global context information\nfrom the full RAW images. Second, we propose an efficient and simple neural ISP\nthat utilizes our proposed module. Our model achieves state-of-the-art results\non different benchmarks using diverse and real smartphone images.\n","authors":["Omar Elezabi","Marcos V. Conde","Radu Timofte"],"pdf_url":"https://arxiv.org/pdf/2404.11569v2.pdf","comment":"IEEE International Conference on Image Processing (ICIP) 2024 - Oral\n  Presentation"},{"id":"http://arxiv.org/abs/2409.17134v1","updated":"2024-09-25T17:51:20Z","published":"2024-09-25T17:51:20Z","title":"Streaming Neural Images","summary":"  Implicit Neural Representations (INRs) are a novel paradigm for signal\nrepresentation that have attracted considerable interest for image compression.\nINRs offer unprecedented advantages in signal resolution and memory efficiency,\nenabling new possibilities for compression techniques. However, the existing\nlimitations of INRs for image compression have not been sufficiently addressed\nin the literature. In this work, we explore the critical yet overlooked\nlimiting factors of INRs, such as computational cost, unstable performance, and\nrobustness. Through extensive experiments and empirical analysis, we provide a\ndeeper and more nuanced understanding of implicit neural image compression\nmethods such as Fourier Feature Networks and Siren. Our work also offers\nvaluable insights for future research in this area.\n","authors":["Marcos V. Conde","Andy Bigos","Radu Timofte"],"pdf_url":"https://arxiv.org/pdf/2409.17134v1.pdf","comment":"IEEE International Conference on Image Processing (ICIP)2024"},{"id":"http://arxiv.org/abs/2409.17122v1","updated":"2024-09-25T17:36:18Z","published":"2024-09-25T17:36:18Z","title":"Classification of Gleason Grading in Prostate Cancer Histopathology\n  Images Using Deep Learning Techniques: YOLO, Vision Transformers, and Vision\n  Mamba","summary":"  Prostate cancer ranks among the leading health issues impacting men, with the\nGleason scoring system serving as the primary method for diagnosis and\nprognosis. This system relies on expert pathologists to evaluate samples of\nprostate tissue and assign a Gleason grade, a task that requires significant\ntime and manual effort. To address this challenge, artificial intelligence (AI)\nsolutions have been explored to automate the grading process. In light of these\nchallenges, this study evaluates and compares the effectiveness of three deep\nlearning methodologies, YOLO, Vision Transformers, and Vision Mamba, in\naccurately classifying Gleason grades from histopathology images. The goal is\nto enhance diagnostic precision and efficiency in prostate cancer management.\nThis study utilized two publicly available datasets, Gleason2019 and SICAPv2,\nto train and test the performance of YOLO, Vision Transformers, and Vision\nMamba models. Each model was assessed based on its ability to classify Gleason\ngrades accurately, considering metrics such as false positive rate, false\nnegative rate, precision, and recall. The study also examined the computational\nefficiency and applicability of each method in a clinical setting. Vision Mamba\ndemonstrated superior performance across all metrics, achieving high precision\nand recall rates while minimizing false positives and negatives. YOLO showed\npromise in terms of speed and efficiency, particularly beneficial for real-time\nanalysis. Vision Transformers excelled in capturing long-range dependencies\nwithin images, although they presented higher computational complexity compared\nto the other models. Vision Mamba emerges as the most effective model for\nGleason grade classification in histopathology images, offering a balance\nbetween accuracy and computational efficiency.\n","authors":["Amin Malekmohammadi","Ali Badiezadeh","Seyed Mostafa Mirhassani","Parisa Gifani","Majid Vafaeezadeh"],"pdf_url":"https://arxiv.org/pdf/2409.17122v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17119v1","updated":"2024-09-25T17:31:17Z","published":"2024-09-25T17:31:17Z","title":"Small data deep learning methodology for in-field disease detection","summary":"  Early detection of diseases in crops is essential to prevent harvest losses\nand improve the quality of the final product. In this context, the combination\nof machine learning and proximity sensors is emerging as a technique capable of\nachieving this detection efficiently and effectively. For example, this machine\nlearning approach has been applied to potato crops -- to detect late blight\n(Phytophthora infestans) -- and grapevine crops -- to detect downy mildew.\nHowever, most of these AI models found in the specialised literature have been\ndeveloped using leaf-by-leaf images taken in the lab, which does not represent\nfield conditions and limits their applicability.\n  In this study, we present the first machine learning model capable of\ndetecting mild symptoms of late blight in potato crops through the analysis of\nhigh-resolution RGB images captured directly in the field, overcoming the\nlimitations of other publications in the literature and presenting real-world\napplicability. Our proposal exploits the availability of high-resolution images\nvia the concept of patching, and is based on deep convolutional neural networks\nwith a focal loss function, which makes the model to focus on the complex\npatterns that arise in field conditions. Additionally, we present a data\naugmentation scheme that facilitates the training of these neural networks with\nfew high-resolution images, which allows for development of models under the\nsmall data paradigm.\n  Our model correctly detects all cases of late blight in the test dataset,\ndemonstrating a high level of accuracy and effectiveness in identifying early\nsymptoms. These promising results reinforce the potential use of machine\nlearning for the early detection of diseases and pests in agriculture, enabling\nbetter treatment and reducing their impact on crops.\n","authors":["David Herrera-Poyato","Jacinto Domínguez-Rull","Rosana Montes","Inés Hernánde","Ignacio Barrio","Carlos Poblete-Echeverria","Javier Tardaguila","Francisco Herrera","Andrés Herrera-Poyatos"],"pdf_url":"https://arxiv.org/pdf/2409.17119v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2409.17110v1","updated":"2024-09-25T17:25:06Z","published":"2024-09-25T17:25:06Z","title":"MorphoSeg: An Uncertainty-Aware Deep Learning Method for Biomedical\n  Segmentation of Complex Cellular Morphologies","summary":"  Deep learning has revolutionized medical and biological imaging, particularly\nin segmentation tasks. However, segmenting biological cells remains challenging\ndue to the high variability and complexity of cell shapes. Addressing this\nchallenge requires high-quality datasets that accurately represent the diverse\nmorphologies found in biological cells. Existing cell segmentation datasets are\noften limited by their focus on regular and uniform shapes. In this paper, we\nintroduce a novel benchmark dataset of Ntera-2 (NT2) cells, a pluripotent\ncarcinoma cell line, exhibiting diverse morphologies across multiple stages of\ndifferentiation, capturing the intricate and heterogeneous cellular structures\nthat complicate segmentation tasks. To address these challenges, we propose an\nuncertainty-aware deep learning framework for complex cellular morphology\nsegmentation (MorphoSeg) by incorporating sampling of virtual outliers from\nlow-likelihood regions during training. Our comprehensive experimental\nevaluations against state-of-the-art baselines demonstrate that MorphoSeg\nsignificantly enhances segmentation accuracy, achieving up to a 7.74% increase\nin the Dice Similarity Coefficient (DSC) and a 28.36% reduction in the\nHausdorff Distance. These findings highlight the effectiveness of our dataset\nand methodology in advancing cell segmentation capabilities, especially for\ncomplex and variable cell morphologies. The dataset and source code is publicly\navailable at https://github.com/RanchoGoose/MorphoSeg.\n","authors":["Tianhao Zhang","Heather J. McCourty","Berardo M. Sanchez-Tafolla","Anton Nikolaev","Lyudmila S. Mihaylova"],"pdf_url":"https://arxiv.org/pdf/2409.17110v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17109v1","updated":"2024-09-25T17:24:27Z","published":"2024-09-25T17:24:27Z","title":"Unveiling Ontological Commitment in Multi-Modal Foundation Models","summary":"  Ontological commitment, i.e., used concepts, relations, and assumptions, are\na corner stone of qualitative reasoning (QR) models. The state-of-the-art for\nprocessing raw inputs, though, are deep neural networks (DNNs), nowadays often\nbased off from multimodal foundation models. These automatically learn rich\nrepresentations of concepts and respective reasoning. Unfortunately, the\nlearned qualitative knowledge is opaque, preventing easy inspection,\nvalidation, or adaptation against available QR models. So far, it is possible\nto associate pre-defined concepts with latent representations of DNNs, but\nextractable relations are mostly limited to semantic similarity. As a next step\ntowards QR for validation and verification of DNNs: Concretely, we propose a\nmethod that extracts the learned superclass hierarchy from a multimodal DNN for\na given set of leaf concepts. Under the hood we (1) obtain leaf concept\nembeddings using the DNN's textual input modality; (2) apply hierarchical\nclustering to them, using that DNNs encode semantic similarities via vector\ndistances; and (3) label the such-obtained parent concepts using search in\navailable ontologies from QR. An initial evaluation study shows that meaningful\nontological class hierarchies can be extracted from state-of-the-art foundation\nmodels. Furthermore, we demonstrate how to validate and verify a DNN's learned\nrepresentations against given ontologies. Lastly, we discuss potential future\napplications in the context of QR.\n","authors":["Mert Keser","Gesina Schwalbe","Niki Amini-Naieni","Matthias Rottmann","Alois Knoll"],"pdf_url":"https://arxiv.org/pdf/2409.17109v1.pdf","comment":"Qualitative Reasoning Workshop 2024 (QR2024) colocated with ECAI2024,\n  camera-ready submission; first two authors contributed equally; 10 pages, 4\n  figures, 3 tables"},{"id":"http://arxiv.org/abs/2409.17106v1","updated":"2024-09-25T17:19:33Z","published":"2024-09-25T17:19:33Z","title":"Text2CAD: Generating Sequential CAD Models from Beginner-to-Expert Level\n  Text Prompts","summary":"  Prototyping complex computer-aided design (CAD) models in modern softwares\ncan be very time-consuming. This is due to the lack of intelligent systems that\ncan quickly generate simpler intermediate parts. We propose Text2CAD, the first\nAI framework for generating text-to-parametric CAD models using\ndesigner-friendly instructions for all skill levels. Furthermore, we introduce\na data annotation pipeline for generating text prompts based on natural\nlanguage instructions for the DeepCAD dataset using Mistral and LLaVA-NeXT. The\ndataset contains $\\sim170$K models and $\\sim660$K text annotations, from\nabstract CAD descriptions (e.g., generate two concentric cylinders) to detailed\nspecifications (e.g., draw two circles with center $(x,y)$ and radius $r_{1}$,\n$r_{2}$, and extrude along the normal by $d$...). Within the Text2CAD\nframework, we propose an end-to-end transformer-based auto-regressive network\nto generate parametric CAD models from input texts. We evaluate the performance\nof our model through a mixture of metrics, including visual quality, parametric\nprecision, and geometrical accuracy. Our proposed framework shows great\npotential in AI-aided design applications. Our source code and annotations will\nbe publicly available.\n","authors":["Mohammad Sadil Khan","Sankalp Sinha","Talha Uddin Sheikh","Didier Stricker","Sk Aziz Ali","Muhammad Zeshan Afzal"],"pdf_url":"https://arxiv.org/pdf/2409.17106v1.pdf","comment":"Accepted in NeurIPS 2024 (Spotlight)"},{"id":"http://arxiv.org/abs/2312.14115v3","updated":"2024-09-25T17:14:15Z","published":"2023-12-21T18:40:34Z","title":"LingoQA: Video Question Answering for Autonomous Driving","summary":"  We introduce LingoQA, a novel dataset and benchmark for visual question\nanswering in autonomous driving. The dataset contains 28K unique short video\nscenarios, and 419K annotations. Evaluating state-of-the-art vision-language\nmodels on our benchmark shows that their performance is below human\ncapabilities, with GPT-4V responding truthfully to 59.6% of the questions\ncompared to 96.6% for humans. For evaluation, we propose a truthfulness\nclassifier, called Lingo-Judge, that achieves a 0.95 Spearman correlation\ncoefficient to human evaluations, surpassing existing techniques like METEOR,\nBLEU, CIDEr, and GPT-4. We establish a baseline vision-language model and run\nextensive ablation studies to understand its performance. We release our\ndataset and benchmark https://github.com/wayveai/LingoQA as an evaluation\nplatform for vision-language models in autonomous driving.\n","authors":["Ana-Maria Marcu","Long Chen","Jan Hünermann","Alice Karnsund","Benoit Hanotte","Prajwal Chidananda","Saurabh Nair","Vijay Badrinarayanan","Alex Kendall","Jamie Shotton","Elahe Arani","Oleg Sinavski"],"pdf_url":"https://arxiv.org/pdf/2312.14115v3.pdf","comment":"Accepted to ECCV 2024. Benchmark and dataset are available at\n  https://github.com/wayveai/LingoQA/"},{"id":"http://arxiv.org/abs/2409.17095v1","updated":"2024-09-25T17:05:55Z","published":"2024-09-25T17:05:55Z","title":"General Detection-based Text Line Recognition","summary":"  We introduce a general detection-based approach to text line recognition, be\nit printed (OCR) or handwritten (HTR), with Latin, Chinese, or ciphered\ncharacters. Detection-based approaches have until now been largely discarded\nfor HTR because reading characters separately is often challenging, and\ncharacter-level annotation is difficult and expensive. We overcome these\nchallenges thanks to three main insights: (i) synthetic pre-training with\nsufficiently diverse data enables learning reasonable character localization\nfor any script; (ii) modern transformer-based detectors can jointly detect a\nlarge number of instances, and, if trained with an adequate masking strategy,\nleverage consistency between the different detections; (iii) once a pre-trained\ndetection model with approximate character localization is available, it is\npossible to fine-tune it with line-level annotation on real data, even with a\ndifferent alphabet. Our approach, dubbed DTLR, builds on a completely different\nparadigm than state-of-the-art HTR methods, which rely on autoregressive\ndecoding, predicting character values one by one, while we treat a complete\nline in parallel. Remarkably, we demonstrate good performance on a large range\nof scripts, usually tackled with specialized approaches. In particular, we\nimprove state-of-the-art performances for Chinese script recognition on the\nCASIA v2 dataset, and for cipher recognition on the Borg and Copiale datasets.\nOur code and models are available at https://github.com/raphael-baena/DTLR.\n","authors":["Raphael Baena","Syrine Kalleli","Mathieu Aubry"],"pdf_url":"https://arxiv.org/pdf/2409.17095v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17093v1","updated":"2024-09-25T17:03:49Z","published":"2024-09-25T17:03:49Z","title":"BitQ: Tailoring Block Floating Point Precision for Improved DNN\n  Efficiency on Resource-Constrained Devices","summary":"  Deep neural networks (DNNs) are powerful for cognitive tasks such as image\nclassification, object detection, and scene segmentation. One drawback however\nis the significant high computational complexity and memory consumption, which\nmakes them unfeasible to run real-time on embedded platforms because of the\nlimited hardware resources. Block floating point (BFP) quantization is one of\nthe representative compression approaches for reducing the memory and\ncomputational burden owing to their capability to effectively capture the broad\ndata distribution of DNN models. Unfortunately, prior works on BFP-based\nquantization empirically choose the block size and the precision that preserve\naccuracy. In this paper, we develop a BFP-based bitwidth-aware analytical\nmodeling framework (called ``BitQ'') for the best BFP implementation of DNN\ninference on embedded platforms. We formulate and resolve an optimization\nproblem to identify the optimal BFP block size and bitwidth distribution by the\ntrade-off of both accuracy and performance loss. Experimental results show that\ncompared with an equal bitwidth setting, the BFP DNNs with optimized bitwidth\nallocation provide efficient computation, preserving accuracy on famous\nbenchmarks. The source code and data are available at\nhttps://github.com/Cheliosoops/BitQ.\n","authors":["Yongqi Xu","Yujian Lee","Gao Yi","Bosheng Liu","Yucong Chen","Peng Liu","Jigang Wu","Xiaoming Chen","Yinhe Han"],"pdf_url":"https://arxiv.org/pdf/2409.17093v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17091v1","updated":"2024-09-25T16:58:19Z","published":"2024-09-25T16:58:19Z","title":"Ctrl-GenAug: Controllable Generative Augmentation for Medical Sequence\n  Classification","summary":"  In the medical field, the limited availability of large-scale datasets and\nlabor-intensive annotation processes hinder the performance of deep models.\nDiffusion-based generative augmentation approaches present a promising solution\nto this issue, having been proven effective in advancing downstream medical\nrecognition tasks. Nevertheless, existing works lack sufficient semantic and\nsequential steerability for challenging video/3D sequence generation, and\nneglect quality control of noisy synthesized samples, resulting in unreliable\nsynthetic databases and severely limiting the performance of downstream tasks.\nIn this work, we present Ctrl-GenAug, a novel and general generative\naugmentation framework that enables highly semantic- and sequential-customized\nsequence synthesis and suppresses incorrectly synthesized samples, to aid\nmedical sequence classification. Specifically, we first design a multimodal\nconditions-guided sequence generator for controllably synthesizing\ndiagnosis-promotive samples. A sequential augmentation module is integrated to\nenhance the temporal/stereoscopic coherence of generated samples. Then, we\npropose a noisy synthetic data filter to suppress unreliable cases at semantic\nand sequential levels. Extensive experiments on 3 medical datasets, using 11\nnetworks trained on 3 paradigms, comprehensively analyze the effectiveness and\ngenerality of Ctrl-GenAug, particularly in underrepresented high-risk\npopulations and out-domain conditions.\n","authors":["Xinrui Zhou","Yuhao Huang","Haoran Dou","Shijing Chen","Ao Chang","Jia Liu","Weiran Long","Jian Zheng","Erjiao Xu","Jie Ren","Ruobing Huang","Jun Cheng","Wufeng Xue","Dong Ni"],"pdf_url":"https://arxiv.org/pdf/2409.17091v1.pdf","comment":"17 pages, 7 figures, 7 tables"},{"id":"http://arxiv.org/abs/2409.17085v1","updated":"2024-09-25T16:49:25Z","published":"2024-09-25T16:49:25Z","title":"Parameter-efficient Bayesian Neural Networks for Uncertainty-aware Depth\n  Estimation","summary":"  State-of-the-art computer vision tasks, like monocular depth estimation\n(MDE), rely heavily on large, modern Transformer-based architectures. However,\ntheir application in safety-critical domains demands reliable predictive\nperformance and uncertainty quantification. While Bayesian neural networks\nprovide a conceptually simple approach to serve those requirements, they suffer\nfrom the high dimensionality of the parameter space. Parameter-efficient\nfine-tuning (PEFT) methods, in particular low-rank adaptations (LoRA), have\nemerged as a popular strategy for adapting large-scale models to down-stream\ntasks by performing parameter inference on lower-dimensional subspaces. In this\nwork, we investigate the suitability of PEFT methods for subspace Bayesian\ninference in large-scale Transformer-based vision models. We show that, indeed,\ncombining BitFit, DiffFit, LoRA, and CoLoRA, a novel LoRA-inspired PEFT method,\nwith Bayesian inference enables more robust and reliable predictive performance\nin MDE.\n","authors":["Richard D. Paul","Alessio Quercia","Vincent Fortuin","Katharina Nöh","Hanno Scharr"],"pdf_url":"https://arxiv.org/pdf/2409.17085v1.pdf","comment":"Presented at UnCV Workshop at ECCV'24"},{"id":"http://arxiv.org/abs/2409.17080v1","updated":"2024-09-25T16:45:02Z","published":"2024-09-25T16:45:02Z","title":"Can Vision Language Models Learn from Visual Demonstrations of Ambiguous\n  Spatial Reasoning?","summary":"  Large vision-language models (VLMs) have become state-of-the-art for many\ncomputer vision tasks, with in-context learning (ICL) as a popular adaptation\nstrategy for new ones. But can VLMs learn novel concepts purely from visual\ndemonstrations, or are they limited to adapting to the output format of ICL\nexamples? We propose a new benchmark we call Spatial Visual Ambiguity Tasks\n(SVAT) that challenges state-of-the-art VLMs to learn new visuospatial tasks\nin-context. We find that VLMs fail to do this zero-shot, and sometimes continue\nto fail after finetuning. However, adding simpler data to the training by\ncurriculum learning leads to improved ICL performance.\n","authors":["Bowen Zhao","Leo Parker Dirac","Paulina Varshavskaya"],"pdf_url":"https://arxiv.org/pdf/2409.17080v1.pdf","comment":"13 pages, 4 figures. Code released at\n  https://github.com/groundlight/vlm-visual-demonstrations"},{"id":"http://arxiv.org/abs/2409.17069v1","updated":"2024-09-25T16:29:21Z","published":"2024-09-25T16:29:21Z","title":"The Effect of Perceptual Metrics on Music Representation Learning for\n  Genre Classification","summary":"  The subjective quality of natural signals can be approximated with objective\nperceptual metrics. Designed to approximate the perceptual behaviour of human\nobservers, perceptual metrics often reflect structures found in natural signals\nand neurological pathways. Models trained with perceptual metrics as loss\nfunctions can capture perceptually meaningful features from the structures held\nwithin these metrics. We demonstrate that using features extracted from\nautoencoders trained with perceptual losses can improve performance on music\nunderstanding tasks, i.e. genre classification, over using these metrics\ndirectly as distances when learning a classifier. This result suggests improved\ngeneralisation to novel signals when using perceptual metrics as loss functions\nfor representation learning.\n","authors":["Tashi Namgyal","Alexander Hepburn","Raul Santos-Rodriguez","Valero Laparra","Jesus Malo"],"pdf_url":"https://arxiv.org/pdf/2409.17069v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2312.03455"},{"id":"http://arxiv.org/abs/2409.17063v1","updated":"2024-09-25T16:21:43Z","published":"2024-09-25T16:21:43Z","title":"Benchmarking Domain Generalization Algorithms in Computational Pathology","summary":"  Deep learning models have shown immense promise in computational pathology\n(CPath) tasks, but their performance often suffers when applied to unseen data\ndue to domain shifts. Addressing this requires domain generalization (DG)\nalgorithms. However, a systematic evaluation of DG algorithms in the CPath\ncontext is lacking. This study aims to benchmark the effectiveness of 30 DG\nalgorithms on 3 CPath tasks of varying difficulty through 7,560\ncross-validation runs. We evaluate these algorithms using a unified and robust\nplatform, incorporating modality-specific techniques and recent advances like\npretrained foundation models. Our extensive cross-validation experiments\nprovide insights into the relative performance of various DG strategies. We\nobserve that self-supervised learning and stain augmentation consistently\noutperform other methods, highlighting the potential of pretrained models and\ndata augmentation. Furthermore, we introduce a new pan-cancer tumor detection\ndataset (HISTOPANTUM) as a benchmark for future research. This study offers\nvaluable guidance to researchers in selecting appropriate DG approaches for\nCPath tasks.\n","authors":["Neda Zamanitajeddin","Mostafa Jahanifar","Kesi Xu","Fouzia Siraj","Nasir Rajpoot"],"pdf_url":"https://arxiv.org/pdf/2409.17063v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17058v1","updated":"2024-09-25T16:15:21Z","published":"2024-09-25T16:15:21Z","title":"Degradation-Guided One-Step Image Super-Resolution with Diffusion Priors","summary":"  Diffusion-based image super-resolution (SR) methods have achieved remarkable\nsuccess by leveraging large pre-trained text-to-image diffusion models as\npriors. However, these methods still face two challenges: the requirement for\ndozens of sampling steps to achieve satisfactory results, which limits\nefficiency in real scenarios, and the neglect of degradation models, which are\ncritical auxiliary information in solving the SR problem. In this work, we\nintroduced a novel one-step SR model, which significantly addresses the\nefficiency issue of diffusion-based SR methods. Unlike existing fine-tuning\nstrategies, we designed a degradation-guided Low-Rank Adaptation (LoRA) module\nspecifically for SR, which corrects the model parameters based on the\npre-estimated degradation information from low-resolution images. This module\nnot only facilitates a powerful data-dependent or degradation-dependent SR\nmodel but also preserves the generative prior of the pre-trained diffusion\nmodel as much as possible. Furthermore, we tailor a novel training pipeline by\nintroducing an online negative sample generation strategy. Combined with the\nclassifier-free guidance strategy during inference, it largely improves the\nperceptual quality of the super-resolution results. Extensive experiments have\ndemonstrated the superior efficiency and effectiveness of the proposed model\ncompared to recent state-of-the-art methods.\n","authors":["Aiping Zhang","Zongsheng Yue","Renjing Pei","Wenqi Ren","Xiaochun Cao"],"pdf_url":"https://arxiv.org/pdf/2409.17058v1.pdf","comment":"The code is available at https://github.com/ArcticHare105/S3Diff"},{"id":"http://arxiv.org/abs/2409.17049v1","updated":"2024-09-25T16:03:33Z","published":"2024-09-25T16:03:33Z","title":"ControlCity: A Multimodal Diffusion Model Based Approach for Accurate\n  Geospatial Data Generation and Urban Morphology Analysis","summary":"  Volunteer Geographic Information (VGI), with its rich variety, large volume,\nrapid updates, and diverse sources, has become a critical source of geospatial\ndata. However, VGI data from platforms like OSM exhibit significant quality\nheterogeneity across different data types, particularly with urban building\ndata. To address this, we propose a multi-source geographic data transformation\nsolution, utilizing accessible and complete VGI data to assist in generating\nurban building footprint data. We also employ a multimodal data generation\nframework to improve accuracy. First, we introduce a pipeline for constructing\nan 'image-text-metadata-building footprint' dataset, primarily based on road\nnetwork data and supplemented by other multimodal data. We then present\nControlCity, a geographic data transformation method based on a multimodal\ndiffusion model. This method first uses a pre-trained text-to-image model to\nalign text, metadata, and building footprint data. An improved ControlNet\nfurther integrates road network and land-use imagery, producing refined\nbuilding footprint data. Experiments across 22 global cities demonstrate that\nControlCity successfully simulates real urban building patterns, achieving\nstate-of-the-art performance. Specifically, our method achieves an average FID\nscore of 50.94, reducing error by 71.01% compared to leading methods, and a\nMIoU score of 0.36, an improvement of 38.46%. Additionally, our model excels in\ntasks like urban morphology transfer, zero-shot city generation, and spatial\ndata completeness assessment. In the zero-shot city task, our method accurately\npredicts and generates similar urban structures, demonstrating strong\ngeneralization. This study confirms the effectiveness of our approach in\ngenerating urban building footprint data and capturing complex city\ncharacteristics.\n","authors":["Fangshuo Zhou","Huaxia Li","Rui Hu","Sensen Wu","Hailin Feng","Zhenhong Du","Liuchang Xu"],"pdf_url":"https://arxiv.org/pdf/2409.17049v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2409.17045v1","updated":"2024-09-25T15:57:59Z","published":"2024-09-25T15:57:59Z","title":"GeoBiked: A Dataset with Geometric Features and Automated Labeling\n  Techniques to Enable Deep Generative Models in Engineering Design","summary":"  We provide a dataset for enabling Deep Generative Models (DGMs) in\nengineering design and propose methods to automate data labeling by utilizing\nlarge-scale foundation models. GeoBiked is curated to contain 4 355 bicycle\nimages, annotated with structural and technical features and is used to\ninvestigate two automated labeling techniques: The utilization of consolidated\nlatent features (Hyperfeatures) from image-generation models to detect\ngeometric correspondences (e.g. the position of the wheel center) in structural\nimages and the generation of diverse text descriptions for structural images.\nGPT-4o, a vision-language-model (VLM), is instructed to analyze images and\nproduce diverse descriptions aligned with the system-prompt. By representing\ntechnical images as Diffusion-Hyperfeatures, drawing geometric correspondences\nbetween them is possible. The detection accuracy of geometric points in unseen\nsamples is improved by presenting multiple annotated source images. GPT-4o has\nsufficient capabilities to generate accurate descriptions of technical images.\nGrounding the generation only on images leads to diverse descriptions but\ncauses hallucinations, while grounding it on categorical labels restricts the\ndiversity. Using both as input balances creativity and accuracy. Successfully\nusing Hyperfeatures for geometric correspondence suggests that this approach\ncan be used for general point-detection and annotation tasks in technical\nimages. Labeling such images with text descriptions using VLMs is possible, but\ndependent on the models detection capabilities, careful prompt-engineering and\nthe selection of input information. Applying foundation models in engineering\ndesign is largely unexplored. We aim to bridge this gap with a dataset to\nexplore training, finetuning and conditioning DGMs in this field and suggesting\napproaches to bootstrap foundation models to process technical images.\n","authors":["Phillip Mueller","Sebastian Mueller","Lars Mikelsons"],"pdf_url":"https://arxiv.org/pdf/2409.17045v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14307v2","updated":"2024-09-25T15:56:46Z","published":"2024-09-22T04:21:29Z","title":"DilateQuant: Accurate and Efficient Diffusion Quantization via Weight\n  Dilation","summary":"  Diffusion models have shown excellent performance on various image generation\ntasks, but the substantial computational costs and huge memory footprint hinder\ntheir low-latency applications in real-world scenarios. Quantization is a\npromising way to compress and accelerate models. Nevertheless, due to the wide\nrange and time-varying activations in diffusion models, existing methods cannot\nmaintain both accuracy and efficiency simultaneously for low-bit quantization.\nTo tackle this issue, we propose DilateQuant, a novel quantization framework\nfor diffusion models that offers comparable accuracy and high efficiency.\nSpecifically, we keenly aware of numerous unsaturated in-channel weights, which\ncan be cleverly exploited to reduce the range of activations without additional\ncomputation cost. Based on this insight, we propose Weight Dilation (WD) that\nmaximally dilates the unsaturated in-channel weights to a constrained range\nthrough a mathematically equivalent scaling. WD costlessly absorbs the\nactivation quantization errors into weight quantization. The range of\nactivations decreases, which makes activations quantization easy. The range of\nweights remains constant, which makes model easy to converge in training stage.\nConsidering the temporal network leads to time-varying activations, we design a\nTemporal Parallel Quantizer (TPQ), which sets time-step quantization parameters\nand supports parallel quantization for different time steps, significantly\nimproving the performance and reducing time cost. To further enhance\nperformance while preserving efficiency, we introduce a Block-wise Knowledge\nDistillation (BKD) to align the quantized models with the full-precision models\nat a block level. The simultaneous training of time-step quantization\nparameters and weights minimizes the time required, and the shorter\nbackpropagation paths decreases the memory footprint of the quantization\nprocess.\n","authors":["Xuewen Liu","Zhikai Li","Qingyi Gu"],"pdf_url":"https://arxiv.org/pdf/2409.14307v2.pdf","comment":"Code: http://github.com/BienLuky/DilateQuant"},{"id":"http://arxiv.org/abs/2409.13084v2","updated":"2024-09-25T15:34:19Z","published":"2024-09-19T20:49:39Z","title":"Real-time estimation of overt attention from dynamic features of the\n  face using deep-learning","summary":"  Students often drift in and out of focus during class. Effective teachers\nrecognize this and re-engage them when necessary. With the shift to remote\nlearning, teachers have lost the visual feedback needed to adapt to varying\nstudent engagement. We propose using readily available front-facing video to\ninfer attention levels based on movements of the eyes, head, and face. We train\na deep learning model to predict a measure of attention based on overt eye\nmovements. Specifically, we measure Inter-Subject Correlation of eye movements\nin ten-second intervals while students watch the same educational videos. In 3\ndifferent experiments (N=83) we show that the trained model predicts this\nobjective metric of attention on unseen data with $R^2$=0.38, and on unseen\nsubjects with $R^2$=0.26-0.30. The deep network relies mostly on a student's\neye movements, but to some extent also on movements of the brows, cheeks, and\nhead. In contrast to Inter-Subject Correlation of the eyes, the model can\nestimate attentional engagement from individual students' movements without\nneeding reference data from an attentive group. This enables a much broader set\nof online applications. The solution is lightweight and can operate on the\nclient side, which mitigates some of the privacy concerns associated with\nonline attention monitoring. GitHub implementation is available at\nhttps://github.com/asortubay/timeISC\n","authors":["Aimar Silvan Ortubay","Lucas C. Parra","Jens Madsen"],"pdf_url":"https://arxiv.org/pdf/2409.13084v2.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2409.17029v1","updated":"2024-09-25T15:32:07Z","published":"2024-09-25T15:32:07Z","title":"EventHDR: from Event to High-Speed HDR Videos and Beyond","summary":"  Event cameras are innovative neuromorphic sensors that asynchronously capture\nthe scene dynamics. Due to the event-triggering mechanism, such cameras record\nevent streams with much shorter response latency and higher intensity\nsensitivity compared to conventional cameras. On the basis of these features,\nprevious works have attempted to reconstruct high dynamic range (HDR) videos\nfrom events, but have either suffered from unrealistic artifacts or failed to\nprovide sufficiently high frame rates. In this paper, we present a recurrent\nconvolutional neural network that reconstruct high-speed HDR videos from event\nsequences, with a key frame guidance to prevent potential error accumulation\ncaused by the sparse event data. Additionally, to address the problem of\nseverely limited real dataset, we develop a new optical system to collect a\nreal-world dataset with paired high-speed HDR videos and event streams,\nfacilitating future research in this field. Our dataset provides the first real\npaired dataset for event-to-HDR reconstruction, avoiding potential inaccuracies\nfrom simulation strategies. Experimental results demonstrate that our method\ncan generate high-quality, high-speed HDR videos. We further explore the\npotential of our work in cross-camera reconstruction and downstream computer\nvision tasks, including object detection, panoramic segmentation, optical flow\nestimation, and monocular depth estimation under HDR scenarios.\n","authors":["Yunhao Zou","Ying Fu","Tsuyoshi Takatani","Yinqiang Zheng"],"pdf_url":"https://arxiv.org/pdf/2409.17029v1.pdf","comment":"TPAMI 2024"},{"id":"http://arxiv.org/abs/2409.17025v1","updated":"2024-09-25T15:27:44Z","published":"2024-09-25T15:27:44Z","title":"Automated Surgical Skill Assessment in Endoscopic Pituitary Surgery\n  using Real-time Instrument Tracking on a High-fidelity Bench-top Phantom","summary":"  Improved surgical skill is generally associated with improved patient\noutcomes, although assessment is subjective; labour-intensive; and requires\ndomain specific expertise. Automated data driven metrics can alleviate these\ndifficulties, as demonstrated by existing machine learning instrument tracking\nmodels in minimally invasive surgery. However, these models have been tested on\nlimited datasets of laparoscopic surgery, with a focus on isolated tasks and\nrobotic surgery. In this paper, a new public dataset is introduced, focusing on\nsimulated surgery, using the nasal phase of endoscopic pituitary surgery as an\nexemplar. Simulated surgery allows for a realistic yet repeatable environment,\nmeaning the insights gained from automated assessment can be used by novice\nsurgeons to hone their skills on the simulator before moving to real surgery.\nPRINTNet (Pituitary Real-time INstrument Tracking Network) has been created as\na baseline model for this automated assessment. Consisting of DeepLabV3 for\nclassification and segmentation; StrongSORT for tracking; and the NVIDIA\nHoloscan SDK for real-time performance, PRINTNet achieved 71.9% Multiple Object\nTracking Precision running at 22 Frames Per Second. Using this tracking output,\na Multilayer Perceptron achieved 87% accuracy in predicting surgical skill\nlevel (novice or expert), with the \"ratio of total procedure time to instrument\nvisible time\" correlated with higher surgical skill. This therefore\ndemonstrates the feasibility of automated surgical skill assessment in\nsimulated endoscopic pituitary surgery. The new publicly available dataset can\nbe found here: https://doi.org/10.5522/04/26511049.\n","authors":["Adrito Das","Bilal Sidiqi","Laurent Mennillo","Zhehua Mao","Mikael Brudfors","Miguel Xochicale","Danyal Z. Khan","Nicola Newall","John G. Hanrahan","Matthew J. Clarkson","Danail Stoyanov","Hani J. Marcus","Sophia Bano"],"pdf_url":"https://arxiv.org/pdf/2409.17025v1.pdf","comment":"7 pages, 6 figures"},{"id":"http://arxiv.org/abs/2409.17023v1","updated":"2024-09-25T15:27:05Z","published":"2024-09-25T15:27:05Z","title":"Enhanced Wavelet Scattering Network for image inpainting detection","summary":"  The rapid advancement of image inpainting tools, especially those aimed at\nremoving artifacts, has made digital image manipulation alarmingly accessible.\nThis paper proposes several innovative ideas for detecting inpainting forgeries\nbased on low level noise analysis by combining Dual-Tree Complex Wavelet\nTransform (DT-CWT) for feature extraction with convolutional neural networks\n(CNN) for forged area detection and localization, and lastly by employing an\ninnovative combination of texture segmentation with noise variance estimations.\nThe DT-CWT offers significant advantages due to its shift-invariance, enhancing\nits robustness against subtle manipulations during the inpainting process.\nFurthermore, its directional selectivity allows for the detection of subtle\nartifacts introduced by inpainting within specific frequency bands and\norientations. Various neural network architectures were evaluated and proposed.\nLastly, we propose a fusion detection module that combines texture analysis\nwith noise variance estimation to give the forged area. Our approach was\nbenchmarked against state-of-the-art methods and demonstrated superior\nperformance over all cited alternatives. The training code (with pretrained\nmodel weights) as long as the dataset will be available at\nhttps://github.com/jmaba/Deep-dual-tree-complex-neural-network-for-image-inpainting-detection\n","authors":["Barglazan Adrian-Alin","Brad Remus"],"pdf_url":"https://arxiv.org/pdf/2409.17023v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17020v1","updated":"2024-09-25T15:23:46Z","published":"2024-09-25T15:23:46Z","title":"PTQ4RIS: Post-Training Quantization for Referring Image Segmentation","summary":"  Referring Image Segmentation (RIS), aims to segment the object referred by a\ngiven sentence in an image by understanding both visual and linguistic\ninformation. However, existing RIS methods tend to explore top-performance\nmodels, disregarding considerations for practical applications on\nresources-limited edge devices. This oversight poses a significant challenge\nfor on-device RIS inference. To this end, we propose an effective and efficient\npost-training quantization framework termed PTQ4RIS. Specifically, we first\nconduct an in-depth analysis of the root causes of performance degradation in\nRIS model quantization and propose dual-region quantization (DRQ) and\nreorder-based outlier-retained quantization (RORQ) to address the quantization\ndifficulties in visual and text encoders. Extensive experiments on three\nbenchmarks with different bits settings (from 8 to 4 bits) demonstrates its\nsuperior performance. Importantly, we are the first PTQ method specifically\ndesigned for the RIS task, highlighting the feasibility of PTQ in RIS\napplications. Code will be available at {https://github.com/gugu511yy/PTQ4RIS}.\n","authors":["Xiaoyan Jiang","Hang Yang","Kaiying Zhu","Xihe Qiu","Shibo Zhao","Sifan Zhou"],"pdf_url":"https://arxiv.org/pdf/2409.17020v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17016v1","updated":"2024-09-25T15:19:04Z","published":"2024-09-25T15:19:04Z","title":"CNN Mixture-of-Depths","summary":"  We introduce Mixture-of-Depths (MoD) for Convolutional Neural Networks\n(CNNs), a novel approach that enhances the computational efficiency of CNNs by\nselectively processing channels based on their relevance to the current\nprediction. This method optimizes computational resources by dynamically\nselecting key channels in feature maps for focused processing within the\nconvolutional blocks (Conv-Blocks), while skipping less relevant channels.\nUnlike conditional computation methods that require dynamic computation graphs,\nCNN MoD uses a static computation graph with fixed tensor sizes which improve\nhardware efficiency. It speeds up the training and inference processes without\nthe need for customized CUDA kernels, unique loss functions, or finetuning. CNN\nMoD either matches the performance of traditional CNNs with reduced inference\ntimes, GMACs, and parameters, or exceeds their performance while maintaining\nsimilar inference times, GMACs, and parameters. For example, on ImageNet,\nResNet86-MoD exceeds the performance of the standard ResNet50 by 0.45% with a\n6% speedup on CPU and 5% on GPU. Moreover, ResNet75-MoD achieves the same\nperformance as ResNet50 with a 25% speedup on CPU and 15% on GPU.\n","authors":["Rinor Cakaj","Jens Mehnert","Bin Yang"],"pdf_url":"https://arxiv.org/pdf/2409.17016v1.pdf","comment":"Conference Paper of the Asian Conference on Computer Vision (ACCV)\n  2024"},{"id":"http://arxiv.org/abs/2409.17001v1","updated":"2024-09-25T15:05:03Z","published":"2024-09-25T15:05:03Z","title":"Adverse Weather Optical Flow: Cumulative Homogeneous-Heterogeneous\n  Adaptation","summary":"  Optical flow has made great progress in clean scenes, while suffers\ndegradation under adverse weather due to the violation of the brightness\nconstancy and gradient continuity assumptions of optical flow. Typically,\nexisting methods mainly adopt domain adaptation to transfer motion knowledge\nfrom clean to degraded domain through one-stage adaptation. However, this\ndirect adaptation is ineffective, since there exists a large gap due to adverse\nweather and scene style between clean and real degraded domains. Moreover, even\nwithin the degraded domain itself, static weather (e.g., fog) and dynamic\nweather (e.g., rain) have different impacts on optical flow. To address above\nissues, we explore synthetic degraded domain as an intermediate bridge between\nclean and real degraded domains, and propose a cumulative\nhomogeneous-heterogeneous adaptation framework for real adverse weather optical\nflow. Specifically, for clean-degraded transfer, our key insight is that static\nweather possesses the depth-association homogeneous feature which does not\nchange the intrinsic motion of the scene, while dynamic weather additionally\nintroduces the heterogeneous feature which results in a significant boundary\ndiscrepancy in warp errors between clean and degraded domains. For\nsynthetic-real transfer, we figure out that cost volume correlation shares a\nsimilar statistical histogram between synthetic and real degraded domains,\nbenefiting to holistically aligning the homogeneous correlation distribution\nfor synthetic-real knowledge distillation. Under this unified framework, the\nproposed method can progressively and explicitly transfer knowledge from clean\nscenes to real adverse weather. In addition, we further collect a real adverse\nweather dataset with manually annotated optical flow labels and perform\nextensive experiments to verify the superiority of the proposed method.\n","authors":["Hanyu Zhou","Yi Chang","Zhiwei Shi","Wending Yan","Gang Chen","Yonghong Tian","Luxin Yan"],"pdf_url":"https://arxiv.org/pdf/2409.17001v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16999v1","updated":"2024-09-25T15:04:21Z","published":"2024-09-25T15:04:21Z","title":"WasteGAN: Data Augmentation for Robotic Waste Sorting through Generative\n  Adversarial Networks","summary":"  Robotic waste sorting poses significant challenges in both perception and\nmanipulation, given the extreme variability of objects that should be\nrecognized on a cluttered conveyor belt. While deep learning has proven\neffective in solving complex tasks, the necessity for extensive data collection\nand labeling limits its applicability in real-world scenarios like waste\nsorting. To tackle this issue, we introduce a data augmentation method based on\na novel GAN architecture called wasteGAN. The proposed method allows to\nincrease the performance of semantic segmentation models, starting from a very\nlimited bunch of labeled examples, such as few as 100. The key innovations of\nwasteGAN include a novel loss function, a novel activation function, and a\nlarger generator block. Overall, such innovations helps the network to learn\nfrom limited number of examples and synthesize data that better mirrors\nreal-world distributions. We then leverage the higher-quality segmentation\nmasks predicted from models trained on the wasteGAN synthetic data to compute\nsemantic-aware grasp poses, enabling a robotic arm to effectively recognizing\ncontaminants and separating waste in a real-world scenario. Through\ncomprehensive evaluation encompassing dataset-based assessments and real-world\nexperiments, our methodology demonstrated promising potential for robotic waste\nsorting, yielding performance gains of up to 5.8\\% in picking contaminants. The\nproject page is available at https://github.com/bach05/wasteGAN.git\n","authors":["Alberto Bacchin","Leonardo Barcellona","Matteo Terreran","Stefano Ghidoni","Emanuele Menegatti","Takuya Kiyokawa"],"pdf_url":"https://arxiv.org/pdf/2409.16999v1.pdf","comment":"Accepted at 2024 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2024)"},{"id":"http://arxiv.org/abs/2409.16998v1","updated":"2024-09-25T15:03:22Z","published":"2024-09-25T15:03:22Z","title":"PitRSDNet: Predicting Intra-operative Remaining Surgery Duration in\n  Endoscopic Pituitary Surgery","summary":"  Accurate intra-operative Remaining Surgery Duration (RSD) predictions allow\nfor anaesthetists to more accurately decide when to administer anaesthetic\nagents and drugs, as well as to notify hospital staff to send in the next\npatient. Therefore RSD plays an important role in improving patient care and\nminimising surgical theatre costs via efficient scheduling. In endoscopic\npituitary surgery, it is uniquely challenging due to variable workflow\nsequences with a selection of optional steps contributing to high variability\nin surgery duration. This paper presents PitRSDNet for predicting RSD during\npituitary surgery, a spatio-temporal neural network model that learns from\nhistorical data focusing on workflow sequences. PitRSDNet integrates workflow\nknowledge into RSD prediction in two forms: 1) multi-task learning for\nconcurrently predicting step and RSD; and 2) incorporating prior steps as\ncontext in temporal learning and inference. PitRSDNet is trained and evaluated\non a new endoscopic pituitary surgery dataset with 88 videos to show\ncompetitive performance improvements over previous statistical and machine\nlearning methods. The findings also highlight how PitRSDNet improve RSD\nprecision on outlier cases utilising the knowledge of prior steps.\n","authors":["Anjana Wijekoon","Adrito Das","Roxana R. Herrera","Danyal Z. Khan","John Hanrahan","Eleanor Carter","Valpuri Luoma","Danail Stoyanov","Hani J. Marcus","Sophia Bano"],"pdf_url":"https://arxiv.org/pdf/2409.16998v1.pdf","comment":"Accepted to the Augmented Environments for Computer-Assisted\n  Interventions (AE-CAI) Workshop at the Medical Image Computing and\n  Computer-Assisted Interventions (MICCAI) Conference 2024"},{"id":"http://arxiv.org/abs/2409.16990v1","updated":"2024-09-25T14:56:37Z","published":"2024-09-25T14:56:37Z","title":"Single Image, Any Face: Generalisable 3D Face Generation","summary":"  The creation of 3D human face avatars from a single unconstrained image is a\nfundamental task that underlies numerous real-world vision and graphics\napplications. Despite the significant progress made in generative models,\nexisting methods are either less suited in design for human faces or fail to\ngeneralise from the restrictive training domain to unconstrained facial images.\nTo address these limitations, we propose a novel model, Gen3D-Face, which\ngenerates 3D human faces with unconstrained single image input within a\nmulti-view consistent diffusion framework. Given a specific input image, our\nmodel first produces multi-view images, followed by neural surface\nconstruction. To incorporate face geometry information in a generalisable\nmanner, we utilise input-conditioned mesh estimation instead of ground-truth\nmesh along with synthetic multi-view training data. Importantly, we introduce a\nmulti-view joint generation scheme to enhance appearance consistency among\ndifferent views. To the best of our knowledge, this is the first attempt and\nbenchmark for creating photorealistic 3D human face avatars from single images\nfor generic human subject across domains. Extensive experiments demonstrate the\nsuperiority of our method over previous alternatives for out-of-domain singe\nimage 3D face generation and top competition for in-domain setting.\n","authors":["Wenqing Wang","Haosen Yang","Josef Kittler","Xiatian Zhu"],"pdf_url":"https://arxiv.org/pdf/2409.16990v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.01322v3","updated":"2024-09-25T14:44:21Z","published":"2024-09-02T15:21:46Z","title":"Guide-and-Rescale: Self-Guidance Mechanism for Effective Tuning-Free\n  Real Image Editing","summary":"  Despite recent advances in large-scale text-to-image generative models,\nmanipulating real images with these models remains a challenging problem. The\nmain limitations of existing editing methods are that they either fail to\nperform with consistent quality on a wide range of image edits or require\ntime-consuming hyperparameter tuning or fine-tuning of the diffusion model to\npreserve the image-specific appearance of the input image. We propose a novel\napproach that is built upon a modified diffusion sampling process via the\nguidance mechanism. In this work, we explore the self-guidance technique to\npreserve the overall structure of the input image and its local regions\nappearance that should not be edited. In particular, we explicitly introduce\nlayout-preserving energy functions that are aimed to save local and global\nstructures of the source image. Additionally, we propose a noise rescaling\nmechanism that allows to preserve noise distribution by balancing the norms of\nclassifier-free guidance and our proposed guiders during generation. Such a\nguiding approach does not require fine-tuning the diffusion model and exact\ninversion process. As a result, the proposed method provides a fast and\nhigh-quality editing mechanism. In our experiments, we show through human\nevaluation and quantitative analysis that the proposed method allows to produce\ndesired editing which is more preferable by humans and also achieves a better\ntrade-off between editing quality and preservation of the original image. Our\ncode is available at https://github.com/MACderRu/Guide-and-Rescale.\n","authors":["Vadim Titov","Madina Khalmatova","Alexandra Ivanova","Dmitry Vetrov","Aibek Alanov"],"pdf_url":"https://arxiv.org/pdf/2409.01322v3.pdf","comment":"Accepted to ECCV 2024. The project page is available at\n  https://macderru.github.io/Guide-and-Rescale"},{"id":"http://arxiv.org/abs/2409.16967v1","updated":"2024-09-25T14:27:37Z","published":"2024-09-25T14:27:37Z","title":"Multi-Robot Informative Path Planning for Efficient Target Mapping using\n  Deep Reinforcement Learning","summary":"  Autonomous robots are being employed in several mapping and data collection\ntasks due to their efficiency and low labor costs. In these tasks, the robots\nare required to map targets-of-interest in an unknown environment while\nconstrained to a given resource budget such as path length or mission time.\nThis is a challenging problem as each robot has to not only detect and avoid\ncollisions from static obstacles in the environment but also has to model other\nrobots' trajectories to avoid inter-robot collisions. We propose a novel deep\nreinforcement learning approach for multi-robot informative path planning to\nmap targets-of-interest in an unknown 3D environment. A key aspect of our\napproach is an augmented graph that models other robots' trajectories to enable\nplanning for communication and inter-robot collision avoidance. We train our\ndecentralized reinforcement learning policy via the centralized training and\ndecentralized execution paradigm. Once trained, our policy is also scalable to\nvarying number of robots and does not require re-training. Our approach\noutperforms other state-of-the-art multi-robot target mapping approaches by\n33.75% in terms of the number of discovered targets-of-interest. We open-source\nour code and model at: https://github.com/AccGen99/marl_ipp\n","authors":["Apoorva Vashisth","Dipam Patel","Damon Conover","Aniket Bera"],"pdf_url":"https://arxiv.org/pdf/2409.16967v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2402.04894"},{"id":"http://arxiv.org/abs/2403.18600v2","updated":"2024-09-25T14:20:39Z","published":"2024-03-27T14:22:40Z","title":"RAP: Retrieval-Augmented Planner for Adaptive Procedure Planning in\n  Instructional Videos","summary":"  Procedure Planning in instructional videos entails generating a sequence of\naction steps based on visual observations of the initial and target states.\nDespite the rapid progress in this task, there remain several critical\nchallenges to be solved: (1) Adaptive procedures: Prior works hold an\nunrealistic assumption that the number of action steps is known and fixed,\nleading to non-generalizable models in real-world scenarios where the sequence\nlength varies. (2) Temporal relation: Understanding the step temporal relation\nknowledge is essential in producing reasonable and executable plans. (3)\nAnnotation cost: Annotating instructional videos with step-level labels (i.e.,\ntimestamp) or sequence-level labels (i.e., action category) is demanding and\nlabor-intensive, limiting its generalizability to large-scale datasets. In this\nwork, we propose a new and practical setting, called adaptive procedure\nplanning in instructional videos, where the procedure length is not fixed or\npre-determined. To address these challenges, we introduce Retrieval-Augmented\nPlanner (RAP) model. Specifically, for adaptive procedures, RAP adaptively\ndetermines the conclusion of actions using an auto-regressive model\narchitecture. For temporal relation, RAP establishes an external memory module\nto explicitly retrieve the most relevant state-action pairs from the training\nvideos and revises the generated procedures. To tackle high annotation cost,\nRAP utilizes a weakly-supervised learning manner to expand the training dataset\nto other task-relevant, unannotated videos by generating pseudo labels for\naction steps. Experiments on CrossTask and COIN benchmarks show the superiority\nof RAP over traditional fixed-length models, establishing it as a strong\nbaseline solution for adaptive procedure planning.\n","authors":["Ali Zare","Yulei Niu","Hammad Ayyubi","Shih-fu Chang"],"pdf_url":"https://arxiv.org/pdf/2403.18600v2.pdf","comment":"Accepted in ECCV 2024"},{"id":"http://arxiv.org/abs/2409.16953v1","updated":"2024-09-25T14:08:37Z","published":"2024-09-25T14:08:37Z","title":"Path-adaptive Spatio-Temporal State Space Model for Event-based\n  Recognition with Arbitrary Duration","summary":"  Event cameras are bio-inspired sensors that capture the intensity changes\nasynchronously and output event streams with distinct advantages, such as high\ntemporal resolution. To exploit event cameras for object/action recognition,\nexisting methods predominantly sample and aggregate events in a second-level\nduration at every fixed temporal interval (or frequency). However, they often\nface difficulties in capturing the spatiotemporal relationships for longer,\ne.g., minute-level, events and generalizing across varying temporal\nfrequencies. To fill the gap, we present a novel framework, dubbed PAST-SSM,\nexhibiting superior capacity in recognizing events with arbitrary duration\n(e.g., 0.1s to 4.5s) and generalizing to varying inference frequencies. Our key\ninsight is to learn the spatiotemporal relationships from the encoded event\nfeatures via the state space model (SSM) -- whose linear complexity makes it\nideal for modeling high temporal resolution events with longer sequences. To\nachieve this goal, we first propose a Path-Adaptive Event Aggregation and Scan\n(PEAS) module to encode events of varying duration into features with fixed\ndimensions by adaptively scanning and selecting aggregated event frames. On top\nof PEAS, we introduce a novel Multi-faceted Selection Guiding (MSG) loss to\nminimize the randomness and redundancy of the encoded features. This subtly\nenhances the model generalization across different inference frequencies.\nLastly, the SSM is employed to better learn the spatiotemporal properties from\nthe encoded features. Moreover, we build a minute-level event-based recognition\ndataset, named ArDVS100, with arbitrary duration for the benefit of the\ncommunity. Extensive experiments prove that our method outperforms prior arts\nby +3.45%, +0.38% and +8.31% on the DVS Action, SeAct and HARDVS datasets,\nrespectively.\n","authors":["Jiazhou Zhou","Kanghao Chen","Lei Zhang","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2409.16953v1.pdf","comment":"First version"},{"id":"http://arxiv.org/abs/2409.16949v1","updated":"2024-09-25T14:02:43Z","published":"2024-09-25T14:02:43Z","title":"DALDA: Data Augmentation Leveraging Diffusion Model and LLM with\n  Adaptive Guidance Scaling","summary":"  In this paper, we present an effective data augmentation framework leveraging\nthe Large Language Model (LLM) and Diffusion Model (DM) to tackle the\nchallenges inherent in data-scarce scenarios. Recently, DMs have opened up the\npossibility of generating synthetic images to complement a few training images.\nHowever, increasing the diversity of synthetic images also raises the risk of\ngenerating samples outside the target distribution. Our approach addresses this\nissue by embedding novel semantic information into text prompts via LLM and\nutilizing real images as visual prompts, thus generating semantically rich\nimages. To ensure that the generated images remain within the target\ndistribution, we dynamically adjust the guidance weight based on each image's\nCLIPScore to control the diversity. Experimental results show that our method\nproduces synthetic images with enhanced diversity while maintaining adherence\nto the target distribution. Consequently, our approach proves to be more\nefficient in the few-shot setting on several benchmarks. Our code is available\nat https://github.com/kkyuhun94/dalda .\n","authors":["Kyuheon Jung","Yongdeuk Seo","Seongwoo Cho","Jaeyoung Kim","Hyun-seok Min","Sungchul Choi"],"pdf_url":"https://arxiv.org/pdf/2409.16949v1.pdf","comment":"Accepted to ECCV Synthetic Data for Computer Vision Workshop (Oral)"},{"id":"http://arxiv.org/abs/2406.05915v2","updated":"2024-09-25T14:01:55Z","published":"2024-06-09T20:58:32Z","title":"Bits-to-Photon: End-to-End Learned Scalable Point Cloud Compression for\n  Direct Rendering","summary":"  Point cloud is a promising 3D representation for volumetric streaming in\nemerging AR/VR applications. Despite recent advances in point cloud\ncompression, decoding and rendering high-quality images from lossy compressed\npoint clouds is still challenging in terms of quality and complexity, making it\na major roadblock to achieve real-time 6-Degree-of-Freedom video streaming. In\nthis paper, we address this problem by developing a point cloud compression\nscheme that generates a bit stream that can be directly decoded to renderable\n3D Gaussians. The encoder and decoder are jointly optimized to consider both\nbit-rates and rendering quality. It significantly improves the rendering\nquality while substantially reducing decoding and rendering time, compared to\nexisting point cloud compression methods. Furthermore, the proposed scheme\ngenerates a scalable bit stream, allowing multiple levels of details at\ndifferent bit-rate ranges. Our method supports real-time color decoding and\nrendering of high quality point clouds, thus paving the way for interactive 3D\nstreaming applications with free view points.\n","authors":["Yueyu Hu","Ran Gong","Yao Wang"],"pdf_url":"https://arxiv.org/pdf/2406.05915v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16947v1","updated":"2024-09-25T13:59:36Z","published":"2024-09-25T13:59:36Z","title":"NTIRE 2024 Challenge on Stereo Image Super-Resolution: Methods and\n  Results","summary":"  This paper summarizes the 3rd NTIRE challenge on stereo image\nsuper-resolution (SR) with a focus on new solutions and results. The task of\nthis challenge is to super-resolve a low-resolution stereo image pair to a\nhigh-resolution one with a magnification factor of x4 under a limited\ncomputational budget. Compared with single image SR, the major challenge of\nthis challenge lies in how to exploit additional information in another\nviewpoint and how to maintain stereo consistency in the results. This challenge\nhas 2 tracks, including one track on bicubic degradation and one track on real\ndegradations. In total, 108 and 70 participants were successfully registered\nfor each track, respectively. In the test phase, 14 and 13 teams successfully\nsubmitted valid results with PSNR (RGB) scores better than the baseline. This\nchallenge establishes a new benchmark for stereo image SR.\n","authors":["Longguang Wang","Yulan Guo","Juncheng Li","Hongda Liu","Yang Zhao","Yingqian Wang","Zhi Jin","Shuhang Gu","Radu Timofte"],"pdf_url":"https://arxiv.org/pdf/2409.16947v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16945v1","updated":"2024-09-25T13:57:16Z","published":"2024-09-25T13:57:16Z","title":"Face Forgery Detection with Elaborate Backbone","summary":"  Face Forgery Detection (FFD), or Deepfake detection, aims to determine\nwhether a digital face is real or fake. Due to different face synthesis\nalgorithms with diverse forgery patterns, FFD models often overfit specific\npatterns in training datasets, resulting in poor generalization to other unseen\nforgeries. This severe challenge requires FFD models to possess strong\ncapabilities in representing complex facial features and extracting subtle\nforgery cues. Although previous FFD models directly employ existing backbones\nto represent and extract facial forgery cues, the critical role of backbones is\noften overlooked, particularly as their knowledge and capabilities are\ninsufficient to address FFD challenges, inevitably limiting generalization.\nTherefore, it is essential to integrate the backbone pre-training\nconfigurations and seek practical solutions by revisiting the complete FFD\nworkflow, from backbone pre-training and fine-tuning to inference of\ndiscriminant results. Specifically, we analyze the crucial contributions of\nbackbones with different configurations in FFD task and propose leveraging the\nViT network with self-supervised learning on real-face datasets to pre-train a\nbackbone, equipping it with superior facial representation capabilities. We\nthen build a competitive backbone fine-tuning framework that strengthens the\nbackbone's ability to extract diverse forgery cues within a competitive\nlearning mechanism. Moreover, we devise a threshold optimization mechanism that\nutilizes prediction confidence to improve the inference reliability.\nComprehensive experiments demonstrate that our FFD model with the elaborate\nbackbone achieves excellent performance in FFD and extra face-related tasks,\ni.e., presentation attack detection. Code and models are available at\nhttps://github.com/zhenglab/FFDBackbone.\n","authors":["Zonghui Guo","Yingjie Liu","Jie Zhang","Haiyong Zheng","Shiguang Shan"],"pdf_url":"https://arxiv.org/pdf/2409.16945v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16944v1","updated":"2024-09-25T13:56:08Z","published":"2024-09-25T13:56:08Z","title":"Go-SLAM: Grounded Object Segmentation and Localization with Gaussian\n  Splatting SLAM","summary":"  We introduce Go-SLAM, a novel framework that utilizes 3D Gaussian Splatting\nSLAM to reconstruct dynamic environments while embedding object-level\ninformation within the scene representations. This framework employs advanced\nobject segmentation techniques, assigning a unique identifier to each Gaussian\nsplat that corresponds to the object it represents. Consequently, our system\nfacilitates open-vocabulary querying, allowing users to locate objects using\nnatural language descriptions. Furthermore, the framework features an optimal\npath generation module that calculates efficient navigation paths for robots\ntoward queried objects, considering obstacles and environmental uncertainties.\nComprehensive evaluations in various scene settings demonstrate the\neffectiveness of our approach in delivering high-fidelity scene\nreconstructions, precise object segmentation, flexible object querying, and\nefficient robot path planning. This work represents an additional step forward\nin bridging the gap between 3D scene reconstruction, semantic object\nunderstanding, and real-time environment interactions.\n","authors":["Phu Pham","Dipam Patel","Damon Conover","Aniket Bera"],"pdf_url":"https://arxiv.org/pdf/2409.16944v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16940v1","updated":"2024-09-25T13:53:48Z","published":"2024-09-25T13:53:48Z","title":"Going Beyond U-Net: Assessing Vision Transformers for Semantic\n  Segmentation in Microscopy Image Analysis","summary":"  Segmentation is a crucial step in microscopy image analysis. Numerous\napproaches have been developed over the past years, ranging from classical\nsegmentation algorithms to advanced deep learning models. While U-Net remains\none of the most popular and well-established models for biomedical segmentation\ntasks, recently developed transformer-based models promise to enhance the\nsegmentation process of microscopy images. In this work, we assess the efficacy\nof transformers, including UNETR, the Segment Anything Model, and Swin-UPerNet,\nand compare them with the well-established U-Net model across various image\nmodalities such as electron microscopy, brightfield, histopathology, and\nphase-contrast. Our evaluation identifies several limitations in the original\nSwin Transformer model, which we address through architectural modifications to\noptimise its performance. The results demonstrate that these modifications\nimprove segmentation performance compared to the classical U-Net model and the\nunmodified Swin-UPerNet. This comparative analysis highlights the promise of\ntransformer models for advancing biomedical image segmentation. It demonstrates\nthat their efficiency and applicability can be improved with careful\nmodifications, facilitating their future use in microscopy image analysis\ntools.\n","authors":["Illia Tsiporenko","Pavel Chizhov","Dmytro Fishman"],"pdf_url":"https://arxiv.org/pdf/2409.16940v1.pdf","comment":"to be published in ECCV 2024 BioImage Computing Workshop"},{"id":"http://arxiv.org/abs/2409.16938v1","updated":"2024-09-25T13:52:50Z","published":"2024-09-25T13:52:50Z","title":"Generative Object Insertion in Gaussian Splatting with a Multi-View\n  Diffusion Model","summary":"  Generating and inserting new objects into 3D content is a compelling approach\nfor achieving versatile scene recreation. Existing methods, which rely on SDS\noptimization or single-view inpainting, often struggle to produce high-quality\nresults. To address this, we propose a novel method for object insertion in 3D\ncontent represented by Gaussian Splatting. Our approach introduces a multi-view\ndiffusion model, dubbed MVInpainter, which is built upon a pre-trained stable\nvideo diffusion model to facilitate view-consistent object inpainting. Within\nMVInpainter, we incorporate a ControlNet-based conditional injection module to\nenable controlled and more predictable multi-view generation. After generating\nthe multi-view inpainted results, we further propose a mask-aware 3D\nreconstruction technique to refine Gaussian Splatting reconstruction from these\nsparse inpainted views. By leveraging these fabricate techniques, our approach\nyields diverse results, ensures view-consistent and harmonious insertions, and\nproduces better object quality. Extensive experiments demonstrate that our\napproach outperforms existing methods.\n","authors":["Hongliang Zhong","Can Wang","Jingbo Zhang","Jing Liao"],"pdf_url":"https://arxiv.org/pdf/2409.16938v1.pdf","comment":"Project Page: https://github.com/JiuTongBro/MultiView_Inpaint"},{"id":"http://arxiv.org/abs/2406.19280v3","updated":"2024-09-25T13:36:27Z","published":"2024-06-27T15:50:41Z","title":"HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into\n  Multimodal LLMs at Scale","summary":"  The rapid development of multimodal large language models (MLLMs), such as\nGPT-4V, has led to significant advancements. However, these models still face\nchallenges in medical multimodal capabilities due to limitations in the\nquantity and quality of medical vision-text data, stemming from data privacy\nconcerns and high annotation costs. While pioneering approaches utilize\nPubMed's large-scale, de-identified medical image-text pairs to address these\nlimitations, they still fall short due to inherent data noise. To tackle this,\nwe refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in\nan 'unblinded' capacity to denoise and reformat the data, resulting in the\ncreation of the PubMedVision dataset with 1.3 million medical VQA samples. Our\nvalidation demonstrates that: (1) PubMedVision can significantly enhance the\nmedical multimodal capabilities of current MLLMs, showing significant\nimprovement in benchmarks including the MMMU Health & Medicine track; (2)\nmanual checks by medical experts and empirical results validate the superior\ndata quality of our dataset compared to other data construction methods. Using\nPubMedVision, we train a 34B medical MLLM HuatuoGPT-Vision, which shows\nsuperior performance in medical multimodal scenarios among open-source MLLMs.\n","authors":["Junying Chen","Chi Gui","Ruyi Ouyang","Anningzhe Gao","Shunian Chen","Guiming Hardy Chen","Xidong Wang","Ruifei Zhang","Zhenyang Cai","Ke Ji","Guangjun Yu","Xiang Wan","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2406.19280v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16925v1","updated":"2024-09-25T13:33:28Z","published":"2024-09-25T13:33:28Z","title":"Game4Loc: A UAV Geo-Localization Benchmark from Game Data","summary":"  The vision-based geo-localization technology for UAV, serving as a secondary\nsource of GPS information in addition to the global navigation satellite\nsystems (GNSS), can still operate independently in the GPS-denied environment.\nRecent deep learning based methods attribute this as the task of image matching\nand retrieval. By retrieving drone-view images in geo-tagged satellite image\ndatabase, approximate localization information can be obtained. However, due to\nhigh costs and privacy concerns, it is usually difficult to obtain large\nquantities of drone-view images from a continuous area. Existing drone-view\ndatasets are mostly composed of small-scale aerial photography with a strong\nassumption that there exists a perfect one-to-one aligned reference image for\nany query, leaving a significant gap from the practical localization scenario.\nIn this work, we construct a large-range contiguous area UAV geo-localization\ndataset named GTA-UAV, featuring multiple flight altitudes, attitudes, scenes,\nand targets using modern computer games. Based on this dataset, we introduce a\nmore practical UAV geo-localization task including partial matches of\ncross-view paired data, and expand the image-level retrieval to the actual\nlocalization in terms of distance (meters). For the construction of drone-view\nand satellite-view pairs, we adopt a weight-based contrastive learning\napproach, which allows for effective learning while avoiding additional\npost-processing matching steps. Experiments demonstrate the effectiveness of\nour data and training method for UAV geo-localization, as well as the\ngeneralization capabilities to real-world scenarios.\n","authors":["Yuxiang Ji","Boyong He","Zhuoyue Tan","Liaoni Wu"],"pdf_url":"https://arxiv.org/pdf/2409.16925v1.pdf","comment":"Project page: https://yux1angji.github.io/game4loc/"},{"id":"http://arxiv.org/abs/2409.16921v1","updated":"2024-09-25T13:27:29Z","published":"2024-09-25T13:27:29Z","title":"Moner: Motion Correction in Undersampled Radial MRI with Unsupervised\n  Neural Representation","summary":"  Motion correction (MoCo) in radial MRI is a challenging problem due to the\nunpredictability of subject's motion. Current state-of-the-art (SOTA) MoCo\nalgorithms often use extensive high-quality MR images to pre-train neural\nnetworks, obtaining excellent reconstructions. However, the need for\nlarge-scale datasets significantly increases costs and limits model\ngeneralization. In this work, we propose Moner, an unsupervised MoCo method\nthat jointly solves artifact-free MR images and accurate motion from\nundersampled, rigid motion-corrupted k-space data, without requiring training\ndata. Our core idea is to leverage the continuous prior of implicit neural\nrepresentation (INR) to constrain this ill-posed inverse problem, enabling\nideal solutions. Specifically, we incorporate a quasi-static motion model into\nthe INR, granting its ability to correct subject's motion. To stabilize model\noptimization, we reformulate radial MRI as a back-projection problem using the\nFourier-slice theorem. Additionally, we propose a novel coarse-to-fine hash\nencoding strategy, significantly enhancing MoCo accuracy. Experiments on\nmultiple MRI datasets show our Moner achieves performance comparable to SOTA\nMoCo techniques on in-domain data, while demonstrating significant improvements\non out-of-domain data.\n","authors":["Qing Wu","Chenhe Du","XuanYu Tian","Jingyi Yu","Yuyao Zhang","Hongjiang Wei"],"pdf_url":"https://arxiv.org/pdf/2409.16921v1.pdf","comment":"18 pages, 13 pages"},{"id":"http://arxiv.org/abs/2309.08482v2","updated":"2024-09-25T13:26:19Z","published":"2023-09-15T15:42:00Z","title":"YCB-Ev 1.1: Event-vision dataset for 6DoF object pose estimation","summary":"  Our work introduces the YCB-Ev dataset, which contains synchronized RGB-D\nframes and event data that enables evaluating 6DoF object pose estimation\nalgorithms using these modalities.\n  This dataset provides ground truth 6DoF object poses for the same 21 YCB\nobjects that were used in the YCB-Video (YCB-V) dataset, allowing for\ncross-dataset algorithm performance evaluation.\n  The dataset consists of 21 synchronized event and RGB-D sequences, totalling\n13,851 frames (7 minutes and 43 seconds of event data). Notably, 12 of these\nsequences feature the same object arrangement as the YCB-V subset used in the\nBOP challenge.\n  Ground truth poses are generated by detecting objects in the RGB-D frames,\ninterpolating the poses to align with the event timestamps, and then\ntransferring them to the event coordinate frame using extrinsic calibration.\n  Our dataset is the first to provide ground truth 6DoF pose data for event\nstreams. Furthermore, we evaluate the generalization capabilities of two\nstate-of-the-art algorithms, which were pre-trained for the BOP challenge,\nusing our novel YCB-V sequences.\n  The dataset is publicly available at https://github.com/paroj/ycbev.\n","authors":["Pavel Rojtberg","Thomas Pöllabauer"],"pdf_url":"https://arxiv.org/pdf/2309.08482v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.00126v3","updated":"2024-09-25T13:13:32Z","published":"2023-04-28T23:43:10Z","title":"Event-Free Moving Object Segmentation from Moving Ego Vehicle","summary":"  Moving object segmentation (MOS) in dynamic scenes is an important,\nchallenging, but under-explored research topic for autonomous driving,\nespecially for sequences obtained from moving ego vehicles. Most segmentation\nmethods leverage motion cues obtained from optical flow maps. However, since\nthese methods are often based on optical flows that are pre-computed from\nsuccessive RGB frames, this neglects the temporal consideration of events\noccurring within the inter-frame, consequently constraining its ability to\ndiscern objects exhibiting relative staticity but genuinely in motion. To\naddress these limitations, we propose to exploit event cameras for better video\nunderstanding, which provide rich motion cues without relying on optical flow.\nTo foster research in this area, we first introduce a novel large-scale dataset\ncalled DSEC-MOS for moving object segmentation from moving ego vehicles, which\nis the first of its kind. For benchmarking, we select various mainstream\nmethods and rigorously evaluate them on our dataset. Subsequently, we devise\nEmoFormer, a novel network able to exploit the event data. For this purpose, we\nfuse the event temporal prior with spatial semantic maps to distinguish\ngenuinely moving objects from the static background, adding another level of\ndense supervision around our object of interest. Our proposed network relies\nonly on event data for training but does not require event input during\ninference, making it directly comparable to frame-only methods in terms of\nefficiency and more widely usable in many application cases. The exhaustive\ncomparison highlights a significant performance improvement of our method over\nall other methods. The source code and dataset are publicly available at:\nhttps://github.com/ZZY-Zhou/DSEC-MOS.\n","authors":["Zhuyun Zhou","Zongwei Wu","Danda Pani Paudel","Rémi Boutteau","Fan Yang","Luc Van Gool","Radu Timofte","Dominique Ginhac"],"pdf_url":"https://arxiv.org/pdf/2305.00126v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16907v1","updated":"2024-09-25T13:12:58Z","published":"2024-09-25T13:12:58Z","title":"An Adaptive Screen-Space Meshing Approach for Normal Integration","summary":"  Reconstructing surfaces from normals is a key component of photometric\nstereo. This work introduces an adaptive surface triangulation in the image\ndomain and afterwards performs the normal integration on a triangle mesh. Our\nkey insight is that surface curvature can be computed from normals. Based on\nthe curvature, we identify flat areas and aggregate pixels into triangles. The\napproximation quality is controlled by a single user parameter facilitating a\nseamless generation of low- to high-resolution meshes. Compared to pixel grids,\nour triangle meshes adapt locally to surface details and allow for a sparser\nrepresentation. Our new mesh-based formulation of the normal integration\nproblem is strictly derived from discrete differential geometry and leads to\nwell-conditioned linear systems. Results on real and synthetic data show that\n10 to 100 times less vertices are required than pixels. Experiments suggest\nthat this sparsity translates into a sublinear runtime in the number of pixels.\nFor 64 MP normal maps, our meshing-first approach generates and integrates\nmeshes in minutes while pixel-based approaches require hours just for the\nintegration.\n","authors":["Moritz Heep","Eduard Zell"],"pdf_url":"https://arxiv.org/pdf/2409.16907v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16902v1","updated":"2024-09-25T13:10:03Z","published":"2024-09-25T13:10:03Z","title":"Towards Underwater Camouflaged Object Tracking: An Experimental\n  Evaluation of SAM and SAM 2","summary":"  Over the past decade, significant progress has been made in visual object\ntracking, largely due to the availability of large-scale training datasets.\nHowever, existing tracking datasets are primarily focused on open-air\nscenarios, which greatly limits the development of object tracking in\nunderwater environments. To address this issue, we take a step forward by\nproposing the first large-scale underwater camouflaged object tracking dataset,\nnamely UW-COT. Based on the proposed dataset, this paper presents an\nexperimental evaluation of several advanced visual object tracking methods and\nthe latest advancements in image and video segmentation. Specifically, we\ncompare the performance of the Segment Anything Model (SAM) and its updated\nversion, SAM 2, in challenging underwater environments. Our findings highlight\nthe improvements in SAM 2 over SAM, demonstrating its enhanced capability to\nhandle the complexities of underwater camouflaged objects. Compared to current\nadvanced visual object tracking methods, the latest video segmentation\nfoundation model SAM 2 also exhibits significant advantages, providing valuable\ninsights into the development of more effective tracking technologies for\nunderwater scenarios. The dataset will be accessible at\n\\color{magenta}{https://github.com/983632847/Awesome-Multimodal-Object-Tracking}.\n","authors":["Chunhui Zhang","Li Liu","Guanjie Huang","Hao Wen","Xi Zhou","Yanfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2409.16902v1.pdf","comment":"Preprint. Work in Progress"},{"id":"http://arxiv.org/abs/2409.16897v1","updated":"2024-09-25T13:07:37Z","published":"2024-09-25T13:07:37Z","title":"HVT: A Comprehensive Vision Framework for Learning in Non-Euclidean\n  Space","summary":"  Data representation in non-Euclidean spaces has proven effective for\ncapturing hierarchical and complex relationships in real-world datasets.\nHyperbolic spaces, in particular, provide efficient embeddings for hierarchical\nstructures. This paper introduces the Hyperbolic Vision Transformer (HVT), a\nnovel extension of the Vision Transformer (ViT) that integrates hyperbolic\ngeometry. While traditional ViTs operate in Euclidean space, our method\nenhances the self-attention mechanism by leveraging hyperbolic distance and\nM\\\"obius transformations. This enables more effective modeling of hierarchical\nand relational dependencies in image data. We present rigorous mathematical\nformulations, showing how hyperbolic geometry can be incorporated into\nattention layers, feed-forward networks, and optimization. We offer improved\nperformance for image classification using the ImageNet dataset.\n","authors":["Jacob Fein-Ashley","Ethan Feng","Minh Pham"],"pdf_url":"https://arxiv.org/pdf/2409.16897v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16865v1","updated":"2024-09-25T12:28:48Z","published":"2024-09-25T12:28:48Z","title":"Linking in Style: Understanding learned features in deep learning models","summary":"  Convolutional neural networks (CNNs) learn abstract features to perform\nobject classification, but understanding these features remains challenging due\nto difficult-to-interpret results or high computational costs. We propose an\nautomatic method to visualize and systematically analyze learned features in\nCNNs. Specifically, we introduce a linking network that maps the penultimate\nlayer of a pre-trained classifier to the latent space of a generative model\n(StyleGAN-XL), thereby enabling an interpretable, human-friendly visualization\nof the classifier's representations. Our findings indicate a congruent semantic\norder in both spaces, enabling a direct linear mapping between them. Training\nthe linking network is computationally inexpensive and decoupled from training\nboth the GAN and the classifier. We introduce an automatic pipeline that\nutilizes such GAN-based visualizations to quantify learned representations by\nanalyzing activation changes in the classifier in the image domain. This\nquantification allows us to systematically study the learned representations in\nseveral thousand units simultaneously and to extract and visualize units\nselective for specific semantic concepts. Further, we illustrate how our method\ncan be used to quantify and interpret the classifier's decision boundary using\ncounterfactual examples. Overall, our method offers systematic and objective\nperspectives on learned abstract representations in CNNs.\nhttps://github.com/kaschube-lab/LinkingInStyle.git\n","authors":["Maren H. Wehrheim","Pamela Osuna-Vargas","Matthias Kaschube"],"pdf_url":"https://arxiv.org/pdf/2409.16865v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.21454v3","updated":"2024-09-25T12:24:35Z","published":"2024-07-31T08:59:33Z","title":"StreetSurfaceVis: a dataset of crowdsourced street-level imagery\n  annotated by road surface type and quality","summary":"  Road unevenness significantly impacts the safety and comfort of traffic\nparticipants, especially vulnerable groups such as cyclists and wheelchair\nusers. To train models for comprehensive road surface assessments, we introduce\nStreetSurfaceVis, a novel dataset comprising 9,122 street-level images mostly\nfrom Germany collected from a crowdsourcing platform and manually annotated by\nroad surface type and quality. By crafting a heterogeneous dataset, we aim to\nenable robust models that maintain high accuracy across diverse image sources.\nAs the frequency distribution of road surface types and qualities is highly\nimbalanced, we propose a sampling strategy incorporating various external label\nprediction resources to ensure sufficient images per class while reducing\nmanual annotation. More precisely, we estimate the impact of (1) enriching the\nimage data with OpenStreetMap tags, (2) iterative training and application of a\ncustom surface type classification model, (3) amplifying underrepresented\nclasses through prompt-based classification with GPT-4o and (4) similarity\nsearch using image embeddings. Combining these strategies effectively reduces\nmanual annotation workload while ensuring sufficient class representation.\n","authors":["Alexandra Kapp","Edith Hoffmann","Esther Weigmann","Helena Mihaljević"],"pdf_url":"https://arxiv.org/pdf/2407.21454v3.pdf","comment":"12 pages, 2 figures"},{"id":"http://arxiv.org/abs/2409.16863v1","updated":"2024-09-25T12:21:31Z","published":"2024-09-25T12:21:31Z","title":"Towards Unified 3D Hair Reconstruction from Single-View Portraits","summary":"  Single-view 3D hair reconstruction is challenging, due to the wide range of\nshape variations among diverse hairstyles. Current state-of-the-art methods are\nspecialized in recovering un-braided 3D hairs and often take braided styles as\ntheir failure cases, because of the inherent difficulty to define priors for\ncomplex hairstyles, whether rule-based or data-based. We propose a novel\nstrategy to enable single-view 3D reconstruction for a variety of hair types\nvia a unified pipeline. To achieve this, we first collect a large-scale\nsynthetic multi-view hair dataset SynMvHair with diverse 3D hair in both\nbraided and un-braided styles, and learn two diffusion priors specialized on\nhair. Then we optimize 3D Gaussian-based hair from the priors with two\nspecially designed modules, i.e. view-wise and pixel-wise Gaussian refinement.\nOur experiments demonstrate that reconstructing braided and un-braided 3D hair\nfrom single-view images via a unified approach is possible and our method\nachieves the state-of-the-art performance in recovering complex hairstyles. It\nis worth to mention that our method shows good generalization ability to real\nimages, although it learns hair priors from synthetic data.\n","authors":["Yujian Zheng","Yuda Qiu","Leyang Jin","Chongyang Ma","Haibin Huang","Di Zhang","Pengfei Wan","Xiaoguang Han"],"pdf_url":"https://arxiv.org/pdf/2409.16863v1.pdf","comment":"SIGGRAPH Asia 2024, project page: https://unihair24.github.io"},{"id":"http://arxiv.org/abs/2409.16861v1","updated":"2024-09-25T12:15:18Z","published":"2024-09-25T12:15:18Z","title":"Limitations of (Procrustes) Alignment in Assessing Multi-Person Human\n  Pose and Shape Estimation","summary":"  We delve into the challenges of accurately estimating 3D human pose and shape\nin video surveillance scenarios. Beginning with the advocacy for metrics like\nW-MPJPE and W-PVE, which omit the (Procrustes) realignment step, to improve\nmodel evaluation, we then introduce RotAvat. This technique aims to enhance\nthese metrics by refining the alignment of 3D meshes with the ground plane.\nThrough qualitative comparisons, we demonstrate RotAvat's effectiveness in\naddressing the limitations of existing aproaches.\n","authors":["Drazic Martin","Pierre Perrault"],"pdf_url":"https://arxiv.org/pdf/2409.16861v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16860v1","updated":"2024-09-25T12:15:15Z","published":"2024-09-25T12:15:15Z","title":"The Role of Language Models in Modern Healthcare: A Comprehensive Review","summary":"  The application of large language models (LLMs) in healthcare has gained\nsignificant attention due to their ability to process complex medical data and\nprovide insights for clinical decision-making. These models have demonstrated\nsubstantial capabilities in understanding and generating natural language,\nwhich is crucial for medical documentation, diagnostics, and patient\ninteraction. This review examines the trajectory of language models from their\nearly stages to the current state-of-the-art LLMs, highlighting their strengths\nin healthcare applications and discussing challenges such as data privacy,\nbias, and ethical considerations. The potential of LLMs to enhance healthcare\ndelivery is explored, alongside the necessary steps to ensure their ethical and\neffective integration into medical practice.\n","authors":["Amna Khalid","Ayma Khalid","Umar Khalid"],"pdf_url":"https://arxiv.org/pdf/2409.16860v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16855v1","updated":"2024-09-25T12:06:30Z","published":"2024-09-25T12:06:30Z","title":"A Versatile and Differentiable Hand-Object Interaction Representation","summary":"  Synthesizing accurate hands-object interactions (HOI) is critical for\napplications in Computer Vision, Augmented Reality (AR), and Mixed Reality\n(MR). Despite recent advances, the accuracy of reconstructed or generated HOI\nleaves room for refinement. Some techniques have improved the accuracy of dense\ncorrespondences by shifting focus from generating explicit contacts to using\nrich HOI fields. Still, they lack full differentiability or continuity and are\ntailored to specific tasks. In contrast, we present a Coarse Hand-Object\nInteraction Representation (CHOIR), a novel, versatile and fully differentiable\nfield for HOI modelling. CHOIR leverages discrete unsigned distances for\ncontinuous shape and pose encoding, alongside multivariate Gaussian\ndistributions to represent dense contact maps with few parameters. To\ndemonstrate the versatility of CHOIR we design JointDiffusion, a diffusion\nmodel to learn a grasp distribution conditioned on noisy hand-object\ninteractions or only object geometries, for both refinement and synthesis\napplications. We demonstrate JointDiffusion's improvements over the SOTA in\nboth applications: it increases the contact F1 score by $5\\%$ for refinement\nand decreases the sim. displacement by $46\\%$ for synthesis. Our experiments\nshow that JointDiffusion with CHOIR yield superior contact accuracy and\nphysical realism compared to SOTA methods designed for specific tasks. Our\nmodels and code will be publicly available to the research community.\n","authors":["Théo Morales","Omid Taheri","Gerard Lacey"],"pdf_url":"https://arxiv.org/pdf/2409.16855v1.pdf","comment":"Accepted at the Winter Applications in Computer Vision 2025\n  conference. 9 pages, 6 figures"},{"id":"http://arxiv.org/abs/2406.19364v3","updated":"2024-09-25T12:03:54Z","published":"2024-06-27T17:46:13Z","title":"SimTxtSeg: Weakly-Supervised Medical Image Segmentation with Simple Text\n  Cues","summary":"  Weakly-supervised medical image segmentation is a challenging task that aims\nto reduce the annotation cost while keep the segmentation performance. In this\npaper, we present a novel framework, SimTxtSeg, that leverages simple text cues\nto generate high-quality pseudo-labels and study the cross-modal fusion in\ntraining segmentation models, simultaneously. Our contribution consists of two\nkey components: an effective Textual-to-Visual Cue Converter that produces\nvisual prompts from text prompts on medical images, and a text-guided\nsegmentation model with Text-Vision Hybrid Attention that fuses text and image\nfeatures. We evaluate our framework on two medical image segmentation tasks:\ncolonic polyp segmentation and MRI brain tumor segmentation, and achieve\nconsistent state-of-the-art performance. Source code is available at:\nhttps://github.com/xyx1024/SimTxtSeg.\n","authors":["Yuxin Xie","Tao Zhou","Yi Zhou","Geng Chen"],"pdf_url":"https://arxiv.org/pdf/2406.19364v3.pdf","comment":"accepted by MICCAI 2024"},{"id":"http://arxiv.org/abs/2409.16850v1","updated":"2024-09-25T11:55:27Z","published":"2024-09-25T11:55:27Z","title":"Robust Scene Change Detection Using Visual Foundation Models and\n  Cross-Attention Mechanisms","summary":"  We present a novel method for scene change detection that leverages the\nrobust feature extraction capabilities of a visual foundational model, DINOv2,\nand integrates full-image cross-attention to address key challenges such as\nvarying lighting, seasonal variations, and viewpoint differences. In order to\neffectively learn correspondences and mis-correspondences between an image pair\nfor the change detection task, we propose to a) ``freeze'' the backbone in\norder to retain the generality of dense foundation features, and b) employ\n``full-image'' cross-attention to better tackle the viewpoint variations\nbetween the image pair. We evaluate our approach on two benchmark datasets,\nVL-CMU-CD and PSCD, along with their viewpoint-varied versions. Our experiments\ndemonstrate significant improvements in F1-score, particularly in scenarios\ninvolving geometric changes between image pairs. The results indicate our\nmethod's superior generalization capabilities over existing state-of-the-art\napproaches, showing robustness against photometric and geometric variations as\nwell as better overall generalization when fine-tuned to adapt to new\nenvironments. Detailed ablation studies further validate the contributions of\neach component in our architecture. Source code will be made publicly available\nupon acceptance.\n","authors":["Chun-Jung Lin","Sourav Garg","Tat-Jun Chin","Feras Dayoub"],"pdf_url":"https://arxiv.org/pdf/2409.16850v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2409.16845v1","updated":"2024-09-25T11:53:58Z","published":"2024-09-25T11:53:58Z","title":"IRASNet: Improved Feature-Level Clutter Reduction for Domain Generalized\n  SAR-ATR","summary":"  Recently, computer-aided design models and electromagnetic simulations have\nbeen used to augment synthetic aperture radar (SAR) data for deep learning.\nHowever, an automatic target recognition (ATR) model struggles with domain\nshift when using synthetic data because the model learns specific clutter\npatterns present in such data, which disturbs performance when applied to\nmeasured data with different clutter distributions. This study proposes a\nframework particularly designed for domain-generalized SAR-ATR called IRASNet,\nenabling effective feature-level clutter reduction and domain-invariant feature\nlearning. First, we propose a clutter reduction module (CRM) that maximizes the\nsignal-to-clutter ratio on feature maps. The module reduces the impact of\nclutter at the feature level while preserving target and shadow information,\nthereby improving ATR performance. Second, we integrate adversarial learning\nwith CRM to extract clutter-reduced domain-invariant features. The integration\nbridges the gap between synthetic and measured datasets without requiring\nmeasured data during training. Third, we improve feature extraction from target\nand shadow regions by implementing a positional supervision task using mask\nground truth encoding. The improvement enhances the ability of the model to\ndiscriminate between classes. Our proposed IRASNet presents new\nstate-of-the-art public SAR datasets utilizing target and shadow information to\nachieve superior performance across various test conditions. IRASNet not only\nenhances generalization performance but also significantly improves\nfeature-level clutter reduction, making it a valuable advancement in the field\nof radar image pattern recognition.\n","authors":["Oh-Tae Jang","Hae-Kang Song","Min-Jun Kim","Kyung-Hwan Lee","Geon Lee","Sung-Ho Kim","Kyung-Tae Kim"],"pdf_url":"https://arxiv.org/pdf/2409.16845v1.pdf","comment":"16 pages, 11 figures"},{"id":"http://arxiv.org/abs/2409.16838v1","updated":"2024-09-25T11:43:29Z","published":"2024-09-25T11:43:29Z","title":"Explicitly Modeling Pre-Cortical Vision with a Neuro-Inspired Front-End\n  Improves CNN Robustness","summary":"  While convolutional neural networks (CNNs) excel at clean image\nclassification, they struggle to classify images corrupted with different\ncommon corruptions, limiting their real-world applicability. Recent work has\nshown that incorporating a CNN front-end block that simulates some features of\nthe primate primary visual cortex (V1) can improve overall model robustness.\nHere, we expand on this approach by introducing two novel biologically-inspired\nCNN model families that incorporate a new front-end block designed to simulate\npre-cortical visual processing. RetinaNet, a hybrid architecture containing the\nnovel front-end followed by a standard CNN back-end, shows a relative\nrobustness improvement of 12.3% when compared to the standard model; and EVNet,\nwhich further adds a V1 block after the pre-cortical front-end, shows a\nrelative gain of 18.5%. The improvement in robustness was observed for all the\ndifferent corruption categories, though accompanied by a small decrease in\nclean image accuracy, and generalized to a different back-end architecture.\nThese findings show that simulating multiple stages of early visual processing\nin CNN early layers provides cumulative benefits for model robustness.\n","authors":["Lucas Piper","Arlindo L. Oliveira","Tiago Marques"],"pdf_url":"https://arxiv.org/pdf/2409.16838v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15503v3","updated":"2024-09-25T11:29:27Z","published":"2024-08-28T03:17:40Z","title":"RoboSense: Large-scale Dataset and Benchmark for Multi-sensor Low-speed\n  Autonomous Driving","summary":"  Robust object detection and tracking under arbitrary sight of view is\nchallenging yet essential for the development of Autonomous Vehicle technology.\nWith the growing demand of unmanned function vehicles, near-field scene\nunderstanding becomes an important research topic in the areas of low-speed\nautonomous driving. Due to the complexity of driving conditions and diversity\nof near obstacles such as blind spots and high occlusion, the perception\ncapability of near-field environment is still inferior than its farther\ncounterpart. To further enhance the intelligent ability of unmanned vehicles,\nin this paper, we construct a multimodal data collection platform based on 3\nmain types of sensors (Camera, LiDAR and Fisheye), which supports flexible\nsensor configurations to enable dynamic sight of view for ego vehicle, either\nglobal view or local view. Meanwhile, a large-scale multi-sensor dataset is\nbuilt, named RoboSense, to facilitate near-field scene understanding. RoboSense\ncontains more than 133K synchronized data with 1.4M 3D bounding box and IDs\nannotated in the full $360^{\\circ}$ view, forming 216K trajectories across 7.6K\ntemporal sequences. It has $270\\times$ and $18\\times$ as many annotations of\nnear-field obstacles within 5$m$ as the previous single-vehicle datasets such\nas KITTI and nuScenes. Moreover, we define a novel matching criterion for\nnear-field 3D perception and prediction metrics. Based on RoboSense, we\nformulate 6 popular tasks to facilitate the future development of related\nresearch, where the detailed data analysis as well as benchmarks are also\nprovided accordingly. Code and dataset will be available at\nhttps://github.com/suhaisheng/RoboSense.\n","authors":["Haisheng Su","Feixiang Song","Cong Ma","Wei Wu","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2408.15503v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16827v1","updated":"2024-09-25T11:24:37Z","published":"2024-09-25T11:24:37Z","title":"Focus Entirety and Perceive Environment for Arbitrary-Shaped Text\n  Detection","summary":"  Due to the diversity of scene text in aspects such as font, color, shape, and\nsize, accurately and efficiently detecting text is still a formidable\nchallenge. Among the various detection approaches, segmentation-based\napproaches have emerged as prominent contenders owing to their flexible\npixel-level predictions. However, these methods typically model text instances\nin a bottom-up manner, which is highly susceptible to noise. In addition, the\nprediction of pixels is isolated without introducing pixel-feature interaction,\nwhich also influences the detection performance. To alleviate these problems,\nwe propose a multi-information level arbitrary-shaped text detector consisting\nof a focus entirety module (FEM) and a perceive environment module (PEM). The\nformer extracts instance-level features and adopts a top-down scheme to model\ntexts to reduce the influence of noises. Specifically, it assigns consistent\nentirety information to pixels within the same instance to improve their\ncohesion. In addition, it emphasizes the scale information, enabling the model\nto distinguish varying scale texts effectively. The latter extracts\nregion-level information and encourages the model to focus on the distribution\nof positive samples in the vicinity of a pixel, which perceives environment\ninformation. It treats the kernel pixels as positive samples and helps the\nmodel differentiate text and kernel features. Extensive experiments demonstrate\nthe FEM's ability to efficiently support the model in handling different scale\ntexts and confirm the PEM can assist in perceiving pixels more accurately by\nfocusing on pixel vicinities. Comparisons show the proposed model outperforms\nexisting state-of-the-art approaches on four public datasets.\n","authors":["Xu Han","Junyu Gao","Chuang Yang","Yuan Yuan","Qi Wang"],"pdf_url":"https://arxiv.org/pdf/2409.16827v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16821v1","updated":"2024-09-25T11:19:42Z","published":"2024-09-25T11:19:42Z","title":"XAI-guided Insulator Anomaly Detection for Imbalanced Datasets","summary":"  Power grids serve as a vital component in numerous industries, seamlessly\ndelivering electrical energy to industrial processes and technologies, making\ntheir safe and reliable operation indispensable. However, powerlines can be\nhard to inspect due to difficult terrain or harsh climatic conditions.\nTherefore, unmanned aerial vehicles are increasingly deployed to inspect\npowerlines, resulting in a substantial stream of visual data which requires\nswift and accurate processing. Deep learning methods have become widely popular\nfor this task, proving to be a valuable asset in fault detection. In\nparticular, the detection of insulator defects is crucial for predicting\npowerline failures, since their malfunction can lead to transmission\ndisruptions. It is therefore of great interest to continuously maintain and\nrigorously inspect insulator components. In this work we propose a novel\npipeline to tackle this task. We utilize state-of-the-art object detection to\ndetect and subsequently classify individual insulator anomalies. Our approach\naddresses dataset challenges such as imbalance and motion-blurred images\nthrough a fine-tuning methodology which allows us to alter the classification\nfocus of the model by increasing the classification accuracy of anomalous\ninsulators. In addition, we employ explainable-AI tools for precise\nlocalization and explanation of anomalies. This proposed method contributes to\nthe field of anomaly detection, particularly vision-based industrial inspection\nand predictive maintenance. We significantly improve defect detection accuracy\nby up to 13%, while also offering a detailed analysis of model\nmis-classifications and localization quality, showcasing the potential of our\nmethod on real-world data.\n","authors":["Maximilian Andreas Hoefler","Karsten Mueller","Wojciech Samek"],"pdf_url":"https://arxiv.org/pdf/2409.16821v1.pdf","comment":"Accepted as a workshop paper at ECCV 2024"},{"id":"http://arxiv.org/abs/2409.16820v1","updated":"2024-09-25T11:19:09Z","published":"2024-09-25T11:19:09Z","title":"Spotlight Text Detector: Spotlight on Candidate Regions Like a Camera","summary":"  The irregular contour representation is one of the tough challenges in scene\ntext detection. Although segmentation-based methods have achieved significant\nprogress with the help of flexible pixel prediction, the overlap of\ngeographically close texts hinders detecting them separately. To alleviate this\nproblem, some shrink-based methods predict text kernels and expand them to\nrestructure texts. However, the text kernel is an artificial object with\nincomplete semantic features that are prone to incorrect or missing detection.\nIn addition, different from the general objects, the geometry features (aspect\nratio, scale, and shape) of scene texts vary significantly, which makes it\ndifficult to detect them accurately. To consider the above problems, we propose\nan effective spotlight text detector (STD), which consists of a spotlight\ncalibration module (SCM) and a multivariate information extraction module\n(MIEM). The former concentrates efforts on the candidate kernel, like a camera\nfocus on the target. It obtains candidate features through a mapping filter and\ncalibrates them precisely to eliminate some false positive samples. The latter\ndesigns different shape schemes to explore multiple geometric features for\nscene texts. It helps extract various spatial relationships to improve the\nmodel's ability to recognize kernel regions. Ablation studies prove the\neffectiveness of the designed SCM and MIEM. Extensive experiments verify that\nour STD is superior to existing state-of-the-art methods on various datasets,\nincluding ICDAR2015, CTW1500, MSRA-TD500, and Total-Text.\n","authors":["Xu Han","Junyu Gao","Chuang Yang","Yuan Yuan","Qi Wang"],"pdf_url":"https://arxiv.org/pdf/2409.16820v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16818v1","updated":"2024-09-25T11:14:47Z","published":"2024-09-25T11:14:47Z","title":"Towards General Text-guided Image Synthesis for Customized Multimodal\n  Brain MRI Generation","summary":"  Multimodal brain magnetic resonance (MR) imaging is indispensable in\nneuroscience and neurology. However, due to the accessibility of MRI scanners\nand their lengthy acquisition time, multimodal MR images are not commonly\navailable. Current MR image synthesis approaches are typically trained on\nindependent datasets for specific tasks, leading to suboptimal performance when\napplied to novel datasets and tasks. Here, we present TUMSyn, a Text-guided\nUniversal MR image Synthesis generalist model, which can flexibly generate\nbrain MR images with demanded imaging metadata from routinely acquired scans\nguided by text prompts. To ensure TUMSyn's image synthesis precision,\nversatility, and generalizability, we first construct a brain MR database\ncomprising 31,407 3D images with 7 MRI modalities from 13 centers. We then\npre-train an MRI-specific text encoder using contrastive learning to\neffectively control MR image synthesis based on text prompts. Extensive\nexperiments on diverse datasets and physician assessments indicate that TUMSyn\ncan generate clinically meaningful MR images with specified imaging metadata in\nsupervised and zero-shot scenarios. Therefore, TUMSyn can be utilized along\nwith acquired MR scan(s) to facilitate large-scale MRI-based screening and\ndiagnosis of brain diseases.\n","authors":["Yulin Wang","Honglin Xiong","Kaicong Sun","Shuwei Bai","Ling Dai","Zhongxiang Ding","Jiameng Liu","Qian Wang","Qian Liu","Dinggang Shen"],"pdf_url":"https://arxiv.org/pdf/2409.16818v1.pdf","comment":"23 pages, 9 figures"},{"id":"http://arxiv.org/abs/2409.16810v1","updated":"2024-09-25T11:02:30Z","published":"2024-09-25T11:02:30Z","title":"Inline Photometrically Calibrated Hybrid Visual SLAM","summary":"  This paper presents an integrated approach to Visual SLAM, merging online\nsequential photometric calibration within a Hybrid direct-indirect visual SLAM\n(H-SLAM). Photometric calibration helps normalize pixel intensity values under\ndifferent lighting conditions, and thereby improves the direct component of our\nH-SLAM. A tangential benefit also results to the indirect component of H-SLAM\ngiven that the detected features are more stable across variable lighting\nconditions. Our proposed photometrically calibrated H-SLAM is tested on several\ndatasets, including the TUM monoVO as well as on a dataset we created.\nCalibrated H-SLAM outperforms other state of the art direct, indirect, and\nhybrid Visual SLAM systems in all the experiments. Furthermore, in online SLAM\ntested at our site, it also significantly outperformed the other SLAM Systems.\n","authors":["Nicolas Abboud","Malak Sayour","Imad H. Elhajj","John Zelek","Daniel Asmar"],"pdf_url":"https://arxiv.org/pdf/2409.16810v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16808v1","updated":"2024-09-25T10:56:49Z","published":"2024-09-25T10:56:49Z","title":"Benchmarking Deep Learning Models for Object Detection on Edge Computing\n  Devices","summary":"  Modern applications, such as autonomous vehicles, require deploying deep\nlearning algorithms on resource-constrained edge devices for real-time image\nand video processing. However, there is limited understanding of the efficiency\nand performance of various object detection models on these devices. In this\npaper, we evaluate state-of-the-art object detection models, including YOLOv8\n(Nano, Small, Medium), EfficientDet Lite (Lite0, Lite1, Lite2), and SSD (SSD\nMobileNet V1, SSDLite MobileDet). We deployed these models on popular edge\ndevices like the Raspberry Pi 3, 4, and 5 with/without TPU accelerators, and\nJetson Orin Nano, collecting key performance metrics such as energy\nconsumption, inference time, and Mean Average Precision (mAP). Our findings\nhighlight that lower mAP models such as SSD MobileNet V1 are more\nenergy-efficient and faster in inference, whereas higher mAP models like YOLOv8\nMedium generally consume more energy and have slower inference, though with\nexceptions when accelerators like TPUs are used. Among the edge devices, Jetson\nOrin Nano stands out as the fastest and most energy-efficient option for\nrequest handling, despite having the highest idle energy consumption. These\nresults emphasize the need to balance accuracy, speed, and energy efficiency\nwhen deploying deep learning models on edge devices, offering valuable guidance\nfor practitioners and researchers selecting models and devices for their\napplications.\n","authors":["Daghash K. Alqahtani","Aamir Cheema","Adel N. Toosi"],"pdf_url":"https://arxiv.org/pdf/2409.16808v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16806v1","updated":"2024-09-25T10:56:08Z","published":"2024-09-25T10:56:08Z","title":"Topological SLAM in colonoscopies leveraging deep features and\n  topological priors","summary":"  We introduce ColonSLAM, a system that combines classical multiple-map metric\nSLAM with deep features and topological priors to create topological maps of\nthe whole colon. The SLAM pipeline by itself is able to create disconnected\nindividual metric submaps representing locations from short video subsections\nof the colon, but is not able to merge covisible submaps due to deformations\nand the limited performance of the SIFT descriptor in the medical domain.\nColonSLAM is guided by topological priors and combines a deep localization\nnetwork trained to distinguish if two images come from the same place or not\nand the soft verification of a transformer-based matching network, being able\nto relate far-in-time submaps during an exploration, grouping them in nodes\nimaging the same colon place, building more complex maps than any other\napproach in the literature. We demonstrate our approach in the Endomapper\ndataset, showing its potential for producing maps of the whole colon in real\nhuman explorations. Code and models are available at:\nhttps://github.com/endomapper/ColonSLAM.\n","authors":["Javier Morlana","Juan D. Tardós","José M. M. Montiel"],"pdf_url":"https://arxiv.org/pdf/2409.16806v1.pdf","comment":"MICCAI 2024"},{"id":"http://arxiv.org/abs/2401.03302v3","updated":"2024-09-25T10:45:52Z","published":"2024-01-06T20:53:02Z","title":"Realism in Action: Anomaly-Aware Diagnosis of Brain Tumors from Medical\n  Images Using YOLOv8 and DeiT","summary":"  In the field of medical sciences, reliable detection and classification of\nbrain tumors from images remains a formidable challenge due to the rarity of\ntumors within the population of patients. Therefore, the ability to detect\ntumors in anomaly scenarios is paramount for ensuring timely interventions and\nimproved patient outcomes. This study addresses the issue by leveraging deep\nlearning (DL) techniques to detect and classify brain tumors in challenging\nsituations. The curated data set from the National Brain Mapping Lab (NBML)\ncomprises 81 patients, including 30 Tumor cases and 51 Normal cases. The\ndetection and classification pipelines are separated into two consecutive\ntasks. The detection phase involved comprehensive data analysis and\npre-processing to modify the number of image samples and the number of patients\nof each class to anomaly distribution (9 Normal per 1 Tumor) to comply with\nreal world scenarios. Next, in addition to common evaluation metrics for the\ntesting, we employed a novel performance evaluation method called Patient to\nPatient (PTP), focusing on the realistic evaluation of the model. In the\ndetection phase, we fine-tuned a YOLOv8n detection model to detect the tumor\nregion. Subsequent testing and evaluation yielded competitive performance both\nin Common Evaluation Metrics and PTP metrics. Furthermore, using the Data\nEfficient Image Transformer (DeiT) module, we distilled a Vision Transformer\n(ViT) model from a fine-tuned ResNet152 as a teacher in the classification\nphase. This approach demonstrates promising strides in reliable tumor detection\nand classification, offering potential advancements in tumor diagnosis for\nreal-world medical imaging scenarios.\n","authors":["Seyed Mohammad Hossein Hashemi","Leila Safari","Amirhossein Dadashzadeh Taromi"],"pdf_url":"https://arxiv.org/pdf/2401.03302v3.pdf","comment":"This work has been submitted to the Elsevier for possible\n  publication. Copyright may be transferred without notice, after which this\n  version may no longer be accessible"},{"id":"http://arxiv.org/abs/2403.15033v4","updated":"2024-09-25T10:34:03Z","published":"2024-03-22T08:32:30Z","title":"Toward Tiny and High-quality Facial Makeup with Data Amplify Learning","summary":"  Contemporary makeup approaches primarily hinge on unpaired learning\nparadigms, yet they grapple with the challenges of inaccurate supervision\n(e.g., face misalignment) and sophisticated facial prompts (including face\nparsing, and landmark detection). These challenges prohibit low-cost deployment\nof facial makeup models, especially on mobile devices. To solve above problems,\nwe propose a brand-new learning paradigm, termed \"Data Amplify Learning (DAL),\"\nalongside a compact makeup model named \"TinyBeauty.\" The core idea of DAL lies\nin employing a Diffusion-based Data Amplifier (DDA) to \"amplify\" limited images\nfor the model training, thereby enabling accurate pixel-to-pixel supervision\nwith merely a handful of annotations. Two pivotal innovations in DDA facilitate\nthe above training approach: (1) A Residual Diffusion Model (RDM) is designed\nto generate high-fidelity detail and circumvent the detail vanishing problem in\nthe vanilla diffusion models; (2) A Fine-Grained Makeup Module (FGMM) is\nproposed to achieve precise makeup control and combination while retaining face\nidentity. Coupled with DAL, TinyBeauty necessitates merely 80K parameters to\nachieve a state-of-the-art performance without intricate face prompts.\nMeanwhile, TinyBeauty achieves a remarkable inference speed of up to 460 fps on\nthe iPhone 13. Extensive experiments show that DAL can produce highly\ncompetitive makeup models using only 5 image pairs.\n","authors":["Qiaoqiao Jin","Xuanhong Chen","Meiguang Jin","Ying Chen","Rui Shi","Yucheng Zheng","Yupeng Zhu","Bingbing Ni"],"pdf_url":"https://arxiv.org/pdf/2403.15033v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16797v1","updated":"2024-09-25T10:30:24Z","published":"2024-09-25T10:30:24Z","title":"Scalable Ensemble Diversification for OOD Generalization and Detection","summary":"  Training a diverse ensemble of models has several practical applications such\nas providing candidates for model selection with better out-of-distribution\n(OOD) generalization, and enabling the detection of OOD samples via Bayesian\nprinciples. An existing approach to diverse ensemble training encourages the\nmodels to disagree on provided OOD samples. However, the approach is\ncomputationally expensive and it requires well-separated ID and OOD examples,\nsuch that it has only been demonstrated in small-scale settings.\n  $\\textbf{Method.}$ This work presents a method for Scalable Ensemble\nDiversification (SED) applicable to large-scale settings (e.g. ImageNet) that\ndoes not require OOD samples. Instead, SED identifies hard training samples on\nthe fly and encourages the ensemble members to disagree on these. To improve\nscaling, we show how to avoid the expensive computations in existing methods of\nexhaustive pairwise disagreements across models.\n  $\\textbf{Results.}$ We evaluate the benefits of diversification with\nexperiments on ImageNet. First, for OOD generalization, we observe large\nbenefits from the diversification in multiple settings including output-space\n(classical) ensembles and weight-space ensembles (model soups). Second, for OOD\ndetection, we turn the diversity of ensemble hypotheses into a novel\nuncertainty score estimator that surpasses a large number of OOD detection\nbaselines.\n  Code is available here:\nhttps://github.com/AlexanderRubinstein/diverse-universe-public.\n","authors":["Alexander Rubinstein","Luca Scimeca","Damien Teney","Seong Joon Oh"],"pdf_url":"https://arxiv.org/pdf/2409.16797v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2409.16793v1","updated":"2024-09-25T10:14:01Z","published":"2024-09-25T10:14:01Z","title":"Spacewalker: Traversing Representation Spaces for Fast Interactive\n  Exploration and Annotation of Unstructured Data","summary":"  Unstructured data in industries such as healthcare, finance, and\nmanufacturing presents significant challenges for efficient analysis and\ndecision making. Detecting patterns within this data and understanding their\nimpact is critical but complex without the right tools. Traditionally, these\ntasks relied on the expertise of data analysts or labor-intensive manual\nreviews. In response, we introduce Spacewalker, an interactive tool designed to\nexplore and annotate data across multiple modalities. Spacewalker allows users\nto extract data representations and visualize them in low-dimensional spaces,\nenabling the detection of semantic similarities. Through extensive user\nstudies, we assess Spacewalker's effectiveness in data annotation and integrity\nverification. Results show that the tool's ability to traverse latent spaces\nand perform multi-modal queries significantly enhances the user's capacity to\nquickly identify relevant data. Moreover, Spacewalker allows for annotation\nspeed-ups far superior to conventional methods, making it a promising tool for\nefficiently navigating unstructured data and improving decision making\nprocesses. The code of this work is open-source and can be found at:\nhttps://github.com/code-lukas/Spacewalker\n","authors":["Lukas Heine","Fabian Hörst","Jana Fragemann","Gijs Luijten","Miriam Balzer","Jan Egger","Fin Bahnsen","M. Saquib Sarfraz","Jens Kleesiek","Constantin Seibold"],"pdf_url":"https://arxiv.org/pdf/2409.16793v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15260v2","updated":"2024-09-25T10:13:14Z","published":"2024-03-22T15:00:29Z","title":"Hyperbolic Metric Learning for Visual Outlier Detection","summary":"  Out-Of-Distribution (OOD) detection is critical to deploy deep learning\nmodels in safety-critical applications. However, the inherent hierarchical\nconcept structure of visual data, which is instrumental to OOD detection, is\noften poorly captured by conventional methods based on Euclidean geometry. This\nwork proposes a metric framework that leverages the strengths of Hyperbolic\ngeometry for OOD detection. Inspired by previous works that refine the decision\nboundary for OOD data with synthetic outliers, we extend this method to\nHyperbolic space. Interestingly, we find that synthetic outliers do not benefit\nOOD detection in Hyperbolic space as they do in Euclidean space. Furthermore we\nexplore the relationship between OOD detection performance and Hyperbolic\nembedding dimension, addressing practical concerns in resource-constrained\nenvironments. Extensive experiments show that our framework improves the FPR95\nfor OOD detection from 22\\% to 15\\% and from 49% to 28% on CIFAR-10 and\nCIFAR-100 respectively compared to Euclidean methods.\n","authors":["Alvaro Gonzalez-Jimenez","Simone Lionetti","Dena Bazazian","Philippe Gottfrois","Fabian Gröger","Marc Pouly","Alexander Navarini"],"pdf_url":"https://arxiv.org/pdf/2403.15260v2.pdf","comment":"European Conference on Computer Vision ECCV 2024 BEW Workshop"},{"id":"http://arxiv.org/abs/2406.19336v3","updated":"2024-09-25T10:09:03Z","published":"2024-06-27T17:10:10Z","title":"LiverUSRecon: Automatic 3D Reconstruction and Volumetry of the Liver\n  with a Few Partial Ultrasound Scans","summary":"  3D reconstruction of the liver for volumetry is important for qualitative\nanalysis and disease diagnosis. Liver volumetry using ultrasound (US) scans,\nalthough advantageous due to less acquisition time and safety, is challenging\ndue to the inherent noisiness in US scans, blurry boundaries, and partial liver\nvisibility. We address these challenges by using the segmentation masks of a\nfew incomplete sagittal-plane US scans of the liver in conjunction with a\nstatistical shape model (SSM) built using a set of CT scans of the liver. We\ncompute the shape parameters needed to warp this canonical SSM to fit the US\nscans through a parametric regression network. The resulting 3D liver\nreconstruction is accurate and leads to automatic liver volume calculation. We\nevaluate the accuracy of the estimated liver volumes with respect to CT\nsegmentation volumes using RMSE. Our volume computation is statistically much\ncloser to the volume estimated using CT scans than the volume computed using\nChilds' method by radiologists: p-value of 0.094 (>0.05) says that there is no\nsignificant difference between CT segmentation volumes and ours in contrast to\nChilds' method. We validate our method using investigations (ablation studies)\non the US image resolution, the number of CT scans used for SSM, the number of\nprincipal components, and the number of input US scans. To the best of our\nknowledge, this is the first automatic liver volumetry system using a few\nincomplete US scans given a set of CT scans of livers for SSM.\n","authors":["Kaushalya Sivayogaraj","Sahan T. Guruge","Udari Liyanage","Jeevani Udupihille","Saroj Jayasinghe","Gerard Fernando","Ranga Rodrigo","M. Rukshani Liyanaarachchi"],"pdf_url":"https://arxiv.org/pdf/2406.19336v3.pdf","comment":"10 pages, Accepted to MICCAI 2024"},{"id":"http://arxiv.org/abs/2409.09369v2","updated":"2024-09-25T09:55:24Z","published":"2024-09-14T08:47:45Z","title":"Interpretable Vision-Language Survival Analysis with Ordinal Inductive\n  Bias for Computational Pathology","summary":"  Histopathology Whole-Slide Images (WSIs) provide an important tool to assess\ncancer prognosis in computational pathology (CPATH). While existing survival\nanalysis (SA) approaches have made exciting progress, they are generally\nlimited to adopting highly-expressive architectures and only coarse-grained\npatient-level labels to learn prognostic visual representations from gigapixel\nWSIs. Such learning paradigm suffers from important performance bottlenecks,\nwhen facing present scarce training data and standard multi-instance learning\n(MIL) framework in CPATH. To overcome it, this paper, for the first time,\nproposes a new Vision-Language-based SA (VLSA) paradigm. Concretely, (1) VLSA\nis driven by pathology VL foundation models. It no longer relies on\nhigh-capability networks and shows the advantage of data efficiency. (2) In\nvision-end, VLSA encodes prognostic language prior and then employs it as\nauxiliary signals to guide the aggregating of prognostic visual features at\ninstance level, thereby compensating for the weak supervision in MIL. Moreover,\ngiven the characteristics of SA, we propose i) ordinal survival prompt learning\nto transform continuous survival labels into textual prompts; and ii) ordinal\nincidence function as prediction target to make SA compatible with VL-based\nprediction. Notably, VLSA's predictions can be interpreted intuitively by our\nShapley values-based method. The extensive experiments on five datasets confirm\nthe effectiveness of our scheme. Our VLSA could pave a new way for SA in CPATH\nby offering weakly-supervised MIL an effective means to learn valuable\nprognostic clues from gigapixel WSIs. Our source code is available at\nhttps://github.com/liupei101/VLSA.\n","authors":["Pei Liu","Luping Ji","Jiaxiang Gou","Bo Fu","Mao Ye"],"pdf_url":"https://arxiv.org/pdf/2409.09369v2.pdf","comment":"24 pages, 11 tables, 6 figures"},{"id":"http://arxiv.org/abs/2403.10942v3","updated":"2024-09-25T09:42:45Z","published":"2024-03-16T14:58:58Z","title":"ScanTalk: 3D Talking Heads from Unregistered Scans","summary":"  Speech-driven 3D talking heads generation has emerged as a significant area\nof interest among researchers, presenting numerous challenges. Existing methods\nare constrained by animating faces with fixed topologies, wherein point-wise\ncorrespondence is established, and the number and order of points remains\nconsistent across all identities the model can animate. In this work, we\npresent \\textbf{ScanTalk}, a novel framework capable of animating 3D faces in\narbitrary topologies including scanned data. Our approach relies on the\nDiffusionNet architecture to overcome the fixed topology constraint, offering\npromising avenues for more flexible and realistic 3D animations. By leveraging\nthe power of DiffusionNet, ScanTalk not only adapts to diverse facial\nstructures but also maintains fidelity when dealing with scanned data, thereby\nenhancing the authenticity and versatility of generated 3D talking heads.\nThrough comprehensive comparisons with state-of-the-art methods, we validate\nthe efficacy of our approach, demonstrating its capacity to generate realistic\ntalking heads comparable to existing techniques. While our primary objective is\nto develop a generic method free from topological constraints, all\nstate-of-the-art methodologies are bound by such limitations. Code for\nreproducing our results, and the pre-trained model are available at\nhttps://github.com/miccunifi/ScanTalk .\n","authors":["Federico Nocentini","Thomas Besnier","Claudio Ferrari","Sylvain Arguillere","Stefano Berretti","Mohamed Daoudi"],"pdf_url":"https://arxiv.org/pdf/2403.10942v3.pdf","comment":"Published in the ECCV 2024 Proceedings"},{"id":"http://arxiv.org/abs/2409.16774v1","updated":"2024-09-25T09:34:44Z","published":"2024-09-25T09:34:44Z","title":"MixPolyp: Integrating Mask, Box and Scribble Supervision for Enhanced\n  Polyp Segmentation","summary":"  Limited by the expensive labeling, polyp segmentation models are plagued by\ndata shortages. To tackle this, we propose the mixed supervised polyp\nsegmentation paradigm (MixPolyp). Unlike traditional models relying on a single\ntype of annotation, MixPolyp combines diverse annotation types (mask, box, and\nscribble) within a single model, thereby expanding the range of available data\nand reducing labeling costs. To achieve this, MixPolyp introduces three novel\nsupervision losses to handle various annotations: Subspace Projection loss\n(L_SP), Binary Minimum Entropy loss (L_BME), and Linear Regularization loss\n(L_LR). For box annotations, L_SP eliminates shape inconsistencies between the\nprediction and the supervision. For scribble annotations, L_BME provides\nsupervision for unlabeled pixels through minimum entropy constraint, thereby\nalleviating supervision sparsity. Furthermore, L_LR provides dense supervision\nby enforcing consistency among the predictions, thus reducing the\nnon-uniqueness. These losses are independent of the model structure, making\nthem generally applicable. They are used only during training, adding no\ncomputational cost during inference. Extensive experiments on five datasets\ndemonstrate MixPolyp's effectiveness.\n","authors":["Yiwen Hu","Jun Wei","Yuncheng Jiang","Haoyang Li","Shuguang Cui","Zhen Li","Song Wu"],"pdf_url":"https://arxiv.org/pdf/2409.16774v1.pdf","comment":"Accepted in IEEE BIBM 2024"},{"id":"http://arxiv.org/abs/2409.07966v2","updated":"2024-09-25T09:32:52Z","published":"2024-09-12T11:53:05Z","title":"ProbTalk3D: Non-Deterministic Emotion Controllable Speech-Driven 3D\n  Facial Animation Synthesis Using VQ-VAE","summary":"  Audio-driven 3D facial animation synthesis has been an active field of\nresearch with attention from both academia and industry. While there are\npromising results in this area, recent approaches largely focus on lip-sync and\nidentity control, neglecting the role of emotions and emotion control in the\ngenerative process. That is mainly due to the lack of emotionally rich facial\nanimation data and algorithms that can synthesize speech animations with\nemotional expressions at the same time. In addition, majority of the models are\ndeterministic, meaning given the same audio input, they produce the same output\nmotion. We argue that emotions and non-determinism are crucial to generate\ndiverse and emotionally-rich facial animations. In this paper, we propose\nProbTalk3D a non-deterministic neural network approach for emotion controllable\nspeech-driven 3D facial animation synthesis using a two-stage VQ-VAE model and\nan emotionally rich facial animation dataset 3DMEAD. We provide an extensive\ncomparative analysis of our model against the recent 3D facial animation\nsynthesis approaches, by evaluating the results objectively, qualitatively, and\nwith a perceptual user study. We highlight several objective metrics that are\nmore suitable for evaluating stochastic outputs and use both in-the-wild and\nground truth data for subjective evaluation. To our knowledge, that is the\nfirst non-deterministic 3D facial animation synthesis method incorporating a\nrich emotion dataset and emotion control with emotion labels and intensity\nlevels. Our evaluation demonstrates that the proposed model achieves superior\nperformance compared to state-of-the-art emotion-controlled, deterministic and\nnon-deterministic models. We recommend watching the supplementary video for\nquality judgement. The entire codebase is publicly available\n(https://github.com/uuembodiedsocialai/ProbTalk3D/).\n","authors":["Sichun Wu","Kazi Injamamul Haque","Zerrin Yumak"],"pdf_url":"https://arxiv.org/pdf/2409.07966v2.pdf","comment":"14 pages, 9 figures, 3 tables. Includes code. Accepted at ACM\n  SIGGRAPH MIG 2024"},{"id":"http://arxiv.org/abs/2409.16766v1","updated":"2024-09-25T09:24:53Z","published":"2024-09-25T09:24:53Z","title":"Let There Be Light: Robust Lensless Imaging Under External Illumination\n  With Deep Learning","summary":"  Lensless cameras relax the design constraints of traditional cameras by\nshifting image formation from analog optics to digital post-processing. While\nnew camera designs and applications can be enabled, lensless imaging is very\nsensitive to unwanted interference (other sources, noise, etc.). In this work,\nwe address a prevalent noise source that has not been studied for lensless\nimaging: external illumination e.g. from ambient and direct lighting. Being\nrobust to a variety of lighting conditions would increase the practicality and\nadoption of lensless imaging. To this end, we propose multiple recovery\napproaches that account for external illumination by incorporating its estimate\ninto the image recovery process. At the core is a physics-based reconstruction\nthat combines learnable image recovery and denoisers, all of whose parameters\nare trained using experimentally gathered data. Compared to standard\nreconstruction methods, our approach yields significant qualitative and\nquantitative improvements. We open-source our implementations and a 25K dataset\nof measurements under multiple lighting conditions.\n","authors":["Eric Bezzam","Stefan Peters","Martin Vetterli"],"pdf_url":"https://arxiv.org/pdf/2409.16766v1.pdf","comment":"4 pages, dataset: https://doi.org/10.57967/hf/2970"},{"id":"http://arxiv.org/abs/2409.16765v1","updated":"2024-09-25T09:24:42Z","published":"2024-09-25T09:24:42Z","title":"MaViLS, a Benchmark Dataset for Video-to-Slide Alignment, Assessing\n  Baseline Accuracy with a Multimodal Alignment Algorithm Leveraging Speech,\n  OCR, and Visual Features","summary":"  This paper presents a benchmark dataset for aligning lecture videos with\ncorresponding slides and introduces a novel multimodal algorithm leveraging\nfeatures from speech, text, and images. It achieves an average accuracy of 0.82\nin comparison to SIFT (0.56) while being approximately 11 times faster. Using\ndynamic programming the algorithm tries to determine the optimal slide\nsequence. The results show that penalizing slide transitions increases\naccuracy. Features obtained via optical character recognition (OCR) contribute\nthe most to a high matching accuracy, followed by image features. The findings\nhighlight that audio transcripts alone provide valuable information for\nalignment and are beneficial if OCR data is lacking. Variations in matching\naccuracy across different lectures highlight the challenges associated with\nvideo quality and lecture style. The novel multimodal algorithm demonstrates\nrobustness to some of these challenges, underscoring the potential of the\napproach.\n","authors":["Katharina Anderer","Andreas Reich","Matthias Wölfel"],"pdf_url":"https://arxiv.org/pdf/2409.16765v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16763v1","updated":"2024-09-25T09:18:19Z","published":"2024-09-25T09:18:19Z","title":"Statewide Visual Geolocalization in the Wild","summary":"  This work presents a method that is able to predict the geolocation of a\nstreet-view photo taken in the wild within a state-sized search region by\nmatching against a database of aerial reference imagery. We partition the\nsearch region into geographical cells and train a model to map cells and\ncorresponding photos into a joint embedding space that is used to perform\nretrieval at test time. The model utilizes aerial images for each cell at\nmultiple levels-of-detail to provide sufficient information about the\nsurrounding scene. We propose a novel layout of the search region with\nconsistent cell resolutions that allows scaling to large geographical regions.\nExperiments demonstrate that the method successfully localizes 60.6% of all\nnon-panoramic street-view photos uploaded to the crowd-sourcing platform\nMapillary in the state of Massachusetts to within 50m of their ground-truth\nlocation. Source code is available at\nhttps://github.com/fferflo/statewide-visual-geolocalization.\n","authors":["Florian Fervers","Sebastian Bullinger","Christoph Bodensteiner","Michael Arens","Rainer Stiefelhagen"],"pdf_url":"https://arxiv.org/pdf/2409.16763v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16756v1","updated":"2024-09-25T09:07:46Z","published":"2024-09-25T09:07:46Z","title":"Navigating the Maze of Explainable AI: A Systematic Approach to\n  Evaluating Methods and Metrics","summary":"  Explainable AI (XAI) is a rapidly growing domain with a myriad of proposed\nmethods as well as metrics aiming to evaluate their efficacy. However, current\nstudies are often of limited scope, examining only a handful of XAI methods and\nignoring underlying design parameters for performance, such as the model\narchitecture or the nature of input data. Moreover, they often rely on one or a\nfew metrics and neglect thorough validation, increasing the risk of selection\nbias and ignoring discrepancies among metrics. These shortcomings leave\npractitioners confused about which method to choose for their problem. In\nresponse, we introduce LATEC, a large-scale benchmark that critically evaluates\n17 prominent XAI methods using 20 distinct metrics. We systematically\nincorporate vital design parameters like varied architectures and diverse input\nmodalities, resulting in 7,560 examined combinations. Through LATEC, we\nshowcase the high risk of conflicting metrics leading to unreliable rankings\nand consequently propose a more robust evaluation scheme. Further, we\ncomprehensively evaluate various XAI methods to assist practitioners in\nselecting appropriate methods aligning with their needs. Curiously, the\nemerging top-performing method, Expected Gradients, is not examined in any\nrelevant related study. LATEC reinforces its role in future XAI research by\npublicly releasing all 326k saliency maps and 378k metric scores as a\n(meta-)evaluation dataset.\n","authors":["Lukas Klein","Carsten T. Lüth","Udo Schlegel","Till J. Bungert","Mennatallah El-Assady","Paul F. Jäger"],"pdf_url":"https://arxiv.org/pdf/2409.16756v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16154v2","updated":"2024-09-25T09:00:27Z","published":"2024-09-24T14:58:27Z","title":"Efficient Motion Prediction: A Lightweight & Accurate Trajectory\n  Prediction Model With Fast Training and Inference Speed","summary":"  For efficient and safe autonomous driving, it is essential that autonomous\nvehicles can predict the motion of other traffic agents. While highly accurate,\ncurrent motion prediction models often impose significant challenges in terms\nof training resource requirements and deployment on embedded hardware. We\npropose a new efficient motion prediction model, which achieves highly\ncompetitive benchmark results while training only a few hours on a single GPU.\nDue to our lightweight architectural choices and the focus on reducing the\nrequired training resources, our model can easily be applied to custom\ndatasets. Furthermore, its low inference latency makes it particularly suitable\nfor deployment in autonomous applications with limited computing resources.\n","authors":["Alexander Prutsch","Horst Bischof","Horst Possegger"],"pdf_url":"https://arxiv.org/pdf/2409.16154v2.pdf","comment":"Accepted to IROS 2024"},{"id":"http://arxiv.org/abs/2409.12431v3","updated":"2024-09-25T08:45:56Z","published":"2024-09-19T03:24:22Z","title":"FlexiTex: Enhancing Texture Generation with Visual Guidance","summary":"  Recent texture generation methods achieve impressive results due to the\npowerful generative prior they leverage from large-scale text-to-image\ndiffusion models. However, abstract textual prompts are limited in providing\nglobal textural or shape information, which results in the texture generation\nmethods producing blurry or inconsistent patterns. To tackle this, we present\nFlexiTex, embedding rich information via visual guidance to generate a\nhigh-quality texture. The core of FlexiTex is the Visual Guidance Enhancement\nmodule, which incorporates more specific information from visual guidance to\nreduce ambiguity in the text prompt and preserve high-frequency details. To\nfurther enhance the visual guidance, we introduce a Direction-Aware Adaptation\nmodule that automatically designs direction prompts based on different camera\nposes, avoiding the Janus problem and maintaining semantically global\nconsistency. Benefiting from the visual guidance, FlexiTex produces\nquantitatively and qualitatively sound results, demonstrating its potential to\nadvance texture generation for real-world applications.\n","authors":["DaDong Jiang","Xianghui Yang","Zibo Zhao","Sheng Zhang","Jiaao Yu","Zeqiang Lai","Shaoxiong Yang","Chunchao Guo","Xiaobo Zhou","Zhihui Ke"],"pdf_url":"https://arxiv.org/pdf/2409.12431v3.pdf","comment":"Project Page: https://flexitex.github.io/FlexiTex/"},{"id":"http://arxiv.org/abs/2409.16736v1","updated":"2024-09-25T08:36:59Z","published":"2024-09-25T08:36:59Z","title":"Commonly Interesting Images","summary":"  Images tell stories, trigger emotions, and let us recall memories -- they\nmake us think. Thus, they have the ability to attract and hold one's attention,\nwhich is the definition of being \"interesting\". Yet, the appeal of an image is\nhighly subjective. Looking at the image of my son taking his first steps will\nalways bring me back to this emotional moment, while it is just a blurry,\nquickly taken snapshot to most others. Preferences vary widely: some adore\ncats, others are dog enthusiasts, and a third group may not be fond of either.\nWe argue that every image can be interesting to a particular observer under\ncertain circumstances. This work particularly emphasizes subjective\npreferences. However, our analysis of 2.5k image collections from diverse users\nof the photo-sharing platform Flickr reveals that specific image\ncharacteristics make them commonly more interesting. For instance, images,\nincluding professionally taken landscapes, appeal broadly due to their\naesthetic qualities. In contrast, subjectively interesting images, such as\nthose depicting personal or niche community events, resonate on a more\nindividual level, often evoking personal memories and emotions.\n","authors":["Fitim Abdullahu","Helmut Grabner"],"pdf_url":"https://arxiv.org/pdf/2409.16736v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2409.16733v1","updated":"2024-09-25T08:31:37Z","published":"2024-09-25T08:31:37Z","title":"The Effect of Lossy Compression on 3D Medical Images Segmentation with\n  Deep Learning","summary":"  Image compression is a critical tool in decreasing the cost of storage and\nimproving the speed of transmission over the internet. While deep learning\napplications for natural images widely adopts the usage of lossy compression\ntechniques, it is not widespread for 3D medical images. Using three CT datasets\n(17 tasks) and one MRI dataset (3 tasks) we demonstrate that lossy compression\nup to 20 times have no negative impact on segmentation quality with deep neural\nnetworks (DNN). In addition, we demonstrate the ability of DNN models trained\non compressed data to predict on uncompressed data and vice versa with no\nquality deterioration.\n","authors":["Anvar Kurmukov","Bogdan Zavolovich","Aleksandra Dalechina","Vladislav Proskurov","Boris Shirokikh"],"pdf_url":"https://arxiv.org/pdf/2409.16733v1.pdf","comment":"12 pages, 5 figures, 2 tables; accepted on MICCAI Workshop on\n  Advancing Data Solutions in Medical Imaging AI"},{"id":"http://arxiv.org/abs/2409.16730v1","updated":"2024-09-25T08:28:54Z","published":"2024-09-25T08:28:54Z","title":"Non-stationary BERT: Exploring Augmented IMU Data For Robust Human\n  Activity Recognition","summary":"  Human Activity Recognition (HAR) has gained great attention from researchers\ndue to the popularity of mobile devices and the need to observe users' daily\nactivity data for better human-computer interaction. In this work, we collect a\nhuman activity recognition dataset called OPPOHAR consisting of phone IMU data.\nTo facilitate the employment of HAR system in mobile phone and to achieve\nuser-specific activity recognition, we propose a novel light-weight network\ncalled Non-stationary BERT with a two-stage training method. We also propose a\nsimple yet effective data augmentation method to explore the deeper\nrelationship between the accelerator and gyroscope data from the IMU. The\nnetwork achieves the state-of-the-art performance testing on various activity\nrecognition datasets and the data augmentation method demonstrates its wide\napplicability.\n","authors":["Ning Sun","Yufei Wang","Yuwei Zhang","Jixiang Wan","Shenyue Wang","Ping Liu","Xudong Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.16730v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16728v1","updated":"2024-09-25T08:23:53Z","published":"2024-09-25T08:23:53Z","title":"SDCL: Students Discrepancy-Informed Correction Learning for\n  Semi-supervised Medical Image Segmentation","summary":"  Semi-supervised medical image segmentation (SSMIS) has been demonstrated the\npotential to mitigate the issue of limited medical labeled data. However,\nconfirmation and cognitive biases may affect the prevalent teacher-student\nbased SSMIS methods due to erroneous pseudo-labels. To tackle this challenge,\nwe improve the mean teacher approach and propose the Students\nDiscrepancy-Informed Correction Learning (SDCL) framework that includes two\nstudents and one non-trainable teacher, which utilizes the segmentation\ndifference between the two students to guide the self-correcting learning. The\nessence of SDCL is to identify the areas of segmentation discrepancy as the\npotential bias areas, and then encourage the model to review the correct\ncognition and rectify their own biases in these areas. To facilitate the bias\ncorrection learning with continuous review and rectification, two correction\nloss functions are employed to minimize the correct segmentation voxel distance\nand maximize the erroneous segmentation voxel entropy. We conducted experiments\non three public medical image datasets: two 3D datasets (CT and MRI) and one 2D\ndataset (MRI). The results show that our SDCL surpasses the current\nState-of-the-Art (SOTA) methods by 2.57\\%, 3.04\\%, and 2.34\\% in the Dice score\non the Pancreas, LA, and ACDC datasets, respectively. In addition, the accuracy\nof our method is very close to the fully supervised method on the ACDC dataset,\nand even exceeds the fully supervised method on the Pancreas and LA dataset.\n(Code available at \\url{https://github.com/pascalcpp/SDCL}).\n","authors":["Bentao Song","Qingfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2409.16728v1.pdf","comment":"Accepted at MICCAI 2024"},{"id":"http://arxiv.org/abs/2409.16723v1","updated":"2024-09-25T08:22:00Z","published":"2024-09-25T08:22:00Z","title":"EAGLE: Towards Efficient Arbitrary Referring Visual Prompts\n  Comprehension for Multimodal Large Language Models","summary":"  Recently, Multimodal Large Language Models (MLLMs) have sparked great\nresearch interests owing to their exceptional content-reasoning and\ninstruction-following capabilities. To effectively instruct an MLLM, in\naddition to conventional language expressions, the practice of referring to\nobjects by painting with brushes on images has emerged as a prevalent tool\n(referred to as \"referring visual prompts\") due to its efficacy in aligning the\nuser's intention with specific image regions. To accommodate the most common\nreferring visual prompts, namely points, boxes, and masks, existing approaches\ninitially utilize specialized feature encoding modules to capture the semantics\nof the highlighted areas indicated by these prompts. Subsequently, these\nencoded region features are adapted to MLLMs through fine-tuning on a\nmeticulously curated multimodal instruction dataset. However, such designs\nsuffer from redundancy in architecture. Moreover, they face challenges in\neffectively generalizing when encountering a diverse range of arbitrary\nreferring visual prompts in real-life scenarios. To address the above issues,\nwe propose EAGLE, a novel MLLM that empowers comprehension of arbitrary\nreferring visual prompts with less training efforts than existing approaches.\nSpecifically, our EAGLE maintains the innate format of the referring visual\nprompts as colored patches rendered on the given image for conducting the\ninstruction tuning. Our approach embeds referring visual prompts as spatial\nconcepts conveying specific spatial areas comprehensible to the MLLM, with the\nsemantic comprehension of these regions originating from the MLLM itself.\nBesides, we also propose a Geometry-Agnostic Learning paradigm (GAL) to further\ndisentangle the MLLM's region-level comprehension with the specific formats of\nreferring visual prompts. Extensive experiments are conducted to prove the\neffectiveness of our proposed method.\n","authors":["Jiacheng Zhang","Yang Jiao","Shaoxiang Chen","Jingjing Chen","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2409.16723v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12246v3","updated":"2024-09-25T08:17:21Z","published":"2024-06-18T03:42:00Z","title":"TroL: Traversal of Layers for Large Language and Vision Models","summary":"  Large language and vision models (LLVMs) have been driven by the\ngeneralization power of large language models (LLMs) and the advent of visual\ninstruction tuning. Along with scaling them up directly, these models enable\nLLVMs to showcase powerful vision language (VL) performances by covering\ndiverse tasks via natural language instructions. However, existing open-source\nLLVMs that perform comparably to closed-source LLVMs such as GPT-4V are often\nconsidered too large (e.g., 26B, 34B, and 110B parameters), having a larger\nnumber of layers. These large models demand costly, high-end resources for both\ntraining and inference. To address this issue, we present a new efficient LLVM\nfamily with 1.8B, 3.8B, and 7B LLM model sizes, Traversal of Layers (TroL),\nwhich enables the reuse of layers in a token-wise manner. This layer traversing\ntechnique simulates the effect of looking back and retracing the answering\nstream while increasing the number of forward propagation layers without\nphysically adding more layers. We demonstrate that TroL employs a simple layer\ntraversing approach yet efficiently outperforms the open-source LLVMs with\nlarger model sizes and rivals the performances of the closed-source LLVMs with\nsubstantial sizes.\n","authors":["Byung-Kwan Lee","Sangyun Chung","Chae Won Kim","Beomchan Park","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2406.12246v3.pdf","comment":"EMNLP 2024. Code is available in https://github.com/ByungKwanLee/TroL"},{"id":"http://arxiv.org/abs/2409.16718v1","updated":"2024-09-25T08:07:18Z","published":"2024-09-25T08:07:18Z","title":"Vision-Language Model Fine-Tuning via Simple Parameter-Efficient\n  Modification","summary":"  Recent advances in fine-tuning Vision-Language Models (VLMs) have witnessed\nthe success of prompt tuning and adapter tuning, while the classic model\nfine-tuning on inherent parameters seems to be overlooked. It is believed that\nfine-tuning the parameters of VLMs with few-shot samples corrupts the\npre-trained knowledge since fine-tuning the CLIP model even degrades\nperformance. In this paper, we revisit this viewpoint, and propose a new\nperspective: fine-tuning the specific parameters instead of all will uncover\nthe power of classic model fine-tuning on VLMs. Through our meticulous study,\nwe propose ClipFit, a simple yet effective method to fine-tune CLIP without\nintroducing any overhead of extra parameters. We demonstrate that by only\nfine-tuning the specific bias terms and normalization layers, ClipFit can\nimprove the performance of zero-shot CLIP by 7.27\\% average harmonic mean\naccuracy. Lastly, to understand how fine-tuning in CLIPFit affects the\npre-trained models, we conducted extensive experimental analyses w.r.t. changes\nin internal parameters and representations. We found that low-level text bias\nlayers and the first layer normalization layer change much more than other\nlayers. The code is available at \\url{https://github.com/minglllli/CLIPFit}.\n","authors":["Ming Li","Jike Zhong","Chenxin Li","Liuzhuozheng Li","Nie Lin","Masashi Sugiyama"],"pdf_url":"https://arxiv.org/pdf/2409.16718v1.pdf","comment":"EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2409.14676v2","updated":"2024-09-25T08:03:46Z","published":"2024-09-23T02:52:49Z","title":"TransUKAN:Computing-Efficient Hybrid KAN-Transformer for Enhanced\n  Medical Image Segmentation","summary":"  U-Net is currently the most widely used architecture for medical image\nsegmentation. Benefiting from its unique encoder-decoder architecture and skip\nconnections, it can effectively extract features from input images to segment\ntarget regions. The commonly used U-Net is typically based on convolutional\noperations or Transformers, modeling the dependencies between local or global\ninformation to accomplish medical image analysis tasks. However, convolutional\nlayers, fully connected layers, and attention mechanisms used in this process\nintroduce a significant number of parameters, often requiring the stacking of\nnetwork layers to model complex nonlinear relationships, which can impact the\ntraining process. To address these issues, we propose TransUKAN. Specifically,\nwe have improved the KAN to reduce memory usage and computational load. On this\nbasis, we explored an effective combination of KAN, Transformer, and U-Net\nstructures. This approach enhances the model's capability to capture nonlinear\nrelationships by introducing only a small number of additional parameters and\ncompensates for the Transformer structure's deficiency in local information\nextraction. We validated TransUKAN on multiple medical image segmentation\ntasks. Experimental results demonstrate that TransUKAN achieves excellent\nperformance with significantly reduced parameters. The code will be available\nathttps://github.com/wuyanlin-wyl/TransUKAN.\n","authors":["Yanlin Wu","Tao Li","Zhihong Wang","Hong Kang","Along He"],"pdf_url":"https://arxiv.org/pdf/2409.14676v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16709v1","updated":"2024-09-25T07:54:53Z","published":"2024-09-25T07:54:53Z","title":"Pose-Guided Fine-Grained Sign Language Video Generation","summary":"  Sign language videos are an important medium for spreading and learning sign\nlanguage. However, most existing human image synthesis methods produce sign\nlanguage images with details that are distorted, blurred, or structurally\nincorrect. They also produce sign language video frames with poor temporal\nconsistency, with anomalies such as flickering and abrupt detail changes\nbetween the previous and next frames. To address these limitations, we propose\na novel Pose-Guided Motion Model (PGMM) for generating fine-grained and\nmotion-consistent sign language videos. Firstly, we propose a new Coarse Motion\nModule (CMM), which completes the deformation of features by optical flow\nwarping, thus transfering the motion of coarse-grained structures without\nchanging the appearance; Secondly, we propose a new Pose Fusion Module (PFM),\nwhich guides the modal fusion of RGB and pose features, thus completing the\nfine-grained generation. Finally, we design a new metric, Temporal Consistency\nDifference (TCD) to quantitatively assess the degree of temporal consistency of\na video by comparing the difference between the frames of the reconstructed\nvideo and the previous and next frames of the target video. Extensive\nqualitative and quantitative experiments show that our method outperforms\nstate-of-the-art methods in most benchmark tests, with visible improvements in\ndetails and temporal consistency.\n","authors":["Tongkai Shi","Lianyu Hu","Fanhua Shang","Jichao Feng","Peidong Liu","Wei Feng"],"pdf_url":"https://arxiv.org/pdf/2409.16709v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2409.16706v1","updated":"2024-09-25T07:51:47Z","published":"2024-09-25T07:51:47Z","title":"Pix2Next: Leveraging Vision Foundation Models for RGB to NIR Image\n  Translation","summary":"  This paper proposes Pix2Next, a novel image-to-image translation framework\ndesigned to address the challenge of generating high-quality Near-Infrared\n(NIR) images from RGB inputs. Our approach leverages a state-of-the-art Vision\nFoundation Model (VFM) within an encoder-decoder architecture, incorporating\ncross-attention mechanisms to enhance feature integration. This design captures\ndetailed global representations and preserves essential spectral\ncharacteristics, treating RGB-to-NIR translation as more than a simple domain\ntransfer problem. A multi-scale PatchGAN discriminator ensures realistic image\ngeneration at various detail levels, while carefully designed loss functions\ncouple global context understanding with local feature preservation. We\nperformed experiments on the RANUS dataset to demonstrate Pix2Next's advantages\nin quantitative metrics and visual quality, improving the FID score by 34.81%\ncompared to existing methods. Furthermore, we demonstrate the practical utility\nof Pix2Next by showing improved performance on a downstream object detection\ntask using generated NIR data to augment limited real NIR datasets. The\nproposed approach enables the scaling up of NIR datasets without additional\ndata acquisition or annotation efforts, potentially accelerating advancements\nin NIR-based computer vision applications.\n","authors":["Youngwan Jin","Incheol Park","Hanbin Song","Hyeongjin Ju","Yagiz Nalcakan","Shiho Kim"],"pdf_url":"https://arxiv.org/pdf/2409.16706v1.pdf","comment":"19 pages,12 figures"},{"id":"http://arxiv.org/abs/2306.05670v2","updated":"2024-09-25T07:51:23Z","published":"2023-06-09T04:59:24Z","title":"One-Shot Machine Unlearning with Mnemonic Code","summary":"  Ethical and privacy issues inherent in artificial intelligence (AI)\napplications have been a growing concern with the rapid spread of deep\nlearning. Machine unlearning (MU) is the research area that addresses these\nissues by making a trained AI model forget about undesirable training data.\nUnfortunately, most existing MU methods incur significant time and\ncomputational costs for forgetting. Therefore, it is often difficult to apply\nthese methods to practical datasets and sophisticated architectures, e.g.,\nImageNet and Transformer. To tackle this problem, we propose a lightweight and\neffective MU method. Our method identifies the model parameters sensitive to\nthe forgetting targets and adds perturbation to such model parameters. We\nidentify the sensitive parameters by calculating the Fisher Information Matrix\n(FIM). This approach does not require time-consuming additional training for\nforgetting. In addition, we introduce class-specific random signals called\nmnemonic code to reduce the cost of FIM calculation, which generally requires\nthe entire training data and incurs significant computational costs. In our\nmethod, we train the model with mnemonic code; when forgetting, we use a small\nnumber of mnemonic codes to calculate the FIM and get the effective\nperturbation for forgetting. Comprehensive experiments demonstrate that our\nmethod is faster and better at forgetting than existing MU methods.\nFurthermore, we show that our method can scale to more practical datasets and\nsophisticated architectures.\n","authors":["Tomoya Yamashita","Masanori Yamada","Takashi Shibata"],"pdf_url":"https://arxiv.org/pdf/2306.05670v2.pdf","comment":"24 pages, welcome coments"},{"id":"http://arxiv.org/abs/2409.16702v1","updated":"2024-09-25T07:48:57Z","published":"2024-09-25T07:48:57Z","title":"3DDX: Bone Surface Reconstruction from a Single Standard-Geometry\n  Radiograph via Dual-Face Depth Estimation","summary":"  Radiography is widely used in orthopedics for its affordability and low\nradiation exposure. 3D reconstruction from a single radiograph, so-called 2D-3D\nreconstruction, offers the possibility of various clinical applications, but\nachieving clinically viable accuracy and computational efficiency is still an\nunsolved challenge. Unlike other areas in computer vision, X-ray imaging's\nunique properties, such as ray penetration and fixed geometry, have not been\nfully exploited. We propose a novel approach that simultaneously learns\nmultiple depth maps (front- and back-surface of multiple bones) derived from\nthe X-ray image to computed tomography registration. The proposed method not\nonly leverages the fixed geometry characteristic of X-ray imaging but also\nenhances the precision of the reconstruction of the whole surface. Our study\ninvolved 600 CT and 2651 X-ray images (4 to 5 posed X-ray images per patient),\ndemonstrating our method's superiority over traditional approaches with a\nsurface reconstruction error reduction from 4.78 mm to 1.96 mm. This\nsignificant accuracy improvement and enhanced computational efficiency suggest\nour approach's potential for clinical application.\n","authors":["Yi Gu","Yoshito Otake","Keisuke Uemura","Masaki Takao","Mazen Soufi","Seiji Okada","Nobuhiko Sugano","Hugues Talbot","Yoshinobu Sato"],"pdf_url":"https://arxiv.org/pdf/2409.16702v1.pdf","comment":"MICCAI 2024. 12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2409.13430v3","updated":"2024-09-25T07:34:27Z","published":"2024-09-20T11:52:47Z","title":"CVT-Occ: Cost Volume Temporal Fusion for 3D Occupancy Prediction","summary":"  Vision-based 3D occupancy prediction is significantly challenged by the\ninherent limitations of monocular vision in depth estimation. This paper\nintroduces CVT-Occ, a novel approach that leverages temporal fusion through the\ngeometric correspondence of voxels over time to improve the accuracy of 3D\noccupancy predictions. By sampling points along the line of sight of each voxel\nand integrating the features of these points from historical frames, we\nconstruct a cost volume feature map that refines current volume features for\nimproved prediction outcomes. Our method takes advantage of parallax cues from\nhistorical observations and employs a data-driven approach to learn the cost\nvolume. We validate the effectiveness of CVT-Occ through rigorous experiments\non the Occ3D-Waymo dataset, where it outperforms state-of-the-art methods in 3D\noccupancy prediction with minimal additional computational cost. The code is\nreleased at \\url{https://github.com/Tsinghua-MARS-Lab/CVT-Occ}.\n","authors":["Zhangchen Ye","Tao Jiang","Chenfeng Xu","Yiming Li","Hang Zhao"],"pdf_url":"https://arxiv.org/pdf/2409.13430v3.pdf","comment":"Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2409.16689v1","updated":"2024-09-25T07:24:43Z","published":"2024-09-25T07:24:43Z","title":"Layout-Corrector: Alleviating Layout Sticking Phenomenon in Discrete\n  Diffusion Model","summary":"  Layout generation is a task to synthesize a harmonious layout with elements\ncharacterized by attributes such as category, position, and size. Human\ndesigners experiment with the placement and modification of elements to create\naesthetic layouts, however, we observed that current discrete diffusion models\n(DDMs) struggle to correct inharmonious layouts after they have been generated.\nIn this paper, we first provide novel insights into layout sticking phenomenon\nin DDMs and then propose a simple yet effective layout-assessment module\nLayout-Corrector, which works in conjunction with existing DDMs to address the\nlayout sticking problem. We present a learning-based module capable of\nidentifying inharmonious elements within layouts, considering overall layout\nharmony characterized by complex composition. During the generation process,\nLayout-Corrector evaluates the correctness of each token in the generated\nlayout, reinitializing those with low scores to the ungenerated state. The DDM\nthen uses the high-scored tokens as clues to regenerate the harmonized tokens.\nLayout-Corrector, tested on common benchmarks, consistently boosts\nlayout-generation performance when in conjunction with various state-of-the-art\nDDMs. Furthermore, our extensive analysis demonstrates that the\nLayout-Corrector (1) successfully identifies erroneous tokens, (2) facilitates\ncontrol over the fidelity-diversity trade-off, and (3) significantly mitigates\nthe performance drop associated with fast sampling.\n","authors":["Shoma Iwai","Atsuki Osanai","Shunsuke Kitada","Shinichiro Omachi"],"pdf_url":"https://arxiv.org/pdf/2409.16689v1.pdf","comment":"Accepted by ECCV2024, Project Page:\n  https://iwa-shi.github.io/Layout-Corrector-Project-Page/"},{"id":"http://arxiv.org/abs/2409.16685v1","updated":"2024-09-25T07:21:43Z","published":"2024-09-25T07:21:43Z","title":"Skyeyes: Ground Roaming using Aerial View Images","summary":"  Integrating aerial imagery-based scene generation into applications like\nautonomous driving and gaming enhances realism in 3D environments, but\nchallenges remain in creating detailed content for occluded areas and ensuring\nreal-time, consistent rendering. In this paper, we introduce Skyeyes, a novel\nframework that can generate photorealistic sequences of ground view images\nusing only aerial view inputs, thereby creating a ground roaming experience.\nMore specifically, we combine a 3D representation with a view consistent\ngeneration model, which ensures coherence between generated images. This method\nallows for the creation of geometrically consistent ground view images, even\nwith large view gaps. The images maintain improved spatial-temporal coherence\nand realism, enhancing scene comprehension and visualization from aerial\nperspectives. To the best of our knowledge, there are no publicly available\ndatasets that contain pairwise geo-aligned aerial and ground view imagery.\nTherefore, we build a large, synthetic, and geo-aligned dataset using Unreal\nEngine. Both qualitative and quantitative analyses on this synthetic dataset\ndisplay superior results compared to other leading synthesis approaches. See\nthe project page for more results:\nhttps://chaoren2357.github.io/website-skyeyes/.\n","authors":["Zhiyuan Gao","Wenbin Teng","Gonglin Chen","Jinsen Wu","Ningli Xu","Rongjun Qin","Andrew Feng","Yajie Zhao"],"pdf_url":"https://arxiv.org/pdf/2409.16685v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.04913v3","updated":"2024-09-25T07:16:41Z","published":"2024-04-07T10:49:59Z","title":"CodecNeRF: Toward Fast Encoding and Decoding, Compact, and High-quality\n  Novel-view Synthesis","summary":"  Neural Radiance Fields (NeRF) have achieved huge success in effectively\ncapturing and representing 3D objects and scenes. However, to establish a\nubiquitous presence in everyday media formats, such as images and videos, we\nneed to fulfill three key objectives: 1. fast encoding and decoding time, 2.\ncompact model sizes, and 3. high-quality renderings. Despite recent\nadvancements, a comprehensive algorithm that adequately addresses all\nobjectives has yet to be fully realized. In this work, we present CodecNeRF, a\nneural codec for NeRF representations, consisting of an encoder and decoder\narchitecture that can generate a NeRF representation in a single forward pass.\nFurthermore, inspired by the recent parameter-efficient finetuning approaches,\nwe propose a finetuning method to efficiently adapt the generated NeRF\nrepresentations to a new test instance, leading to high-quality image\nrenderings and compact code sizes. The proposed CodecNeRF, a newly suggested\nencoding-decoding-finetuning pipeline for NeRF, achieved unprecedented\ncompression performance of more than 100x and remarkable reduction in encoding\ntime while maintaining (or improving) the image quality on widely used 3D\nobject datasets.\n","authors":["Gyeongjin Kang","Younggeun Lee","Seungjun Oh","Eunbyung Park"],"pdf_url":"https://arxiv.org/pdf/2404.04913v3.pdf","comment":"Project page: https://gynjn.github.io/CodecNeRF/"},{"id":"http://arxiv.org/abs/2409.16678v1","updated":"2024-09-25T07:09:04Z","published":"2024-09-25T07:09:04Z","title":"TSBP: Improving Object Detection in Histology Images via Test-time\n  Self-guided Bounding-box Propagation","summary":"  A global threshold (e.g., 0.5) is often applied to determine which bounding\nboxes should be included in the final results for an object detection task. A\nhigher threshold reduces false positives but may result in missing a\nsignificant portion of true positives. A lower threshold can increase detection\nrecall but may also result in more false positives. Because of this, using a\npreset global threshold (e.g., 0.5) applied to all the bounding box candidates\nmay lead to suboptimal solutions. In this paper, we propose a Test-time\nSelf-guided Bounding-box Propagation (TSBP) method, leveraging Earth Mover's\nDistance (EMD) to enhance object detection in histology images. TSBP utilizes\nbounding boxes with high confidence to influence those with low confidence,\nleveraging visual similarities between them. This propagation mechanism enables\nbounding boxes to be selected in a controllable, explainable, and robust\nmanner, which surpasses the effectiveness of using simple thresholds and\nuncertainty calibration methods. Importantly, TSBP does not necessitate\nadditional labeled samples for model training or parameter estimation, unlike\ncalibration methods. We conduct experiments on gland detection and cell\ndetection tasks in histology images. The results show that our proposed TSBP\nsignificantly improves detection outcomes when working in conjunction with\nstate-of-the-art deep learning-based detection networks. Compared to other\nmethods such as uncertainty calibration, TSBP yields more robust and accurate\nobject detection predictions while using no additional labeled samples. The\ncode is available at https://github.com/jwhgdeu/TSBP.\n","authors":["Tingting Yang","Liang Xiao","Yizhe Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.16678v1.pdf","comment":"MICCAI 2024"},{"id":"http://arxiv.org/abs/2403.13652v2","updated":"2024-09-25T07:08:46Z","published":"2024-03-20T14:58:09Z","title":"ZoDi: Zero-Shot Domain Adaptation with Diffusion-Based Image Transfer","summary":"  Deep learning models achieve high accuracy in segmentation tasks among\nothers, yet domain shift often degrades the models' performance, which can be\ncritical in real-world scenarios where no target images are available. This\npaper proposes a zero-shot domain adaptation method based on diffusion models,\ncalled ZoDi, which is two-fold by the design: zero-shot image transfer and\nmodel adaptation. First, we utilize an off-the-shelf diffusion model to\nsynthesize target-like images by transferring the domain of source images to\nthe target domain. In this we specifically try to maintain the layout and\ncontent by utilising layout-to-image diffusion models with stochastic\ninversion. Secondly, we train the model using both source images and\nsynthesized images with the original segmentation maps while maximizing the\nfeature similarity of images from the two domains to learn domain-robust\nrepresentations. Through experiments we show benefits of ZoDi in the task of\nimage segmentation over state-of-the-art methods. It is also more applicable\nthan existing CLIP-based methods because it assumes no specific backbone or\nmodels, and it enables to estimate the model's performance without target\nimages by inspecting generated images. Our implementation will be publicly\navailable.\n","authors":["Hiroki Azuma","Yusuke Matsui","Atsuto Maki"],"pdf_url":"https://arxiv.org/pdf/2403.13652v2.pdf","comment":"ECCV2024 Workshop"},{"id":"http://arxiv.org/abs/2409.16666v1","updated":"2024-09-25T06:51:57Z","published":"2024-09-25T06:51:57Z","title":"TalkinNeRF: Animatable Neural Fields for Full-Body Talking Humans","summary":"  We introduce a novel framework that learns a dynamic neural radiance field\n(NeRF) for full-body talking humans from monocular videos. Prior work\nrepresents only the body pose or the face. However, humans communicate with\ntheir full body, combining body pose, hand gestures, as well as facial\nexpressions. In this work, we propose TalkinNeRF, a unified NeRF-based network\nthat represents the holistic 4D human motion. Given a monocular video of a\nsubject, we learn corresponding modules for the body, face, and hands, that are\ncombined together to generate the final result. To capture complex finger\narticulation, we learn an additional deformation field for the hands. Our\nmulti-identity representation enables simultaneous training for multiple\nsubjects, as well as robust animation under completely unseen poses. It can\nalso generalize to novel identities, given only a short video as input. We\ndemonstrate state-of-the-art performance for animating full-body talking\nhumans, with fine-grained hand articulation and facial expressions.\n","authors":["Aggelina Chatziagapi","Bindita Chaudhuri","Amit Kumar","Rakesh Ranjan","Dimitris Samaras","Nikolaos Sarafianos"],"pdf_url":"https://arxiv.org/pdf/2409.16666v1.pdf","comment":"Accepted by ECCVW 2024. Project page:\n  https://aggelinacha.github.io/TalkinNeRF/"},{"id":"http://arxiv.org/abs/2409.16663v1","updated":"2024-09-25T06:48:25Z","published":"2024-09-25T06:48:25Z","title":"Mitigating Covariate Shift in Imitation Learning for Autonomous Vehicles\n  Using Latent Space Generative World Models","summary":"  We propose the use of latent space generative world models to address the\ncovariate shift problem in autonomous driving. A world model is a neural\nnetwork capable of predicting an agent's next state given past states and\nactions. By leveraging a world model during training, the driving policy\neffectively mitigates covariate shift without requiring an excessive amount of\ntraining data. During end-to-end training, our policy learns how to recover\nfrom errors by aligning with states observed in human demonstrations, so that\nat runtime it can recover from perturbations outside the training distribution.\nAdditionally, we introduce a novel transformer-based perception encoder that\nemploys multi-view cross-attention and a learned scene query. We present\nqualitative and quantitative results, demonstrating significant improvements\nupon prior state of the art in closed-loop testing in the CARLA simulator, as\nwell as showing the ability to handle perturbations in both CARLA and NVIDIA's\nDRIVE Sim.\n","authors":["Alexander Popov","Alperen Degirmenci","David Wehr","Shashank Hegde","Ryan Oldja","Alexey Kamenev","Bertrand Douillard","David Nistér","Urs Muller","Ruchi Bhargava","Stan Birchfield","Nikolai Smolyanskiy"],"pdf_url":"https://arxiv.org/pdf/2409.16663v1.pdf","comment":"7 pages, 6 figures, for ICRA 2025 conference, for associated video\n  file, see https://youtu.be/9FpDFD9aiFU"},{"id":"http://arxiv.org/abs/2409.16652v1","updated":"2024-09-25T06:16:32Z","published":"2024-09-25T06:16:32Z","title":"Progressive Representation Learning for Real-Time UAV Tracking","summary":"  Visual object tracking has significantly promoted autonomous applications for\nunmanned aerial vehicles (UAVs). However, learning robust object\nrepresentations for UAV tracking is especially challenging in complex dynamic\nenvironments, when confronted with aspect ratio change and occlusion. These\nchallenges severely alter the original information of the object. To handle the\nabove issues, this work proposes a novel progressive representation learning\nframework for UAV tracking, i.e., PRL-Track. Specifically, PRL-Track is divided\ninto coarse representation learning and fine representation learning. For\ncoarse representation learning, two innovative regulators, which rely on\nappearance and semantic information, are designed to mitigate appearance\ninterference and capture semantic information. Furthermore, for fine\nrepresentation learning, a new hierarchical modeling generator is developed to\nintertwine coarse object representations. Exhaustive experiments demonstrate\nthat the proposed PRL-Track delivers exceptional performance on three\nauthoritative UAV tracking benchmarks. Real-world tests indicate that the\nproposed PRL-Track realizes superior tracking performance with 42.6 frames per\nsecond on the typical UAV platform equipped with an edge smart camera. The\ncode, model, and demo videos are available at\n\\url{https://github.com/vision4robotics/PRL-Track}.\n","authors":["Changhong Fu","Xiang Lei","Haobo Zuo","Liangliang Yao","Guangze Zheng","Jia Pan"],"pdf_url":"https://arxiv.org/pdf/2409.16652v1.pdf","comment":"Accepted by the 2024 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2024)"},{"id":"http://arxiv.org/abs/2408.00498v2","updated":"2024-09-25T05:32:25Z","published":"2024-08-01T12:08:20Z","title":"How Effective are Self-Supervised Models for Contact Identification in\n  Videos","summary":"  The exploration of video content via Self-Supervised Learning (SSL) models\nhas unveiled a dynamic field of study, emphasizing both the complex challenges\nand unique opportunities inherent in this area. Despite the growing body of\nresearch, the ability of SSL models to detect physical contacts in videos\nremains largely unexplored, particularly the effectiveness of methods such as\ndownstream supervision with linear probing or full fine-tuning. This work aims\nto bridge this gap by employing eight different convolutional neural networks\n(CNNs) based video SSL models to identify instances of physical contact within\nvideo sequences specifically. The Something-Something v2 (SSv2) and\nEpic-Kitchen (EK-100) datasets were chosen for evaluating these approaches due\nto the promising results on UCF101 and HMDB51, coupled with their limited prior\nassessment on SSv2 and EK-100. Additionally, these datasets feature diverse\nenvironments and scenarios, essential for testing the robustness and accuracy\nof video-based models. This approach not only examines the effectiveness of\neach model in recognizing physical contacts but also explores the performance\nin the action recognition downstream task. By doing so, valuable insights into\nthe adaptability of SSL models in interpreting complex, dynamic visual\ninformation are contributed.\n","authors":["Malitha Gunawardhana","Limalka Sadith","Liel David","Daniel Harari","Muhammad Haris Khan"],"pdf_url":"https://arxiv.org/pdf/2408.00498v2.pdf","comment":"15 pages, 6 figures"},{"id":"http://arxiv.org/abs/2409.16637v1","updated":"2024-09-25T05:30:18Z","published":"2024-09-25T05:30:18Z","title":"Deep-Learning Recognition of Scanning Transmission Electron Microscopy:\n  Quantifying and Mitigating the Influence of Gaussian Noises","summary":"  Scanning transmission electron microscopy (STEM) is a powerful tool to reveal\nthe morphologies and structures of materials, thereby attracting intensive\ninterests from the scientific and industrial communities. The outstanding\nspatial (atomic level) and temporal (ms level) resolutions of the STEM\ntechniques generate fruitful amounts of high-definition data, thereby enabling\nthe high-volume and high-speed analysis of materials. On the other hand,\nprocessing of the big dataset generated by STEM is time-consuming and beyond\nthe capability of human-based manual work, which urgently calls for\ncomputer-based automation. In this work, we present a deep-learning mask\nregion-based neural network (Mask R-CNN) for the recognition of nanoparticles\nimaged by STEM, as well as generating the associated dimensional analysis. The\nMask R-CNN model was tested on simulated STEM-HAADF results with different\nGaussian noises, particle shapes and particle sizes, and the results indicated\nthat Gaussian noise has determining influence on the accuracy of recognition.\nBy applying Gaussian and Non-Local Means filters on the noise-containing\nSTEM-HAADF results, the influences of noises are largely mitigated, and\nrecognition accuracy is significantly improved. This filtering-recognition\napproach was further applied to experimental STEM-HAADF results, which yields\nsatisfying accuracy compared with the traditional threshold methods. The\ndeep-learning-based method developed in this work has great potentials in\nanalysis of the complicated structures and large data generated by STEM-HAADF.\n","authors":["Hanlei Zhang","Jincheng Bai","Xiabo Chen","Can Li","Chuanjian Zhong","Jiye Fang","Guangwen Zhou"],"pdf_url":"https://arxiv.org/pdf/2409.16637v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16631v1","updated":"2024-09-25T05:19:35Z","published":"2024-09-25T05:19:35Z","title":"Enhancing Nighttime UAV Tracking with Light Distribution Suppression","summary":"  Visual object tracking has boosted extensive intelligent applications for\nunmanned aerial vehicles (UAVs). However, the state-of-the-art (SOTA) enhancers\nfor nighttime UAV tracking always neglect the uneven light distribution in\nlow-light images, inevitably leading to excessive enhancement in scenarios with\ncomplex illumination. To address these issues, this work proposes a novel\nenhancer, i.e., LDEnhancer, enhancing nighttime UAV tracking with light\ndistribution suppression. Specifically, a novel image content refinement module\nis developed to decompose the light distribution information and image content\ninformation in the feature space, allowing for the targeted enhancement of the\nimage content information. Then this work designs a new light distribution\ngeneration module to capture light distribution effectively. The features with\nlight distribution information and image content information are fed into the\ndifferent parameter estimation modules, respectively, for the parameter map\nprediction. Finally, leveraging two parameter maps, an innovative interweave\niteration adjustment is proposed for the collaborative pixel-wise adjustment of\nlow-light images. Additionally, a challenging nighttime UAV tracking dataset\nwith uneven light distribution, namely NAT2024-2, is constructed to provide a\ncomprehensive evaluation, which contains 40 challenging sequences with over 74K\nframes in total. Experimental results on the authoritative UAV benchmarks and\nthe proposed NAT2024-2 demonstrate that LDEnhancer outperforms other SOTA\nlow-light enhancers for nighttime UAV tracking. Furthermore, real-world tests\non a typical UAV platform with an NVIDIA Orin NX confirm the practicality and\nefficiency of LDEnhancer. The code is available at\nhttps://github.com/vision4robotics/LDEnhancer.\n","authors":["Liangliang Yao","Changhong Fu","Yiheng Wang","Haobo Zuo","Kunhan Lu"],"pdf_url":"https://arxiv.org/pdf/2409.16631v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16630v1","updated":"2024-09-25T05:18:17Z","published":"2024-09-25T05:18:17Z","title":"Stochastic Subsampling With Average Pooling","summary":"  Regularization of deep neural networks has been an important issue to achieve\nhigher generalization performance without overfitting problems. Although the\npopular method of Dropout provides a regularization effect, it causes\ninconsistent properties in the output, which may degrade the performance of\ndeep neural networks. In this study, we propose a new module called stochastic\naverage pooling, which incorporates Dropout-like stochasticity in pooling. We\ndescribe the properties of stochastic subsampling and average pooling and\nleverage them to design a module without any inconsistency problem. The\nstochastic average pooling achieves a regularization effect without any\npotential performance degradation due to the inconsistency issue and can easily\nbe plugged into existing architectures of deep neural networks. Experiments\ndemonstrate that replacing existing average pooling with stochastic average\npooling yields consistent improvements across a variety of tasks, datasets, and\nmodels.\n","authors":["Bum Jun Kim","Sang Woo Kim"],"pdf_url":"https://arxiv.org/pdf/2409.16630v1.pdf","comment":"17 pages, 8 figures"},{"id":"http://arxiv.org/abs/2409.16615v1","updated":"2024-09-25T04:43:59Z","published":"2024-09-25T04:43:59Z","title":"DeformStream: Deformation-based Adaptive Volumetric Video Streaming","summary":"  Volumetric video streaming offers immersive 3D experiences but faces\nsignificant challenges due to high bandwidth requirements and latency issues in\ntransmitting detailed content in real time. Traditional methods like point\ncloud streaming compromise visual quality when zoomed in, and neural rendering\ntechniques are too computationally intensive for real-time use. Though\nmesh-based streaming stands out by preserving surface detail and connectivity,\noffering a more refined representation for 3D content, traditional mesh\nstreaming methods typically transmit data on a per-frame basis, failing to take\nfull advantage of temporal redundancies across frames. This results in\ninefficient bandwidth usage and poor adaptability to fluctuating network\nconditions. We introduce Deformation-based Adaptive Volumetric Video Streaming,\na novel framework that enhances volumetric video streaming performance by\nleveraging the inherent deformability of mesh-based representations.\nDeformStream uses embedded deformation to reconstruct subsequent frames from\ninter-frame motion, significantly reducing bandwidth usage while ensuring\nvisual coherence between frames. To address frame reconstruction overhead and\nnetwork adaptability, we formulate a new QoE model that accounts for\nclient-side deformation latency and design a dynamic programming algorithm to\noptimize the trade-off between visual quality and bandwidth consumption under\nvarying network conditions. Our evaluation demonstrates that Deformation-based\nAdaptive Volumetric Video Streaming outperforms existing mesh-based streaming\nsystems in both bandwidth efficiency and visual quality, offering a robust\nsolution for real-time volumetric video applications.\n","authors":["Boyan Li","Yongting Chen","Dayou Zhang","Fangxin Wang"],"pdf_url":"https://arxiv.org/pdf/2409.16615v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04511v2","updated":"2024-09-25T04:36:14Z","published":"2024-06-06T21:08:07Z","title":"Classification of Non-native Handwritten Characters Using Convolutional\n  Neural Network","summary":"  The use of convolutional neural networks (CNNs) has accelerated the progress\nof handwritten character classification/recognition. Handwritten character\nrecognition (HCR) has found applications in various domains, such as traffic\nsignal detection, language translation, and document information extraction.\nHowever, the widespread use of existing HCR technology is yet to be seen as it\ndoes not provide reliable character recognition with outstanding accuracy. One\nof the reasons for unreliable HCR is that existing HCR methods do not take the\nhandwriting styles of non-native writers into account. Hence, further\nimprovement is needed to ensure the reliability and extensive deployment of\ncharacter recognition technologies for critical tasks. In this work, the\nclassification of English characters written by non-native users is performed\nby proposing a custom-tailored CNN model. We train this CNN with a new dataset\ncalled the handwritten isolated English character (HIEC) dataset. This dataset\nconsists of 16,496 images collected from 260 persons. This paper also includes\nan ablation study of our CNN by adjusting hyperparameters to identify the best\nmodel for the HIEC dataset. The proposed model with five convolutional layers\nand one hidden layer outperforms state-of-the-art models in terms of character\nrecognition accuracy and achieves an accuracy of $\\mathbf{97.04}$%. Compared\nwith the second-best model, the relative improvement of our model in terms of\nclassification accuracy is $\\mathbf{4.38}$%.\n","authors":["F. A. Mamun","S. A. H. Chowdhury","J. E. Giti","H. Sarker"],"pdf_url":"https://arxiv.org/pdf/2406.04511v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16604v1","updated":"2024-09-25T04:05:32Z","published":"2024-09-25T04:05:32Z","title":"Semi-LLIE: Semi-supervised Contrastive Learning with Mamba-based\n  Low-light Image Enhancement","summary":"  Despite the impressive advancements made in recent low-light image\nenhancement techniques, the scarcity of paired data has emerged as a\nsignificant obstacle to further advancements. This work proposes a\nmean-teacher-based semi-supervised low-light enhancement (Semi-LLIE) framework\nthat integrates the unpaired data into model training. The mean-teacher\ntechnique is a prominent semi-supervised learning method, successfully adopted\nfor addressing high-level and low-level vision tasks. However, two primary\nissues hinder the naive mean-teacher method from attaining optimal performance\nin low-light image enhancement. Firstly, pixel-wise consistency loss is\ninsufficient for transferring realistic illumination distribution from the\nteacher to the student model, which results in color cast in the enhanced\nimages. Secondly, cutting-edge image enhancement approaches fail to effectively\ncooperate with the mean-teacher framework to restore detailed information in\ndark areas due to their tendency to overlook modeling structured information\nwithin local regions. To mitigate the above issues, we first introduce a\nsemantic-aware contrastive loss to faithfully transfer the illumination\ndistribution, contributing to enhancing images with natural colors. Then, we\ndesign a Mamba-based low-light image enhancement backbone to effectively\nenhance Mamba's local region pixel relationship representation ability with a\nmulti-scale feature learning scheme, facilitating the generation of images with\nrich textural details. Further, we propose novel perceptive loss based on the\nlarge-scale vision-language Recognize Anything Model (RAM) to help generate\nenhanced images with richer textual details. The experimental results indicate\nthat our Semi-LLIE surpasses existing methods in both quantitative and\nqualitative metrics.\n","authors":["Guanlin Li","Ke Zhang","Ting Wang","Ming Li","Bin Zhao","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2409.16604v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17278v2","updated":"2024-09-25T03:59:55Z","published":"2024-05-27T15:40:24Z","title":"EF-Calib: Spatiotemporal Calibration of Event- and Frame-Based Cameras\n  Using Continuous-Time Trajectories","summary":"  Event camera, a bio-inspired asynchronous triggered camera, offers promising\nprospects for fusion with frame-based cameras owing to its low latency and high\ndynamic range. However, calibrating stereo vision systems that incorporate both\nevent and frame-based cameras remains a significant challenge. In this letter,\nwe present EF-Calib, a spatiotemporal calibration framework for event- and\nframe-based cameras using continuous-time trajectories. A novel calibration\npattern applicable to both camera types and the corresponding event recognition\nalgorithm is proposed. Leveraging the asynchronous nature of events, a\nderivable piece-wise B-spline to represent camera pose continuously is\nintroduced, enabling calibration for intrinsic parameters, extrinsic\nparameters, and time offset, with analytical Jacobians provided. Various\nexperiments are carried out to evaluate the calibration performance of\nEF-Calib, including calibration experiments for intrinsic parameters, extrinsic\nparameters, and time offset. Experimental results show that EF-Calib achieves\nthe most accurate intrinsic parameters compared to current SOTA, the close\naccuracy of the extrinsic parameters compared to the frame-based results, and\naccurate time offset estimation. EF-Calib provides a convenient and accurate\ntoolbox for calibrating the system that fuses events and frames. The code of\nthis paper will also be open-sourced at: https://github.com/wsakobe/EF-Calib.\n","authors":["Shaoan Wang","Zhanhua Xin","Yaoqing Hu","Dongyue Li","Mingzhu Zhu","Junzhi Yu"],"pdf_url":"https://arxiv.org/pdf/2405.17278v2.pdf","comment":"Accepted by IEEE Robotics and Automation Letters"},{"id":"http://arxiv.org/abs/2409.16600v1","updated":"2024-09-25T03:54:01Z","published":"2024-09-25T03:54:01Z","title":"FAFA: Frequency-Aware Flow-Aided Self-Supervision for Underwater Object\n  Pose Estimation","summary":"  Although methods for estimating the pose of objects in indoor scenes have\nachieved great success, the pose estimation of underwater objects remains\nchallenging due to difficulties brought by the complex underwater environment,\nsuch as degraded illumination, blurring, and the substantial cost of obtaining\nreal annotations. In response, we introduce FAFA, a Frequency-Aware Flow-Aided\nself-supervised framework for 6D pose estimation of unmanned underwater\nvehicles (UUVs). Essentially, we first train a frequency-aware flow-based pose\nestimator on synthetic data, where an FFT-based augmentation approach is\nproposed to facilitate the network in capturing domain-invariant features and\ntarget domain styles from a frequency perspective. Further, we perform\nself-supervised training by enforcing flow-aided multi-level consistencies to\nadapt it to the real-world underwater environment. Our framework relies solely\non the 3D model and RGB images, alleviating the need for any real pose\nannotations or other-modality data like depths. We evaluate the effectiveness\nof FAFA on common underwater object pose benchmarks and showcase significant\nperformance improvements compared to state-of-the-art methods. Code is\navailable at github.com/tjy0703/FAFA.\n","authors":["Jingyi Tang","Gu Wang","Zeyu Chen","Shengquan Li","Xiu Li","Xiangyang Ji"],"pdf_url":"https://arxiv.org/pdf/2409.16600v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2409.07267v3","updated":"2024-09-25T03:53:39Z","published":"2024-09-11T13:43:01Z","title":"MiniDrive: More Efficient Vision-Language Models with Multi-Level 2D\n  Features as Text Tokens for Autonomous Driving","summary":"  Vision-language models (VLMs) serve as general-purpose end-to-end models in\nautonomous driving, performing subtasks such as prediction, planning, and\nperception through question-and-answer interactions. However, most existing\nmethods rely on computationally expensive visual encoders and large language\nmodels (LLMs), making them difficult to deploy in real-world scenarios and\nreal-time applications. Meanwhile, most existing VLMs lack the ability to\nprocess multiple images, making it difficult to adapt to multi-camera\nperception in autonomous driving. To address these issues, we propose a novel\nframework called MiniDrive, which incorporates our proposed Feature Engineering\nMixture of Experts (FE-MoE) module and Dynamic Instruction Adapter\n(DI-Adapter). The FE-MoE effectively maps 2D features into visual token\nembeddings before being input into the language model. The DI-Adapter enables\nthe visual token embeddings to dynamically change with the instruction text\nembeddings, resolving the issue of static visual token embeddings for the same\nimage in previous approaches. Compared to previous works, MiniDrive achieves\nstate-of-the-art performance in terms of parameter size, floating point\noperations, and response efficiency, with the smallest version containing only\n83M parameters.\n","authors":["Enming Zhang","Xingyuan Dai","Yisheng Lv","Qinghai Miao"],"pdf_url":"https://arxiv.org/pdf/2409.07267v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.11480v2","updated":"2024-09-25T03:50:28Z","published":"2024-08-21T09:47:54Z","title":"OAPT: Offset-Aware Partition Transformer for Double JPEG Artifacts\n  Removal","summary":"  Deep learning-based methods have shown remarkable performance in single JPEG\nartifacts removal task. However, existing methods tend to degrade on double\nJPEG images, which are prevalent in real-world scenarios. To address this\nissue, we propose Offset-Aware Partition Transformer for double JPEG artifacts\nremoval, termed as OAPT. We conduct an analysis of double JPEG compression that\nresults in up to four patterns within each 8x8 block and design our model to\ncluster the similar patterns to remedy the difficulty of restoration. Our OAPT\nconsists of two components: compression offset predictor and image\nreconstructor. Specifically, the predictor estimates pixel offsets between the\nfirst and second compression, which are then utilized to divide different\npatterns. The reconstructor is mainly based on several Hybrid Partition\nAttention Blocks (HPAB), combining vanilla window-based self-attention and\nsparse attention for clustered pattern features. Extensive experiments\ndemonstrate that OAPT outperforms the state-of-the-art method by more than\n0.16dB in double JPEG image restoration task. Moreover, without increasing any\ncomputation cost, the pattern clustering module in HPAB can serve as a plugin\nto enhance other transformer-based image restoration methods. The code will be\navailable at https://github.com/QMoQ/OAPT.git .\n","authors":["Qiao Mo","Yukang Ding","Jinhua Hao","Qiang Zhu","Ming Sun","Chao Zhou","Feiyu Chen","Shuyuan Zhu"],"pdf_url":"https://arxiv.org/pdf/2408.11480v2.pdf","comment":"14 pages, 9 figures. Codes and models are available at\n  https://github.com/QMoQ/OAPT.git"},{"id":"http://arxiv.org/abs/2409.16597v1","updated":"2024-09-25T03:49:46Z","published":"2024-09-25T03:49:46Z","title":"EventHallusion: Diagnosing Event Hallucinations in Video LLMs","summary":"  Recently, Multimodal Large Language Models (MLLMs) have made significant\nprogress in the video comprehension field. Despite remarkable content reasoning\nand instruction following capabilities they demonstrated, the hallucination\nproblem of these VideoLLMs is less explored compared with its counterpart in\nthe image domain. To mitigate this gap, we first propose EventHallusion, a\nnovel benchmark that focuses on assessing the VideoLMMs' hallucination\nphenomenon on video event comprehension. Based on the observation that existing\nVideoLLMs are entangled with the priors stemming from their foundation models,\nour EventHallusion is curated by meticulously collecting videos and annotating\nquestions to intentionally mislead the VideoLLMs into interpreting events based\non these priors rather than accurately understanding the video content. On the\nother hand, we also propose a simple yet effective method, called Temporal\nContrastive Decoding (TCD), to tackle the hallucination problems of VideoLLMs.\nThe proposed TCD suppresses the model's preference toward their priors by\ncomparing the original video with a constructed counterpart, whose temporal\ncues are disrupted, during the autoregressive decoding stage. Through\ncomprehensive evaluation of eight open-source and two closed-source VideoLLMs\non the proposed EventHallusion benchmark, we find that the open-source models\nsuffer significantly from hallucination problems, whereas the closed-source\nmodels perform markedly better. By further equipping open-sourced VideoLLMs\nwith the proposed TCD approach, evident performance improvements are achieved\nacross most metrics in the EventHallusion benchmark. Our codes and benchmark\ndata are available at https://github.com/Stevetich/EventHallusion.\n","authors":["Jiacheng Zhang","Yang Jiao","Shaoxiang Chen","Jingjing Chen","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2409.16597v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04102v2","updated":"2024-09-25T03:42:09Z","published":"2024-08-07T21:44:29Z","title":"ArtVLM: Attribute Recognition Through Vision-Based Prefix Language\n  Modeling","summary":"  Recognizing and disentangling visual attributes from objects is a foundation\nto many computer vision applications. While large vision language\nrepresentations like CLIP had largely resolved the task of zero-shot object\nrecognition, zero-shot visual attribute recognition remains a challenge because\nCLIP's contrastively-learned vision-language representation cannot effectively\ncapture object-attribute dependencies. In this paper, we target this weakness\nand propose a sentence generation-based retrieval formulation for attribute\nrecognition that is novel in 1) explicitly modeling a to-be-measured and\nretrieved object-attribute relation as a conditional probability graph, which\nconverts the recognition problem into a dependency-sensitive language-modeling\nproblem, and 2) applying a large pretrained Vision-Language Model (VLM) on this\nreformulation and naturally distilling its knowledge of image-object-attribute\nrelations to use towards attribute recognition. Specifically, for each\nattribute to be recognized on an image, we measure the visual-conditioned\nprobability of generating a short sentence encoding the attribute's relation to\nobjects on the image. Unlike contrastive retrieval, which measures likelihood\nby globally aligning elements of the sentence to the image, generative\nretrieval is sensitive to the order and dependency of objects and attributes in\nthe sentence. We demonstrate through experiments that generative retrieval\nconsistently outperforms contrastive retrieval on two visual reasoning\ndatasets, Visual Attribute in the Wild (VAW), and our newly-proposed Visual\nGenome Attribute Ranking (VGARank).\n","authors":["William Yicheng Zhu","Keren Ye","Junjie Ke","Jiahui Yu","Leonidas Guibas","Peyman Milanfar","Feng Yang"],"pdf_url":"https://arxiv.org/pdf/2408.04102v2.pdf","comment":"Accepted at ECCV 2024"},{"id":"http://arxiv.org/abs/2409.08695v3","updated":"2024-09-25T03:34:45Z","published":"2024-09-13T10:27:27Z","title":"Precision Aquaculture: An Integrated Computer Vision and IoT Approach\n  for Optimized Tilapia Feeding","summary":"  Traditional fish farming practices often lead to inefficient feeding,\nresulting in environmental issues and reduced productivity. We developed an\ninnovative system combining computer vision and IoT technologies for precise\nTilapia feeding. Our solution uses real-time IoT sensors to monitor water\nquality parameters and computer vision algorithms to analyze fish size and\ncount, determining optimal feed amounts. A mobile app enables remote monitoring\nand control. We utilized YOLOv8 for keypoint detection to measure Tilapia\nweight from length, achieving \\textbf{94\\%} precision on 3,500 annotated\nimages. Pixel-based measurements were converted to centimeters using depth\nestimation for accurate feeding calculations. Our method, with data collection\nmirroring inference conditions, significantly improved results. Preliminary\nestimates suggest this approach could increase production up to 58 times\ncompared to traditional farms. Our models, code, and dataset are\nopen-source~\\footnote{The code, dataset, and models are available upon\nreasonable request.\n","authors":["Rania Hossam","Ahmed Heakl","Walid Gomaa"],"pdf_url":"https://arxiv.org/pdf/2409.08695v3.pdf","comment":"8 pages, 6 figures, 3 tables, 21th International Conference on\n  Informatics in Control, Automation, and Robotics"},{"id":"http://arxiv.org/abs/2409.16581v1","updated":"2024-09-25T03:19:29Z","published":"2024-09-25T03:19:29Z","title":"SelectiveKD: A semi-supervised framework for cancer detection in DBT\n  through Knowledge Distillation and Pseudo-labeling","summary":"  When developing Computer Aided Detection (CAD) systems for Digital Breast\nTomosynthesis (DBT), the complexity arising from the volumetric nature of the\nmodality poses significant technical challenges for obtaining large-scale\naccurate annotations. Without access to large-scale annotations, the resulting\nmodel may not generalize to different domains. Given the costly nature of\nobtaining DBT annotations, how to effectively increase the amount of data used\nfor training DBT CAD systems remains an open challenge.\n  In this paper, we present SelectiveKD, a semi-supervised learning framework\nfor building cancer detection models for DBT, which only requires a limited\nnumber of annotated slices to reach high performance. We achieve this by\nutilizing unlabeled slices available in a DBT stack through a knowledge\ndistillation framework in which the teacher model provides a supervisory signal\nto the student model for all slices in the DBT volume. Our framework mitigates\nthe potential noise in the supervisory signal from a sub-optimal teacher by\nimplementing a selective dataset expansion strategy using pseudo labels.\n  We evaluate our approach with a large-scale real-world dataset of over 10,000\nDBT exams collected from multiple device manufacturers and locations. The\nresulting SelectiveKD process effectively utilizes unannotated slices from a\nDBT stack, leading to significantly improved cancer classification performance\n(AUC) and generalization performance.\n","authors":["Laurent Dillard","Hyeonsoo Lee","Weonsuk Lee","Tae Soo Kim","Ali Diba","Thijs Kooi"],"pdf_url":"https://arxiv.org/pdf/2409.16581v1.pdf","comment":"10 pages, 2 figures, 1 table"},{"id":"http://arxiv.org/abs/2407.12346v2","updated":"2024-09-25T03:18:14Z","published":"2024-07-17T06:42:14Z","title":"Object-Aware Query Perturbation for Cross-Modal Image-Text Retrieval","summary":"  The pre-trained vision and language (V\\&L) models have substantially improved\nthe performance of cross-modal image-text retrieval. In general, however, V\\&L\nmodels have limited retrieval performance for small objects because of the\nrough alignment between words and the small objects in the image. In contrast,\nit is known that human cognition is object-centric, and we pay more attention\nto important objects, even if they are small. To bridge this gap between the\nhuman cognition and the V\\&L model's capability, we propose a cross-modal\nimage-text retrieval framework based on ``object-aware query perturbation.''\nThe proposed method generates a key feature subspace of the detected objects\nand perturbs the corresponding queries using this subspace to improve the\nobject awareness in the image. In our proposed method, object-aware cross-modal\nimage-text retrieval is possible while keeping the rich expressive power and\nretrieval performance of existing V\\&L models without additional fine-tuning.\nComprehensive experiments on four public datasets show that our method\noutperforms conventional algorithms. Our code is publicly available at\n\\url{https://github.com/NEC-N-SOGI/query-perturbation}.\n","authors":["Naoya Sogi","Takashi Shibata","Makoto Terao"],"pdf_url":"https://arxiv.org/pdf/2407.12346v2.pdf","comment":"ECCV 2024. Code: https://github.com/NEC-N-SOGI/query-perturbation"},{"id":"http://arxiv.org/abs/2409.16578v1","updated":"2024-09-25T03:15:17Z","published":"2024-09-25T03:15:17Z","title":"FLaRe: Achieving Masterful and Adaptive Robot Policies with Large-Scale\n  Reinforcement Learning Fine-Tuning","summary":"  In recent years, the Robotics field has initiated several efforts toward\nbuilding generalist robot policies through large-scale multi-task Behavior\nCloning. However, direct deployments of these policies have led to\nunsatisfactory performance, where the policy struggles with unseen states and\ntasks. How can we break through the performance plateau of these models and\nelevate their capabilities to new heights? In this paper, we propose FLaRe, a\nlarge-scale Reinforcement Learning fine-tuning framework that integrates robust\npre-trained representations, large-scale training, and gradient stabilization\ntechniques. Our method aligns pre-trained policies towards task completion,\nachieving state-of-the-art (SoTA) performance both on previously demonstrated\nand on entirely novel tasks and embodiments. Specifically, on a set of\nlong-horizon mobile manipulation tasks, FLaRe achieves an average success rate\nof 79.5% in unseen environments, with absolute improvements of +23.6% in\nsimulation and +30.7% on real robots over prior SoTA methods. By utilizing only\nsparse rewards, our approach can enable generalizing to new capabilities beyond\nthe pretraining data with minimal human effort. Moreover, we demonstrate rapid\nadaptation to new embodiments and behaviors with less than a day of\nfine-tuning. Videos can be found on the project website at\nhttps://robot-flare.github.io/\n","authors":["Jiaheng Hu","Rose Hendrix","Ali Farhadi","Aniruddha Kembhavi","Roberto Martin-Martin","Peter Stone","Kuo-Hao Zeng","Kiana Ehsan"],"pdf_url":"https://arxiv.org/pdf/2409.16578v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00877v2","updated":"2024-09-25T03:13:27Z","published":"2023-12-30T10:22:59Z","title":"Improving the Stability and Efficiency of Diffusion Models for Content\n  Consistent Super-Resolution","summary":"  The generative priors of pre-trained latent diffusion models (DMs) have\ndemonstrated great potential to enhance the visual quality of image\nsuper-resolution (SR) results. However, the noise sampling process in DMs\nintroduces randomness in the SR outputs, and the generated contents can differ\na lot with different noise samples. The multi-step diffusion process can be\naccelerated by distilling methods, but the generative capacity is difficult to\ncontrol. To address these issues, we analyze the respective advantages of DMs\nand generative adversarial networks (GANs) and propose to partition the\ngenerative SR process into two stages, where the DM is employed for\nreconstructing image structures and the GAN is employed for improving\nfine-grained details. Specifically, we propose a non-uniform timestep sampling\nstrategy in the first stage. A single timestep sampling is first applied to\nextract the coarse information from the input image, then a few reverse steps\nare used to reconstruct the main structures. In the second stage, we finetune\nthe decoder of the pre-trained variational auto-encoder by adversarial GAN\ntraining for deterministic detail enhancement. Once trained, our proposed\nmethod, namely content consistent super-resolution (CCSR),allows flexible use\nof different diffusion steps in the inference stage without re-training.\nExtensive experiments show that with 2 or even 1 diffusion step, CCSR can\nsignificantly improve the content consistency of SR outputs while keeping high\nperceptual quality. Codes and models can be found at\n\\href{https://github.com/csslc/CCSR}{https://github.com/csslc/CCSR}.\n","authors":["Lingchen Sun","Rongyuan Wu","Jie Liang","Zhengqiang Zhang","Hongwei Yong","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.00877v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15991v2","updated":"2024-09-25T03:05:05Z","published":"2024-08-28T17:58:17Z","title":"Distribution Backtracking Builds A Faster Convergence Trajectory for\n  Diffusion Distillation","summary":"  Accelerating the sampling speed of diffusion models remains a significant\nchallenge. Recent score distillation methods distill a heavy teacher model into\na student generator to achieve one-step generation, which is optimized by\ncalculating the difference between the two score functions on the samples\ngenerated by the student model. However, there is a score mismatch issue in the\nearly stage of the distillation process, because existing methods mainly focus\non using the endpoint of pre-trained diffusion models as teacher models,\noverlooking the importance of the convergence trajectory between the student\ngenerator and the teacher model. To address this issue, we extend the score\ndistillation process by introducing the entire convergence trajectory of\nteacher models and propose Distribution Backtracking Distillation (DisBack).\nDisBask is composed of two stages: Degradation Recording and Distribution\nBacktracking. Degradation Recording is designed to obtain the convergence\ntrajectory of the teacher model, which records the degradation path from the\ntrained teacher model to the untrained initial student generator. The\ndegradation path implicitly represents the teacher model's intermediate\ndistributions, and its reverse can be viewed as the convergence trajectory from\nthe student generator to the teacher model. Then Distribution Backtracking\ntrains a student generator to backtrack the intermediate distributions along\nthe path to approximate the convergence trajectory of teacher models. Extensive\nexperiments show that DisBack achieves faster and better convergence than the\nexisting distillation method and accomplishes comparable generation\nperformance, with FID score of 1.38 on ImageNet 64x64 dataset. Notably, DisBack\nis easy to implement and can be generalized to existing distillation methods to\nboost performance. Our code is publicly available on\nhttps://github.com/SYZhang0805/DisBack.\n","authors":["Shengyuan Zhang","Ling Yang","Zejian Li","An Zhao","Chenye Meng","Changyuan Yang","Guang Yang","Zhiyuan Yang","Lingyun Sun"],"pdf_url":"https://arxiv.org/pdf/2408.15991v2.pdf","comment":"https://github.com/SYZhang0805/DisBack"},{"id":"http://arxiv.org/abs/2408.00591v3","updated":"2024-09-25T02:50:45Z","published":"2024-08-01T14:20:47Z","title":"Regional quality estimation for echocardiography using deep learning","summary":"  Automatic estimation of cardiac ultrasound image quality can be beneficial\nfor guiding operators and ensuring the accuracy of clinical measurements.\nPrevious work often fails to distinguish the view correctness of the\nechocardiogram from the image quality. Additionally, previous studies only\nprovide a global image quality value, which limits their practical utility. In\nthis work, we developed and compared three methods to estimate image quality:\n1) classic pixel-based metrics like the generalized contrast-to-noise ratio\n(gCNR) on myocardial segments as region of interest and left ventricle lumen as\nbackground, obtained using a U-Net segmentation 2) local image coherence\nderived from a U-Net model that predicts coherence from B-Mode images 3) a deep\nconvolutional network that predicts the quality of each region directly in an\nend-to-end fashion. We evaluate each method against manual regional image\nquality annotations by three experienced cardiologists. The results indicate\npoor performance of the gCNR metric, with Spearman correlation to the\nannotations of rho = 0.24. The end-to-end learning model obtains the best\nresult, rho = 0.69, comparable to the inter-observer correlation, rho = 0.63.\nFinally, the coherence-based method, with rho = 0.58, outperformed the\nclassical metrics and is more generic than the end-to-end approach.\n","authors":["Gilles Van De Vyver","Svein-Erik Måsøy","Håvard Dalen","Bjørnar Leangen Grenne","Espen Holte","Sindre Hellum Olaisen","John Nyberg","Andreas Østvik","Lasse Løvstakken","Erik Smistad"],"pdf_url":"https://arxiv.org/pdf/2408.00591v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15898v2","updated":"2024-09-25T02:48:53Z","published":"2024-09-24T09:17:08Z","title":"FedRepOpt: Gradient Re-parameterized Optimizers in Federated Learning","summary":"  Federated Learning (FL) has emerged as a privacy-preserving method for\ntraining machine learning models in a distributed manner on edge devices.\nHowever, on-device models face inherent computational power and memory\nlimitations, potentially resulting in constrained gradient updates. As the\nmodel's size increases, the frequency of gradient updates on edge devices\ndecreases, ultimately leading to suboptimal training outcomes during any\nparticular FL round. This limits the feasibility of deploying advanced and\nlarge-scale models on edge devices, hindering the potential for performance\nenhancements. To address this issue, we propose FedRepOpt, a gradient\nre-parameterized optimizer for FL. The gradient re-parameterized method allows\ntraining a simple local model with a similar performance as a complex model by\nmodifying the optimizer's gradients according to a set of model-specific\nhyperparameters obtained from the complex models. In this work, we focus on\nVGG-style and Ghost-style models in the FL environment. Extensive experiments\ndemonstrate that models using FedRepOpt obtain a significant boost in\nperformance of 16.7% and 11.4% compared to the RepGhost-style and RepVGG-style\nnetworks, while also demonstrating a faster convergence time of 11.7% and 57.4%\ncompared to their complex structure.\n","authors":["Kin Wai Lau","Yasar Abbas Ur Rehman","Pedro Porto Buarque de Gusmão","Lai-Man Po","Lan Ma","Yuyang Xie"],"pdf_url":"https://arxiv.org/pdf/2409.15898v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04918v3","updated":"2024-09-25T02:28:08Z","published":"2024-09-07T21:52:58Z","title":"Training-free Zero-shot Composed Image Retrieval via Weighted Modality\n  Fusion and Similarity","summary":"  Composed image retrieval (CIR), which formulates the query as a combination\nof a reference image and modified text, has emerged as a new form of image\nsearch due to its enhanced ability to capture users' intentions. However,\ntraining a CIR model in a supervised manner typically requires labor-intensive\ncollection of (reference image, text modifier, target image) triplets. While\nexisting zero-shot CIR (ZS-CIR) methods eliminate the need for training on\nspecific downstream datasets, they still require additional pretraining on\nlarge-scale image datasets. In this paper, we introduce a training-free\napproach for ZS-CIR. Our approach, Weighted Modality fusion and similarity for\nCIR (WeiMoCIR), operates under the assumption that image and text modalities\ncan be effectively combined using a simple weighted average. This allows the\nquery representation to be constructed directly from the reference image and\ntext modifier. To further enhance retrieval performance, we employ multimodal\nlarge language models (MLLMs) to generate image captions for the database\nimages and incorporate these textual captions into the similarity computation\nby combining them with image information using a weighted average. Our approach\nis simple, easy to implement, and its effectiveness is validated through\nexperiments on the FashionIQ and CIRR datasets.\n","authors":["Ren-Di Wu","Yu-Yen Lin","Huei-Fang Yang"],"pdf_url":"https://arxiv.org/pdf/2409.04918v3.pdf","comment":"13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2409.12953v3","updated":"2024-09-25T01:46:10Z","published":"2024-09-19T17:58:16Z","title":"JourneyBench: A Challenging One-Stop Vision-Language Understanding\n  Benchmark of Generated Images","summary":"  Existing vision-language understanding benchmarks largely consist of images\nof objects in their usual contexts. As a consequence, recent multimodal large\nlanguage models can perform well with only a shallow visual understanding by\nrelying on background language biases. Thus, strong performance on these\nbenchmarks does not necessarily correlate with strong visual understanding. In\nthis paper, we release JourneyBench, a comprehensive human-annotated benchmark\nof generated images designed to assess the model's fine-grained multimodal\nreasoning abilities across five tasks: complementary multimodal chain of\nthought, multi-image VQA, imaginary image captioning, VQA with hallucination\ntriggers, and fine-grained retrieval with sample-specific distractors. Unlike\nexisting benchmarks, JourneyBench explicitly requires fine-grained multimodal\nreasoning in unusual imaginary scenarios where language bias and holistic image\ngist are insufficient. We benchmark state-of-the-art models on JourneyBench and\nanalyze performance along a number of fine-grained dimensions. Results across\nall five tasks show that JourneyBench is exceptionally challenging for even the\nbest models, indicating that models' visual reasoning abilities are not as\nstrong as they first appear. We discuss the implications of our findings and\npropose avenues for further research.\n","authors":["Zhecan Wang","Junzhang Liu","Chia-Wei Tang","Hani Alomari","Anushka Sivakumar","Rui Sun","Wenhao Li","Md. Atabuzzaman","Hammad Ayyubi","Haoxuan You","Alvi Ishmam","Kai-Wei Chang","Shih-Fu Chang","Chris Thomas"],"pdf_url":"https://arxiv.org/pdf/2409.12953v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.13827v2","updated":"2024-09-25T01:29:37Z","published":"2024-02-21T14:16:49Z","title":"Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering\n  of 3D Gaussian Splatting","summary":"  3D Gaussian splatting (3D-GS) is a new rendering approach that outperforms\nthe neural radiance field (NeRF) in terms of both speed and image quality.\n3D-GS represents 3D scenes by utilizing millions of 3D Gaussians and projects\nthese Gaussians onto the 2D image plane for rendering. However, during the\nrendering process, a substantial number of unnecessary 3D Gaussians exist for\nthe current view direction, resulting in significant computation costs\nassociated with their identification. In this paper, we propose a computational\nreduction technique that quickly identifies unnecessary 3D Gaussians in\nreal-time for rendering the current view without compromising image quality.\nThis is accomplished through the offline clustering of 3D Gaussians that are\nclose in distance, followed by the projection of these clusters onto a 2D image\nplane during runtime. Additionally, we analyze the bottleneck associated with\nthe proposed technique when executed on GPUs and propose an efficient hardware\narchitecture that seamlessly supports the proposed scheme. For the Mip-NeRF360\ndataset, the proposed technique excludes 63% of 3D Gaussians on average before\nthe 2D image projection, which reduces the overall rendering computation by\nalmost 38.3% without sacrificing peak-signal-to-noise-ratio (PSNR). The\nproposed accelerator also achieves a speedup of 10.7x compared to a GPU.\n","authors":["Joongho Jo","Hyeongwon Kim","Jongsun Park"],"pdf_url":"https://arxiv.org/pdf/2402.13827v2.pdf","comment":"Our claim that Step 1 of 3D Gaussian splatting accounts for ~50% of\n  rendering (Fig. 2) was incorrect. Rerunning simulations showed it's only\n  ~20%. Consequently, our method's performance decreased by ~40% from initial\n  reports. We're exploring new directions but have no concrete plans yet. To\n  avoid reader confusion, we're withdrawing the paper and will resubmit once\n  revised"},{"id":"http://arxiv.org/abs/2409.16538v1","updated":"2024-09-25T01:22:10Z","published":"2024-09-25T01:22:10Z","title":"Source-Free Domain Adaptation for YOLO Object Detection","summary":"  Source-free domain adaptation (SFDA) is a challenging problem in object\ndetection, where a pre-trained source model is adapted to a new target domain\nwithout using any source domain data for privacy and efficiency reasons. Most\nstate-of-the-art SFDA methods for object detection have been proposed for\nFaster-RCNN, a detector that is known to have high computational complexity.\nThis paper focuses on domain adaptation techniques for real-world vision\nsystems, particularly for the YOLO family of single-shot detectors known for\ntheir fast baselines and practical applications. Our proposed SFDA method -\nSource-Free YOLO (SF-YOLO) - relies on a teacher-student framework in which the\nstudent receives images with a learned, target domain-specific augmentation,\nallowing the model to be trained with only unlabeled target data and without\nrequiring feature alignment. A challenge with self-training using a\nmean-teacher architecture in the absence of labels is the rapid decline of\naccuracy due to noisy or drifting pseudo-labels. To address this issue, a\nteacher-to-student communication mechanism is introduced to help stabilize the\ntraining and reduce the reliance on annotated target data for model selection.\nDespite its simplicity, our approach is competitive with state-of-the-art\ndetectors on several challenging benchmark datasets, even sometimes\noutperforming methods that use source data for adaptation.\n","authors":["Simon Varailhon","Masih Aminbeidokhti","Marco Pedersoli","Eric Granger"],"pdf_url":"https://arxiv.org/pdf/2409.16538v1.pdf","comment":"ECCV 2024: European Conference on Computer Vision - Workshop on\n  Out-of-Distribution Generalization in Computer Vision Foundation Models,\n  Milan Italy"},{"id":"http://arxiv.org/abs/2409.16535v1","updated":"2024-09-25T01:02:30Z","published":"2024-09-25T01:02:30Z","title":"Prompt Sliders for Fine-Grained Control, Editing and Erasing of Concepts\n  in Diffusion Models","summary":"  Diffusion models have recently surpassed GANs in image synthesis and editing,\noffering superior image quality and diversity. However, achieving precise\ncontrol over attributes in generated images remains a challenge. Concept\nSliders introduced a method for fine-grained image control and editing by\nlearning concepts (attributes/objects). However, this approach adds parameters\nand increases inference time due to the loading and unloading of Low-Rank\nAdapters (LoRAs) used for learning concepts. These adapters are model-specific\nand require retraining for different architectures, such as Stable Diffusion\n(SD) v1.5 and SD-XL. In this paper, we propose a straightforward textual\ninversion method to learn concepts through text embeddings, which are\ngeneralizable across models that share the same text encoder, including\ndifferent versions of the SD model. We refer to our method as Prompt Sliders.\nBesides learning new concepts, we also show that Prompt Sliders can be used to\nerase undesirable concepts such as artistic styles or mature content. Our\nmethod is 30% faster than using LoRAs because it eliminates the need to load\nand unload adapters and introduces no additional parameters aside from the\ntarget concept text embedding. Each concept embedding only requires 3KB of\nstorage compared to the 8922KB or more required for each LoRA adapter, making\nour approach more computationally efficient. Project Page:\nhttps://deepaksridhar.github.io/promptsliders.github.io/\n","authors":["Deepak Sridhar","Nuno Vasconcelos"],"pdf_url":"https://arxiv.org/pdf/2409.16535v1.pdf","comment":"ECCV'24 - Unlearning and Model Editing Workshop. Code:\n  https://github.com/DeepakSridhar/promptsliders"},{"id":"http://arxiv.org/abs/2211.05207v5","updated":"2024-09-25T00:35:55Z","published":"2022-11-09T21:33:40Z","title":"Improving Clinician Performance in Classification of EEG Patterns on the\n  Ictal-Interictal-Injury Continuum using Interpretable Machine Learning","summary":"  In intensive care units (ICUs), critically ill patients are monitored with\nelectroencephalograms (EEGs) to prevent serious brain injury. The number of\npatients who can be monitored is constrained by the availability of trained\nphysicians to read EEGs, and EEG interpretation can be subjective and prone to\ninter-observer variability. Automated deep learning systems for EEG could\nreduce human bias and accelerate the diagnostic process. However, black box\ndeep learning models are untrustworthy, difficult to troubleshoot, and lack\naccountability in real-world applications, leading to a lack of trust and\nadoption by clinicians. To address these challenges, we propose a novel\ninterpretable deep learning model that not only predicts the presence of\nharmful brainwave patterns but also provides high-quality case-based\nexplanations of its decisions. Our model performs better than the corresponding\nblack box model, despite being constrained to be interpretable. The learned 2D\nembedded space provides the first global overview of the structure of\nictal-interictal-injury continuum brainwave patterns. The ability to understand\nhow our model arrived at its decisions will not only help clinicians to\ndiagnose and treat harmful brain activities more accurately but also increase\ntheir trust and adoption of machine learning models in clinical practice; this\ncould be an integral component of the ICU neurologists' standard workflow.\n","authors":["Alina Jade Barnett","Zhicheng Guo","Jin Jing","Wendong Ge","Peter W. Kaplan","Wan Yee Kong","Ioannis Karakis","Aline Herlopian","Lakshman Arcot Jayagopal","Olga Taraschenko","Olga Selioutski","Gamaleldin Osman","Daniel Goldenholz","Cynthia Rudin","M. Brandon Westover"],"pdf_url":"https://arxiv.org/pdf/2211.05207v5.pdf","comment":"24 pages including appendices, 9 figures, published at NEJM AI"},{"id":"http://arxiv.org/abs/2405.16226v3","updated":"2024-09-25T00:09:58Z","published":"2024-05-25T13:34:16Z","title":"Detecting Adversarial Data via Perturbation Forgery","summary":"  As a defense strategy against adversarial attacks, adversarial detection aims\nto identify and filter out adversarial data from the data flow based on\ndiscrepancies in distribution and noise patterns between natural and\nadversarial data. Although previous detection methods achieve high performance\nin detecting gradient-based adversarial attacks, new attacks based on\ngenerative models with imbalanced and anisotropic noise patterns evade\ndetection. Even worse, existing techniques either necessitate access to attack\ndata before deploying a defense or incur a significant time cost for inference,\nrendering them impractical for defending against newly emerging attacks that\nare unseen by defenders. In this paper, we explore the proximity relationship\nbetween adversarial noise distributions and demonstrate the existence of an\nopen covering for them. By learning to distinguish this open covering from the\ndistribution of natural data, we can develop a detector with strong\ngeneralization capabilities against all types of adversarial attacks. Based on\nthis insight, we heuristically propose Perturbation Forgery, which includes\nnoise distribution perturbation, sparse mask generation, and pseudo-adversarial\ndata production, to train an adversarial detector capable of detecting unseen\ngradient-based, generative-model-based, and physical adversarial attacks, while\nremaining agnostic to any specific models. Comprehensive experiments conducted\non multiple general and facial datasets, with a wide spectrum of attacks,\nvalidate the strong generalization of our method.\n","authors":["Qian Wang","Chen Li","Yuchen Luo","Hefei Ling","Ping Li","Jiazhong Chen","Shijuan Huang","Ning Yu"],"pdf_url":"https://arxiv.org/pdf/2405.16226v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09481v4","updated":"2024-09-25T00:07:50Z","published":"2023-12-15T01:38:26Z","title":"Continual Adversarial Defense","summary":"  In response to the rapidly evolving nature of adversarial attacks against\nvisual classifiers on a monthly basis, numerous defenses have been proposed to\ngeneralize against as many known attacks as possible. However, designing a\ndefense method that generalizes to all types of attacks is not realistic\nbecause the environment in which defense systems operate is dynamic and\ncomprises various unique attacks that emerge as time goes on. A well-matched\napproach to the dynamic environment lies in a defense system that continuously\ncollects adversarial data online to quickly improve itself. Therefore, we put\nforward a practical defense deployment against a challenging threat model and\npropose, for the first time, the Continual Adversarial Defense (CAD) framework\nthat adapts to attack sequences under four principles: (1) continual adaptation\nto new attacks without catastrophic forgetting, (2) few-shot adaptation, (3)\nmemory-efficient adaptation, and (4) high accuracy on both clean and\nadversarial data. We explore and integrate cutting-edge continual learning,\nfew-shot learning, and ensemble learning techniques to qualify the principles.\nExtensive experiments validate the effectiveness of our approach against\nmultiple stages of modern adversarial attacks and demonstrate significant\nimprovements over numerous baseline methods. In particular, CAD is capable of\nquickly adapting with minimal budget and a low cost of defense failure while\nmaintaining good performance against previous attacks. Our research sheds light\non a brand-new paradigm for continual defense adaptation against dynamic and\nevolving attacks.\n","authors":["Qian Wang","Yaoyao Liu","Hefei Ling","Yingwei Li","Qihao Liu","Ping Li","Jiazhong Chen","Alan Yuille","Ning Yu"],"pdf_url":"https://arxiv.org/pdf/2312.09481v4.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2306.04833v2","updated":"2024-09-25T17:01:50Z","published":"2023-06-07T23:24:50Z","title":"Unified Embedding Based Personalized Retrieval in Etsy Search","summary":"  Embedding-based neural retrieval is a prevalent approach to address the\nsemantic gap problem which often arises in product search on tail queries. In\ncontrast, popular queries typically lack context and have a broad intent where\nadditional context from users historical interaction can be helpful. In this\npaper, we share our novel approach to address both: the semantic gap problem\nfollowed by an end to end trained model for personalized semantic retrieval. We\npropose learning a unified embedding model incorporating graph, transformer and\nterm-based embeddings end to end and share our design choices for optimal\ntradeoff between performance and efficiency. We share our learnings in feature\nengineering, hard negative sampling strategy, and application of transformer\nmodel, including a novel pre-training strategy and other tricks for improving\nsearch relevance and deploying such a model at industry scale. Our personalized\nretrieval model significantly improves the overall search experience, as\nmeasured by a 5.58% increase in search purchase rate and a 2.63% increase in\nsite-wide conversion rate, aggregated across multiple A/B tests - on live\ntraffic.\n","authors":["Rishikesh Jha","Siddharth Subramaniyam","Ethan Benjamin","Thrivikrama Taula"],"pdf_url":"https://arxiv.org/pdf/2306.04833v2.pdf","comment":"To appear at FMLDS 2024"},{"id":"http://arxiv.org/abs/2402.12997v5","updated":"2024-09-25T14:37:39Z","published":"2024-02-20T13:25:16Z","title":"Towards Trustworthy Reranking: A Simple yet Effective Abstention\n  Mechanism","summary":"  Neural Information Retrieval (NIR) has significantly improved upon\nheuristic-based Information Retrieval (IR) systems. Yet, failures remain\nfrequent, the models used often being unable to retrieve documents relevant to\nthe user's query. We address this challenge by proposing a lightweight\nabstention mechanism tailored for real-world constraints, with particular\nemphasis placed on the reranking phase. We introduce a protocol for evaluating\nabstention strategies in black-box scenarios (typically encountered when\nrelying on API services), demonstrating their efficacy, and propose a simple\nyet effective data-driven mechanism. We provide open-source code for experiment\nreplication and abstention implementation, fostering wider adoption and\napplication in diverse contexts.\n","authors":["Hippolyte Gisserot-Boukhlef","Manuel Faysse","Emmanuel Malherbe","Céline Hudelot","Pierre Colombo"],"pdf_url":"https://arxiv.org/pdf/2402.12997v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.02135v2","updated":"2024-09-25T14:00:18Z","published":"2024-06-04T09:24:04Z","title":"Robust Interaction-Based Relevance Modeling for Online e-Commerce Search","summary":"  Semantic relevance calculation is crucial for e-commerce search engines, as\nit ensures that the items selected closely align with customer intent.\nInadequate attention to this aspect can detrimentally affect user experience\nand engagement. Traditional text-matching techniques are prevalent but often\nfail to capture the nuances of search intent accurately, so neural networks now\nhave become a preferred solution to processing such complex text matching.\nExisting methods predominantly employ representation-based architectures, which\nstrike a balance between high traffic capacity and low latency. However, they\nexhibit significant shortcomings in generalization and robustness when compared\nto interaction-based architectures. In this work, we introduce a robust\ninteraction-based modeling paradigm to address these shortcomings. It\nencompasses 1) a dynamic length representation scheme for expedited inference,\n2) a professional terms recognition method to identify subjects and core\nattributes from complex sentence structures, and 3) a contrastive adversarial\ntraining protocol to bolster the model's robustness and matching capabilities.\nExtensive offline evaluations demonstrate the superior robustness and\neffectiveness of our approach, and online A/B testing confirms its ability to\nimprove relevance in the same exposure position, resulting in more clicks and\nconversions. To the best of our knowledge, this method is the first\ninteraction-based approach for large e-commerce search relevance calculation.\nNotably, we have deployed it for the entire search traffic on alibaba.com, the\nlargest B2B e-commerce platform in the world.\n","authors":["Ben Chen","Huangyu Dai","Xiang Ma","Wen Jiang","Wei Ning"],"pdf_url":"https://arxiv.org/pdf/2406.02135v2.pdf","comment":"Accepted by ECML-PKDD'24 as Outstanding Paper. 8 pages, 2 figures, 7\n  tables"},{"id":"http://arxiv.org/abs/2304.08851v2","updated":"2024-09-25T12:57:08Z","published":"2023-04-18T09:37:22Z","title":"A Personality-Guided Preference Aggregator for Ephemeral Group\n  Recommendation","summary":"  Ephemeral group recommendation (EGR) aims to suggest items for a group of\nusers who come together for the first time. Existing work typically consider\nindividual preferences as the sole factor in aggregating group preferences.\nHowever, they neglect to take into account the importance of the individual\ninherent factors, such as personality, and thus fail to accurately simulate the\ngroup decision-making process. Additionally, these methods often struggle due\nto insufficient interactive records. To tackle these issues, a\nPersonality-Guided Preference Aggregator (PEGA) is proposed, which guides the\npreference aggregation of group members based on their personalities, rather\nthan relying solely on their preferences. Specifically, implicit personalities\nare first extracted from user reviews. Hyper-rectangles are then used to\naggregate individual personalities to obtain the \"Group Personality\", which\nallows for the learning of personality distributions within the group.\nSubsequently, a personality attention mechanism is employed to aggregate group\npreferences, and a preference-based fine-tuning module is used to balance the\nweights of personality and preferences. The role of personality in this\napproach is twofold: (1) To estimate the importance of individual users in a\ngroup and provide explainability; (2) To alleviate the data sparsity issue\nencountered in ephemeral groups. Experimental results demonstrate that, on four\nreal-world datasets, the PEGA model significantly outperforms related baseline\nmodels in terms of classification accuracy and interpretability. Moreover,\nempirical evidence supports the idea that personality plays a pivotal role in\nenhancing the performance of EGR tasks.\n","authors":["Guangze Ye","Wen Wu","Liye Shi","Wenxin Hu","Xin Chen","Liang He"],"pdf_url":"https://arxiv.org/pdf/2304.08851v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16793v1","updated":"2024-09-25T10:14:01Z","published":"2024-09-25T10:14:01Z","title":"Spacewalker: Traversing Representation Spaces for Fast Interactive\n  Exploration and Annotation of Unstructured Data","summary":"  Unstructured data in industries such as healthcare, finance, and\nmanufacturing presents significant challenges for efficient analysis and\ndecision making. Detecting patterns within this data and understanding their\nimpact is critical but complex without the right tools. Traditionally, these\ntasks relied on the expertise of data analysts or labor-intensive manual\nreviews. In response, we introduce Spacewalker, an interactive tool designed to\nexplore and annotate data across multiple modalities. Spacewalker allows users\nto extract data representations and visualize them in low-dimensional spaces,\nenabling the detection of semantic similarities. Through extensive user\nstudies, we assess Spacewalker's effectiveness in data annotation and integrity\nverification. Results show that the tool's ability to traverse latent spaces\nand perform multi-modal queries significantly enhances the user's capacity to\nquickly identify relevant data. Moreover, Spacewalker allows for annotation\nspeed-ups far superior to conventional methods, making it a promising tool for\nefficiently navigating unstructured data and improving decision making\nprocesses. The code of this work is open-source and can be found at:\nhttps://github.com/code-lukas/Spacewalker\n","authors":["Lukas Heine","Fabian Hörst","Jana Fragemann","Gijs Luijten","Miriam Balzer","Jan Egger","Fin Bahnsen","M. Saquib Sarfraz","Jens Kleesiek","Constantin Seibold"],"pdf_url":"https://arxiv.org/pdf/2409.16793v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16332v2","updated":"2024-09-25T09:36:49Z","published":"2024-06-24T06:10:13Z","title":"DemoRank: Selecting Effective Demonstrations for Large Language Models\n  in Ranking Task","summary":"  Recently, there has been increasing interest in applying large language\nmodels (LLMs) as zero-shot passage rankers. However, few studies have explored\nhow to select appropriate in-context demonstrations for the passage ranking\ntask, which is the focus of this paper. Previous studies mainly use LLM's\nfeedback to train a retriever for demonstration selection. These studies apply\nthe LLM to score each demonstration independently, which ignores the\ndependencies between demonstrations (especially important in ranking task),\nleading to inferior performance of top-$k$ retrieved demonstrations. To\nmitigate this issue, we introduce a demonstration reranker to rerank the\nretrieved demonstrations so that top-$k$ ranked ones are more suitable for ICL.\nHowever, generating training data for such reranker is quite challenging. On\nthe one hand, different from demonstration retriever, the training samples of\nreranker need to incorporate demonstration dependencies. On the other hand,\nobtaining the gold ranking from the retrieved demonstrations is an NP-hard\nproblem, which is hard to implement. To overcome these challenges, we propose a\nmethod to approximate the optimal demonstration list iteratively and utilize\nLLM to score demonstration lists of varying lengths. By doing so, the search\nspace is greatly reduced and demonstration dependencies are considered. Based\non these scored demonstration lists, we further design a list-pairwise training\napproach which compares a pair of lists that only differ in the last\ndemonstration, to teach the reranker how to select the next demonstration given\na previous sequence. In this paper, we propose a demonstration selection\nframework DemoRank for ranking task and conduct extensive experiments to prove\nits strong ability.\n","authors":["Wenhan Liu","Yutao Zhu","Zhicheng Dou"],"pdf_url":"https://arxiv.org/pdf/2406.16332v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07054v2","updated":"2024-09-25T09:19:00Z","published":"2023-11-13T03:42:17Z","title":"A Study of Implicit Ranking Unfairness in Large Language Models","summary":"  Recently, Large Language Models (LLMs) have demonstrated a superior ability\nto serve as ranking models. However, concerns have arisen as LLMs will exhibit\ndiscriminatory ranking behaviors based on users' sensitive attributes (\\eg\ngender). Worse still, in this paper, we identify a subtler form of\ndiscrimination in LLMs, termed \\textit{implicit ranking unfairness}, where LLMs\nexhibit discriminatory ranking patterns based solely on non-sensitive user\nprofiles, such as user names. Such implicit unfairness is more widespread but\nless noticeable, threatening the ethical foundation. To comprehensively explore\nsuch unfairness, our analysis will focus on three research aspects: (1) We\npropose an evaluation method to investigate the severity of implicit ranking\nunfairness. (2) We uncover the reasons for causing such unfairness. (3) To\nmitigate such unfairness effectively, we utilize a pair-wise regression method\nto conduct fair-aware data augmentation for LLM fine-tuning. The experiment\ndemonstrates that our method outperforms existing approaches in ranking\nfairness, achieving this with only a small reduction in accuracy. Lastly, we\nemphasize the need for the community to identify and mitigate the implicit\nunfairness, aiming to avert the potential deterioration in the reinforced\nhuman-LLMs ecosystem deterioration.\n","authors":["Chen Xu","Wenjie Wang","Yuxin Li","Liang Pang","Jun Xu","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2311.07054v2.pdf","comment":"Accepted in EMNLP 2024 findings"},{"id":"http://arxiv.org/abs/2409.16760v1","updated":"2024-09-25T09:16:46Z","published":"2024-09-25T09:16:46Z","title":"Enhancing Automatic Keyphrase Labelling with Text-to-Text Transfer\n  Transformer (T5) Architecture: A Framework for Keyphrase Generation and\n  Filtering","summary":"  Automatic keyphrase labelling stands for the ability of models to retrieve\nwords or short phrases that adequately describe documents' content. Previous\nwork has put much effort into exploring extractive techniques to address this\ntask; however, these methods cannot produce keyphrases not found in the text.\nGiven this limitation, keyphrase generation approaches have arisen lately. This\npaper presents a keyphrase generation model based on the Text-to-Text Transfer\nTransformer (T5) architecture. Having a document's title and abstract as input,\nwe learn a T5 model to generate keyphrases which adequately define its content.\nWe name this model docT5keywords. We not only perform the classic inference\napproach, where the output sequence is directly selected as the predicted\nvalues, but we also report results from a majority voting approach. In this\napproach, multiple sequences are generated, and the keyphrases are ranked based\non their frequency of occurrence across these sequences. Along with this model,\nwe present a novel keyphrase filtering technique based on the T5 architecture.\nWe train a T5 model to learn whether a given keyphrase is relevant to a\ndocument. We devise two evaluation methodologies to prove our model's\ncapability to filter inadequate keyphrases. First, we perform a binary\nevaluation where our model has to predict if a keyphrase is relevant for a\ngiven document. Second, we filter the predicted keyphrases by several AKG\nmodels and check if the evaluation scores are improved. Experimental results\ndemonstrate that our keyphrase generation model significantly outperforms all\nthe baselines, with gains exceeding 100\\% in some cases. The proposed filtering\ntechnique also achieves near-perfect accuracy in eliminating false positives\nacross all datasets.\n","authors":["Jorge Gabín","M. Eduardo Ares","Javier Parapar"],"pdf_url":"https://arxiv.org/pdf/2409.16760v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14913v2","updated":"2024-09-25T08:52:49Z","published":"2024-09-23T11:08:04Z","title":"Towards a Realistic Long-Term Benchmark for Open-Web Research Agents","summary":"  We present initial results of a forthcoming benchmark for evaluating LLM\nagents on white-collar tasks of economic value. We evaluate agents on\nreal-world \"messy\" open-web research tasks of the type that are routine in\nfinance and consulting. In doing so, we lay the groundwork for an LLM agent\nevaluation suite where good performance directly corresponds to a large\neconomic and societal impact. We built and tested several agent architectures\nwith o1-preview, GPT-4o, Claude-3.5 Sonnet, Llama 3.1 (405b), and GPT-4o-mini.\nOn average, LLM agents powered by Claude-3.5 Sonnet and o1-preview\nsubstantially outperformed agents using GPT-4o, with agents based on Llama 3.1\n(405b) and GPT-4o-mini lagging noticeably behind. Across LLMs, a ReAct\narchitecture with the ability to delegate subtasks to subagents performed best.\nIn addition to quantitative evaluations, we qualitatively assessed the\nperformance of the LLM agents by inspecting their traces and reflecting on\ntheir observations. Our evaluation represents the first in-depth assessment of\nagents' abilities to conduct challenging, economically valuable analyst-style\nresearch on the real open web.\n","authors":["Peter Mühlbacher","Nikos I. Bosse","Lawrence Phillips"],"pdf_url":"https://arxiv.org/pdf/2409.14913v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16674v1","updated":"2024-09-25T07:06:14Z","published":"2024-09-25T07:06:14Z","title":"A Prompting-Based Representation Learning Method for Recommendation with\n  Large Language Models","summary":"  In recent years, Recommender Systems (RS) have witnessed a transformative\nshift with the advent of Large Language Models (LLMs) in the field of Natural\nLanguage Processing (NLP). Models such as GPT-3.5/4, Llama, have demonstrated\nunprecedented capabilities in understanding and generating human-like text. The\nextensive information pre-trained by these LLMs allows for the potential to\ncapture a more profound semantic representation from different contextual\ninformation of users and items.\n  While the great potential lies behind the thriving of LLMs, the challenge of\nleveraging user-item preferences from contextual information and its alignment\nwith the improvement of Recommender Systems needs to be addressed. Believing\nthat a better understanding of the user or item itself can be the key factor in\nimproving recommendation performance, we conduct research on generating\ninformative profiles using state-of-the-art LLMs.\n  To boost the linguistic abilities of LLMs in Recommender Systems, we\nintroduce the Prompting-Based Representation Learning Method for Recommendation\n(P4R). In our P4R framework, we utilize the LLM prompting strategy to create\npersonalized item profiles. These profiles are then transformed into semantic\nrepresentation spaces using a pre-trained BERT model for text embedding.\nFurthermore, we incorporate a Graph Convolution Network (GCN) for collaborative\nfiltering representation. The P4R framework aligns these two embedding spaces\nin order to address the general recommendation tasks. In our evaluation, we\ncompare P4R with state-of-the-art Recommender models and assess the quality of\nprompt-based profile generation.\n","authors":["Junyi Chen","Toyotaro Suzumura"],"pdf_url":"https://arxiv.org/pdf/2409.16674v1.pdf","comment":"Risks: The 1st International Workshop on Risks, Opportunities, and\n  Evaluation of Generative Models in Recommendation"},{"id":"http://arxiv.org/abs/2403.00781v3","updated":"2024-09-25T06:31:09Z","published":"2024-02-18T06:07:17Z","title":"ChatDiet: Empowering Personalized Nutrition-Oriented Food Recommender\n  Chatbots through an LLM-Augmented Framework","summary":"  The profound impact of food on health necessitates advanced\nnutrition-oriented food recommendation services. Conventional methods often\nlack the crucial elements of personalization, explainability, and\ninteractivity. While Large Language Models (LLMs) bring interpretability and\nexplainability, their standalone use falls short of achieving true\npersonalization. In this paper, we introduce ChatDiet, a novel LLM-powered\nframework designed specifically for personalized nutrition-oriented food\nrecommendation chatbots. ChatDiet integrates personal and population models,\ncomplemented by an orchestrator, to seamlessly retrieve and process pertinent\ninformation. The personal model leverages causal discovery and inference\ntechniques to assess personalized nutritional effects for a specific user,\nwhereas the population model provides generalized information on food\nnutritional content. The orchestrator retrieves, synergizes and delivers the\noutput of both models to the LLM, providing tailored food recommendations\ndesigned to support targeted health outcomes. The result is a dynamic delivery\nof personalized and explainable food recommendations, tailored to individual\nuser preferences. Our evaluation of ChatDiet includes a compelling case study,\nwhere we establish a causal personal model to estimate individual nutrition\neffects. Our assessments, including a food recommendation test showcasing a\n92\\% effectiveness rate, coupled with illustrative dialogue examples,\nunderscore ChatDiet's strengths in explainability, personalization, and\ninteractivity.\n","authors":["Zhongqi Yang","Elahe Khatibi","Nitish Nagesh","Mahyar Abbasian","Iman Azimi","Ramesh Jain","Amir M. Rahmani"],"pdf_url":"https://arxiv.org/pdf/2403.00781v3.pdf","comment":"Published on Smart Health"},{"id":"http://arxiv.org/abs/2409.16633v1","updated":"2024-09-25T05:23:26Z","published":"2024-09-25T05:23:26Z","title":"PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System\n  Inferences","summary":"  Deep Learning Recommendation Models (DLRMs) have become increasingly popular\nand prevalent in today's datacenters, consuming most of the AI inference\ncycles. The performance of DLRMs is heavily influenced by available bandwidth\ndue to their large vector sizes in embedding tables and concurrent accesses. To\nachieve substantial improvements over existing solutions, novel approaches\ntowards DLRM optimization are needed, especially, in the context of emerging\ninterconnect technologies like CXL. This study delves into exploring\nCXL-enabled systems, implementing a process-in-fabric-switch (PIFS) solution to\naccelerate DLRMs while optimizing their memory and bandwidth scalability. We\npresent an in-depth characterization of industry-scale DLRM workloads running\non CXL-ready systems, identifying the predominant bottlenecks in existing CXL\nsystems. We, therefore, propose PIFS-Rec, a PIFS-based scheme that implements\nnear-data processing through downstream ports of the fabric switch. PIFS-Rec\nachieves a latency that is 3.89x lower than Pond, an industry-standard\nCXL-based system, and also outperforms BEACON, a state-of-the-art scheme, by\n2.03x.\n","authors":["Pingyi Huo","Anusha Devulapally","Hasan Al Maruf","Minseo Park","Krishnakumar Nair","Meena Arunachalam","Gulsum Gudukbay Akbulut","Mahmut Taylan Kandemir","Vijaykrishnan Narayanan"],"pdf_url":"https://arxiv.org/pdf/2409.16633v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16627v1","updated":"2024-09-25T05:12:07Z","published":"2024-09-25T05:12:07Z","title":"Train Once, Deploy Anywhere: Matryoshka Representation Learning for\n  Multimodal Recommendation","summary":"  Despite recent advancements in language and vision modeling, integrating rich\nmultimodal knowledge into recommender systems continues to pose significant\nchallenges. This is primarily due to the need for efficient recommendation,\nwhich requires adaptive and interactive responses. In this study, we focus on\nsequential recommendation and introduce a lightweight framework called\nfull-scale Matryoshka representation learning for multimodal recommendation\n(fMRLRec). Our fMRLRec captures item features at different granularities,\nlearning informative representations for efficient recommendation across\nmultiple dimensions. To integrate item features from diverse modalities,\nfMRLRec employs a simple mapping to project multimodal item features into an\naligned feature space. Additionally, we design an efficient linear\ntransformation that embeds smaller features into larger ones, substantially\nreducing memory requirements for large-scale training on recommendation data.\nCombined with improved state space modeling techniques, fMRLRec scales to\ndifferent dimensions and only requires one-time training to produce multiple\nmodels tailored to various granularities. We demonstrate the effectiveness and\nefficiency of fMRLRec on multiple benchmark datasets, which consistently\nachieves superior performance over state-of-the-art baseline methods.\n","authors":["Yueqi Wang","Zhenrui Yue","Huimin Zeng","Dong Wang","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2409.16627v1.pdf","comment":"Accepted to EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2409.16605v1","updated":"2024-09-25T04:12:38Z","published":"2024-09-25T04:12:38Z","title":"Evaluating and Enhancing Large Language Models for Novelty Assessment in\n  Scholarly Publications","summary":"  Recent studies have evaluated the creativity/novelty of large language models\n(LLMs) primarily from a semantic perspective, using benchmarks from cognitive\nscience. However, accessing the novelty in scholarly publications is a largely\nunexplored area in evaluating LLMs. In this paper, we introduce a scholarly\nnovelty benchmark (SchNovel) to evaluate LLMs' ability to assess novelty in\nscholarly papers. SchNovel consists of 15000 pairs of papers across six fields\nsampled from the arXiv dataset with publication dates spanning 2 to 10 years\napart. In each pair, the more recently published paper is assumed to be more\nnovel. Additionally, we propose RAG-Novelty, which simulates the review process\ntaken by human reviewers by leveraging the retrieval of similar papers to\nassess novelty. Extensive experiments provide insights into the capabilities of\ndifferent LLMs to assess novelty and demonstrate that RAG-Novelty outperforms\nrecent baseline models.\n","authors":["Ethan Lin","Zhiyuan Peng","Yi Fang"],"pdf_url":"https://arxiv.org/pdf/2409.16605v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2409.16594v1","updated":"2024-09-25T03:39:14Z","published":"2024-09-25T03:39:14Z","title":"Generative Pre-trained Ranking Model with Over-parameterization at\n  Web-Scale (Extended Abstract)","summary":"  Learning to rank (LTR) is widely employed in web searches to prioritize\npertinent webpages from retrieved content based on input queries. However,\ntraditional LTR models encounter two principal obstacles that lead to\nsuboptimal performance: (1) the lack of well-annotated query-webpage pairs with\nranking scores covering a diverse range of search query popularities, which\nhampers their ability to address queries across the popularity spectrum, and\n(2) inadequately trained models that fail to induce generalized representations\nfor LTR, resulting in overfitting. To address these challenges, we propose a\n\\emph{\\uline{G}enerative \\uline{S}emi-\\uline{S}upervised \\uline{P}re-trained}\n(GS2P) LTR model. We conduct extensive offline experiments on both a publicly\navailable dataset and a real-world dataset collected from a large-scale search\nengine. Furthermore, we deploy GS2P in a large-scale web search engine with\nrealistic traffic, where we observe significant improvements in the real-world\napplication.\n","authors":["Yuchen Li","Haoyi Xiong","Linghe Kong","Jiang Bian","Shuaiqiang Wang","Guihai Chen","Dawei Yin"],"pdf_url":"https://arxiv.org/pdf/2409.16594v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16590v1","updated":"2024-09-25T03:33:47Z","published":"2024-09-25T03:33:47Z","title":"Pre-trained Graphformer-based Ranking at Web-scale Search (Extended\n  Abstract)","summary":"  Both Transformer and Graph Neural Networks (GNNs) have been employed in the\ndomain of learning to rank (LTR). However, these approaches adhere to two\ndistinct yet complementary problem formulations: ranking score regression based\non query-webpage pairs, and link prediction within query-webpage bipartite\ngraphs, respectively. While it is possible to pre-train GNNs or Transformers on\nsource datasets and subsequently fine-tune them on sparsely annotated LTR\ndatasets, the distributional shifts between the pair-based and bipartite graph\ndomains present significant challenges in integrating these heterogeneous\nmodels into a unified LTR framework at web scale. To address this, we introduce\nthe novel MPGraf model, which leverages a modular and capsule-based\npre-training strategy, aiming to cohesively integrate the regression\ncapabilities of Transformers with the link prediction strengths of GNNs. We\nconduct extensive offline and online experiments to rigorously evaluate the\nperformance of MPGraf.\n","authors":["Yuchen Li","Haoyi Xiong","Linghe Kong","Zeyi Sun","Hongyang Chen","Shuaiqiang Wang","Dawei Yin"],"pdf_url":"https://arxiv.org/pdf/2409.16590v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12346v2","updated":"2024-09-25T03:18:14Z","published":"2024-07-17T06:42:14Z","title":"Object-Aware Query Perturbation for Cross-Modal Image-Text Retrieval","summary":"  The pre-trained vision and language (V\\&L) models have substantially improved\nthe performance of cross-modal image-text retrieval. In general, however, V\\&L\nmodels have limited retrieval performance for small objects because of the\nrough alignment between words and the small objects in the image. In contrast,\nit is known that human cognition is object-centric, and we pay more attention\nto important objects, even if they are small. To bridge this gap between the\nhuman cognition and the V\\&L model's capability, we propose a cross-modal\nimage-text retrieval framework based on ``object-aware query perturbation.''\nThe proposed method generates a key feature subspace of the detected objects\nand perturbs the corresponding queries using this subspace to improve the\nobject awareness in the image. In our proposed method, object-aware cross-modal\nimage-text retrieval is possible while keeping the rich expressive power and\nretrieval performance of existing V\\&L models without additional fine-tuning.\nComprehensive experiments on four public datasets show that our method\noutperforms conventional algorithms. Our code is publicly available at\n\\url{https://github.com/NEC-N-SOGI/query-perturbation}.\n","authors":["Naoya Sogi","Takashi Shibata","Makoto Terao"],"pdf_url":"https://arxiv.org/pdf/2407.12346v2.pdf","comment":"ECCV 2024. Code: https://github.com/NEC-N-SOGI/query-perturbation"},{"id":"http://arxiv.org/abs/2409.16576v1","updated":"2024-09-25T03:14:01Z","published":"2024-09-25T03:14:01Z","title":"FusionANNS: An Efficient CPU/GPU Cooperative Processing Architecture for\n  Billion-scale Approximate Nearest Neighbor Search","summary":"  Approximate nearest neighbor search (ANNS) has emerged as a crucial component\nof database and AI infrastructure. Ever-increasing vector datasets pose\nsignificant challenges in terms of performance, cost, and accuracy for ANNS\nservices. None of modern ANNS systems can address these issues simultaneously.\nWe present FusionANNS, a high-throughput, low-latency, cost-efficient, and\nhigh-accuracy ANNS system for billion-scale datasets using SSDs and only one\nentry-level GPU. The key idea of FusionANNS lies in CPU/GPU collaborative\nfiltering and re-ranking mechanisms, which significantly reduce I/O operations\nacross CPUs, GPU, and SSDs to break through the I/O performance bottleneck.\nSpecifically, we propose three novel designs: (1) multi-tiered indexing to\navoid data swapping between CPUs and GPU, (2) heuristic re-ranking to eliminate\nunnecessary I/Os and computations while guaranteeing high accuracy, and (3)\nredundant-aware I/O deduplication to further improve I/O efficiency. We\nimplement FusionANNS and compare it with the state-of-the-art SSD-based ANNS\nsystem--SPANN and GPU-accelerated in-memory ANNS system--RUMMY. Experimental\nresults show that FusionANNS achieves 1) 9.4-13.1X higher query per second\n(QPS) and 5.7-8.8X higher cost efficiency compared with SPANN; 2) and 2-4.9X\nhigher QPS and 2.3-6.8X higher cost efficiency compared with RUMMY, while\nguaranteeing low latency and high accuracy.\n","authors":["Bing Tian","Haikun Liu","Yuhang Tang","Shihai Xiao","Zhuohui Duan","Xiaofei Liao","Xuecang Zhang","Junhua Zhu","Yu Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.16576v1.pdf","comment":"15 pages, 26 figures"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2409.17146v1","updated":"2024-09-25T17:59:51Z","published":"2024-09-25T17:59:51Z","title":"Molmo and PixMo: Open Weights and Open Data for State-of-the-Art\n  Multimodal Models","summary":"  Today's most advanced multimodal models remain proprietary. The strongest\nopen-weight models rely heavily on synthetic data from proprietary VLMs to\nachieve good performance, effectively distilling these closed models into open\nones. As a result, the community is still missing foundational knowledge about\nhow to build performant VLMs from scratch. We present Molmo, a new family of\nVLMs that are state-of-the-art in their class of openness. Our key innovation\nis a novel, highly detailed image caption dataset collected entirely from human\nannotators using speech-based descriptions. To enable a wide array of user\ninteractions, we also introduce a diverse dataset mixture for fine-tuning that\nincludes in-the-wild Q&A and innovative 2D pointing data. The success of our\napproach relies on careful choices for the model architecture details, a\nwell-tuned training pipeline, and, most critically, the quality of our newly\ncollected datasets, all of which will be released. The best-in-class 72B model\nwithin the Molmo family not only outperforms others in the class of open weight\nand data models but also compares favorably against proprietary systems like\nGPT-4o, Claude 3.5, and Gemini 1.5 on both academic benchmarks and human\nevaluation.\n  We will be releasing all of our model weights, captioning and fine-tuning\ndata, and source code in the near future. Select model weights, inference code,\nand demo are available at https://molmo.allenai.org.\n","authors":["Matt Deitke","Christopher Clark","Sangho Lee","Rohun Tripathi","Yue Yang","Jae Sung Park","Mohammadreza Salehi","Niklas Muennighoff","Kyle Lo","Luca Soldaini","Jiasen Lu","Taira Anderson","Erin Bransom","Kiana Ehsani","Huong Ngo","YenSung Chen","Ajay Patel","Mark Yatskar","Chris Callison-Burch","Andrew Head","Rose Hendrix","Favyen Bastani","Eli VanderBilt","Nathan Lambert","Yvonne Chou","Arnavi Chheda","Jenna Sparks","Sam Skjonsberg","Michael Schmitz","Aaron Sarnat","Byron Bischoff","Pete Walsh","Chris Newell","Piper Wolters","Tanmay Gupta","Kuo-Hao Zeng","Jon Borchardt","Dirk Groeneveld","Jen Dumas","Crystal Nam","Sophie Lebrecht","Caitlin Wittlif","Carissa Schoenick","Oscar Michel","Ranjay Krishna","Luca Weihs","Noah A. Smith","Hannaneh Hajishirzi","Ross Girshick","Ali Farhadi","Aniruddha Kembhavi"],"pdf_url":"https://arxiv.org/pdf/2409.17146v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17145v1","updated":"2024-09-25T17:59:45Z","published":"2024-09-25T17:59:45Z","title":"DreamWaltz-G: Expressive 3D Gaussian Avatars from Skeleton-Guided 2D\n  Diffusion","summary":"  Leveraging pretrained 2D diffusion models and score distillation sampling\n(SDS), recent methods have shown promising results for text-to-3D avatar\ngeneration. However, generating high-quality 3D avatars capable of expressive\nanimation remains challenging. In this work, we present DreamWaltz-G, a novel\nlearning framework for animatable 3D avatar generation from text. The core of\nthis framework lies in Skeleton-guided Score Distillation and Hybrid 3D\nGaussian Avatar representation. Specifically, the proposed skeleton-guided\nscore distillation integrates skeleton controls from 3D human templates into 2D\ndiffusion models, enhancing the consistency of SDS supervision in terms of view\nand human pose. This facilitates the generation of high-quality avatars,\nmitigating issues such as multiple faces, extra limbs, and blurring. The\nproposed hybrid 3D Gaussian avatar representation builds on the efficient 3D\nGaussians, combining neural implicit fields and parameterized 3D meshes to\nenable real-time rendering, stable SDS optimization, and expressive animation.\nExtensive experiments demonstrate that DreamWaltz-G is highly effective in\ngenerating and animating 3D avatars, outperforming existing methods in both\nvisual quality and animation expressiveness. Our framework further supports\ndiverse applications, including human video reenactment and multi-subject scene\ncomposition.\n","authors":["Yukun Huang","Jianan Wang","Ailing Zeng","Zheng-Jun Zha","Lei Zhang","Xihui Liu"],"pdf_url":"https://arxiv.org/pdf/2409.17145v1.pdf","comment":"Project page: https://yukun-huang.github.io/DreamWaltz-G/"},{"id":"http://arxiv.org/abs/2409.17144v1","updated":"2024-09-25T17:59:32Z","published":"2024-09-25T17:59:32Z","title":"Differential Privacy Regularization: Protecting Training Data Through\n  Loss Function Regularization","summary":"  Training machine learning models based on neural networks requires large\ndatasets, which may contain sensitive information. The models, however, should\nnot expose private information from these datasets. Differentially private SGD\n[DP-SGD] requires the modification of the standard stochastic gradient descent\n[SGD] algorithm for training new models. In this short paper, a novel\nregularization strategy is proposed to achieve the same goal in a more\nefficient manner.\n","authors":["Francisco Aguilera-Martínez","Fernando Berzal"],"pdf_url":"https://arxiv.org/pdf/2409.17144v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07726v2","updated":"2024-09-25T17:59:18Z","published":"2024-06-11T21:09:45Z","title":"A Concise Mathematical Description of Active Inference in Discrete Time","summary":"  In this paper we present a concise mathematical description of active\ninference in discrete time. The main part of the paper serves as a basic\nintroduction to the topic, including a detailed example illustrating the theory\non action selection. In the appendix the more subtle mathematical details are\ndiscussed. This part is aimed at readers who have already studied the active\ninference literature but struggle to make sense of the mathematical details and\nderivations. Throughout the whole manuscript, special attention has been paid\nto adopting notation that is both precise and in line with standard\nmathematical texts. All equations and derivations are linked to specific\nequation numbers in other popular text on the topic. Furthermore, Python code\nis provided that implements the action selection mechanism described in this\npaper and is compatible with pymdp environments.\n","authors":["Jesse van Oostrum","Carlotta Langer","Nihat Ay"],"pdf_url":"https://arxiv.org/pdf/2406.07726v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17141v1","updated":"2024-09-25T17:58:35Z","published":"2024-09-25T17:58:35Z","title":"FineZip : Pushing the Limits of Large Language Models for Practical\n  Lossless Text Compression","summary":"  While the language modeling objective has been shown to be deeply connected\nwith compression, it is surprising that modern LLMs are not employed in\npractical text compression systems. In this paper, we provide an in-depth\nanalysis of neural network and transformer-based compression techniques to\nanswer this question. We compare traditional text compression systems with\nneural network and LLM-based text compression methods. Although LLM-based\nsystems significantly outperform conventional compression methods, they are\nhighly impractical. Specifically, LLMZip, a recent text compression system\nusing Llama3-8B requires 9.5 days to compress just 10 MB of text, although with\nhuge improvements in compression ratios. To overcome this, we present FineZip -\na novel LLM-based text compression system that combines ideas of online\nmemorization and dynamic context to reduce the compression time immensely.\nFineZip can compress the above corpus in approximately 4 hours compared to 9.5\ndays, a 54 times improvement over LLMZip and comparable performance. FineZip\noutperforms traditional algorithmic compression methods with a large margin,\nimproving compression ratios by approximately 50\\%. With this work, we take the\nfirst step towards making lossless text compression with LLMs a reality. While\nFineZip presents a significant step in that direction, LLMs are still not a\nviable solution for large-scale text compression. We hope our work paves the\nway for future research and innovation to solve this problem.\n","authors":["Fazal Mittu","Yihuan Bu","Akshat Gupta","Ashok Devireddy","Alp Eren Ozdarendeli","Anant Singh","Gopala Anumanchipalli"],"pdf_url":"https://arxiv.org/pdf/2409.17141v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.16201v2","updated":"2024-09-25T17:58:21Z","published":"2023-11-27T07:19:26Z","title":"Pre-trained Language Models Do Not Help Auto-regressive Text-to-Image\n  Generation","summary":"  Recent advances in image tokenizers, such as VQ-VAE, have enabled\ntext-to-image generation using auto-regressive methods, similar to language\nmodeling. However, these methods have yet to leverage pre-trained language\nmodels, despite their adaptability to various downstream tasks. In this work,\nwe explore this gap by adapting a pre-trained language model for\nauto-regressive text-to-image generation, and find that pre-trained language\nmodels offer limited help. We provide a two-fold explanation by analyzing\ntokens from each modality. First, we demonstrate that image tokens possess\nsignificantly different semantics compared to text tokens, rendering\npre-trained language models no more effective in modeling them than randomly\ninitialized ones. Second, the text tokens in the image-text datasets are too\nsimple compared to normal language model pre-training data, which causes the\ncatastrophic degradation of language models' capability.\n","authors":["Yuhui Zhang","Brandon McKinzie","Zhe Gan","Vaishaal Shankar","Alexander Toshev"],"pdf_url":"https://arxiv.org/pdf/2311.16201v2.pdf","comment":"Published at EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2409.17139v1","updated":"2024-09-25T17:57:04Z","published":"2024-09-25T17:57:04Z","title":"Learning with Dynamics: Autonomous Regulation of UAV Based Communication\n  Networks with Dynamic UAV Crew","summary":"  Unmanned Aerial Vehicle (UAV) based communication networks (UCNs) are a key\ncomponent in future mobile networking. To handle the dynamic environments in\nUCNs, reinforcement learning (RL) has been a promising solution attributed to\nits strong capability of adaptive decision-making free of the environment\nmodels. However, most existing RL-based research focus on control strategy\ndesign assuming a fixed set of UAVs. Few works have investigated how UCNs\nshould be adaptively regulated when the serving UAVs change dynamically. This\narticle discusses RL-based strategy design for adaptive UCN regulation given a\ndynamic UAV set, addressing both reactive strategies in general UCNs and\nproactive strategies in solar-powered UCNs. An overview of the UCN and the RL\nframework is first provided. Potential research directions with key challenges\nand possible solutions are then elaborated. Some of our recent works are\npresented as case studies to inspire innovative ways to handle dynamic UAV crew\nwith different RL algorithms.\n","authors":["Ran Zhang","Bowei Li","Liyuan Zhang"," Jiang"," Xie","Miao Wang"],"pdf_url":"https://arxiv.org/pdf/2409.17139v1.pdf","comment":"7 pages, 6 figures, magazine paper"},{"id":"http://arxiv.org/abs/2409.17138v1","updated":"2024-09-25T17:56:02Z","published":"2024-09-25T17:56:02Z","title":"Landscape of Policy Optimization for Finite Horizon MDPs with General\n  State and Action","summary":"  Policy gradient methods are widely used in reinforcement learning. Yet, the\nnonconvexity of policy optimization imposes significant challenges in\nunderstanding the global convergence of policy gradient methods. For a class of\nfinite-horizon Markov Decision Processes (MDPs) with general state and action\nspaces, we develop a framework that provides a set of easily verifiable\nassumptions to ensure the Kurdyka-Lojasiewicz (KL) condition of the policy\noptimization. Leveraging the KL condition, policy gradient methods converge to\nthe globally optimal policy with a non-asymptomatic rate despite nonconvexity.\nOur results find applications in various control and operations models,\nincluding entropy-regularized tabular MDPs, Linear Quadratic Regulator (LQR)\nproblems, stochastic inventory models, and stochastic cash balance problems,\nfor which we show an $\\epsilon$-optimal policy can be obtained using a sample\nsize in $\\tilde{\\mathcal{O}}(\\epsilon^{-1})$ and polynomial in terms of the\nplanning horizon by stochastic policy gradient methods. Our result establishes\nthe first sample complexity for multi-period inventory systems with\nMarkov-modulated demands and stochastic cash balance problems in the\nliterature.\n","authors":["Xin Chen","Yifan Hu","Minda Zhao"],"pdf_url":"https://arxiv.org/pdf/2409.17138v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17137v1","updated":"2024-09-25T17:56:00Z","published":"2024-09-25T17:56:00Z","title":"PACE: marrying generalization in PArameter-efficient fine-tuning with\n  Consistency rEgularization","summary":"  Parameter-Efficient Fine-Tuning (PEFT) effectively adapts pre-trained vision\ntransformers to downstream tasks. However, the optimization for tasks\nperformance often comes at the cost of generalizability in fine-tuned models.\nTo address this issue, we theoretically connect smaller weight gradient norms\nduring training and larger datasets to the improved model generalization.\nMotivated by this connection, we propose reducing gradient norms for enhanced\ngeneralization and aligning fine-tuned model with the pre-trained counterpart\nto retain knowledge from large-scale pre-training data. Yet, naive alignment\ndoes not guarantee gradient reduction and can potentially cause gradient\nexplosion, complicating efforts to manage gradients. To address such issues, we\npropose PACE, marrying generalization of PArameter-efficient fine-tuning with\nConsistency rEgularization. We perturb features learned from the adapter with\nthe multiplicative noise and ensure the fine-tuned model remains consistent for\nsame sample under different perturbations. Theoretical analysis shows that PACE\nnot only implicitly regularizes gradients for enhanced generalization, but also\nimplicitly aligns the fine-tuned and pre-trained models to retain knowledge.\nExperimental evidence supports our theories. PACE outperforms existing PEFT\nmethods in four visual adaptation tasks: VTAB-1k, FGVC, few-shot learning and\ndomain adaptation. Code will be available at\nhttps://github.com/MaxwellYaoNi/PACE\n","authors":["Yao Ni","Shan Zhang","Piotr Koniusz"],"pdf_url":"https://arxiv.org/pdf/2409.17137v1.pdf","comment":"Accepted by NeurIPS 2024 as a spotlight. This preliminary version\n  will soon be extended with the experiments and analyses from the rebuttal"},{"id":"http://arxiv.org/abs/2404.11569v2","updated":"2024-09-25T17:53:48Z","published":"2024-04-17T17:11:47Z","title":"Simple Image Signal Processing using Global Context Guidance","summary":"  In modern smartphone cameras, the Image Signal Processor (ISP) is the core\nelement that converts the RAW readings from the sensor into perceptually\npleasant RGB images for the end users. The ISP is typically proprietary and\nhandcrafted and consists of several blocks such as white balance, color\ncorrection, and tone mapping. Deep learning-based ISPs aim to transform RAW\nimages into DSLR-like RGB images using deep neural networks. However, most\nlearned ISPs are trained using patches (small regions) due to computational\nlimitations. Such methods lack global context, which limits their efficacy on\nfull-resolution images and harms their ability to capture global properties\nsuch as color constancy or illumination. First, we propose a novel module that\ncan be integrated into any neural ISP to capture the global context information\nfrom the full RAW images. Second, we propose an efficient and simple neural ISP\nthat utilizes our proposed module. Our model achieves state-of-the-art results\non different benchmarks using diverse and real smartphone images.\n","authors":["Omar Elezabi","Marcos V. Conde","Radu Timofte"],"pdf_url":"https://arxiv.org/pdf/2404.11569v2.pdf","comment":"IEEE International Conference on Image Processing (ICIP) 2024 - Oral\n  Presentation"},{"id":"http://arxiv.org/abs/2409.17126v1","updated":"2024-09-25T17:42:20Z","published":"2024-09-25T17:42:20Z","title":"Blox-Net: Generative Design-for-Robot-Assembly Using VLM Supervision,\n  Physics Simulation, and a Robot with Reset","summary":"  Generative AI systems have shown impressive capabilities in creating text,\ncode, and images. Inspired by the rich history of research in industrial\n''Design for Assembly'', we introduce a novel problem: Generative\nDesign-for-Robot-Assembly (GDfRA). The task is to generate an assembly based on\na natural language prompt (e.g., ''giraffe'') and an image of available\nphysical components, such as 3D-printed blocks. The output is an assembly, a\nspatial arrangement of these components, and instructions for a robot to build\nthis assembly. The output must 1) resemble the requested object and 2) be\nreliably assembled by a 6 DoF robot arm with a suction gripper. We then present\nBlox-Net, a GDfRA system that combines generative vision language models with\nwell-established methods in computer vision, simulation, perturbation analysis,\nmotion planning, and physical robot experimentation to solve a class of GDfRA\nproblems with minimal human supervision. Blox-Net achieved a Top-1 accuracy of\n63.5% in the ''recognizability'' of its designed assemblies (eg, resembling\ngiraffe as judged by a VLM). These designs, after automated perturbation\nredesign, were reliably assembled by a robot, achieving near-perfect success\nacross 10 consecutive assembly iterations with human intervention only during\nreset prior to assembly. Surprisingly, this entire design process from textual\nword (''giraffe'') to reliable physical assembly is performed with zero human\nintervention.\n","authors":["Andrew Goldberg","Kavish Kondap","Tianshuang Qiu","Zehan Ma","Letian Fu","Justin Kerr","Huang Huang","Kaiyuan Chen","Kuan Fang","Ken Goldberg"],"pdf_url":"https://arxiv.org/pdf/2409.17126v1.pdf","comment":"8 pages, 7 Figures"},{"id":"http://arxiv.org/abs/2409.17120v1","updated":"2024-09-25T17:31:45Z","published":"2024-09-25T17:31:45Z","title":"Deep Learning and Machine Learning, Advancing Big Data Analytics and\n  Management: Handy Appetizer","summary":"  This book explores the role of Artificial Intelligence (AI), Machine Learning\n(ML), and Deep Learning (DL) in driving the progress of big data analytics and\nmanagement. The book focuses on simplifying the complex mathematical concepts\nbehind deep learning, offering intuitive visualizations and practical case\nstudies to help readers understand how neural networks and technologies like\nConvolutional Neural Networks (CNNs) work. It introduces several classic models\nand technologies such as Transformers, GPT, ResNet, BERT, and YOLO,\nhighlighting their applications in fields like natural language processing,\nimage recognition, and autonomous driving. The book also emphasizes the\nimportance of pre-trained models and how they can enhance model performance and\naccuracy, with instructions on how to apply these models in various real-world\nscenarios. Additionally, it provides an overview of key big data management\ntechnologies like SQL and NoSQL databases, as well as distributed computing\nframeworks such as Apache Hadoop and Spark, explaining their importance in\nmanaging and processing vast amounts of data. Ultimately, the book underscores\nthe value of mastering deep learning and big data management skills as critical\ntools for the future workforce, making it an essential resource for both\nbeginners and experienced professionals.\n","authors":["Benji Peng","Xuanhe Pan","Yizhu Wen","Ziqian Bi","Keyu Chen","Ming Li","Ming Liu","Qian Niu","Junyu Liu","Jinlang Wang","Sen Zhang","Jiawei Xu","Pohsun Feng"],"pdf_url":"https://arxiv.org/pdf/2409.17120v1.pdf","comment":"This book contains 93 pages and 60 figures"},{"id":"http://arxiv.org/abs/2409.17115v1","updated":"2024-09-25T17:28:13Z","published":"2024-09-25T17:28:13Z","title":"Programming Every Example: Lifting Pre-training Data Quality like\n  Experts at Scale","summary":"  Large language model pre-training has traditionally relied on human experts\nto craft heuristics for improving the corpora quality, resulting in numerous\nrules developed to date. However, these rules lack the flexibility to address\nthe unique characteristics of individual example effectively. Meanwhile,\napplying tailored rules to every example is impractical for human experts. In\nthis paper, we demonstrate that even small language models, with as few as 0.3B\nparameters, can exhibit substantial data refining capabilities comparable to\nthose of human experts. We introduce Programming Every Example (ProX), a novel\nframework that treats data refinement as a programming task, enabling models to\nrefine corpora by generating and executing fine-grained operations, such as\nstring normalization, for each individual example at scale. Experimental\nresults show that models pre-trained on ProX-curated data outperform either\noriginal data or data filtered by other selection methods by more than 2%\nacross various downstream benchmarks. Its effectiveness spans various model\nsizes and pre-training corpora, including C4, RedPajama-V2, and FineWeb.\nFurthermore, ProX exhibits significant potential in domain-specific continual\npre-training: without domain specific design, models trained on OpenWebMath\nrefined by ProX outperform human-crafted rule-based methods, improving average\naccuracy by 7.6% over Mistral-7B, with 14.6% for Llama-2-7B and 20.3% for\nCodeLlama-7B, all within 10B tokens to be comparable to models like Llemma-7B\ntrained on 200B tokens. Further analysis highlights that ProX significantly\nsaves training FLOPs, offering a promising path for efficient LLM\npre-training.We are open-sourcing ProX with >100B corpus, models, and sharing\nall training and implementation details for reproducible research and future\ninnovation. Code: https://github.com/GAIR-NLP/ProX\n","authors":["Fan Zhou","Zengzhi Wang","Qian Liu","Junlong Li","Pengfei Liu"],"pdf_url":"https://arxiv.org/pdf/2409.17115v1.pdf","comment":"45 pages, 13 figures, 34 tables"},{"id":"http://arxiv.org/abs/2409.17113v1","updated":"2024-09-25T17:27:02Z","published":"2024-09-25T17:27:02Z","title":"Characterizing stable regions in the residual stream of LLMs","summary":"  We identify \"stable regions\" in the residual stream of Transformers, where\nthe model's output remains insensitive to small activation changes, but\nexhibits high sensitivity at region boundaries. These regions emerge during\ntraining and become more defined as training progresses or model size\nincreases. The regions appear to be much larger than previously studied\npolytopes. Our analysis suggests that these stable regions align with semantic\ndistinctions, where similar prompts cluster within regions, and activations\nfrom the same region lead to similar next token predictions.\n","authors":["Jett Janiak","Jacek Karwowski","Chatrik Singh Mangat","Giorgi Giglemiani","Nora Petrova","Stefan Heimersheim"],"pdf_url":"https://arxiv.org/pdf/2409.17113v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17107v1","updated":"2024-09-25T17:21:09Z","published":"2024-09-25T17:21:09Z","title":"Non-asymptotic convergence analysis of the stochastic gradient\n  Hamiltonian Monte Carlo algorithm with discontinuous stochastic gradient with\n  applications to training of ReLU neural networks","summary":"  In this paper, we provide a non-asymptotic analysis of the convergence of the\nstochastic gradient Hamiltonian Monte Carlo (SGHMC) algorithm to a target\nmeasure in Wasserstein-1 and Wasserstein-2 distance. Crucially, compared to the\nexisting literature on SGHMC, we allow its stochastic gradient to be\ndiscontinuous. This allows us to provide explicit upper bounds, which can be\ncontrolled to be arbitrarily small, for the expected excess risk of non-convex\nstochastic optimization problems with discontinuous stochastic gradients,\nincluding, among others, the training of neural networks with ReLU activation\nfunction. To illustrate the applicability of our main results, we consider\nnumerical experiments on quantile estimation and on several optimization\nproblems involving ReLU neural networks relevant in finance and artificial\nintelligence.\n","authors":["Luxu Liang","Ariel Neufeld","Ying Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.17107v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.00082v3","updated":"2024-09-25T17:16:24Z","published":"2024-03-29T10:48:32Z","title":"Data-Driven Room Acoustic Modeling Via Differentiable Feedback Delay\n  Networks With Learnable Delay Lines","summary":"  Over the past few decades, extensive research has been devoted to the design\nof artificial reverberation algorithms aimed at emulating the room acoustics of\nphysical environments. Despite significant advancements, automatic parameter\ntuning of delay-network models remains an open challenge. We introduce a novel\nmethod for finding the parameters of a Feedback Delay Network (FDN) such that\nits output renders target attributes of a measured room impulse response. The\nproposed approach involves the implementation of a differentiable FDN with\ntrainable delay lines, which, for the first time, allows us to simultaneously\nlearn each and every delay-network parameter via backpropagation. The iterative\noptimization process seeks to minimize a perceptually-motivated time-domain\nloss function incorporating differentiable terms accounting for energy decay\nand echo density. Through experimental validation, we show that the proposed\nmethod yields time-invariant frequency-independent FDNs capable of closely\nmatching the desired acoustical characteristics, and outperforms existing\nmethods based on genetic algorithms and analytical FDN design.\n","authors":["Alessandro Ilic Mezza","Riccardo Giampiccolo","Enzo De Sena","Alberto Bernardini"],"pdf_url":"https://arxiv.org/pdf/2404.00082v3.pdf","comment":"The article is scheduled to be published in EURASIP Journal on Audio,\n  Speech, and Music Processing"},{"id":"http://arxiv.org/abs/2409.17092v1","updated":"2024-09-25T16:58:35Z","published":"2024-09-25T16:58:35Z","title":"Accumulator-Aware Post-Training Quantization","summary":"  Several recent studies have investigated low-precision accumulation,\nreporting improvements in throughput, power, and area across various platforms.\nHowever, the accompanying proposals have only considered the quantization-aware\ntraining (QAT) paradigm, in which models are fine-tuned or trained from scratch\nwith quantization in the loop. As models continue to grow in size, QAT\ntechniques become increasingly more expensive, which has motivated the recent\nsurge in post-training quantization (PTQ) research. To the best of our\nknowledge, ours marks the first formal study of accumulator-aware quantization\nin the PTQ setting. To bridge this gap, we introduce AXE, a practical framework\nof accumulator-aware extensions designed to endow overflow avoidance guarantees\nto existing layer-wise PTQ algorithms. We theoretically motivate AXE and\ndemonstrate its flexibility by implementing it on top of two state-of-the-art\nPTQ algorithms: GPFQ and OPTQ. We further generalize AXE to support multi-stage\naccumulation for the first time, opening the door for full datapath\noptimization and scaling to large language models (LLMs). We evaluate AXE\nacross image classification and language generation models, and observe\nsignificant improvements in the trade-off between accumulator bit width and\nmodel accuracy over baseline methods.\n","authors":["Ian Colbert","Fabian Grob","Giuseppe Franco","Jinjie Zhang","Rayan Saab"],"pdf_url":"https://arxiv.org/pdf/2409.17092v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17091v1","updated":"2024-09-25T16:58:19Z","published":"2024-09-25T16:58:19Z","title":"Ctrl-GenAug: Controllable Generative Augmentation for Medical Sequence\n  Classification","summary":"  In the medical field, the limited availability of large-scale datasets and\nlabor-intensive annotation processes hinder the performance of deep models.\nDiffusion-based generative augmentation approaches present a promising solution\nto this issue, having been proven effective in advancing downstream medical\nrecognition tasks. Nevertheless, existing works lack sufficient semantic and\nsequential steerability for challenging video/3D sequence generation, and\nneglect quality control of noisy synthesized samples, resulting in unreliable\nsynthetic databases and severely limiting the performance of downstream tasks.\nIn this work, we present Ctrl-GenAug, a novel and general generative\naugmentation framework that enables highly semantic- and sequential-customized\nsequence synthesis and suppresses incorrectly synthesized samples, to aid\nmedical sequence classification. Specifically, we first design a multimodal\nconditions-guided sequence generator for controllably synthesizing\ndiagnosis-promotive samples. A sequential augmentation module is integrated to\nenhance the temporal/stereoscopic coherence of generated samples. Then, we\npropose a noisy synthetic data filter to suppress unreliable cases at semantic\nand sequential levels. Extensive experiments on 3 medical datasets, using 11\nnetworks trained on 3 paradigms, comprehensively analyze the effectiveness and\ngenerality of Ctrl-GenAug, particularly in underrepresented high-risk\npopulations and out-domain conditions.\n","authors":["Xinrui Zhou","Yuhao Huang","Haoran Dou","Shijing Chen","Ao Chang","Jia Liu","Weiran Long","Jian Zheng","Erjiao Xu","Jie Ren","Ruobing Huang","Jun Cheng","Wufeng Xue","Dong Ni"],"pdf_url":"https://arxiv.org/pdf/2409.17091v1.pdf","comment":"17 pages, 7 figures, 7 tables"},{"id":"http://arxiv.org/abs/2409.17090v1","updated":"2024-09-25T16:57:47Z","published":"2024-09-25T16:57:47Z","title":"Locally Regularized Sparse Graph by Fast Proximal Gradient Descent","summary":"  Sparse graphs built by sparse representation has been demonstrated to be\neffective in clustering high-dimensional data. Albeit the compelling empirical\nperformance, the vanilla sparse graph ignores the geometric information of the\ndata by performing sparse representation for each datum separately. In order to\nobtain a sparse graph aligned with the local geometric structure of data, we\npropose a novel Support Regularized Sparse Graph, abbreviated as SRSG, for data\nclustering. SRSG encourages local smoothness on the neighborhoods of nearby\ndata points by a well-defined support regularization term. We propose a fast\nproximal gradient descent method to solve the non-convex optimization problem\nof SRSG with the convergence matching the Nesterov's optimal convergence rate\nof first-order methods on smooth and convex objective function with Lipschitz\ncontinuous gradient. Extensive experimental results on various real data sets\ndemonstrate the superiority of SRSG over other competing clustering methods.\n","authors":["Dongfang Sun","Yingzhen Yang"],"pdf_url":"https://arxiv.org/pdf/2409.17090v1.pdf","comment":"Accepted by UAI2023"},{"id":"http://arxiv.org/abs/2309.17012v3","updated":"2024-09-25T16:57:20Z","published":"2023-09-29T06:53:10Z","title":"Benchmarking Cognitive Biases in Large Language Models as Evaluators","summary":"  Large Language Models are cognitively biased judges. Large Language Models\n(LLMs) have recently been shown to be effective as automatic evaluators with\nsimple prompting and in-context learning. In this work, we assemble 15 LLMs of\nfour different size ranges and evaluate their output responses by preference\nranking from the other LLMs as evaluators, such as System Star is better than\nSystem Square. We then evaluate the quality of ranking outputs introducing the\nCognitive Bias Benchmark for LLMs as Evaluators (CoBBLEr), a benchmark to\nmeasure six different cognitive biases in LLM evaluation outputs, such as the\nEgocentric bias where a model prefers to rank its own outputs highly in\nevaluation. We find that LLMs are biased text quality evaluators, exhibiting\nstrong indications on our bias benchmark (average of 40% of comparisons across\nall models) within each of their evaluations that question their robustness as\nevaluators. Furthermore, we examine the correlation between human and machine\npreferences and calculate the average Rank-Biased Overlap (RBO) score to be\n49.6%, indicating that machine preferences are misaligned with humans.\nAccording to our findings, LLMs may still be unable to be utilized for\nautomatic annotation aligned with human preferences. Our project page is at:\nhttps://minnesotanlp.github.io/cobbler.\n","authors":["Ryan Koo","Minhwa Lee","Vipul Raheja","Jong Inn Park","Zae Myung Kim","Dongyeop Kang"],"pdf_url":"https://arxiv.org/pdf/2309.17012v3.pdf","comment":"Publishsed at ACL 2024. 29 pages, 9 figures, 14 tables"},{"id":"http://arxiv.org/abs/2409.17087v1","updated":"2024-09-25T16:50:59Z","published":"2024-09-25T16:50:59Z","title":"SEN12-WATER: A New Dataset for Hydrological Applications and its\n  Benchmarking","summary":"  Climate change and increasing droughts pose significant challenges to water\nresource management around the world. These problems lead to severe water\nshortages that threaten ecosystems, agriculture, and human communities. To\nadvance the fight against these challenges, we present a new dataset,\nSEN12-WATER, along with a benchmark using a novel end-to-end Deep Learning (DL)\nframework for proactive drought-related analysis. The dataset, identified as a\nspatiotemporal datacube, integrates SAR polarization, elevation, slope, and\nmultispectral optical bands. Our DL framework enables the analysis and\nestimation of water losses over time in reservoirs of interest, revealing\nsignificant insights into water dynamics for drought analysis by examining\ntemporal changes in physical quantities such as water volume. Our methodology\ntakes advantage of the multitemporal and multimodal characteristics of the\nproposed dataset, enabling robust generalization and advancing understanding of\ndrought, contributing to climate change resilience and sustainable water\nresource management. The proposed framework involves, among the several\ncomponents, speckle noise removal from SAR data, a water body segmentation\nthrough a U-Net architecture, the time series analysis, and the predictive\ncapability of a Time-Distributed-Convolutional Neural Network (TD-CNN). Results\nare validated through ground truth data acquired on-ground via dedicated\nsensors and (tailored) metrics, such as Precision, Recall, Intersection over\nUnion, Mean Squared Error, Structural Similarity Index Measure and Peak\nSignal-to-Noise Ratio.\n","authors":["Luigi Russo","Francesco Mauro","Alessandro Sebastianelli","Paolo Gamba","Silvia Liberata Ullo"],"pdf_url":"https://arxiv.org/pdf/2409.17087v1.pdf","comment":"Submitted to IEEE Transactions on Geoscience and Remote Sensing.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2405.06080v2","updated":"2024-09-25T16:50:55Z","published":"2024-05-09T20:12:46Z","title":"Scalable Learning of Segment-Level Traffic Congestion Functions","summary":"  We propose and study a data-driven framework for identifying traffic\ncongestion functions (numerical relationships between observations of traffic\nvariables) at global scale and segment-level granularity. In contrast to\nmethods that estimate a separate set of parameters for each roadway, ours\nlearns a single black-box function over all roadways in a metropolitan area.\nFirst, we pool traffic data from all segments into one dataset, combining\nstatic attributes with dynamic time-dependent features. Second, we train a\nfeed-forward neural network on this dataset, which we can then use on any\nsegment in the area. We evaluate how well our framework identifies congestion\nfunctions on observed segments and how it generalizes to unobserved segments\nand predicts segment attributes on a large dataset covering multiple cities\nworldwide. For identification error on observed segments, our single\ndata-driven congestion function compares favorably to segment-specific\nmodel-based functions on highway roads, but has room to improve on arterial\nroads. For generalization, our approach shows strong performance across cities\nand road types: both on unobserved segments in the same city and on zero-shot\ntransfer learning between cities. Finally, for predicting segment attributes,\nwe find that our approach can approximate critical densities for individual\nsegments using their static properties.\n","authors":["Shushman Choudhury","Abdul Rahman Kreidieh","Iveel Tsogsuren","Neha Arora","Carolina Osorio","Alexandre Bayen"],"pdf_url":"https://arxiv.org/pdf/2405.06080v2.pdf","comment":"Published at IEEE ITSC 2024"},{"id":"http://arxiv.org/abs/2409.07028v2","updated":"2024-09-25T16:41:07Z","published":"2024-09-11T05:55:51Z","title":"Adaptive Error-Bounded Hierarchical Matrices for Efficient Neural\n  Network Compression","summary":"  This paper introduces a dynamic, error-bounded hierarchical matrix (H-matrix)\ncompression method tailored for Physics-Informed Neural Networks (PINNs). The\nproposed approach reduces the computational complexity and memory demands of\nlarge-scale physics-based models while preserving the essential properties of\nthe Neural Tangent Kernel (NTK). By adaptively refining hierarchical matrix\napproximations based on local error estimates, our method ensures efficient\ntraining and robust model performance. Empirical results demonstrate that this\ntechnique outperforms traditional compression methods, such as Singular Value\nDecomposition (SVD), pruning, and quantization, by maintaining high accuracy\nand improving generalization capabilities. Additionally, the dynamic H-matrix\nmethod enhances inference speed, making it suitable for real-time applications.\nThis approach offers a scalable and efficient solution for deploying PINNs in\ncomplex scientific and engineering domains, bridging the gap between\ncomputational feasibility and real-world applicability.\n","authors":["John Mango","Ronald Katende"],"pdf_url":"https://arxiv.org/pdf/2409.07028v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17077v1","updated":"2024-09-25T16:40:51Z","published":"2024-09-25T16:40:51Z","title":"Efficient Feature Interactions with Transformers: Improving User\n  Spending Propensity Predictions in Gaming","summary":"  Dream11 is a fantasy sports platform that allows users to create their own\nvirtual teams for real-life sports events. We host multiple sports and matches\nfor our 200M+ user base. In this RMG (real money gaming) setting, users pay an\nentry amount to participate in various contest products that we provide to\nusers. In our current work, we discuss the problem of predicting the user's\npropensity to spend in a gaming round, so it can be utilized for various\ndownstream applications. e.g. Upselling users by incentivizing them marginally\nas per their spending propensity, or personalizing the product listing based on\nthe user's propensity to spend.\n  We aim to model the spending propensity of each user based on past\ntransaction data. In this paper, we benchmark tree-based and deep-learning\nmodels that show good results on structured data, and we propose a new\narchitecture change that is specifically designed to capture the rich\ninteractions among the input features. We show that our proposed architecture\noutperforms the existing models on the task of predicting the user's propensity\nto spend in a gaming round. Our new transformer model surpasses the\nstate-of-the-art FT-Transformer, improving MAE by 2.5\\% and MSE by 21.8\\%.\n","authors":["Ved Prakash","Kartavya Kothari"],"pdf_url":"https://arxiv.org/pdf/2409.17077v1.pdf","comment":"6 pages, 3 figures"},{"id":"http://arxiv.org/abs/2409.17069v1","updated":"2024-09-25T16:29:21Z","published":"2024-09-25T16:29:21Z","title":"The Effect of Perceptual Metrics on Music Representation Learning for\n  Genre Classification","summary":"  The subjective quality of natural signals can be approximated with objective\nperceptual metrics. Designed to approximate the perceptual behaviour of human\nobservers, perceptual metrics often reflect structures found in natural signals\nand neurological pathways. Models trained with perceptual metrics as loss\nfunctions can capture perceptually meaningful features from the structures held\nwithin these metrics. We demonstrate that using features extracted from\nautoencoders trained with perceptual losses can improve performance on music\nunderstanding tasks, i.e. genre classification, over using these metrics\ndirectly as distances when learning a classifier. This result suggests improved\ngeneralisation to novel signals when using perceptual metrics as loss functions\nfor representation learning.\n","authors":["Tashi Namgyal","Alexander Hepburn","Raul Santos-Rodriguez","Valero Laparra","Jesus Malo"],"pdf_url":"https://arxiv.org/pdf/2409.17069v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2312.03455"},{"id":"http://arxiv.org/abs/2409.17063v1","updated":"2024-09-25T16:21:43Z","published":"2024-09-25T16:21:43Z","title":"Benchmarking Domain Generalization Algorithms in Computational Pathology","summary":"  Deep learning models have shown immense promise in computational pathology\n(CPath) tasks, but their performance often suffers when applied to unseen data\ndue to domain shifts. Addressing this requires domain generalization (DG)\nalgorithms. However, a systematic evaluation of DG algorithms in the CPath\ncontext is lacking. This study aims to benchmark the effectiveness of 30 DG\nalgorithms on 3 CPath tasks of varying difficulty through 7,560\ncross-validation runs. We evaluate these algorithms using a unified and robust\nplatform, incorporating modality-specific techniques and recent advances like\npretrained foundation models. Our extensive cross-validation experiments\nprovide insights into the relative performance of various DG strategies. We\nobserve that self-supervised learning and stain augmentation consistently\noutperform other methods, highlighting the potential of pretrained models and\ndata augmentation. Furthermore, we introduce a new pan-cancer tumor detection\ndataset (HISTOPANTUM) as a benchmark for future research. This study offers\nvaluable guidance to researchers in selecting appropriate DG approaches for\nCPath tasks.\n","authors":["Neda Zamanitajeddin","Mostafa Jahanifar","Kesi Xu","Fouzia Siraj","Nasir Rajpoot"],"pdf_url":"https://arxiv.org/pdf/2409.17063v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17055v1","updated":"2024-09-25T16:13:57Z","published":"2024-09-25T16:13:57Z","title":"DRIM: Learning Disentangled Representations from Incomplete Multimodal\n  Healthcare Data","summary":"  Real-life medical data is often multimodal and incomplete, fueling the\ngrowing need for advanced deep learning models capable of integrating them\nefficiently. The use of diverse modalities, including histopathology slides,\nMRI, and genetic data, offers unprecedented opportunities to improve prognosis\nprediction and to unveil new treatment pathways. Contrastive learning, widely\nused for deriving representations from paired data in multimodal tasks, assumes\nthat different views contain the same task-relevant information and leverages\nonly shared information. This assumption becomes restrictive when handling\nmedical data since each modality also harbors specific knowledge relevant to\ndownstream tasks. We introduce DRIM, a new multimodal method for capturing\nthese shared and unique representations, despite data sparsity. More\nspecifically, given a set of modalities, we aim to encode a representation for\neach one that can be divided into two components: one encapsulating\npatient-related information common across modalities and the other,\nencapsulating modality-specific details. This is achieved by increasing the\nshared information among different patient modalities while minimizing the\noverlap between shared and unique components within each modality. Our method\noutperforms state-of-the-art algorithms on glioma patients survival prediction\ntasks, while being robust to missing modalities. To promote reproducibility,\nthe code is made publicly available at https://github.com/Lucas-rbnt/DRIM\n","authors":["Lucas Robinet","Ahmad Berjaoui","Ziad Kheil","Elizabeth Cohen-Jonathan Moyal"],"pdf_url":"https://arxiv.org/pdf/2409.17055v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17048v1","updated":"2024-09-25T16:02:45Z","published":"2024-09-25T16:02:45Z","title":"Predictive Covert Communication Against Multi-UAV Surveillance Using\n  Graph Koopman Autoencoder","summary":"  Low Probability of Detection (LPD) communication aims to obscure the presence\nof radio frequency (RF) signals to evade surveillance. In the context of mobile\nsurveillance utilizing unmanned aerial vehicles (UAVs), achieving LPD\ncommunication presents significant challenges due to the UAVs' rapid and\ncontinuous movements, which are characterized by unknown nonlinear dynamics.\nTherefore, accurately predicting future locations of UAVs is essential for\nenabling real-time LPD communication. In this paper, we introduce a novel\nframework termed predictive covert communication, aimed at minimizing\ndetectability in terrestrial ad-hoc networks under multi-UAV surveillance. Our\ndata-driven method synergistically integrates graph neural networks (GNN) with\nKoopman theory to model the complex interactions within a multi-UAV network and\nfacilitating long-term predictions by linearizing the dynamics, even with\nlimited historical data. Extensive simulation results substantiate that the\npredicted trajectories using our method result in at least 63%-75% lower\nprobability of detection when compared to well-known state-of-the-art baseline\napproaches, showing promise in enabling low-latency covert operations in\npractical scenarios.\n","authors":["Sivaram Krishnan","Jihong Park","Gregory Sherman","Benjamin Campbell","Jinho Choi"],"pdf_url":"https://arxiv.org/pdf/2409.17048v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17044v1","updated":"2024-09-25T15:54:29Z","published":"2024-09-25T15:54:29Z","title":"How to Connect Speech Foundation Models and Large Language Models? What\n  Matters and What Does Not","summary":"  The remarkable performance achieved by Large Language Models (LLM) has driven\nresearch efforts to leverage them for a wide range of tasks and input\nmodalities. In speech-to-text (S2T) tasks, the emerging solution consists of\nprojecting the output of the encoder of a Speech Foundational Model (SFM) into\nthe LLM embedding space through an adapter module. However, no work has yet\ninvestigated how much the downstream-task performance depends on each component\n(SFM, adapter, LLM) nor whether the best design of the adapter depends on the\nchosen SFM and LLM. To fill this gap, we evaluate the combination of 5 adapter\nmodules, 2 LLMs (Mistral and Llama), and 2 SFMs (Whisper and SeamlessM4T) on\ntwo widespread S2T tasks, namely Automatic Speech Recognition and Speech\nTranslation. Our results demonstrate that the SFM plays a pivotal role in\ndownstream performance, while the adapter choice has moderate impact and\ndepends on the SFM and LLM.\n","authors":["Francesco Verdini","Pierfrancesco Melucci","Stefano Perna","Francesco Cariaggi","Marco Gaido","Sara Papi","Szymon Mazurek","Marek Kasztelnik","Luisa Bentivogli","Sébastien Bratières","Paolo Merialdo","Simone Scardapane"],"pdf_url":"https://arxiv.org/pdf/2409.17044v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15647v2","updated":"2024-09-25T15:52:24Z","published":"2024-09-24T01:21:17Z","title":"Looped Transformers for Length Generalization","summary":"  Recent work has shown that Transformers trained from scratch can successfully\nsolve various arithmetic and algorithmic tasks, such as adding numbers and\ncomputing parity. While these Transformers generalize well on unseen inputs of\nthe same length, they struggle with length generalization, i.e., handling\ninputs of unseen lengths. In this work, we demonstrate that looped Transformers\nwith an adaptive number of steps significantly improve length generalization.\nWe focus on tasks with a known iterative solution, involving multiple\niterations of a RASP-L operation - a length-generalizable operation that can be\nexpressed by a finite-sized Transformer. We train looped Transformers using our\nproposed learning algorithm and observe that they learn highly\nlength-generalizable solutions for various tasks.\n","authors":["Ying Fan","Yilun Du","Kannan Ramchandran","Kangwook Lee"],"pdf_url":"https://arxiv.org/pdf/2409.15647v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15126v4","updated":"2024-09-25T15:47:40Z","published":"2024-08-27T15:07:27Z","title":"Force-Guided Bridge Matching for Full-Atom Time-Coarsened Dynamics of\n  Peptides","summary":"  Molecular Dynamics (MD) is crucial in various fields such as materials\nscience, chemistry, and pharmacology to name a few. Conventional MD software\nstruggles with the balance between time cost and prediction accuracy, which\nrestricts its wider application. Recently, data-driven approaches based on deep\ngenerative models have been devised for time-coarsened dynamics, which aim at\nlearning dynamics of diverse molecular systems over a long timestep, enjoying\nboth universality and efficiency. Nevertheless, most current methods are\ndesigned solely to learn from the data distribution regardless of the\nunderlying Boltzmann distribution, and the physics priors such as energies and\nforces are constantly overlooked. In this work, we propose a conditional\ngenerative model called Force-guided Bridge Matching (FBM), which learns\nfull-atom time-coarsened dynamics and targets the Boltzmann-constrained\ndistribution. With the guidance of our delicately-designed intermediate force\nfield, FBM leverages favourable physics priors into the generation process,\ngiving rise to enhanced simulations. Experiments on two datasets consisting of\npeptides verify our superiority in terms of comprehensive metrics and\ndemonstrate transferability to unseen systems.\n","authors":["Ziyang Yu","Wenbing Huang","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2408.15126v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17027v1","updated":"2024-09-25T15:30:24Z","published":"2024-09-25T15:30:24Z","title":"Counterfactual Token Generation in Large Language Models","summary":"  \"Sure, I am happy to generate a story for you: Captain Lyra stood at the helm\nof her trusty ship, the Maelstrom's Fury, gazing out at the endless sea. [...]\nLyra's eyes welled up with tears as she realized the bitter truth - she had\nsacrificed everything for fleeting riches, and lost the love of her crew, her\nfamily, and herself.\" Although this story, generated by a large language model,\nis captivating, one may wonder -- how would the story have unfolded if the\nmodel had chosen \"Captain Maeve\" as the protagonist instead? We cannot know.\nState-of-the-art large language models are stateless -- they maintain no\ninternal memory or state. Given a prompt, they generate a sequence of tokens as\nan output using an autoregressive process. As a consequence, they cannot reason\nabout counterfactual alternatives to tokens they have generated in the past. In\nthis work, our goal is to enhance them with this functionality. To this end, we\ndevelop a causal model of token generation that builds upon the Gumbel-Max\nstructural causal model. Our model allows any large language model to perform\ncounterfactual token generation at almost no cost in comparison with vanilla\ntoken generation, it is embarrassingly simple to implement, and it does not\nrequire any fine-tuning nor prompt engineering. We implement our model on Llama\n3 8B-instruct and conduct both qualitative and quantitative analyses of\ncounterfactually generated text. We conclude with a demonstrative application\nof counterfactual token generation for bias detection, unveiling interesting\ninsights about the model of the world constructed by large language models.\n","authors":["Ivi Chatzi","Nina Corvelo Benz","Eleni Straitouri","Stratis Tsirtsis","Manuel Gomez-Rodriguez"],"pdf_url":"https://arxiv.org/pdf/2409.17027v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17021v1","updated":"2024-09-25T15:26:09Z","published":"2024-09-25T15:26:09Z","title":"CombU: A Combined Unit Activation for Fitting Mathematical Expressions\n  with Neural Networks","summary":"  The activation functions are fundamental to neural networks as they introduce\nnon-linearity into data relationships, thereby enabling deep networks to\napproximate complex data relations. Existing efforts to enhance neural network\nperformance have predominantly focused on developing new mathematical\nfunctions. However, we find that a well-designed combination of existing\nactivation functions within a neural network can also achieve this objective.\nIn this paper, we introduce the Combined Units activation (CombU), which\nemploys different activation functions at various dimensions across different\nlayers. This approach can be theoretically proven to fit most mathematical\nexpressions accurately. The experiments conducted on four mathematical\nexpression datasets, compared against six State-Of-The-Art (SOTA) activation\nfunction algorithms, demonstrate that CombU outperforms all SOTA algorithms in\n10 out of 16 metrics and ranks in the top three for the remaining six metrics.\n","authors":["Jiayu Li","Zilong Zhao","Kevin Yee","Uzair Javaid","Biplab Sikdar"],"pdf_url":"https://arxiv.org/pdf/2409.17021v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09441v3","updated":"2024-09-25T15:23:24Z","published":"2024-07-12T17:27:43Z","title":"The $μ\\mathcal{G}$ Language for Programming Graph Neural Networks","summary":"  Graph neural networks form a class of deep learning architectures\nspecifically designed to work with graph-structured data. As such, they share\nthe inherent limitations and problems of deep learning, especially regarding\nthe issues of explainability and trustworthiness. We propose $\\mu\\mathcal{G}$,\nan original domain-specific language for the specification of graph neural\nnetworks that aims to overcome these issues. The language's syntax is\nintroduced, and its meaning is rigorously defined by a denotational semantics.\nAn equivalent characterization in the form of an operational semantics is also\nprovided and, together with a type system, is used to prove the type soundness\nof $\\mu\\mathcal{G}$. We show how $\\mu\\mathcal{G}$ programs can be represented\nin a more user-friendly graphical visualization, and provide examples of its\ngenerality by showing how it can be used to define some of the most popular\ngraph neural network models, or to develop any custom graph processing\napplication.\n","authors":["Matteo Belenchia","Flavio Corradini","Michela Quadrini","Michele Loreti"],"pdf_url":"https://arxiv.org/pdf/2407.09441v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17016v1","updated":"2024-09-25T15:19:04Z","published":"2024-09-25T15:19:04Z","title":"CNN Mixture-of-Depths","summary":"  We introduce Mixture-of-Depths (MoD) for Convolutional Neural Networks\n(CNNs), a novel approach that enhances the computational efficiency of CNNs by\nselectively processing channels based on their relevance to the current\nprediction. This method optimizes computational resources by dynamically\nselecting key channels in feature maps for focused processing within the\nconvolutional blocks (Conv-Blocks), while skipping less relevant channels.\nUnlike conditional computation methods that require dynamic computation graphs,\nCNN MoD uses a static computation graph with fixed tensor sizes which improve\nhardware efficiency. It speeds up the training and inference processes without\nthe need for customized CUDA kernels, unique loss functions, or finetuning. CNN\nMoD either matches the performance of traditional CNNs with reduced inference\ntimes, GMACs, and parameters, or exceeds their performance while maintaining\nsimilar inference times, GMACs, and parameters. For example, on ImageNet,\nResNet86-MoD exceeds the performance of the standard ResNet50 by 0.45% with a\n6% speedup on CPU and 5% on GPU. Moreover, ResNet75-MoD achieves the same\nperformance as ResNet50 with a 25% speedup on CPU and 15% on GPU.\n","authors":["Rinor Cakaj","Jens Mehnert","Bin Yang"],"pdf_url":"https://arxiv.org/pdf/2409.17016v1.pdf","comment":"Conference Paper of the Asian Conference on Computer Vision (ACCV)\n  2024"},{"id":"http://arxiv.org/abs/2409.16998v1","updated":"2024-09-25T15:03:22Z","published":"2024-09-25T15:03:22Z","title":"PitRSDNet: Predicting Intra-operative Remaining Surgery Duration in\n  Endoscopic Pituitary Surgery","summary":"  Accurate intra-operative Remaining Surgery Duration (RSD) predictions allow\nfor anaesthetists to more accurately decide when to administer anaesthetic\nagents and drugs, as well as to notify hospital staff to send in the next\npatient. Therefore RSD plays an important role in improving patient care and\nminimising surgical theatre costs via efficient scheduling. In endoscopic\npituitary surgery, it is uniquely challenging due to variable workflow\nsequences with a selection of optional steps contributing to high variability\nin surgery duration. This paper presents PitRSDNet for predicting RSD during\npituitary surgery, a spatio-temporal neural network model that learns from\nhistorical data focusing on workflow sequences. PitRSDNet integrates workflow\nknowledge into RSD prediction in two forms: 1) multi-task learning for\nconcurrently predicting step and RSD; and 2) incorporating prior steps as\ncontext in temporal learning and inference. PitRSDNet is trained and evaluated\non a new endoscopic pituitary surgery dataset with 88 videos to show\ncompetitive performance improvements over previous statistical and machine\nlearning methods. The findings also highlight how PitRSDNet improve RSD\nprecision on outlier cases utilising the knowledge of prior steps.\n","authors":["Anjana Wijekoon","Adrito Das","Roxana R. Herrera","Danyal Z. Khan","John Hanrahan","Eleanor Carter","Valpuri Luoma","Danail Stoyanov","Hani J. Marcus","Sophia Bano"],"pdf_url":"https://arxiv.org/pdf/2409.16998v1.pdf","comment":"Accepted to the Augmented Environments for Computer-Assisted\n  Interventions (AE-CAI) Workshop at the Medical Image Computing and\n  Computer-Assisted Interventions (MICCAI) Conference 2024"},{"id":"http://arxiv.org/abs/2409.16997v1","updated":"2024-09-25T15:02:25Z","published":"2024-09-25T15:02:25Z","title":"INT-FlashAttention: Enabling Flash Attention for INT8 Quantization","summary":"  As the foundation of large language models (LLMs), self-attention module\nfaces the challenge of quadratic time and memory complexity with respect to\nsequence length. FlashAttention accelerates attention computation and reduces\nits memory usage by leveraging the GPU memory hierarchy. A promising research\ndirection is to integrate FlashAttention with quantization methods. This paper\nintroduces INT-FlashAttention, the first INT8 quantization architecture\ncompatible with the forward workflow of FlashAttention, which significantly\nimproves the inference speed of FlashAttention on Ampere GPUs. We implement our\nINT-FlashAttention prototype with fully INT8 activations and general\nmatrix-multiplication (GEMM) kernels, making it the first attention operator\nwith fully INT8 input. As a general token-level post-training quantization\nframework, INT-FlashAttention is also compatible with other data formats like\nINT4, etc. Experimental results show INT-FlashAttention achieves 72% faster\ninference speed and 82% smaller quantization error compared to standard\nFlashAttention with FP16 and FP8 data format.\n","authors":["Shimao Chen","Zirui Liu","Zhiying Wu","Ce Zheng","Peizhuang Cong","Zihan Jiang","Lei Su","Tong Yang"],"pdf_url":"https://arxiv.org/pdf/2409.16997v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16991v1","updated":"2024-09-25T14:57:07Z","published":"2024-09-25T14:57:07Z","title":"What is the relationship between Slow Feature Analysis and the Successor\n  Representation?","summary":"  (This is a work in progress. Feedback is welcome) An analytical comparison is\nmade between slow feature analysis (SFA) and the successor representation (SR).\nWhile SFA and the SR stem from distinct areas of machine learning, they share\nimportant properties, both in terms of their mathematics and the types of\ninformation they are sensitive to. This work studies their connection along\nthese two axes. In particular, multiple variants of the SFA algorithm are\nexplored analytically and then applied to the setting of an MDP, leading to a\nfamily of eigenvalue problems involving the SR and other related quantities.\nThese resulting eigenvalue problems are then illustrated in the toy setting of\na gridworld, where it is demonstrated that the place- and grid-like fields\noften associated to the SR can equally be generated using SFA.\n","authors":["Eddie Seabrook","Laurenz Wiskott"],"pdf_url":"https://arxiv.org/pdf/2409.16991v1.pdf","comment":"52 pages, 5 figures"},{"id":"http://arxiv.org/abs/2409.16978v1","updated":"2024-09-25T14:40:26Z","published":"2024-09-25T14:40:26Z","title":"Towards User-Focused Research in Training Data Attribution for\n  Human-Centered Explainable AI","summary":"  While Explainable AI (XAI) aims to make AI understandable and useful to\nhumans, it has been criticised for relying too much on formalism and\nsolutionism, focusing more on mathematical soundness than user needs. We\npropose an alternative to this bottom-up approach inspired by design thinking:\nthe XAI research community should adopt a top-down, user-focused perspective to\nensure user relevance. We illustrate this with a relatively young subfield of\nXAI, Training Data Attribution (TDA). With the surge in TDA research and\ngrowing competition, the field risks repeating the same patterns of\nsolutionism. We conducted a needfinding study with a diverse group of AI\npractitioners to identify potential user needs related to TDA. Through\ninterviews (N=10) and a systematic survey (N=31), we uncovered new TDA tasks\nthat are currently largely overlooked. We invite the TDA and XAI communities to\nconsider these novel tasks and improve the user relevance of their research\noutcomes.\n","authors":["Elisa Nguyen","Johannes Bertram","Evgenii Kortukov","Jean Y. Song","Seong Joon Oh"],"pdf_url":"https://arxiv.org/pdf/2409.16978v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14660v3","updated":"2024-09-25T14:36:44Z","published":"2024-09-23T02:02:02Z","title":"Fourier neural operators for spatiotemporal dynamics in two-dimensional\n  turbulence","summary":"  High-fidelity direct numerical simulation of turbulent flows for most\nreal-world applications remains an outstanding computational challenge. Several\nmachine learning approaches have recently been proposed to alleviate the\ncomputational cost even though they become unstable or unphysical for long time\npredictions. We identify that the Fourier neural operator (FNO) based models\ncombined with a partial differential equation (PDE) solver can accelerate fluid\ndynamic simulations and thus address computational expense of large-scale\nturbulence simulations. We treat the FNO model on the same footing as a PDE\nsolver and answer important questions about the volume and temporal resolution\nof data required to build pre-trained models for turbulence. We also discuss\nthe pitfalls of purely data-driven approaches that need to be avoided by the\nmachine learning models to become viable and competitive tools for long time\nsimulations of turbulence.\n","authors":["Mohammad Atif","Pulkit Dubey","Pratik P. Aghor","Vanessa Lopez-Marrero","Tao Zhang","Abdullah Sharfuddin","Kwangmin Yu","Fan Yang","Foluso Ladeinde","Yangang Liu","Meifeng Lin","Lingda Li"],"pdf_url":"https://arxiv.org/pdf/2409.14660v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16973v1","updated":"2024-09-25T14:35:06Z","published":"2024-09-25T14:35:06Z","title":"Adaptive Self-Supervised Learning Strategies for Dynamic On-Device LLM\n  Personalization","summary":"  Large language models (LLMs) have revolutionized how we interact with\ntechnology, but their personalization to individual user preferences remains a\nsignificant challenge, particularly in on-device applications. Traditional\nmethods often depend heavily on labeled datasets and can be resource-intensive.\nTo address these issues, we present Adaptive Self-Supervised Learning\nStrategies (ASLS), which utilizes self-supervised learning techniques to\npersonalize LLMs dynamically. The framework comprises a user profiling layer\nfor collecting interaction data and a neural adaptation layer for real-time\nmodel fine-tuning. This innovative approach enables continuous learning from\nuser feedback, allowing the model to generate responses that align closely with\nuser-specific contexts. The adaptive mechanisms of ASLS minimize computational\ndemands and enhance personalization efficiency. Experimental results across\nvarious user scenarios illustrate the superior performance of ASLS in boosting\nuser engagement and satisfaction, highlighting its potential to redefine LLMs\nas highly responsive and context-aware systems on-device.\n","authors":["Rafael Mendoza","Isabella Cruz","Richard Liu","Aarav Deshmukh","David Williams","Jesscia Peng","Rohan Iyer"],"pdf_url":"https://arxiv.org/pdf/2409.16973v1.pdf","comment":"First ASLS"},{"id":"http://arxiv.org/abs/2409.16968v1","updated":"2024-09-25T14:28:42Z","published":"2024-09-25T14:28:42Z","title":"Bridge to Real Environment with Hardware-in-the-loop for Wireless\n  Artificial Intelligence Paradigms","summary":"  Nowadays, many machine learning (ML) solutions to improve the wireless\nstandard IEEE802.11p for Vehicular Adhoc Network (VANET) are commonly evaluated\nin the simulated world. At the same time, this approach could be cost-effective\ncompared to real-world testing due to the high cost of vehicles. There is a\nrisk of unexpected outcomes when these solutions are implemented in the real\nworld, potentially leading to wasted resources. To mitigate this challenge, the\nhardware-in-the-loop is the way to move forward as it enables the opportunity\nto test in the real world and simulated worlds together. Therefore, we have\ndeveloped what we believe is the pioneering hardware-in-the-loop for testing\nartificial intelligence, multiple services, and HD map data (LiDAR), in both\nsimulated and real-world settings.\n","authors":["Jeffrey Redondo","Nauman Aslam","Juan Zhang","Zhenhui Yuan"],"pdf_url":"https://arxiv.org/pdf/2409.16968v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16965v1","updated":"2024-09-25T14:26:07Z","published":"2024-09-25T14:26:07Z","title":"ABCFair: an Adaptable Benchmark approach for Comparing Fairness Methods","summary":"  Numerous methods have been implemented that pursue fairness with respect to\nsensitive features by mitigating biases in machine learning. Yet, the problem\nsettings that each method tackles vary significantly, including the stage of\nintervention, the composition of sensitive features, the fairness notion, and\nthe distribution of the output. Even in binary classification, these subtle\ndifferences make it highly complicated to benchmark fairness methods, as their\nperformance can strongly depend on exactly how the bias mitigation problem was\noriginally framed.\n  Hence, we introduce ABCFair, a benchmark approach which allows adapting to\nthe desiderata of the real-world problem setting, enabling proper comparability\nbetween methods for any use case. We apply ABCFair to a range of pre-, in-, and\npostprocessing methods on both large-scale, traditional datasets and on a dual\nlabel (biased and unbiased) dataset to sidestep the fairness-accuracy\ntrade-off.\n","authors":["MaryBeth Defrance","Maarten Buyl","Tijl De Bie"],"pdf_url":"https://arxiv.org/pdf/2409.16965v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16956v1","updated":"2024-09-25T14:12:50Z","published":"2024-09-25T14:12:50Z","title":"Informed deep hierarchical classification: a non-standard analysis\n  inspired approach","summary":"  This work proposes a novel approach to the deep hierarchical classification\ntask, i.e., the problem of classifying data according to multiple labels\norganized in a rigid parent-child structure. It consists in a multi-output deep\nneural network equipped with specific projection operators placed before each\noutput layer. The design of such an architecture, called lexicographic hybrid\ndeep neural network (LH-DNN), has been possible by combining tools from\ndifferent and quite distant research fields: lexicographic multi-objective\noptimization, non-standard analysis, and deep learning. To assess the efficacy\nof the approach, the resulting network is compared against the B-CNN, a\nconvolutional neural network tailored for hierarchical classification tasks, on\nthe CIFAR10, CIFAR100 (where it has been originally and recently proposed\nbefore being adopted and tuned for multiple real-world applications) and\nFashion-MNIST benchmarks. Evidence states that an LH-DNN can achieve comparable\nif not superior performance, especially in the learning of the hierarchical\nrelations, in the face of a drastic reduction of the learning parameters,\ntraining epochs, and computational time, without the need for ad-hoc loss\nfunctions weighting values.\n","authors":["Lorenzo Fiaschi","Marco Cococcioni"],"pdf_url":"https://arxiv.org/pdf/2409.16956v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16950v1","updated":"2024-09-25T14:03:58Z","published":"2024-09-25T14:03:58Z","title":"Dynamic Obstacle Avoidance through Uncertainty-Based Adaptive Planning\n  with Diffusion","summary":"  By framing reinforcement learning as a sequence modeling problem, recent work\nhas enabled the use of generative models, such as diffusion models, for\nplanning. While these models are effective in predicting long-horizon state\ntrajectories in deterministic environments, they face challenges in dynamic\nsettings with moving obstacles. Effective collision avoidance demands\ncontinuous monitoring and adaptive decision-making. While replanning at every\ntimestep could ensure safety, it introduces substantial computational overhead\ndue to the repetitive prediction of overlapping state sequences -- a process\nthat is particularly costly with diffusion models, known for their intensive\niterative sampling procedure. We propose an adaptive generative planning\napproach that dynamically adjusts replanning frequency based on the uncertainty\nof action predictions. Our method minimizes the need for frequent,\ncomputationally expensive, and redundant replanning while maintaining robust\ncollision avoidance performance. In experiments, we obtain a 13.5% increase in\nthe mean trajectory length and a 12.7% increase in mean reward over\nlong-horizon planning, indicating a reduction in collision rates and an\nimproved ability to navigate the environment safely.\n","authors":["Vineet Punyamoorty","Pascal Jutras-Dubé","Ruqi Zhang","Vaneet Aggarwal","Damon Conover","Aniket Bera"],"pdf_url":"https://arxiv.org/pdf/2409.16950v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15866v2","updated":"2024-09-25T13:47:44Z","published":"2024-09-24T08:40:04Z","title":"Multi-UAV Pursuit-Evasion with Online Planning in Unknown Environments\n  by Deep Reinforcement Learning","summary":"  Multi-UAV pursuit-evasion, where pursuers aim to capture evaders, poses a key\nchallenge for UAV swarm intelligence. Multi-agent reinforcement learning (MARL)\nhas demonstrated potential in modeling cooperative behaviors, but most RL-based\napproaches remain constrained to simplified simulations with limited dynamics\nor fixed scenarios. Previous attempts to deploy RL policy to real-world\npursuit-evasion are largely restricted to two-dimensional scenarios, such as\nground vehicles or UAVs at fixed altitudes. In this paper, we address multi-UAV\npursuit-evasion by considering UAV dynamics and physical constraints. We\nintroduce an evader prediction-enhanced network to tackle partial observability\nin cooperative strategy learning. Additionally, we propose an adaptive\nenvironment generator within MARL training, enabling higher exploration\nefficiency and better policy generalization across diverse scenarios.\nSimulations show our method significantly outperforms all baselines in\nchallenging scenarios, generalizing to unseen scenarios with a 100% capture\nrate. Finally, we derive a feasible policy via a two-stage reward refinement\nand deploy the policy on real quadrotors in a zero-shot manner. To our\nknowledge, this is the first work to derive and deploy an RL-based policy using\ncollective thrust and body rates control commands for multi-UAV pursuit-evasion\nin unknown environments. The open-source code and videos are available at\nhttps://sites.google.com/view/pursuit-evasion-rl.\n","authors":["Jiayu Chen","Chao Yu","Guosheng Li","Wenhao Tang","Xinyi Yang","Botian Xu","Huazhong Yang","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2409.15866v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19280v3","updated":"2024-09-25T13:36:27Z","published":"2024-06-27T15:50:41Z","title":"HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into\n  Multimodal LLMs at Scale","summary":"  The rapid development of multimodal large language models (MLLMs), such as\nGPT-4V, has led to significant advancements. However, these models still face\nchallenges in medical multimodal capabilities due to limitations in the\nquantity and quality of medical vision-text data, stemming from data privacy\nconcerns and high annotation costs. While pioneering approaches utilize\nPubMed's large-scale, de-identified medical image-text pairs to address these\nlimitations, they still fall short due to inherent data noise. To tackle this,\nwe refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in\nan 'unblinded' capacity to denoise and reformat the data, resulting in the\ncreation of the PubMedVision dataset with 1.3 million medical VQA samples. Our\nvalidation demonstrates that: (1) PubMedVision can significantly enhance the\nmedical multimodal capabilities of current MLLMs, showing significant\nimprovement in benchmarks including the MMMU Health & Medicine track; (2)\nmanual checks by medical experts and empirical results validate the superior\ndata quality of our dataset compared to other data construction methods. Using\nPubMedVision, we train a 34B medical MLLM HuatuoGPT-Vision, which shows\nsuperior performance in medical multimodal scenarios among open-source MLLMs.\n","authors":["Junying Chen","Chi Gui","Ruyi Ouyang","Anningzhe Gao","Shunian Chen","Guiming Hardy Chen","Xidong Wang","Ruifei Zhang","Zhenyang Cai","Ke Ji","Guangjun Yu","Xiang Wan","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2406.19280v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.06433v4","updated":"2024-09-25T13:32:51Z","published":"2024-05-10T12:25:06Z","title":"Fair Mixed Effects Support Vector Machine","summary":"  To ensure unbiased and ethical automated predictions, fairness must be a core\nprinciple in machine learning applications. Fairness in machine learning aims\nto mitigate biases present in the training data and model imperfections that\ncould lead to discriminatory outcomes. This is achieved by preventing the model\nfrom making decisions based on sensitive characteristics like ethnicity or\nsexual orientation. A fundamental assumption in machine learning is the\nindependence of observations. However, this assumption often does not hold true\nfor data describing social phenomena, where data points are often clustered\nbased. Hence, if the machine learning models do not account for the cluster\ncorrelations, the results may be biased. Especially high is the bias in cases\nwhere the cluster assignment is correlated to the variable of interest. We\npresent a fair mixed effects support vector machine algorithm that can handle\nboth problems simultaneously. With a reproducible simulation study we\ndemonstrate the impact of clustered data on the quality of fair machine\nlearning predictions.\n","authors":["João Vitor Pamplona","Jan Pablo Burgard"],"pdf_url":"https://arxiv.org/pdf/2405.06433v4.pdf","comment":"17 pages, 8 figures"},{"id":"http://arxiv.org/abs/2409.16922v1","updated":"2024-09-25T13:27:41Z","published":"2024-09-25T13:27:41Z","title":"Decomposition of Equivariant Maps via Invariant Maps: Application to\n  Universal Approximation under Symmetry","summary":"  In this paper, we develop a theory about the relationship between invariant\nand equivariant maps with regard to a group $G$. We then leverage this theory\nin the context of deep neural networks with group symmetries in order to obtain\nnovel insight into their mechanisms. More precisely, we establish a one-to-one\nrelationship between equivariant maps and certain invariant maps. This allows\nus to reduce arguments for equivariant maps to those for invariant maps and\nvice versa. As an application, we propose a construction of universal\nequivariant architectures built from universal invariant networks. We, in turn,\nexplain how the universal architectures arising from our construction differ\nfrom standard equivariant architectures known to be universal. Furthermore, we\nexplore the complexity, in terms of the number of free parameters, of our\nmodels, and discuss the relation between invariant and equivariant networks'\ncomplexity. Finally, we also give an approximation rate for G-equivariant deep\nneural networks with ReLU activation functions for finite group G.\n","authors":["Akiyoshi Sannai","Yuuki Takai","Matthieu Cordonnier"],"pdf_url":"https://arxiv.org/pdf/2409.16922v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16904v1","updated":"2024-09-25T13:11:17Z","published":"2024-09-25T13:11:17Z","title":"Discriminative Anchor Learning for Efficient Multi-view Clustering","summary":"  Multi-view clustering aims to study the complementary information across\nviews and discover the underlying structure. For solving the relatively high\ncomputational cost for the existing approaches, works based on anchor have been\npresented recently. Even with acceptable clustering performance, these methods\ntend to map the original representation from multiple views into a fixed shared\ngraph based on the original dataset. However, most studies ignore the\ndiscriminative property of the learned anchors, which ruin the representation\ncapability of the built model. Moreover, the complementary information among\nanchors across views is neglected to be ensured by simply learning the shared\nanchor graph without considering the quality of view-specific anchors. In this\npaper, we propose discriminative anchor learning for multi-view clustering\n(DALMC) for handling the above issues. We learn discriminative view-specific\nfeature representations according to the original dataset and build anchors\nfrom different views based on these representations, which increase the quality\nof the shared anchor graph. The discriminative feature learning and consensus\nanchor graph construction are integrated into a unified framework to improve\neach other for realizing the refinement. The optimal anchors from multiple\nviews and the consensus anchor graph are learned with the orthogonal\nconstraints. We give an iterative algorithm to deal with the formulated\nproblem. Extensive experiments on different datasets show the effectiveness and\nefficiency of our method compared with other methods.\n","authors":["Yalan Qin","Nan Pu","Hanzhou Wu","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2409.16904v1.pdf","comment":"This work has been accepted by TMM"},{"id":"http://arxiv.org/abs/2409.00134v3","updated":"2024-09-25T13:09:35Z","published":"2024-08-29T12:55:10Z","title":"MAPF-GPT: Imitation Learning for Multi-Agent Pathfinding at Scale","summary":"  Multi-agent pathfinding (MAPF) is a challenging computational problem that\ntypically requires to find collision-free paths for multiple agents in a shared\nenvironment. Solving MAPF optimally is NP-hard, yet efficient solutions are\ncritical for numerous applications, including automated warehouses and\ntransportation systems. Recently, learning-based approaches to MAPF have gained\nattention, particularly those leveraging deep reinforcement learning. Following\ncurrent trends in machine learning, we have created a foundation model for the\nMAPF problems called MAPF-GPT. Using imitation learning, we have trained a\npolicy on a set of pre-collected sub-optimal expert trajectories that can\ngenerate actions in conditions of partial observability without additional\nheuristics, reward functions, or communication with other agents. The resulting\nMAPF-GPT model demonstrates zero-shot learning abilities when solving the MAPF\nproblem instances that were not present in the training dataset. We show that\nMAPF-GPT notably outperforms the current best-performing learnable-MAPF solvers\non a diverse range of problem instances and is efficient in terms of\ncomputation (in the inference mode).\n","authors":["Anton Andreychuk","Konstantin Yakovlev","Aleksandr Panov","Alexey Skrynnik"],"pdf_url":"https://arxiv.org/pdf/2409.00134v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.04854v7","updated":"2024-09-25T12:57:03Z","published":"2024-02-07T13:54:06Z","title":"Hierarchical Tree-structured Knowledge Graph For Academic Insight Survey","summary":"  Research surveys have always posed a challenge for beginner researchers who\nlack of research training. These researchers struggle to understand the\ndirections within their research topic, and the discovery of new research\nfindings within a short time. One way to provide intuitive assistance to\nbeginner researchers is by offering relevant knowledge graphs(KG) and\nrecommending related academic papers. However, existing navigation knowledge\ngraphs primarily rely on keywords in the research field and often fail to\npresent the logical hierarchy among multiple related papers clearly. Moreover,\nmost recommendation systems for academic papers simply rely on high text\nsimilarity, which can leave researchers confused as to why a particular article\nis being recommended. They may lack of grasp important information about the\ninsight connection between \"Issue resolved\" and \"Issue finding\" that they hope\nto obtain. To address these issues, this study aims to support research insight\nsurveys for beginner researchers by establishing a hierarchical tree-structured\nknowledge graph that reflects the inheritance insight of research topics and\nthe relevance insight among the academic papers.\n","authors":["Jinghong Li","Huy Phan","Wen Gu","Koichi Ota","Shinobu Hasegawa"],"pdf_url":"https://arxiv.org/pdf/2402.04854v7.pdf","comment":"This paper has been published by 'The 18TH International Conference\n  on INnovations in Intelligent SysTems and Applications (INISTA 2024)'"},{"id":"http://arxiv.org/abs/2201.13250v7","updated":"2024-09-25T12:51:59Z","published":"2022-01-31T13:59:28Z","title":"Differentiating and Integrating ZX Diagrams with Applications to Quantum\n  Machine Learning","summary":"  ZX-calculus has proved to be a useful tool for quantum technology with a wide\nrange of successful applications. Most of these applications are of an\nalgebraic nature. However, other tasks that involve differentiation and\nintegration remain unreachable with current ZX techniques. Here we elevate ZX\nto an analytical perspective by realising differentiation and integration\nentirely within the framework of ZX-calculus. We explicitly illustrate the new\nanalytic framework of ZX-calculus by applying it in context of quantum machine\nlearning for the analysis of barren plateaus.\n","authors":["Quanlong Wang","Richie Yeung","Mark Koch"],"pdf_url":"https://arxiv.org/pdf/2201.13250v7.pdf","comment":"43 pages"},{"id":"http://arxiv.org/abs/2409.16882v1","updated":"2024-09-25T12:50:01Z","published":"2024-09-25T12:50:01Z","title":"Revisiting Space Mission Planning: A Reinforcement Learning-Guided\n  Approach for Multi-Debris Rendezvous","summary":"  This research introduces a novel application of a masked Proximal Policy\nOptimization (PPO) algorithm from the field of deep reinforcement learning\n(RL), for determining the most efficient sequence of space debris visitation,\nutilizing the Lambert solver as per Izzo's adaptation for individual\nrendezvous. The aim is to optimize the sequence in which all the given debris\nshould be visited to get the least total time for rendezvous for the entire\nmission. A neural network (NN) policy is developed, trained on simulated space\nmissions with varying debris fields. After training, the neural network\ncalculates approximately optimal paths using Izzo's adaptation of Lambert\nmaneuvers. Performance is evaluated against standard heuristics in mission\nplanning. The reinforcement learning approach demonstrates a significant\nimprovement in planning efficiency by optimizing the sequence for debris\nrendezvous, reducing the total mission time by an average of approximately\n{10.96\\%} and {13.66\\%} compared to the Genetic and Greedy algorithms,\nrespectively. The model on average identifies the most time-efficient sequence\nfor debris visitation across various simulated scenarios with the fastest\ncomputational speed. This approach signifies a step forward in enhancing\nmission planning strategies for space debris clearance.\n","authors":["Agni Bandyopadhyay","Guenther Waxenegger-Wilfing"],"pdf_url":"https://arxiv.org/pdf/2409.16882v1.pdf","comment":"Accepted for publication at the 2024 International Conference on\n  Space Robotics (iSpaRo)"},{"id":"http://arxiv.org/abs/2405.08790v2","updated":"2024-09-25T12:47:46Z","published":"2024-05-14T17:38:17Z","title":"Kolmogorov-Arnold Networks (KANs) for Time Series Analysis","summary":"  This paper introduces a novel application of Kolmogorov-Arnold Networks\n(KANs) to time series forecasting, leveraging their adaptive activation\nfunctions for enhanced predictive modeling. Inspired by the Kolmogorov-Arnold\nrepresentation theorem, KANs replace traditional linear weights with\nspline-parametrized univariate functions, allowing them to learn activation\npatterns dynamically. We demonstrate that KANs outperforms conventional\nMulti-Layer Perceptrons (MLPs) in a real-world satellite traffic forecasting\ntask, providing more accurate results with considerably fewer number of\nlearnable parameters. We also provide an ablation study of KAN-specific\nparameters impact on performance. The proposed approach opens new avenues for\nadaptive forecasting models, emphasizing the potential of KANs as a powerful\ntool in predictive analytics.\n","authors":["Cristian J. Vaca-Rubio","Luis Blanco","Roberto Pereira","Màrius Caus"],"pdf_url":"https://arxiv.org/pdf/2405.08790v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16875v1","updated":"2024-09-25T12:40:07Z","published":"2024-09-25T12:40:07Z","title":"Feedforward Controllers from Learned Dynamic Local Model Networks with\n  Application to Excavator Assistance Functions","summary":"  Complicated first principles modelling and controller synthesis can be\nprohibitively slow and expensive for high-mix, low-volume products such as\nhydraulic excavators. Instead, in a data-driven approach, recorded trajectories\nfrom the real system can be used to train local model networks (LMNs), for\nwhich feedforward controllers are derived via feedback linearization. However,\nprevious works required LMNs without zero dynamics for feedback linearization,\nwhich restricts the model structure and thus modelling capacity of LMNs. In\nthis paper, we overcome this restriction by providing a criterion for when\nfeedback linearization of LMNs with zero dynamics yields a valid controller. As\na criterion we propose the bounded-input bounded-output stability of the\nresulting controller. In two additional contributions, we extend this approach\nto consider measured disturbance signals and multiple inputs and outputs. We\nillustrate the effectiveness of our contributions in a hydraulic excavator\ncontrol application with hardware experiments. To this end, we train LMNs from\nrecorded, noisy data and derive feedforward controllers used as part of a\nleveling assistance system on the excavator. In our experiments, incorporating\ndisturbance signals and multiple inputs and outputs enhances tracking\nperformance of the learned controller. A video of our experiments is available\nat https://youtu.be/lrrWBx2ASaE.\n","authors":["Leon Greiser","Ozan Demir","Benjamin Hartmann","Henrik Hose","Sebastian Trimpe"],"pdf_url":"https://arxiv.org/pdf/2409.16875v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16872v1","updated":"2024-09-25T12:39:28Z","published":"2024-09-25T12:39:28Z","title":"Ethical and Scalable Automation: A Governance and Compliance Framework\n  for Business Applications","summary":"  The popularisation of applying AI in businesses poses significant challenges\nrelating to ethical principles, governance, and legal compliance. Although\nbusinesses have embedded AI into their day-to-day processes, they lack a\nunified approach for mitigating its potential risks. This paper introduces a\nframework ensuring that AI must be ethical, controllable, viable, and\ndesirable. Balancing these factors ensures the design of a framework that\naddresses its trade-offs, such as balancing performance against explainability.\nA successful framework provides practical advice for businesses to meet\nregulatory requirements in sectors such as finance and healthcare, where it is\ncritical to comply with standards like GPDR and the EU AI Act. Different case\nstudies validate this framework by integrating AI in both academic and\npractical environments. For instance, large language models are cost-effective\nalternatives for generating synthetic opinions that emulate attitudes to\nenvironmental issues. These case studies demonstrate how having a structured\nframework could enhance transparency and maintain performance levels as shown\nfrom the alignment between synthetic and expected distributions. This alignment\nis quantified using metrics like Chi-test scores, normalized mutual\ninformation, and Jaccard indexes. Future research should explore the\nframework's empirical validation in diverse industrial settings further,\nensuring the model's scalability and adaptability.\n","authors":["Haocheng Lin"],"pdf_url":"https://arxiv.org/pdf/2409.16872v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08282v2","updated":"2024-09-25T12:38:48Z","published":"2024-08-26T02:58:37Z","title":"LSR-IGRU: Stock Trend Prediction Based on Long Short-Term Relationships\n  and Improved GRU","summary":"  Stock price prediction is a challenging problem in the field of finance and\nreceives widespread attention. In recent years, with the rapid development of\ntechnologies such as deep learning and graph neural networks, more research\nmethods have begun to focus on exploring the interrelationships between stocks.\nHowever, existing methods mostly focus on the short-term dynamic relationships\nof stocks and directly integrating relationship information with temporal\ninformation. They often overlook the complex nonlinear dynamic characteristics\nand potential higher-order interaction relationships among stocks in the stock\nmarket. Therefore, we propose a stock price trend prediction model named\nLSR-IGRU in this paper, which is based on long short-term stock relationships\nand an improved GRU input. Firstly, we construct a long short-term relationship\nmatrix between stocks, where secondary industry information is employed for the\nfirst time to capture long-term relationships of stocks, and overnight price\ninformation is utilized to establish short-term relationships. Next, we improve\nthe inputs of the GRU model at each step, enabling the model to more\neffectively integrate temporal information and long short-term relationship\ninformation, thereby significantly improving the accuracy of predicting stock\ntrend changes. Finally, through extensive experiments on multiple datasets from\nstock markets in China and the United States, we validate the superiority of\nthe proposed LSR-IGRU model over the current state-of-the-art baseline models.\nWe also apply the proposed model to the algorithmic trading system of a\nfinancial company, achieving significantly higher cumulative portfolio returns\ncompared to other baseline methods. Our sources are released at\nhttps://github.com/ZP1481616577/Baselines_LSR-IGRU.\n","authors":["Peng Zhu","Yuante Li","Yifan Hu","Qinyuan Liu","Dawei Cheng","Yuqi Liang"],"pdf_url":"https://arxiv.org/pdf/2409.08282v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16870v1","updated":"2024-09-25T12:36:14Z","published":"2024-09-25T12:36:14Z","title":"Quantifying Visual Properties of GAM Shape Plots: Impact on Perceived\n  Cognitive Load and Interpretability","summary":"  Generalized Additive Models (GAMs) offer a balance between performance and\ninterpretability in machine learning. The interpretability aspect of GAMs is\nexpressed through shape plots, representing the model's decision-making\nprocess. However, the visual properties of these plots, e.g. number of kinks\n(number of local maxima and minima), can impact their complexity and the\ncognitive load imposed on the viewer, compromising interpretability. Our study,\nincluding 57 participants, investigates the relationship between the visual\nproperties of GAM shape plots and cognitive load they induce. We quantify\nvarious visual properties of shape plots and evaluate their alignment with\nparticipants' perceived cognitive load, based on 144 plots. Our results\nindicate that the number of kinks metric is the most effective, explaining\n86.4% of the variance in users' ratings. We develop a simple model based on\nnumber of kinks that provides a practical tool for predicting cognitive load,\nenabling the assessment of one aspect of GAM interpretability without direct\nuser involvement.\n","authors":["Sven Kruschel","Lasse Bohlen","Julian Rosenberger","Patrick Zschech","Mathias Kraus"],"pdf_url":"https://arxiv.org/pdf/2409.16870v1.pdf","comment":"to be published in proceedings of the 58th Hawaii International\n  Conference on System Sciences (HICSS)"},{"id":"http://arxiv.org/abs/2409.16866v1","updated":"2024-09-25T12:32:22Z","published":"2024-09-25T12:32:22Z","title":"Risk-averse learning with delayed feedback","summary":"  In real-world scenarios, the impacts of decisions may not manifest\nimmediately. Taking these delays into account facilitates accurate assessment\nand management of risk in real-world environments, thereby ensuring the\nefficacy of strategies. In this paper, we investigate risk-averse learning\nusing Conditional Value at Risk (CVaR) as risk measure, while incorporating\ndelayed feedback with unknown but bounded delays. We develop two risk-averse\nlearning algorithms that rely on one-point and two-point zeroth-order\noptimization approaches, respectively. The regret achieved by the algorithms is\nanalyzed in terms of the cumulative delay and the number of total samplings.\nThe results suggest that the two-point risk-averse learning achieves a smaller\nregret bound than the one-point algorithm. Furthermore, the one-point\nrisk-averse learning algorithm attains sublinear regret under certain delay\nconditions, and the two-point risk-averse learning algorithm achieves sublinear\nregret with minimal restrictions on the delay. We provide numerical experiments\non a dynamic pricing problem to demonstrate the performance of the proposed\nalgorithms.\n","authors":["Siyi Wang","Zifan Wang","Karl Henrik Johansson","Sandra Hirche"],"pdf_url":"https://arxiv.org/pdf/2409.16866v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16843v1","updated":"2024-09-25T11:51:00Z","published":"2024-09-25T11:51:00Z","title":"Optimal starting point for time series forecasting","summary":"  Recent advances on time series forecasting mainly focus on improving the\nforecasting models themselves. However, managing the length of the input data\ncan also significantly enhance prediction performance. In this paper, we\nintroduce a novel approach called Optimal Starting Point Time Series Forecast\n(OSP-TSP) to capture the intrinsic characteristics of time series data. By\nadjusting the sequence length via leveraging the XGBoost and LightGBM models,\nthe proposed approach can determine optimal starting point (OSP) of the time\nseries and thus enhance the prediction performances. The performances of the\nOSP-TSP approach are then evaluated across various frequencies on the M4\ndataset and other real-world datasets. Empirical results indicate that\npredictions based on the OSP-TSP approach consistently outperform those using\nthe complete dataset. Moreover, recognizing the necessity of sufficient data to\neffectively train models for OSP identification, we further propose targeted\nsolutions to address the issue of data insufficiency.\n","authors":["Yiming Zhong","Yinuo Ren","Guangyao Cao","Feng Li","Haobo Qi"],"pdf_url":"https://arxiv.org/pdf/2409.16843v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.05200v2","updated":"2024-09-25T11:43:59Z","published":"2024-02-07T19:10:36Z","title":"Are LLMs Ready for Real-World Materials Discovery?","summary":"  Large Language Models (LLMs) create exciting possibilities for powerful\nlanguage processing tools to accelerate research in materials science. While\nLLMs have great potential to accelerate materials understanding and discovery,\nthey currently fall short in being practical materials science tools. In this\nposition paper, we show relevant failure cases of LLMs in materials science\nthat reveal current limitations of LLMs related to comprehending and reasoning\nover complex, interconnected materials science knowledge. Given those\nshortcomings, we outline a framework for developing Materials Science LLMs\n(MatSci-LLMs) that are grounded in materials science knowledge and hypothesis\ngeneration followed by hypothesis testing. The path to attaining performant\nMatSci-LLMs rests in large part on building high-quality, multi-modal datasets\nsourced from scientific literature where various information extraction\nchallenges persist. As such, we describe key materials science information\nextraction challenges which need to be overcome in order to build large-scale,\nmulti-modal datasets that capture valuable materials science knowledge.\nFinally, we outline a roadmap for applying future MatSci-LLMs for real-world\nmaterials discovery via: 1. Automated Knowledge Base Generation; 2. Automated\nIn-Silico Material Design; and 3. MatSci-LLM Integrated Self-Driving Materials\nLaboratories.\n","authors":["Santiago Miret","N M Anoop Krishnan"],"pdf_url":"https://arxiv.org/pdf/2402.05200v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16837v1","updated":"2024-09-25T11:39:16Z","published":"2024-09-25T11:39:16Z","title":"Demo2Vec: Learning Region Embedding with Demographic Information","summary":"  Demographic data, such as income, education level, and employment rate,\ncontain valuable information of urban regions, yet few studies have integrated\ndemographic information to generate region embedding. In this study, we show\nhow the simple and easy-to-access demographic data can improve the quality of\nstate-of-the-art region embedding and provide better predictive performances in\nurban areas across three common urban tasks, namely check-in prediction, crime\nrate prediction, and house price prediction. We find that existing pre-train\nmethods based on KL divergence are potentially biased towards mobility\ninformation and propose to use Jenson-Shannon divergence as a more appropriate\nloss function for multi-view representation learning. Experimental results from\nboth New York and Chicago show that mobility + income is the best pre-train\ndata combination, providing up to 10.22\\% better predictive performances than\nexisting models. Considering that mobility big data can be hardly accessible in\nmany developing cities, we suggest geographic proximity + income to be a simple\nbut effective data combination for region embedding pre-training.\n","authors":["Ya Wen","Yulun Zhou"],"pdf_url":"https://arxiv.org/pdf/2409.16837v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16832v1","updated":"2024-09-25T11:33:32Z","published":"2024-09-25T11:33:32Z","title":"Asynchronous Fractional Multi-Agent Deep Reinforcement Learning for\n  Age-Minimal Mobile Edge Computing","summary":"  In the realm of emerging real-time networked applications like cyber-physical\nsystems (CPS), the Age of Information (AoI) has merged as a pivotal metric for\nevaluating the timeliness. To meet the high computational demands, such as\nthose in intelligent manufacturing within CPS, mobile edge computing (MEC)\npresents a promising solution for optimizing computing and reducing AoI. In\nthis work, we study the timeliness of computational-intensive updates and\nexplores jointly optimize the task updating and offloading policies to minimize\nAoI. Specifically, we consider edge load dynamics and formulate a task\nscheduling problem to minimize the expected time-average AoI. The fractional\nobjective introduced by AoI and the semi-Markov game nature of the problem\nrender this challenge particularly difficult, with existing approaches not\ndirectly applicable. To this end, we present a comprehensive framework to\nfractional reinforcement learning (RL). We first introduce a fractional\nsingle-agent RL framework and prove its linear convergence. We then extend this\nto a fractional multi-agent RL framework with a convergence analysis. To tackle\nthe challenge of asynchronous control in semi-Markov game, we further design an\nasynchronous model-free fractional multi-agent RL algorithm, where each device\nmakes scheduling decisions with the hybrid action space without knowing the\nsystem dynamics and decisions of other devices. Experimental results show that\nour proposed algorithms reduce the average AoI by up to 52.6% compared with the\nbest baseline algorithm in our experiments.\n","authors":["Lyudong Jin","Ming Tang","Jiayu Pan","Meng Zhang","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2409.16832v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16826v1","updated":"2024-09-25T11:24:18Z","published":"2024-09-25T11:24:18Z","title":"Learning phase-space flows using time-discrete implicit Runge-Kutta\n  PINNs","summary":"  We present a computational framework for obtaining multidimensional\nphase-space solutions of systems of non-linear coupled differential equations,\nusing high-order implicit Runge-Kutta Physics- Informed Neural Networks\n(IRK-PINNs) schemes. Building upon foundational work originally solving\ndifferential equations for fields depending on coordinates [J. Comput. Phys.\n378, 686 (2019)], we adapt the scheme to a context where the coordinates are\ntreated as functions. This modification enables us to efficiently solve\nequations of motion for a particle in an external field. Our scheme is\nparticularly useful for explicitly time-independent and periodic fields. We\napply this approach to successfully solve the equations of motion for a mass\nparticle placed in a central force field and a charged particle in a periodic\nelectric field.\n","authors":["Álvaro Fernández Corral","Nicolás Mendoza","Armin Iske","Andrey Yachmenev","Jochen Küpper"],"pdf_url":"https://arxiv.org/pdf/2409.16826v1.pdf","comment":"10 pages, 4 figures, published in the International Conference on\n  Scientific Computing and Machine Learning, see http://scml.jp"},{"id":"http://arxiv.org/abs/2409.16824v1","updated":"2024-09-25T11:22:29Z","published":"2024-09-25T11:22:29Z","title":"Uncertainty Representations in State-Space Layers for Deep Reinforcement\n  Learning under Partial Observability","summary":"  Optimal decision-making under partial observability requires reasoning about\nthe uncertainty of the environment's hidden state. However, most reinforcement\nlearning architectures handle partial observability with sequence models that\nhave no internal mechanism to incorporate uncertainty in their hidden state\nrepresentation, such as recurrent neural networks, deterministic state-space\nmodels and transformers. Inspired by advances in probabilistic world models for\nreinforcement learning, we propose a standalone Kalman filter layer that\nperforms closed-form Gaussian inference in linear state-space models and train\nit end-to-end within a model-free architecture to maximize returns. Similar to\nefficient linear recurrent layers, the Kalman filter layer processes sequential\ndata using a parallel scan, which scales logarithmically with the sequence\nlength. By design, Kalman filter layers are a drop-in replacement for other\nrecurrent layers in standard model-free architectures, but importantly they\ninclude an explicit mechanism for probabilistic filtering of the latent state\nrepresentation. Experiments in a wide variety of tasks with partial\nobservability show that Kalman filter layers excel in problems where\nuncertainty reasoning is key for decision-making, outperforming other stateful\nmodels.\n","authors":["Carlos E. Luis","Alessandro G. Bottero","Julia Vinogradska","Felix Berkenkamp","Jan Peters"],"pdf_url":"https://arxiv.org/pdf/2409.16824v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07430v2","updated":"2024-09-25T11:19:47Z","published":"2023-10-11T12:32:13Z","title":"Non-backtracking Graph Neural Networks","summary":"  The celebrated message-passing updates for graph neural networks allow\nrepresenting large-scale graphs with local and computationally tractable\nupdates. However, the updates suffer from backtracking, i.e., a message flowing\nthrough the same edge twice and revisiting the previously visited node. Since\nthe number of message flows increases exponentially with the number of updates,\nthe redundancy in local updates prevents the graph neural network from\naccurately recognizing a particular message flow relevant for downstream tasks.\nIn this work, we propose to resolve such a redundancy issue via the\nnon-backtracking graph neural network (NBA-GNN) that updates a message without\nincorporating the message from the previously visited node. We theoretically\ninvestigate how NBA-GNN alleviates the over-squashing of GNNs, and establish a\nconnection between NBA-GNN and the impressive performance of non-backtracking\nupdates for stochastic block model recovery. Furthermore, we empirically verify\nthe effectiveness of our NBA-GNN on the long-range graph benchmark and\ntransductive node classification problems.\n","authors":["Seonghyun Park","Narae Ryu","Gahee Kim","Dongyeop Woo","Se-Young Yun","Sungsoo Ahn"],"pdf_url":"https://arxiv.org/pdf/2310.07430v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16817v1","updated":"2024-09-25T11:13:50Z","published":"2024-09-25T11:13:50Z","title":"A parametric framework for kernel-based dynamic mode decomposition using\n  deep learning","summary":"  Surrogate modelling is widely applied in computational science and\nengineering to mitigate computational efficiency issues for the real-time\nsimulations of complex and large-scale computational models or for many-query\nscenarios, such as uncertainty quantification and design optimisation. In this\nwork, we propose a parametric framework for kernel-based dynamic mode\ndecomposition method based on the linear and nonlinear disambiguation\noptimization (LANDO) algorithm. The proposed parametric framework consists of\ntwo stages, offline and online. The offline stage prepares the essential\ncomponent for prediction, namely a series of LANDO models that emulate the\ndynamics of the system with particular parameters from a training dataset. The\nonline stage leverages those LANDO models to generate new data at a desired\ntime instant, and approximate the mapping between parameters and the state with\nthe data using deep learning techniques. Moreover, dimensionality reduction\ntechnique is applied to high-dimensional dynamical systems to reduce the\ncomputational cost of training. Three numerical examples including\nLotka-Volterra model, heat equation and reaction-diffusion equation are\npresented to demonstrate the efficiency and effectiveness of the proposed\nframework.\n","authors":["Konstantinos Kevopoulos","Dongwei Ye"],"pdf_url":"https://arxiv.org/pdf/2409.16817v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16815v1","updated":"2024-09-25T11:10:33Z","published":"2024-09-25T11:10:33Z","title":"Accelerating TinyML Inference on Microcontrollers through Approximate\n  Kernels","summary":"  The rapid growth of microcontroller-based IoT devices has opened up numerous\napplications, from smart manufacturing to personalized healthcare. Despite the\nwidespread adoption of energy-efficient microcontroller units (MCUs) in the\nTiny Machine Learning (TinyML) domain, they still face significant limitations\nin terms of performance and memory (RAM, Flash). In this work, we combine\napproximate computing and software kernel design to accelerate the inference of\napproximate CNN models on MCUs. Our kernel-based approximation framework\nfirstly unpacks the operands of each convolution layer and then conducts an\noffline calculation to determine the significance of each operand.\nSubsequently, through a design space exploration, it employs a computation\nskipping approximation strategy based on the calculated significance. Our\nevaluation on an STM32-Nucleo board and 2 popular CNNs trained on the CIFAR-10\ndataset shows that, compared to state-of-the-art exact inference, our Pareto\noptimal solutions can feature on average 21% latency reduction with no\ndegradation in Top-1 classification accuracy, while for lower accuracy\nrequirements, the corresponding reduction becomes even more pronounced.\n","authors":["Giorgos Armeniakos","Georgios Mentzos","Dimitrios Soudris"],"pdf_url":"https://arxiv.org/pdf/2409.16815v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03302v3","updated":"2024-09-25T10:45:52Z","published":"2024-01-06T20:53:02Z","title":"Realism in Action: Anomaly-Aware Diagnosis of Brain Tumors from Medical\n  Images Using YOLOv8 and DeiT","summary":"  In the field of medical sciences, reliable detection and classification of\nbrain tumors from images remains a formidable challenge due to the rarity of\ntumors within the population of patients. Therefore, the ability to detect\ntumors in anomaly scenarios is paramount for ensuring timely interventions and\nimproved patient outcomes. This study addresses the issue by leveraging deep\nlearning (DL) techniques to detect and classify brain tumors in challenging\nsituations. The curated data set from the National Brain Mapping Lab (NBML)\ncomprises 81 patients, including 30 Tumor cases and 51 Normal cases. The\ndetection and classification pipelines are separated into two consecutive\ntasks. The detection phase involved comprehensive data analysis and\npre-processing to modify the number of image samples and the number of patients\nof each class to anomaly distribution (9 Normal per 1 Tumor) to comply with\nreal world scenarios. Next, in addition to common evaluation metrics for the\ntesting, we employed a novel performance evaluation method called Patient to\nPatient (PTP), focusing on the realistic evaluation of the model. In the\ndetection phase, we fine-tuned a YOLOv8n detection model to detect the tumor\nregion. Subsequent testing and evaluation yielded competitive performance both\nin Common Evaluation Metrics and PTP metrics. Furthermore, using the Data\nEfficient Image Transformer (DeiT) module, we distilled a Vision Transformer\n(ViT) model from a fine-tuned ResNet152 as a teacher in the classification\nphase. This approach demonstrates promising strides in reliable tumor detection\nand classification, offering potential advancements in tumor diagnosis for\nreal-world medical imaging scenarios.\n","authors":["Seyed Mohammad Hossein Hashemi","Leila Safari","Amirhossein Dadashzadeh Taromi"],"pdf_url":"https://arxiv.org/pdf/2401.03302v3.pdf","comment":"This work has been submitted to the Elsevier for possible\n  publication. Copyright may be transferred without notice, after which this\n  version may no longer be accessible"},{"id":"http://arxiv.org/abs/2409.16799v1","updated":"2024-09-25T10:32:18Z","published":"2024-09-25T10:32:18Z","title":"Large Language Model Predicts Above Normal All India Summer Monsoon\n  Rainfall in 2024","summary":"  Reliable prediction of the All India Summer Monsoon Rainfall (AISMR) is\npivotal for informed policymaking for the country, impacting the lives of\nbillions of people. However, accurate simulation of AISMR has been a persistent\nchallenge due to the complex interplay of various muti-scale factors and the\ninherent variability of the monsoon system. This research focuses on adapting\nand fine-tuning the latest LLM model, PatchTST, to accurately predict AISMR\nwith a lead time of three months. The fine-tuned PatchTST model, trained with\nhistorical AISMR data, the Ni\\~no3.4 index, and categorical Indian Ocean Dipole\nvalues, outperforms several popular neural network models and statistical\nmodels. This fine-tuned LLM model exhibits an exceptionally low RMSE percentage\nof 0.07% and a Spearman correlation of 0.976. This is particularly impressive,\nsince it is nearly 80% more accurate than the best-performing NN models. The\nmodel predicts an above-normal monsoon for the year 2024, with an accumulated\nrainfall of 921.6 mm in the month of June-September for the entire country.\n","authors":["Ujjawal Sharma","Madhav Biyani","Akhil Dev Suresh","Debi Prasad Bhuyan","Saroj Kanta Mishra","Tanmoy Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2409.16799v1.pdf","comment":"3 figures"},{"id":"http://arxiv.org/abs/2409.16797v1","updated":"2024-09-25T10:30:24Z","published":"2024-09-25T10:30:24Z","title":"Scalable Ensemble Diversification for OOD Generalization and Detection","summary":"  Training a diverse ensemble of models has several practical applications such\nas providing candidates for model selection with better out-of-distribution\n(OOD) generalization, and enabling the detection of OOD samples via Bayesian\nprinciples. An existing approach to diverse ensemble training encourages the\nmodels to disagree on provided OOD samples. However, the approach is\ncomputationally expensive and it requires well-separated ID and OOD examples,\nsuch that it has only been demonstrated in small-scale settings.\n  $\\textbf{Method.}$ This work presents a method for Scalable Ensemble\nDiversification (SED) applicable to large-scale settings (e.g. ImageNet) that\ndoes not require OOD samples. Instead, SED identifies hard training samples on\nthe fly and encourages the ensemble members to disagree on these. To improve\nscaling, we show how to avoid the expensive computations in existing methods of\nexhaustive pairwise disagreements across models.\n  $\\textbf{Results.}$ We evaluate the benefits of diversification with\nexperiments on ImageNet. First, for OOD generalization, we observe large\nbenefits from the diversification in multiple settings including output-space\n(classical) ensembles and weight-space ensembles (model soups). Second, for OOD\ndetection, we turn the diversity of ensemble hypotheses into a novel\nuncertainty score estimator that surpasses a large number of OOD detection\nbaselines.\n  Code is available here:\nhttps://github.com/AlexanderRubinstein/diverse-universe-public.\n","authors":["Alexander Rubinstein","Luca Scimeca","Damien Teney","Seong Joon Oh"],"pdf_url":"https://arxiv.org/pdf/2409.16797v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2409.16791v1","updated":"2024-09-25T10:09:47Z","published":"2024-09-25T10:09:47Z","title":"Symbolic State Partition for Reinforcement Learning","summary":"  Tabular reinforcement learning methods cannot operate directly on continuous\nstate spaces. One solution for this problem is to partition the state space. A\ngood partitioning enables generalization during learning and more efficient\nexploitation of prior experiences. Consequently, the learning process becomes\nfaster and produces more reliable policies. However, partitioning introduces\napproximation, which is particularly harmful in the presence of nonlinear\nrelations between state components. An ideal partition should be as coarse as\npossible, while capturing the key structure of the state space for the given\nproblem. This work extracts partitions from the environment dynamics by\nsymbolic execution. We show that symbolic partitioning improves state space\ncoverage with respect to environmental behavior and allows reinforcement\nlearning to perform better for sparse rewards. We evaluate symbolic state space\npartitioning with respect to precision, scalability, learning agent performance\nand state space coverage for the learnt policies.\n","authors":["Mohsen Ghaffari","Mahsa Varshosaz","Einar Broch Johnsen","Andrzej Wąsowski"],"pdf_url":"https://arxiv.org/pdf/2409.16791v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16787v1","updated":"2024-09-25T09:50:51Z","published":"2024-09-25T09:50:51Z","title":"Enhancing Feature Selection and Interpretability in AI Regression Tasks\n  Through Feature Attribution","summary":"  Research in Explainable Artificial Intelligence (XAI) is increasing, aiming\nto make deep learning models more transparent. Most XAI methods focus on\njustifying the decisions made by Artificial Intelligence (AI) systems in\nsecurity-relevant applications. However, relatively little attention has been\ngiven to using these methods to improve the performance and robustness of deep\nlearning algorithms. Additionally, much of the existing XAI work primarily\naddresses classification problems. In this study, we investigate the potential\nof feature attribution methods to filter out uninformative features in input\ndata for regression problems, thereby improving the accuracy and stability of\npredictions. We introduce a feature selection pipeline that combines Integrated\nGradients with k-means clustering to select an optimal set of variables from\nthe initial data space. To validate the effectiveness of this approach, we\napply it to a real-world industrial problem - blade vibration analysis in the\ndevelopment process of turbo machinery.\n","authors":["Alexander Hinterleitner","Thomas Bartz-Beielstein","Richard Schulz","Sebastian Spengler","Thomas Winter","Christoph Leitenmeier"],"pdf_url":"https://arxiv.org/pdf/2409.16787v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16784v1","updated":"2024-09-25T09:47:31Z","published":"2024-09-25T09:47:31Z","title":"World Model-based Perception for Visual Legged Locomotion","summary":"  Legged locomotion over various terrains is challenging and requires precise\nperception of the robot and its surroundings from both proprioception and\nvision. However, learning directly from high-dimensional visual input is often\ndata-inefficient and intricate. To address this issue, traditional methods\nattempt to learn a teacher policy with access to privileged information first\nand then learn a student policy to imitate the teacher's behavior with visual\ninput. Despite some progress, this imitation framework prevents the student\npolicy from achieving optimal performance due to the information gap between\ninputs. Furthermore, the learning process is unnatural since animals\nintuitively learn to traverse different terrains based on their understanding\nof the world without privileged knowledge. Inspired by this natural ability, we\npropose a simple yet effective method, World Model-based Perception (WMP),\nwhich builds a world model of the environment and learns a policy based on the\nworld model. We illustrate that though completely trained in simulation, the\nworld model can make accurate predictions of real-world trajectories, thus\nproviding informative signals for the policy controller. Extensive simulated\nand real-world experiments demonstrate that WMP outperforms state-of-the-art\nbaselines in traversability and robustness. Videos and Code are available at:\nhttps://wmp-loco.github.io/.\n","authors":["Hang Lai","Jiahang Cao","Jiafeng Xu","Hongtao Wu","Yunfeng Lin","Tao Kong","Yong Yu","Weinan Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.16784v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2409.16769v1","updated":"2024-09-25T09:27:17Z","published":"2024-09-25T09:27:17Z","title":"Super Level Sets and Exponential Decay: A Synergistic Approach to Stable\n  Neural Network Training","summary":"  The objective of this paper is to enhance the optimization process for neural\nnetworks by developing a dynamic learning rate algorithm that effectively\nintegrates exponential decay and advanced anti-overfitting strategies. Our\nprimary contribution is the establishment of a theoretical framework where we\ndemonstrate that the optimization landscape, under the influence of our\nalgorithm, exhibits unique stability characteristics defined by Lyapunov\nstability principles. Specifically, we prove that the superlevel sets of the\nloss function, as influenced by our adaptive learning rate, are always\nconnected, ensuring consistent training dynamics. Furthermore, we establish the\n\"equiconnectedness\" property of these superlevel sets, which maintains uniform\nstability across varying training conditions and epochs. This paper contributes\nto the theoretical understanding of dynamic learning rate mechanisms in neural\nnetworks and also pave the way for the development of more efficient and\nreliable neural optimization techniques. This study intends to formalize and\nvalidate the equiconnectedness of loss function as superlevel sets in the\ncontext of neural network training, opening newer avenues for future research\nin adaptive machine learning algorithms. We leverage previous theoretical\ndiscoveries to propose training mechanisms that can effectively handle complex\nand high-dimensional data landscapes, particularly in applications requiring\nhigh precision and reliability.\n","authors":["Jatin Chaudhary","Dipak Nidhi","Jukka Heikkonen","Haari Merisaari","Rajiv Kanth"],"pdf_url":"https://arxiv.org/pdf/2409.16769v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16768v1","updated":"2024-09-25T09:26:19Z","published":"2024-09-25T09:26:19Z","title":"Interpreting Deep Neural Network-Based Receiver Under Varying\n  Signal-To-Noise Ratios","summary":"  We propose a novel method for interpreting neural networks, focusing on\nconvolutional neural network-based receiver model. The method identifies which\nunit or units of the model contain most (or least) information about the\nchannel parameter(s) of the interest, providing insights at both global and\nlocal levels -- with global explanations aggregating local ones. Experiments on\nlink-level simulations demonstrate the method's effectiveness in identifying\nunits that contribute most (and least) to signal-to-noise ratio processing.\nAlthough we focus on a radio receiver model, the method generalizes to other\nneural network architectures and applications, offering robust estimation even\nin high-dimensional settings.\n","authors":["Marko Tuononen","Dani Korpi","Ville Hautamäki"],"pdf_url":"https://arxiv.org/pdf/2409.16768v1.pdf","comment":"7+1 pages, 8 figures"},{"id":"http://arxiv.org/abs/2409.16767v1","updated":"2024-09-25T09:26:06Z","published":"2024-09-25T09:26:06Z","title":"Exploring Information-Theoretic Metrics Associated with Neural Collapse\n  in Supervised Training","summary":"  In this paper, we utilize information-theoretic metrics like matrix entropy\nand mutual information to analyze supervised learning. We explore the\ninformation content of data representations and classification head weights and\ntheir information interplay during supervised training. Experiments show that\nmatrix entropy cannot solely describe the interaction of the information\ncontent of data representation and classification head weights but it can\neffectively reflect the similarity and clustering behavior of the data.\nInspired by this, we propose a cross-modal alignment loss to improve the\nalignment between the representations of the same class from different\nmodalities. Moreover, in order to assess the interaction of the information\ncontent of data representation and classification head weights more accurately,\nwe utilize new metrics like matrix mutual information ratio (MIR) and matrix\ninformation entropy difference ratio (HDR). Through theory and experiment, we\nshow that HDR and MIR can not only effectively describe the information\ninterplay of supervised training but also improve the performance of supervised\nand semi-supervised learning.\n","authors":["Kun Song","Zhiquan Tan","Bochao Zou","Jiansheng Chen","Huimin Ma","Weiran Huang"],"pdf_url":"https://arxiv.org/pdf/2409.16767v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2406.03999"},{"id":"http://arxiv.org/abs/2409.16765v1","updated":"2024-09-25T09:24:42Z","published":"2024-09-25T09:24:42Z","title":"MaViLS, a Benchmark Dataset for Video-to-Slide Alignment, Assessing\n  Baseline Accuracy with a Multimodal Alignment Algorithm Leveraging Speech,\n  OCR, and Visual Features","summary":"  This paper presents a benchmark dataset for aligning lecture videos with\ncorresponding slides and introduces a novel multimodal algorithm leveraging\nfeatures from speech, text, and images. It achieves an average accuracy of 0.82\nin comparison to SIFT (0.56) while being approximately 11 times faster. Using\ndynamic programming the algorithm tries to determine the optimal slide\nsequence. The results show that penalizing slide transitions increases\naccuracy. Features obtained via optical character recognition (OCR) contribute\nthe most to a high matching accuracy, followed by image features. The findings\nhighlight that audio transcripts alone provide valuable information for\nalignment and are beneficial if OCR data is lacking. Variations in matching\naccuracy across different lectures highlight the challenges associated with\nvideo quality and lecture style. The novel multimodal algorithm demonstrates\nrobustness to some of these challenges, underscoring the potential of the\napproach.\n","authors":["Katharina Anderer","Andreas Reich","Matthias Wölfel"],"pdf_url":"https://arxiv.org/pdf/2409.16765v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16764v1","updated":"2024-09-25T09:22:23Z","published":"2024-09-25T09:22:23Z","title":"Offline and Distributional Reinforcement Learning for Radio Resource\n  Management","summary":"  Reinforcement learning (RL) has proved to have a promising role in future\nintelligent wireless networks. Online RL has been adopted for radio resource\nmanagement (RRM), taking over traditional schemes. However, due to its reliance\non online interaction with the environment, its role becomes limited in\npractical, real-world problems where online interaction is not feasible. In\naddition, traditional RL stands short in front of the uncertainties and risks\nin real-world stochastic environments. In this manner, we propose an offline\nand distributional RL scheme for the RRM problem, enabling offline training\nusing a static dataset without any interaction with the environment and\nconsidering the sources of uncertainties using the distributions of the return.\nSimulation results demonstrate that the proposed scheme outperforms\nconventional resource management models. In addition, it is the only scheme\nthat surpasses online RL and achieves a $16 \\%$ gain over online RL.\n","authors":["Eslam Eldeeb","Hirley Alves"],"pdf_url":"https://arxiv.org/pdf/2409.16764v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16075v2","updated":"2024-09-25T08:59:26Z","published":"2024-09-24T13:21:21Z","title":"Ultra-low latency quantum-inspired machine learning predictors\n  implemented on FPGA","summary":"  Tensor Networks (TNs) are a computational paradigm used for representing\nquantum many-body systems. Recent works have shown how TNs can also be applied\nto perform Machine Learning (ML) tasks, yielding comparable results to standard\nsupervised learning techniques. In this work, we study the use of Tree Tensor\nNetworks (TTNs) in high-frequency real-time applications by exploiting the\nlow-latency hardware of the Field-Programmable Gate Array (FPGA) technology. We\npresent different implementations of TTN classifiers, capable of performing\ninference on classical ML datasets as well as on complex physics data. A\npreparatory analysis of bond dimensions and weight quantization is realized in\nthe training phase, together with entanglement entropy and correlation\nmeasurements, that help setting the choice of the TTN architecture. The\ngenerated TTNs are then deployed on a hardware accelerator; using an FPGA\nintegrated into a server, the inference of the TTN is completely offloaded.\nEventually, a classifier for High Energy Physics (HEP) applications is\nimplemented and executed fully pipelined with sub-microsecond latency.\n","authors":["Lorenzo Borella","Alberto Coppi","Jacopo Pazzini","Andrea Stanco","Marco Trenti","Andrea Triossi","Marco Zanetti"],"pdf_url":"https://arxiv.org/pdf/2409.16075v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14913v2","updated":"2024-09-25T08:52:49Z","published":"2024-09-23T11:08:04Z","title":"Towards a Realistic Long-Term Benchmark for Open-Web Research Agents","summary":"  We present initial results of a forthcoming benchmark for evaluating LLM\nagents on white-collar tasks of economic value. We evaluate agents on\nreal-world \"messy\" open-web research tasks of the type that are routine in\nfinance and consulting. In doing so, we lay the groundwork for an LLM agent\nevaluation suite where good performance directly corresponds to a large\neconomic and societal impact. We built and tested several agent architectures\nwith o1-preview, GPT-4o, Claude-3.5 Sonnet, Llama 3.1 (405b), and GPT-4o-mini.\nOn average, LLM agents powered by Claude-3.5 Sonnet and o1-preview\nsubstantially outperformed agents using GPT-4o, with agents based on Llama 3.1\n(405b) and GPT-4o-mini lagging noticeably behind. Across LLMs, a ReAct\narchitecture with the ability to delegate subtasks to subagents performed best.\nIn addition to quantitative evaluations, we qualitatively assessed the\nperformance of the LLM agents by inspecting their traces and reflecting on\ntheir observations. Our evaluation represents the first in-depth assessment of\nagents' abilities to conduct challenging, economically valuable analyst-style\nresearch on the real open web.\n","authors":["Peter Mühlbacher","Nikos I. Bosse","Lawrence Phillips"],"pdf_url":"https://arxiv.org/pdf/2409.14913v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16735v1","updated":"2024-09-25T08:33:01Z","published":"2024-09-25T08:33:01Z","title":"GB-RVFL: Fusion of Randomized Neural Network and Granular Ball Computing","summary":"  The random vector functional link (RVFL) network is a prominent\nclassification model with strong generalization ability. However, RVFL treats\nall samples uniformly, ignoring whether they are pure or noisy, and its\nscalability is limited due to the need for inverting the entire training\nmatrix. To address these issues, we propose granular ball RVFL (GB-RVFL) model,\nwhich uses granular balls (GBs) as inputs instead of training samples. This\napproach enhances scalability by requiring only the inverse of the GB center\nmatrix and improves robustness against noise and outliers through the coarse\ngranularity of GBs. Furthermore, RVFL overlooks the dataset's geometric\nstructure. To address this, we propose graph embedding GB-RVFL (GE-GB-RVFL)\nmodel, which fuses granular computing and graph embedding (GE) to preserve the\ntopological structure of GBs. The proposed GB-RVFL and GE-GB-RVFL models are\nevaluated on KEEL, UCI, NDC and biomedical datasets, demonstrating superior\nperformance compared to baseline models.\n","authors":["M. Sajid","A. Quadir","M. Tanveer"],"pdf_url":"https://arxiv.org/pdf/2409.16735v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16726v1","updated":"2024-09-25T08:23:37Z","published":"2024-09-25T08:23:37Z","title":"Verified Relative Safety Margins for Neural Network Twins","summary":"  Given two Deep Neural Network (DNN) classifiers with the same input and\noutput domains, our goal is to quantify the robustness of the two networks in\nrelation to each other. Towards this, we introduce the notion of Relative\nSafety Margins (RSMs). Intuitively, given two classes and a common input, RSM\nof one classifier with respect to another reflects the relative margins with\nwhich decisions are made. The proposed notion is relevant in the context of\nseveral applications domains, including to compare a trained network and its\ncorresponding compact network (e.g., pruned, quantized, distilled network). Not\nonly can RSMs establish whether decisions are preserved, but they can also\nquantify their qualities. We also propose a framework to establish safe bounds\non RSM gains or losses given an input and a family of perturbations. We\nevaluate our approach using the MNIST, CIFAR10, and two real-world medical\ndatasets, to show the relevance of our results.\n","authors":["Anahita Baninajjar","Kamran Hosseini","Ahmed Rezine","Amir Aminifar"],"pdf_url":"https://arxiv.org/pdf/2409.16726v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16722v1","updated":"2024-09-25T08:20:24Z","published":"2024-09-25T08:20:24Z","title":"PMSS: Pretrained Matrices Skeleton Selection for LLM Fine-tuning","summary":"  Low-rank adaptation (LoRA) and its variants have recently gained much\ninterest due to their ability to avoid excessive inference costs. However, LoRA\nstill encounters the following challenges: (1) Limitation of low-rank\nassumption; and (2) Its initialization method may be suboptimal. To this end,\nwe propose PMSS(Pre-trained Matrices Skeleton Selection), which enables\nhigh-rank updates with low costs while leveraging semantic and linguistic\ninformation inherent in pre-trained weight. It achieves this by selecting\nskeletons from the pre-trained weight matrix and only learning a small matrix\ninstead. Experiments demonstrate that PMSS outperforms LoRA and other\nfine-tuning methods across tasks with much less trainable parameters. We\ndemonstrate its effectiveness, especially in handling complex tasks such as\nDROP benchmark(+3.4%/+5.9% on LLaMA2-7B/13B) and math\nreasoning(+12.89%/+5.61%/+3.11% on LLaMA2-7B, Mistral-7B and Gemma-7B of\nGSM8K). The code and model will be released soon.\n","authors":["Qibin Wang","Xiaolin Hu","Weikai Xu","Wei Liu","Jian Luan","Bin Wang"],"pdf_url":"https://arxiv.org/pdf/2409.16722v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12246v3","updated":"2024-09-25T08:17:21Z","published":"2024-06-18T03:42:00Z","title":"TroL: Traversal of Layers for Large Language and Vision Models","summary":"  Large language and vision models (LLVMs) have been driven by the\ngeneralization power of large language models (LLMs) and the advent of visual\ninstruction tuning. Along with scaling them up directly, these models enable\nLLVMs to showcase powerful vision language (VL) performances by covering\ndiverse tasks via natural language instructions. However, existing open-source\nLLVMs that perform comparably to closed-source LLVMs such as GPT-4V are often\nconsidered too large (e.g., 26B, 34B, and 110B parameters), having a larger\nnumber of layers. These large models demand costly, high-end resources for both\ntraining and inference. To address this issue, we present a new efficient LLVM\nfamily with 1.8B, 3.8B, and 7B LLM model sizes, Traversal of Layers (TroL),\nwhich enables the reuse of layers in a token-wise manner. This layer traversing\ntechnique simulates the effect of looking back and retracing the answering\nstream while increasing the number of forward propagation layers without\nphysically adding more layers. We demonstrate that TroL employs a simple layer\ntraversing approach yet efficiently outperforms the open-source LLVMs with\nlarger model sizes and rivals the performances of the closed-source LLVMs with\nsubstantial sizes.\n","authors":["Byung-Kwan Lee","Sangyun Chung","Chae Won Kim","Beomchan Park","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2406.12246v3.pdf","comment":"EMNLP 2024. Code is available in https://github.com/ByungKwanLee/TroL"},{"id":"http://arxiv.org/abs/2409.16720v1","updated":"2024-09-25T08:09:52Z","published":"2024-09-25T08:09:52Z","title":"Dashing for the Golden Snitch: Multi-Drone Time-Optimal Motion Planning\n  with Multi-Agent Reinforcement Learning","summary":"  Recent innovations in autonomous drones have facilitated time-optimal flight\nin single-drone configurations and enhanced maneuverability in multi-drone\nsystems through the application of optimal control and learning-based methods.\nHowever, few studies have achieved time-optimal motion planning for multi-drone\nsystems, particularly during highly agile maneuvers or in dynamic scenarios.\nThis paper presents a decentralized policy network for time-optimal multi-drone\nflight using multi-agent reinforcement learning. To strike a balance between\nflight efficiency and collision avoidance, we introduce a soft collision\npenalty inspired by optimization-based methods. By customizing PPO in a\ncentralized training, decentralized execution (CTDE) fashion, we unlock higher\nefficiency and stability in training, while ensuring lightweight\nimplementation. Extensive simulations show that, despite slight performance\ntrade-offs compared to single-drone systems, our multi-drone approach maintains\nnear-time-optimal performance with low collision rates. Real-world experiments\nvalidate our method, with two quadrotors using the same network as simulation\nachieving a maximum speed of 13.65 m/s and a maximum body rate of 13.4 rad/s in\na 5.5 m * 5.5 m * 2.0 m space across various tracks, relying entirely on\nonboard computation.\n","authors":["Xian Wang","Jin Zhou","Yuanli Feng","Jiahao Mei","Jiming Chen","Shuo Li"],"pdf_url":"https://arxiv.org/pdf/2409.16720v1.pdf","comment":"7 pages, 6 figures"},{"id":"http://arxiv.org/abs/2409.16718v1","updated":"2024-09-25T08:07:18Z","published":"2024-09-25T08:07:18Z","title":"Vision-Language Model Fine-Tuning via Simple Parameter-Efficient\n  Modification","summary":"  Recent advances in fine-tuning Vision-Language Models (VLMs) have witnessed\nthe success of prompt tuning and adapter tuning, while the classic model\nfine-tuning on inherent parameters seems to be overlooked. It is believed that\nfine-tuning the parameters of VLMs with few-shot samples corrupts the\npre-trained knowledge since fine-tuning the CLIP model even degrades\nperformance. In this paper, we revisit this viewpoint, and propose a new\nperspective: fine-tuning the specific parameters instead of all will uncover\nthe power of classic model fine-tuning on VLMs. Through our meticulous study,\nwe propose ClipFit, a simple yet effective method to fine-tune CLIP without\nintroducing any overhead of extra parameters. We demonstrate that by only\nfine-tuning the specific bias terms and normalization layers, ClipFit can\nimprove the performance of zero-shot CLIP by 7.27\\% average harmonic mean\naccuracy. Lastly, to understand how fine-tuning in CLIPFit affects the\npre-trained models, we conducted extensive experimental analyses w.r.t. changes\nin internal parameters and representations. We found that low-level text bias\nlayers and the first layer normalization layer change much more than other\nlayers. The code is available at \\url{https://github.com/minglllli/CLIPFit}.\n","authors":["Ming Li","Jike Zhong","Chenxin Li","Liuzhuozheng Li","Nie Lin","Masashi Sugiyama"],"pdf_url":"https://arxiv.org/pdf/2409.16718v1.pdf","comment":"EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2306.05670v2","updated":"2024-09-25T07:51:23Z","published":"2023-06-09T04:59:24Z","title":"One-Shot Machine Unlearning with Mnemonic Code","summary":"  Ethical and privacy issues inherent in artificial intelligence (AI)\napplications have been a growing concern with the rapid spread of deep\nlearning. Machine unlearning (MU) is the research area that addresses these\nissues by making a trained AI model forget about undesirable training data.\nUnfortunately, most existing MU methods incur significant time and\ncomputational costs for forgetting. Therefore, it is often difficult to apply\nthese methods to practical datasets and sophisticated architectures, e.g.,\nImageNet and Transformer. To tackle this problem, we propose a lightweight and\neffective MU method. Our method identifies the model parameters sensitive to\nthe forgetting targets and adds perturbation to such model parameters. We\nidentify the sensitive parameters by calculating the Fisher Information Matrix\n(FIM). This approach does not require time-consuming additional training for\nforgetting. In addition, we introduce class-specific random signals called\nmnemonic code to reduce the cost of FIM calculation, which generally requires\nthe entire training data and incurs significant computational costs. In our\nmethod, we train the model with mnemonic code; when forgetting, we use a small\nnumber of mnemonic codes to calculate the FIM and get the effective\nperturbation for forgetting. Comprehensive experiments demonstrate that our\nmethod is faster and better at forgetting than existing MU methods.\nFurthermore, we show that our method can scale to more practical datasets and\nsophisticated architectures.\n","authors":["Tomoya Yamashita","Masanori Yamada","Takashi Shibata"],"pdf_url":"https://arxiv.org/pdf/2306.05670v2.pdf","comment":"24 pages, welcome coments"},{"id":"http://arxiv.org/abs/2409.16697v1","updated":"2024-09-25T07:43:48Z","published":"2024-09-25T07:43:48Z","title":"Numerical Approximation Capacity of Neural Networks with Bounded\n  Parameters: Do Limits Exist, and How Can They Be Measured?","summary":"  The Universal Approximation Theorem posits that neural networks can\ntheoretically possess unlimited approximation capacity with a suitable\nactivation function and a freely chosen or trained set of parameters. However,\na more practical scenario arises when these neural parameters, especially the\nnonlinear weights and biases, are bounded. This leads us to question:\n\\textbf{Does the approximation capacity of a neural network remain universal,\nor does it have a limit when the parameters are practically bounded? And if it\nhas a limit, how can it be measured?}\n  Our theoretical study indicates that while universal approximation is\ntheoretically feasible, in practical numerical scenarios, Deep Neural Networks\n(DNNs) with any analytic activation functions (such as Tanh and Sigmoid) can\nonly be approximated by a finite-dimensional vector space under a bounded\nnonlinear parameter space (NP space), whether in a continuous or discrete\nsense. Based on this study, we introduce the concepts of \\textit{$\\epsilon$\nouter measure} and \\textit{Numerical Span Dimension (NSdim)} to quantify the\napproximation capacity limit of a family of networks both theoretically and\npractically.\n  Furthermore, drawing on our new theoretical study and adopting a fresh\nperspective, we strive to understand the relationship between back-propagation\nneural networks and random parameter networks (such as the Extreme Learning\nMachine (ELM)) with both finite and infinite width. We also aim to provide\nfresh insights into regularization, the trade-off between width and depth,\nparameter space, width redundancy, condensation, and other related important\nissues.\n","authors":["Li Liu","Tengchao Yu","Heng Yong"],"pdf_url":"https://arxiv.org/pdf/2409.16697v1.pdf","comment":"Universal Approximation; Bounded Weights; Analytic Function;\n  Numerical Span Dimension; Infinite Width Neural Network}"},{"id":"http://arxiv.org/abs/2310.00646v2","updated":"2024-09-25T07:40:20Z","published":"2023-10-01T12:02:57Z","title":"Source Attribution for Large Language Model-Generated Data","summary":"  The impressive performances of Large Language Models (LLMs) and their immense\npotential for commercialization have given rise to serious concerns over the\nIntellectual Property (IP) of their training data. In particular, the synthetic\ntexts generated by LLMs may infringe the IP of the data being used to train the\nLLMs. To this end, it is imperative to be able to perform source attribution by\nidentifying the data provider who contributed to the generation of a synthetic\ntext by an LLM. In this paper, we show that this problem can be tackled by\nwatermarking, i.e., by enabling an LLM to generate synthetic texts with\nembedded watermarks that contain information about their source(s). We identify\nthe key properties of such watermarking frameworks (e.g., source attribution\naccuracy, robustness against adversaries), and propose a source attribution\nframework that satisfies these key properties due to our algorithmic designs.\nOur framework enables an LLM to learn an accurate mapping from the generated\ntexts to data providers, which sets the foundation for effective source\nattribution. Extensive empirical evaluations show that our framework achieves\neffective source attribution.\n","authors":["Jingtan Wang","Xinyang Lu","Zitong Zhao","Zhongxiang Dai","Chuan-Sheng Foo","See-Kiong Ng","Bryan Kian Hsiang Low"],"pdf_url":"https://arxiv.org/pdf/2310.00646v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2107.13289v3","updated":"2024-09-25T07:38:43Z","published":"2021-07-28T11:33:18Z","title":"The loss landscape of deep linear neural networks: a second-order\n  analysis","summary":"  We study the optimization landscape of deep linear neural networks with the\nsquare loss. It is known that, under weak assumptions, there are no spurious\nlocal minima and no local maxima. However, the existence and diversity of\nnon-strict saddle points, which can play a role in first-order algorithms'\ndynamics, have only been lightly studied. We go a step further with a full\nanalysis of the optimization landscape at order 2. We characterize, among all\ncritical points, which are global minimizers, strict saddle points, and\nnon-strict saddle points. We enumerate all the associated critical values. The\ncharacterization is simple, involves conditions on the ranks of partial matrix\nproducts, and sheds some light on global convergence or implicit regularization\nthat have been proved or observed when optimizing linear neural networks. In\npassing, we provide an explicit parameterization of the set of all global\nminimizers and exhibit large sets of strict and non-strict saddle points.\n","authors":["El Mehdi Achour","François Malgouyres","Sébastien Gerchinovitz"],"pdf_url":"https://arxiv.org/pdf/2107.13289v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16694v1","updated":"2024-09-25T07:38:02Z","published":"2024-09-25T07:38:02Z","title":"A Survey of Low-bit Large Language Models: Basics, Systems, and\n  Algorithms","summary":"  Large language models (LLMs) have achieved remarkable advancements in natural\nlanguage processing, showcasing exceptional performance across various tasks.\nHowever, the expensive memory and computational requirements present\nsignificant challenges for their practical deployment. Low-bit quantization has\nemerged as a critical approach to mitigate these challenges by reducing the\nbit-width of model parameters, activations, and gradients, thus decreasing\nmemory usage and computational demands. This paper presents a comprehensive\nsurvey of low-bit quantization methods tailored for LLMs, covering the\nfundamental principles, system implementations, and algorithmic strategies. An\noverview of basic concepts and new data formats specific to low-bit LLMs is\nfirst introduced, followed by a review of frameworks and systems that\nfacilitate low-bit LLMs across various hardware platforms. Then, we categorize\nand analyze techniques and toolkits for efficient low-bit training and\ninference of LLMs. Finally, we conclude with a discussion of future trends and\npotential advancements of low-bit LLMs. Our systematic overview from basic,\nsystem, and algorithm perspectives can offer valuable insights and guidelines\nfor future works to enhance the efficiency and applicability of LLMs through\nlow-bit quantization.\n","authors":["Ruihao Gong","Yifu Ding","Zining Wang","Chengtao Lv","Xingyu Zheng","Jinyang Du","Haotong Qin","Jinyang Guo","Michele Magno","Xianglong Liu"],"pdf_url":"https://arxiv.org/pdf/2409.16694v1.pdf","comment":"Ruihao Gong leads the overall organization of the survey, with Yifu\n  Ding and Jinyang Du contributing to Sections 2 and 3. Xingyu Zheng is\n  responsible for authoring Section 4, while Chengtao Lv and Zining Wang\n  collaborate on Section 5. Haotong Qin, Jinyang Guo, Michele Magno, and\n  Xianglong Liu provide guidance during the whole process and assist in\n  refining the final manuscript"},{"id":"http://arxiv.org/abs/2409.16689v1","updated":"2024-09-25T07:24:43Z","published":"2024-09-25T07:24:43Z","title":"Layout-Corrector: Alleviating Layout Sticking Phenomenon in Discrete\n  Diffusion Model","summary":"  Layout generation is a task to synthesize a harmonious layout with elements\ncharacterized by attributes such as category, position, and size. Human\ndesigners experiment with the placement and modification of elements to create\naesthetic layouts, however, we observed that current discrete diffusion models\n(DDMs) struggle to correct inharmonious layouts after they have been generated.\nIn this paper, we first provide novel insights into layout sticking phenomenon\nin DDMs and then propose a simple yet effective layout-assessment module\nLayout-Corrector, which works in conjunction with existing DDMs to address the\nlayout sticking problem. We present a learning-based module capable of\nidentifying inharmonious elements within layouts, considering overall layout\nharmony characterized by complex composition. During the generation process,\nLayout-Corrector evaluates the correctness of each token in the generated\nlayout, reinitializing those with low scores to the ungenerated state. The DDM\nthen uses the high-scored tokens as clues to regenerate the harmonized tokens.\nLayout-Corrector, tested on common benchmarks, consistently boosts\nlayout-generation performance when in conjunction with various state-of-the-art\nDDMs. Furthermore, our extensive analysis demonstrates that the\nLayout-Corrector (1) successfully identifies erroneous tokens, (2) facilitates\ncontrol over the fidelity-diversity trade-off, and (3) significantly mitigates\nthe performance drop associated with fast sampling.\n","authors":["Shoma Iwai","Atsuki Osanai","Shunsuke Kitada","Shinichiro Omachi"],"pdf_url":"https://arxiv.org/pdf/2409.16689v1.pdf","comment":"Accepted by ECCV2024, Project Page:\n  https://iwa-shi.github.io/Layout-Corrector-Project-Page/"},{"id":"http://arxiv.org/abs/2409.16684v1","updated":"2024-09-25T07:20:59Z","published":"2024-09-25T07:20:59Z","title":"Erase then Rectify: A Training-Free Parameter Editing Approach for\n  Cost-Effective Graph Unlearning","summary":"  Graph unlearning, which aims to eliminate the influence of specific nodes,\nedges, or attributes from a trained Graph Neural Network (GNN), is essential in\napplications where privacy, bias, or data obsolescence is a concern. However,\nexisting graph unlearning techniques often necessitate additional training on\nthe remaining data, leading to significant computational costs, particularly\nwith large-scale graphs. To address these challenges, we propose a two-stage\ntraining-free approach, Erase then Rectify (ETR), designed for efficient and\nscalable graph unlearning while preserving the model utility. Specifically, we\nfirst build a theoretical foundation showing that masking parameters critical\nfor unlearned samples enables effective unlearning. Building on this insight,\nthe Erase stage strategically edits model parameters to eliminate the impact of\nunlearned samples and their propagated influence on intercorrelated nodes. To\nfurther ensure the GNN's utility, the Rectify stage devises a gradient\napproximation method to estimate the model's gradient on the remaining dataset,\nwhich is then used to enhance model performance. Overall, ETR achieves graph\nunlearning without additional training or full training data access,\nsignificantly reducing computational overhead and preserving data privacy.\nExtensive experiments on seven public datasets demonstrate the consistent\nsuperiority of ETR in model utility, unlearning efficiency, and unlearning\neffectiveness, establishing it as a promising solution for real-world graph\nunlearning challenges.\n","authors":["Zhe-Rui Yang","Jindong Han","Chang-Dong Wang","Hao Liu"],"pdf_url":"https://arxiv.org/pdf/2409.16684v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2409.16678v1","updated":"2024-09-25T07:09:04Z","published":"2024-09-25T07:09:04Z","title":"TSBP: Improving Object Detection in Histology Images via Test-time\n  Self-guided Bounding-box Propagation","summary":"  A global threshold (e.g., 0.5) is often applied to determine which bounding\nboxes should be included in the final results for an object detection task. A\nhigher threshold reduces false positives but may result in missing a\nsignificant portion of true positives. A lower threshold can increase detection\nrecall but may also result in more false positives. Because of this, using a\npreset global threshold (e.g., 0.5) applied to all the bounding box candidates\nmay lead to suboptimal solutions. In this paper, we propose a Test-time\nSelf-guided Bounding-box Propagation (TSBP) method, leveraging Earth Mover's\nDistance (EMD) to enhance object detection in histology images. TSBP utilizes\nbounding boxes with high confidence to influence those with low confidence,\nleveraging visual similarities between them. This propagation mechanism enables\nbounding boxes to be selected in a controllable, explainable, and robust\nmanner, which surpasses the effectiveness of using simple thresholds and\nuncertainty calibration methods. Importantly, TSBP does not necessitate\nadditional labeled samples for model training or parameter estimation, unlike\ncalibration methods. We conduct experiments on gland detection and cell\ndetection tasks in histology images. The results show that our proposed TSBP\nsignificantly improves detection outcomes when working in conjunction with\nstate-of-the-art deep learning-based detection networks. Compared to other\nmethods such as uncertainty calibration, TSBP yields more robust and accurate\nobject detection predictions while using no additional labeled samples. The\ncode is available at https://github.com/jwhgdeu/TSBP.\n","authors":["Tingting Yang","Liang Xiao","Yizhe Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.16678v1.pdf","comment":"MICCAI 2024"},{"id":"http://arxiv.org/abs/2310.17032v3","updated":"2024-09-25T07:06:47Z","published":"2023-10-25T22:19:05Z","title":"Quantum Long Short-Term Memory (QLSTM) vs Classical LSTM in Time Series\n  Forecasting: A Comparative Study in Solar Power Forecasting","summary":"  Accurate solar power forecasting is pivotal for the global transition towards\nsustainable energy systems. This study conducts a meticulous comparison between\nQuantum Long Short-Term Memory (QLSTM) and classical Long Short-Term Memory\n(LSTM) models for solar power production forecasting. The primary objective is\nto evaluate the potential advantages of QLSTMs, leveraging their exponential\nrepresentational capabilities, in capturing the intricate spatiotemporal\npatterns inherent in renewable energy data. Through controlled experiments on\nreal-world photovoltaic datasets, our findings reveal promising improvements\noffered by QLSTMs, including accelerated training convergence and substantially\nreduced test loss within the initial epoch compared to classical LSTMs. These\nempirical results demonstrate QLSTM's potential to swiftly assimilate complex\ntime series relationships, enabled by quantum phenomena like superposition.\nHowever, realizing QLSTM's full capabilities necessitates further research into\nmodel validation across diverse conditions, systematic hyperparameter\noptimization, hardware noise resilience, and applications to correlated\nrenewable forecasting problems. With continued progress, quantum machine\nlearning can offer a paradigm shift in renewable energy time series prediction,\npotentially ushering in an era of unprecedented accuracy and reliability in\nsolar power forecasting worldwide. This pioneering work provides initial\nevidence substantiating quantum advantages over classical LSTM models while\nacknowledging present limitations. Through rigorous benchmarking grounded in\nreal-world data, our study illustrates a promising trajectory for quantum\nlearning in renewable forecasting.\n","authors":["Saad Zafar Khan","Nazeefa Muzammil","Salman Ghafoor","Haibat Khan","Syed Mohammad Hasan Zaidi","Abdulah Jeza Aljohani","Imran Aziz"],"pdf_url":"https://arxiv.org/pdf/2310.17032v3.pdf","comment":"21 pages, 9 figures"},{"id":"http://arxiv.org/abs/2409.16675v1","updated":"2024-09-25T07:06:14Z","published":"2024-09-25T07:06:14Z","title":"CryptoTrain: Fast Secure Training on Encrypted Datase","summary":"  Secure training, while protecting the confidentiality of both data and model\nweights, typically incurs significant training overhead. Traditional Fully\nHomomorphic Encryption (FHE)-based non-inter-active training models are heavily\nburdened by computationally demanding bootstrapping. To develop an efficient\nsecure training system, we established a foundational framework, CryptoTrain-B,\nutilizing a hybrid cryptographic protocol that merges FHE with Oblivious\nTransfer (OT) for handling linear and non-linear operations, respectively. This\nintegration eliminates the need for costly bootstrapping. Although\nCryptoTrain-B sets a new baseline in performance, reducing its training\noverhead remains essential. We found that ciphertext-ciphertext multiplication\n(CCMul) is a critical bottleneck in operations involving encrypted inputs and\nmodels. Our solution, the CCMul-Precompute technique, involves precomputing\nCCMul offline and resorting to the less resource-intensive ciphertext-plaintext\nmultiplication (CPMul) during private training. Furthermore, conventional\npolynomial convolution in FHE systems tends to encode irrelevant and redundant\nvalues into polynomial slots, necessitating additional polynomials and\nciphertexts for input representation and leading to extra multiplications.\nAddressing this, we introduce correlated polynomial convolution, which encodes\nonly related input values into polynomials, thus drastically reducing the\nnumber of computations and overheads. By integrating CCMul-Precompute and\ncorrelated polynomial convolution into CryptoTrain-B, we facilitate a rapid and\nefficient secure training framework, CryptoTrain. Extensive experiments\ndemonstrate that CryptoTrain achieves a ~5.3X training time reduction compared\nto prior methods.\n","authors":["Jiaqi Xue","Yancheng Zhang","Yanshan Wang","Xueqiang Wang","Hao Zheng","Qian Lou"],"pdf_url":"https://arxiv.org/pdf/2409.16675v1.pdf","comment":"Accepted by CCS-LAMPS 2024"},{"id":"http://arxiv.org/abs/2409.16673v1","updated":"2024-09-25T07:05:44Z","published":"2024-09-25T07:05:44Z","title":"SWE2: SubWord Enriched and Significant Word Emphasized Framework for\n  Hate Speech Detection","summary":"  Hate speech detection on online social networks has become one of the\nemerging hot topics in recent years. With the broad spread and fast propagation\nspeed across online social networks, hate speech makes significant impacts on\nsociety by increasing prejudice and hurting people. Therefore, there are\naroused attention and concern from both industry and academia. In this paper,\nwe address the hate speech problem and propose a novel hate speech detection\nframework called SWE2, which only relies on the content of messages and\nautomatically identifies hate speech. In particular, our framework exploits\nboth word-level semantic information and sub-word knowledge. It is intuitively\npersuasive and also practically performs well under a situation with/without\ncharacter-level adversarial attack. Experimental results show that our proposed\nmodel achieves 0.975 accuracy and 0.953 macro F1, outperforming 7\nstate-of-the-art baselines under no adversarial attack. Our model robustly and\nsignificantly performed well under extreme adversarial attack (manipulation of\n50% messages), achieving 0.967 accuracy and 0.934 macro F1.\n","authors":["Guanyi Mou","Pengyi Ye","Kyumin Lee"],"pdf_url":"https://arxiv.org/pdf/2409.16673v1.pdf","comment":"Published in CIKM 2020"},{"id":"http://arxiv.org/abs/2409.15868v2","updated":"2024-09-25T07:03:05Z","published":"2024-09-24T08:41:26Z","title":"Privacy Evaluation Benchmarks for NLP Models","summary":"  By inducing privacy attacks on NLP models, attackers can obtain sensitive\ninformation such as training data and model parameters, etc. Although\nresearchers have studied, in-depth, several kinds of attacks in NLP models,\nthey are non-systematic analyses. It lacks a comprehensive understanding of the\nimpact caused by the attacks. For example, we must consider which scenarios can\napply to which attacks, what the common factors are that affect the performance\nof different attacks, the nature of the relationships between different\nattacks, and the influence of various datasets and models on the effectiveness\nof the attacks, etc. Therefore, we need a benchmark to holistically assess the\nprivacy risks faced by NLP models. In this paper, we present a privacy attack\nand defense evaluation benchmark in the field of NLP, which includes the\nconventional/small models and large language models (LLMs). This benchmark\nsupports a variety of models, datasets, and protocols, along with standardized\nmodules for comprehensive evaluation of attacks and defense strategies. Based\non the above framework, we present a study on the association between auxiliary\ndata from different domains and the strength of privacy attacks. And we provide\nan improved attack method in this scenario with the help of Knowledge\nDistillation (KD). Furthermore, we propose a chained framework for privacy\nattacks. Allowing a practitioner to chain multiple attacks to achieve a\nhigher-level attack objective. Based on this, we provide some defense and\nenhanced attack strategies. The code for reproducing the results can be found\nat https://github.com/user2311717757/nlp_doctor.\n","authors":["Wei Huang","Yinggui Wang","Cen Chen"],"pdf_url":"https://arxiv.org/pdf/2409.15868v2.pdf","comment":"Needs further optimization"},{"id":"http://arxiv.org/abs/2409.16671v1","updated":"2024-09-25T06:57:43Z","published":"2024-09-25T06:57:43Z","title":"Wildlife Product Trading in Online Social Networks: A Case Study on\n  Ivory-Related Product Sales Promotion Posts","summary":"  Wildlife trafficking (WLT) has emerged as a global issue, with traffickers\nexpanding their operations from offline to online platforms, utilizing\ne-commerce websites and social networks to enhance their illicit trade. This\npaper addresses the challenge of detecting and recognizing wildlife product\nsales promotion behaviors in online social networks, a crucial aspect in\ncombating these environmentally harmful activities. To counter these\nenvironmentally damaging illegal operations, in this research, we focus on\nwildlife product sales promotion behaviors in online social networks.\nSpecifically, 1) A scalable dataset related to wildlife product trading is\ncollected using a network-based approach. This dataset is labeled through a\nhuman-in-the-loop machine learning process, distinguishing positive class\nsamples containing wildlife product selling posts and hard-negatives\nrepresenting normal posts misclassified as potential WLT posts, subsequently\ncorrected by human annotators. 2) We benchmark the machine learning results on\nthe proposed dataset and build a practical framework that automatically\nidentifies suspicious wildlife selling posts and accounts, sufficiently\nleveraging the multi-modal nature of online social networks. 3) This research\ndelves into an in-depth analysis of trading posts, shedding light on the\nsystematic and organized selling behaviors prevalent in the current landscape.\nWe provide detailed insights into the nature of these behaviors, contributing\nvaluable information for understanding and countering illegal wildlife product\ntrading.\n","authors":["Guanyi Mou","Yun Yue","Kyumin Lee","Ziming Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.16671v1.pdf","comment":"ICWSM 2024"},{"id":"http://arxiv.org/abs/2409.16670v1","updated":"2024-09-25T06:57:42Z","published":"2024-09-25T06:57:42Z","title":"GraphLoRA: Structure-Aware Contrastive Low-Rank Adaptation for\n  Cross-Graph Transfer Learning","summary":"  Graph Neural Networks (GNNs) have demonstrated remarkable proficiency in\nhandling a range of graph analytical tasks across various domains, such as\ne-commerce and social networks. Despite their versatility, GNNs face\nsignificant challenges in transferability, limiting their utility in real-world\napplications. Existing research in GNN transfer learning overlooks\ndiscrepancies in distribution among various graph datasets, facing challenges\nwhen transferring across different distributions. How to effectively adopt a\nwell-trained GNN to new graphs with varying feature and structural\ndistributions remains an under-explored problem. Taking inspiration from the\nsuccess of Low-Rank Adaptation (LoRA) in adapting large language models to\nvarious domains, we propose GraphLoRA, an effective and parameter-efficient\nmethod for transferring well-trained GNNs to diverse graph domains.\nSpecifically, we first propose a Structure-aware Maximum Mean Discrepancy\n(SMMD) to align divergent node feature distributions across source and target\ngraphs. Moreover, we introduce low-rank adaptation by injecting a small\ntrainable GNN alongside the pre-trained one, effectively bridging structural\ndistribution gaps while mitigating the catastrophic forgetting. Additionally, a\nstructure-aware regularization objective is proposed to enhance the\nadaptability of the pre-trained GNN to target graph with scarce supervision\nlabels. Extensive experiments on six real-world datasets demonstrate the\neffectiveness of GraphLoRA against eleven baselines by tuning only 20% of\nparameters, even across disparate graph domains. The code is available at\nhttps://anonymous.4open.science/r/GraphLoRA.\n","authors":["Zhe-Rui Yang","Jindong Han","Chang-Dong Wang","Hao Liu"],"pdf_url":"https://arxiv.org/pdf/2409.16670v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2409.02572v3","updated":"2024-09-25T06:50:29Z","published":"2024-09-04T09:46:33Z","title":"Advancing Cyber Incident Timeline Analysis Through Rule Based AI and\n  Large Language Models","summary":"  Timeline Analysis (TA) plays a crucial role in Timeline Forensics (TF) within\nthe field of Digital Forensics (DF). It focuses on examining and analyzing\ntime-based digital artefacts, such as timestamps derived from event logs, file\nmetadata, and other relevant data, to correlate events linked to cyber\nincidents and reconstruct their chronological sequence. Traditional tools often\nstruggle to efficiently handle the large volume and variety of data generated\nduring DF investigations and Incident Response (IR) processes. This paper\nintroduces a novel framework, GenDFIR, which combines Rule-Based Artificial\nIntelligence (R-BAI) algorithms with Large Language Models (LLMs) to enhance\nand automate the TA process. The proposed approach consists of two key stages:\n(1) R-BAI is used to identify and select anomalous digital artefacts based on\npredefined rules. (2) The selected artefacts are then transformed into\nembeddings for processing by an LLM with the assistance of a\nRetrieval-Augmented Generation (RAG) agent. The LLM uses its capabilities to\nperform automated TA on the artefacts and predict potential incident outcomes.\nTo validate the framework, we evaluated its performance, efficiency, and\nreliability. Several metrics were applied to simulated cyber incident\nscenarios, which were presented as forensic case documents. Our findings\ndemonstrate the significant potential of integrating R-BAI and LLMs for TA.\nThis innovative approach underscores the power of Generative AI (GenAI),\nparticularly LLMs, and opens up new possibilities for advanced threat detection\nand incident reconstruction, marking a significant advancement in the field.\n","authors":["Fatma Yasmine Loumachi","Mohamed Chahine Ghanem"],"pdf_url":"https://arxiv.org/pdf/2409.02572v3.pdf","comment":"22 pages V3.1"},{"id":"http://arxiv.org/abs/2409.16663v1","updated":"2024-09-25T06:48:25Z","published":"2024-09-25T06:48:25Z","title":"Mitigating Covariate Shift in Imitation Learning for Autonomous Vehicles\n  Using Latent Space Generative World Models","summary":"  We propose the use of latent space generative world models to address the\ncovariate shift problem in autonomous driving. A world model is a neural\nnetwork capable of predicting an agent's next state given past states and\nactions. By leveraging a world model during training, the driving policy\neffectively mitigates covariate shift without requiring an excessive amount of\ntraining data. During end-to-end training, our policy learns how to recover\nfrom errors by aligning with states observed in human demonstrations, so that\nat runtime it can recover from perturbations outside the training distribution.\nAdditionally, we introduce a novel transformer-based perception encoder that\nemploys multi-view cross-attention and a learned scene query. We present\nqualitative and quantitative results, demonstrating significant improvements\nupon prior state of the art in closed-loop testing in the CARLA simulator, as\nwell as showing the ability to handle perturbations in both CARLA and NVIDIA's\nDRIVE Sim.\n","authors":["Alexander Popov","Alperen Degirmenci","David Wehr","Shashank Hegde","Ryan Oldja","Alexey Kamenev","Bertrand Douillard","David Nistér","Urs Muller","Ruchi Bhargava","Stan Birchfield","Nikolai Smolyanskiy"],"pdf_url":"https://arxiv.org/pdf/2409.16663v1.pdf","comment":"7 pages, 6 figures, for ICRA 2025 conference, for associated video\n  file, see https://youtu.be/9FpDFD9aiFU"},{"id":"http://arxiv.org/abs/2409.15355v2","updated":"2024-09-25T06:46:42Z","published":"2024-09-14T02:34:26Z","title":"Block-Attention for Efficient RAG","summary":"  We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Unlike existing works that encodes the whole context, its main idea\nlies in dividing the retrieved documents into blocks, where each block\ncalculates key-value (KV) states independently except for the final block. In\nRAG scenarios, by defining each passage as a block, Block-Attention enables us\nto pre-compute the KV states for all passages and cache them in memory,\nsignificantly reducing the latency and the computation cost during inference.\nThe implementation involves block segmentation, positional encoding\ncalculation, and fine-tuning the LLM to adapt to the Block-Attention mechanism.\nExperiments on four RAG benchmarks demonstrate that after block fine-tuning,\nthe Block Attention model can achieve performance comparable to (68.4\\% vs\n67.9\\% on Llama3) or even better (62.8\\% vs 59.6\\% on Mistral) than\nself-attention models. Notably, Block-Attention reduces the TTFT (the time to\nfirst token) and FLOPs (floating point operations) to a very low level. It only\ntakes 45 ms to output the first token for an input sequence with a total length\nof 32K. Compared with the self-attention model, the time consumption and\ncorresponding FLOPs are reduced by 98.7\\% and 99.8\\%, respectively.\n","authors":["East Sun","Yan Wang","Lan Tian"],"pdf_url":"https://arxiv.org/pdf/2409.15355v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.20524v2","updated":"2024-09-25T06:34:42Z","published":"2023-10-31T15:04:53Z","title":"Group-Feature (Sensor) Selection With Controlled Redundancy Using Neural\n  Networks","summary":"  In this paper, we present a novel embedded feature selection method based on\na Multi-layer Perceptron (MLP) network and generalize it for group-feature or\nsensor selection problems, which can control the level of redundancy among the\nselected features or groups. Additionally, we have generalized the group lasso\npenalty for feature selection to encompass a mechanism for selecting valuable\ngroup features while simultaneously maintaining a control over redundancy. We\nestablish the monotonicity and convergence of the proposed algorithm, with a\nsmoothed version of the penalty terms, under suitable assumptions. Experimental\nresults on several benchmark datasets demonstrate the promising performance of\nthe proposed methodology for both feature selection and group feature selection\nover some state-of-the-art methods.\n","authors":["Aytijhya Saha","Nikhil R. Pal"],"pdf_url":"https://arxiv.org/pdf/2310.20524v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.00781v3","updated":"2024-09-25T06:31:09Z","published":"2024-02-18T06:07:17Z","title":"ChatDiet: Empowering Personalized Nutrition-Oriented Food Recommender\n  Chatbots through an LLM-Augmented Framework","summary":"  The profound impact of food on health necessitates advanced\nnutrition-oriented food recommendation services. Conventional methods often\nlack the crucial elements of personalization, explainability, and\ninteractivity. While Large Language Models (LLMs) bring interpretability and\nexplainability, their standalone use falls short of achieving true\npersonalization. In this paper, we introduce ChatDiet, a novel LLM-powered\nframework designed specifically for personalized nutrition-oriented food\nrecommendation chatbots. ChatDiet integrates personal and population models,\ncomplemented by an orchestrator, to seamlessly retrieve and process pertinent\ninformation. The personal model leverages causal discovery and inference\ntechniques to assess personalized nutritional effects for a specific user,\nwhereas the population model provides generalized information on food\nnutritional content. The orchestrator retrieves, synergizes and delivers the\noutput of both models to the LLM, providing tailored food recommendations\ndesigned to support targeted health outcomes. The result is a dynamic delivery\nof personalized and explainable food recommendations, tailored to individual\nuser preferences. Our evaluation of ChatDiet includes a compelling case study,\nwhere we establish a causal personal model to estimate individual nutrition\neffects. Our assessments, including a food recommendation test showcasing a\n92\\% effectiveness rate, coupled with illustrative dialogue examples,\nunderscore ChatDiet's strengths in explainability, personalization, and\ninteractivity.\n","authors":["Zhongqi Yang","Elahe Khatibi","Nitish Nagesh","Mahyar Abbasian","Iman Azimi","Ramesh Jain","Amir M. Rahmani"],"pdf_url":"https://arxiv.org/pdf/2403.00781v3.pdf","comment":"Published on Smart Health"},{"id":"http://arxiv.org/abs/2409.16653v1","updated":"2024-09-25T06:16:45Z","published":"2024-09-25T06:16:45Z","title":"The Credibility Transformer","summary":"  Inspired by the large success of Transformers in Large Language Models, these\narchitectures are increasingly applied to tabular data. This is achieved by\nembedding tabular data into low-dimensional Euclidean spaces resulting in\nsimilar structures as time-series data. We introduce a novel credibility\nmechanism to this Transformer architecture. This credibility mechanism is based\non a special token that should be seen as an encoder that consists of a\ncredibility weighted average of prior information and observation based\ninformation. We demonstrate that this novel credibility mechanism is very\nbeneficial to stabilize training, and our Credibility Transformer leads to\npredictive models that are superior to state-of-the-art deep learning models.\n","authors":["Ronald Richman","Salvatore Scognamiglio","Mario V. Wüthrich"],"pdf_url":"https://arxiv.org/pdf/2409.16653v1.pdf","comment":"30 pages"},{"id":"http://arxiv.org/abs/2407.21439v2","updated":"2024-09-25T06:14:03Z","published":"2024-07-31T08:43:17Z","title":"MLLM Is a Strong Reranker: Advancing Multimodal Retrieval-augmented\n  Generation via Knowledge-enhanced Reranking and Noise-injected Training","summary":"  Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in processing and generating content across multiple data\nmodalities. However, a significant drawback of MLLMs is their reliance on\nstatic training data, leading to outdated information and limited contextual\nawareness. This static nature hampers their ability to provide accurate and\nup-to-date responses, particularly in dynamic or rapidly evolving contexts.\nThough integrating Multimodal Retrieval-augmented Generation (Multimodal RAG)\noffers a promising solution, the system would inevitably encounter the\nmulti-granularity noisy correspondence (MNC) problem, which hinders accurate\nretrieval and generation. In this work, we propose RagVL, a novel framework\nwith knowledge-enhanced reranking and noise-injected training, to address these\nlimitations. We instruction-tune the MLLM with a simple yet effective\ninstruction template to induce its ranking ability and serve it as a reranker\nto precisely filter the top-k retrieved images. For generation, we inject\nvisual noise during training at the data and token levels to enhance the\ngenerator's robustness. Extensive experiments on the subsets of two datasets\nthat require retrieving and reasoning over images to answer a given query\nverify the effectiveness of our method. Code and models are available at\nhttps://github.com/IDEA-FinAI/RagVL.\n","authors":["Zhanpeng Chen","Chengjin Xu","Yiyan Qi","Jian Guo"],"pdf_url":"https://arxiv.org/pdf/2407.21439v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16651v1","updated":"2024-09-25T06:08:35Z","published":"2024-09-25T06:08:35Z","title":"Learning Representation for Multitask learning through Self Supervised\n  Auxiliary learning","summary":"  Multi-task learning is a popular machine learning approach that enables\nsimultaneous learning of multiple related tasks, improving algorithmic\nefficiency and effectiveness. In the hard parameter sharing approach, an\nencoder shared through multiple tasks generates data representations passed to\ntask-specific predictors. Therefore, it is crucial to have a shared encoder\nthat provides decent representations for every and each task. However, despite\nrecent advances in multi-task learning, the question of how to improve the\nquality of representations generated by the shared encoder remains open. To\naddress this gap, we propose a novel approach called Dummy Gradient norm\nRegularization that aims to improve the universality of the representations\ngenerated by the shared encoder. Specifically, the method decreases the norm of\nthe gradient of the loss function with repect to dummy task-specific predictors\nto improve the universality of the shared encoder's representations. Through\nexperiments on multiple multi-task learning benchmark datasets, we demonstrate\nthat DGR effectively improves the quality of the shared representations,\nleading to better multi-task prediction performances. Applied to various\nclassifiers, the shared representations generated by DGR also show superior\nperformance compared to existing multi-task learning methods. Moreover, our\napproach takes advantage of computational efficiency due to its simplicity. The\nsimplicity also allows us to seamlessly integrate DGR with the existing\nmulti-task learning algorithms.\n","authors":["Seokwon Shin","Hyungrok Do","Youngdoo Son"],"pdf_url":"https://arxiv.org/pdf/2409.16651v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16647v1","updated":"2024-09-25T06:04:03Z","published":"2024-09-25T06:04:03Z","title":"Domain-Independent Automatic Generation of Descriptive Texts for\n  Time-Series Data","summary":"  Due to scarcity of time-series data annotated with descriptive texts,\ntraining a model to generate descriptive texts for time-series data is\nchallenging. In this study, we propose a method to systematically generate\ndomain-independent descriptive texts from time-series data. We identify two\ndistinct approaches for creating pairs of time-series data and descriptive\ntexts: the forward approach and the backward approach. By implementing the\nnovel backward approach, we create the Temporal Automated Captions for\nObservations (TACO) dataset. Experimental results demonstrate that a\ncontrastive learning based model trained using the TACO dataset is capable of\ngenerating descriptive texts for time-series data in novel domains.\n","authors":["Kota Dohi","Aoi Ito","Harsh Purohit","Tomoya Nishida","Takashi Endo","Yohei Kawaguchi"],"pdf_url":"https://arxiv.org/pdf/2409.16647v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15955v2","updated":"2024-09-25T06:01:08Z","published":"2024-09-24T10:36:40Z","title":"Historical Trajectory Assisted Zeroth-Order Federated Optimization","summary":"  Federated learning is a distributed learning framework which enables clients\nto train models individually and to upload their model updates for aggregation.\nThe local training process heavily relies on distributed gradient descent\ntechniques. In the situation where gradient information is not available, the\ngradients need to be estimated from zeroth-order information, which typically\ninvolves computing finite-differences along isotropic random directions. This\nmethod suffers from high estimation errors, as the geometric features of the\nobjective landscape may be overlooked during the isotropic sampling. In this\nwork, we propose a non-isotropic sampling method to improve the gradient\nestimation procedure. Gradients in our method are estimated in a subspace\nspanned by historical trajectories of solutions, aiming to encourage the\nexploration of promising regions and hence improve the convergence. We\nimplement this method in zeroth-order federated settings, and show that the\nconvergence rate aligns with existing ones while introducing no significant\noverheads in communication or local computation. The effectiveness of our\nproposal is verified on several numerical experiments in comparison to several\ncommonly-used zeroth-order federated optimization algorithms.\n","authors":["Xiaoyu He","Chenlin Wu","Zike Li","Zibin Zheng"],"pdf_url":"https://arxiv.org/pdf/2409.15955v2.pdf","comment":"28 pages with theoretical proof"},{"id":"http://arxiv.org/abs/2305.05920v3","updated":"2024-09-25T05:57:51Z","published":"2023-05-10T06:17:50Z","title":"Fast Distributed Inference Serving for Large Language Models","summary":"  Large language models (LLMs) power a new generation of interactive AI\napplications exemplified by ChatGPT. The interactive nature of these\napplications demands low latency for LLM inference. Existing LLM serving\nsystems use run-to-completion processing for inference jobs, which suffers from\nhead-of-line blocking and long latency.\n  We present FastServe, a distributed inference serving system for LLMs.\nFastServe exploits the autoregressive pattern of LLM inference to enable\npreemption at the granularity of each output token. FastServe uses preemptive\nscheduling to minimize latency with a novel skip-join Multi-Level Feedback\nQueue scheduler. Based on the new semi-information-agnostic setting of LLM\ninference, the scheduler leverages the input length information to assign an\nappropriate initial queue for each arrival job to join. The higher priority\nqueues than the joined queue are skipped to reduce demotions. We design an\nefficient GPU memory management mechanism that proactively offloads and uploads\nintermediate state between GPU memory and host memory for LLM inference. We\nbuild a system prototype of FastServe and experimental results show that\ncompared to the state-of-the-art solution vLLM, FastServe improves the\nthroughput by up to 31.4x and 17.9x under the same average and tail latency\nrequirements, respectively.\n","authors":["Bingyang Wu","Yinmin Zhong","Zili Zhang","Shengyu Liu","Fangyue Liu","Yuanhang Sun","Gang Huang","Xuanzhe Liu","Xin Jin"],"pdf_url":"https://arxiv.org/pdf/2305.05920v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16645v1","updated":"2024-09-25T05:56:00Z","published":"2024-09-25T05:56:00Z","title":"Task Addition in Multi-Task Learning by Geometrical Alignment","summary":"  Training deep learning models on limited data while maintaining\ngeneralization is one of the fundamental challenges in molecular property\nprediction. One effective solution is transferring knowledge extracted from\nabundant datasets to those with scarce data. Recently, a novel algorithm called\nGeometrically Aligned Transfer Encoder (GATE) has been introduced, which uses\nsoft parameter sharing by aligning the geometrical shapes of task-specific\nlatent spaces. However, GATE faces limitations in scaling to multiple tasks due\nto computational costs. In this study, we propose a task addition approach for\nGATE to improve performance on target tasks with limited data while minimizing\ncomputational complexity. It is achieved through supervised multi-task\npre-training on a large dataset, followed by the addition and training of\ntask-specific modules for each target task. Our experiments demonstrate the\nsuperior performance of the task addition strategy for GATE over conventional\nmulti-task methods, with comparable computational costs.\n","authors":["Soorin Yim","Dae-Woong Jeong","Sung Moon Ko","Sumin Lee","Hyunseung Kim","Chanhui Lee","Sehui Han"],"pdf_url":"https://arxiv.org/pdf/2409.16645v1.pdf","comment":"11 pages, 5 figures, Accepted at AI for Science Workshop at 41st\n  International Conference on Machine Learning"},{"id":"http://arxiv.org/abs/2409.16639v1","updated":"2024-09-25T05:38:09Z","published":"2024-09-25T05:38:09Z","title":"Examining the Rat in the Tunnel: Interpretable Multi-Label\n  Classification of Tor-based Malware","summary":"  Despite being the most popular privacy-enhancing network, Tor is increasingly\nadopted by cybercriminals to obfuscate malicious traffic, hindering the\nidentification of malware-related communications between compromised devices\nand Command and Control (C&C) servers. This malicious traffic can induce\ncongestion and reduce Tor's performance, while encouraging network\nadministrators to block Tor traffic. Recent research, however, demonstrates the\npotential for accurately classifying captured Tor traffic as malicious or\nbenign. While existing efforts have addressed malware class identification,\ntheir performance remains limited, with micro-average precision and recall\nvalues around 70%. Accurately classifying specific malware classes is crucial\nfor effective attack prevention and mitigation. Furthermore, understanding the\nunique patterns and attack vectors employed by different malware classes helps\nthe development of robust and adaptable defence mechanisms.\n  We utilise a multi-label classification technique based on Message-Passing\nNeural Networks, demonstrating its superiority over previous approaches such as\nBinary Relevance, Classifier Chains, and Label Powerset, by achieving\nmicro-average precision (MAP) and recall (MAR) exceeding 90%. Compared to\nprevious work, we significantly improve performance by 19.98%, 10.15%, and\n59.21% in MAP, MAR, and Hamming Loss, respectively. Next, we employ Explainable\nArtificial Intelligence (XAI) techniques to interpret the decision-making\nprocess within these models. Finally, we assess the robustness of all\ntechniques by crafting adversarial perturbations capable of manipulating\nclassifier predictions and generating false positives and negatives.\n","authors":["Ishan Karunanayake","Mashael AlSabah","Nadeem Ahmed","Sanjay Jha"],"pdf_url":"https://arxiv.org/pdf/2409.16639v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16633v1","updated":"2024-09-25T05:23:26Z","published":"2024-09-25T05:23:26Z","title":"PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System\n  Inferences","summary":"  Deep Learning Recommendation Models (DLRMs) have become increasingly popular\nand prevalent in today's datacenters, consuming most of the AI inference\ncycles. The performance of DLRMs is heavily influenced by available bandwidth\ndue to their large vector sizes in embedding tables and concurrent accesses. To\nachieve substantial improvements over existing solutions, novel approaches\ntowards DLRM optimization are needed, especially, in the context of emerging\ninterconnect technologies like CXL. This study delves into exploring\nCXL-enabled systems, implementing a process-in-fabric-switch (PIFS) solution to\naccelerate DLRMs while optimizing their memory and bandwidth scalability. We\npresent an in-depth characterization of industry-scale DLRM workloads running\non CXL-ready systems, identifying the predominant bottlenecks in existing CXL\nsystems. We, therefore, propose PIFS-Rec, a PIFS-based scheme that implements\nnear-data processing through downstream ports of the fabric switch. PIFS-Rec\nachieves a latency that is 3.89x lower than Pond, an industry-standard\nCXL-based system, and also outperforms BEACON, a state-of-the-art scheme, by\n2.03x.\n","authors":["Pingyi Huo","Anusha Devulapally","Hasan Al Maruf","Minseo Park","Krishnakumar Nair","Meena Arunachalam","Gulsum Gudukbay Akbulut","Mahmut Taylan Kandemir","Vijaykrishnan Narayanan"],"pdf_url":"https://arxiv.org/pdf/2409.16633v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16632v1","updated":"2024-09-25T05:23:01Z","published":"2024-09-25T05:23:01Z","title":"Functional Stochastic Gradient MCMC for Bayesian Neural Networks","summary":"  Classical variational inference for Bayesian neural networks (BNNs) in\nparameter space usually suffers from unresolved prior issues such as knowledge\nencoding intractability and pathological behaviors in deep networks, which\ncould lead to an improper posterior inference. Hence, functional variational\ninference has been proposed recently to resolve these issues via stochastic\nprocess priors. Beyond variational inference, stochastic gradient Markov Chain\nMonte Carlo (SGMCMC) is another scalable and effective inference method for\nBNNs to asymptotically generate samples from true posterior by simulating a\ncontinuous dynamic. However, the existing SGMCMC methods only work in\nparametric space, which has the same issues of parameter-space variational\ninference, and extending the parameter-space dynamics to function-space\ndynamics is not a trivial undertaking. In this paper, we introduce a new\nfunctional SGMCMC scheme via newly designed diffusion dynamics, which can\nincorporate more informative functional priors. Moreover, we prove that the\nstationary distribution of these functional dynamics is the target posterior\ndistribution over functions. We demonstrate better performance in both accuracy\nand uncertainty quantification of our functional SGMCMC on several tasks\ncompared with naive SGMCMC and functional variational inference methods.\n","authors":["Mengjing Wu","Junyu Xuan","Jie Lu"],"pdf_url":"https://arxiv.org/pdf/2409.16632v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16630v1","updated":"2024-09-25T05:18:17Z","published":"2024-09-25T05:18:17Z","title":"Stochastic Subsampling With Average Pooling","summary":"  Regularization of deep neural networks has been an important issue to achieve\nhigher generalization performance without overfitting problems. Although the\npopular method of Dropout provides a regularization effect, it causes\ninconsistent properties in the output, which may degrade the performance of\ndeep neural networks. In this study, we propose a new module called stochastic\naverage pooling, which incorporates Dropout-like stochasticity in pooling. We\ndescribe the properties of stochastic subsampling and average pooling and\nleverage them to design a module without any inconsistency problem. The\nstochastic average pooling achieves a regularization effect without any\npotential performance degradation due to the inconsistency issue and can easily\nbe plugged into existing architectures of deep neural networks. Experiments\ndemonstrate that replacing existing average pooling with stochastic average\npooling yields consistent improvements across a variety of tasks, datasets, and\nmodels.\n","authors":["Bum Jun Kim","Sang Woo Kim"],"pdf_url":"https://arxiv.org/pdf/2409.16630v1.pdf","comment":"17 pages, 8 figures"},{"id":"http://arxiv.org/abs/2409.16626v1","updated":"2024-09-25T05:11:58Z","published":"2024-09-25T05:11:58Z","title":"Ascend HiFloat8 Format for Deep Learning","summary":"  This preliminary white paper proposes a novel 8-bit floating-point data\nformat HiFloat8 (abbreviated as HiF8) for deep learning. HiF8 features tapered\nprecision. For normal value encoding, it provides 7 exponents with 3-bit\nmantissa, 8 exponents with 2-bit mantissa, and 16 exponents with 1-bit\nmantissa. For denormal or subnormal value encoding, it extends the dynamic\nrange by 7 extra powers of 2, from 31 to 38 binades (notice that FP16 covers 40\nbinades). Meanwhile, HiF8 encodes all the special values except that positive\nzero and negative zero are represented by only one bit-pattern. Thanks to the\nbetter balance between precision and dynamic range, HiF8 can be simultaneously\nused in both forward and backward passes of AI training. In this paper, we will\ndescribe the definition and rounding methods of HiF8, as well as the tentative\ntraining and inference solutions. To demonstrate the efficacy of HiF8 format,\nmassive simulation results on various neural networks, including traditional\nneural networks and large language models (LLMs), will also be presented.\n","authors":["Yuanyong Luo","Zhongxing Zhang","Richard Wu","Hu Liu","Ying Jin","Kai Zheng","Minmin Wang","Zhanying He","Guipeng Hu","Luyao Chen","Tianchi Hu","Junsong Wang","Minqi Chen","Mikhaylov Dmitry","Korviakov Vladimir","Bobrin Maxim","Yuhao Hu","Guanfu Chen","Zeyi Huang"],"pdf_url":"https://arxiv.org/pdf/2409.16626v1.pdf","comment":"13 Pages, 4 Figures, 9 Tables"},{"id":"http://arxiv.org/abs/2409.14248v2","updated":"2024-09-25T04:46:27Z","published":"2024-08-09T03:50:58Z","title":"Higher-order-ReLU-KANs (HRKANs) for solving physics-informed neural\n  networks (PINNs) more accurately, robustly and faster","summary":"  Finding solutions to partial differential equations (PDEs) is an important\nand essential component in many scientific and engineering discoveries. One of\nthe common approaches empowered by deep learning is Physics-informed Neural\nNetworks (PINNs). Recently, a new type of fundamental neural network model,\nKolmogorov-Arnold Networks (KANs), has been proposed as a substitute of\nMultilayer Perceptions (MLPs), and possesses trainable activation functions. To\nenhance KANs in fitting accuracy, a modification of KANs, so called ReLU-KANs,\nusing \"square of ReLU\" as the basis of its activation functions, has been\nsuggested. In this work, we propose another basis of activation functions,\nnamely, Higherorder-ReLU (HR), which is simpler than the basis of activation\nfunctions used in KANs, namely, Bsplines; allows efficient KAN matrix\noperations; and possesses smooth and non-zero higher-order derivatives,\nessential to physicsinformed neural networks. We name such KANs with\nHigher-order-ReLU (HR) as their activations, HRKANs. Our detailed experiments\non two famous and representative PDEs, namely, the linear Poisson equation and\nnonlinear Burgers' equation with viscosity, reveal that our proposed\nHigher-order-ReLU-KANs (HRKANs) achieve the highest fitting accuracy and\ntraining robustness and lowest training time significantly among KANs,\nReLU-KANs and HRKANs. The codes to replicate our experiments are available at\nhttps://github.com/kelvinhkcs/HRKAN.\n","authors":["Chi Chiu So","Siu Pang Yung"],"pdf_url":"https://arxiv.org/pdf/2409.14248v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16609v1","updated":"2024-09-25T04:18:53Z","published":"2024-09-25T04:18:53Z","title":"Random Forest Regression Feature Importance for Climate Impact Pathway\n  Detection","summary":"  Disturbances to the climate system, both natural and anthropogenic, have far\nreaching impacts that are not always easy to identify or quantify using\ntraditional climate science analyses or causal modeling techniques. In this\npaper, we develop a novel technique for discovering and ranking the chain of\nspatio-temporal downstream impacts of a climate source, referred to herein as a\nsource-impact pathway, using Random Forest Regression (RFR) and SHapley\nAdditive exPlanation (SHAP) feature importances. Rather than utilizing RFR for\nclassification or regression tasks (the most common use case for RFR), we\npropose a fundamentally new RFR-based workflow in which we: (i) train random\nforest (RF) regressors on a set of spatio-temporal features of interest, (ii)\ncalculate their pair-wise feature importances using the SHAP weights associated\nwith those features, and (iii) translate these feature importances into a\nweighted pathway network (i.e., a weighted directed graph), which can be used\nto trace out and rank interdependencies between climate features and/or\nmodalities. We adopt a tiered verification approach to verify our new pathway\nidentification methodology. In this approach, we apply our method to ensembles\nof data generated by running two increasingly complex benchmarks: (i) a set of\nsynthetic coupled equations, and (ii) a fully coupled simulation of the 1991\neruption of Mount Pinatubo in the Philippines performed using a modified\nversion 2 of the U.S. Department of Energy's Energy Exascale Earth System Model\n(E3SMv2). We find that our RFR feature importance-based approach can accurately\ndetect known pathways of impact for both test cases.\n","authors":["Meredith G. L. Brown","Matt Peterson","Irina Tezaur","Kara Peterson","Diana Bull"],"pdf_url":"https://arxiv.org/pdf/2409.16609v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16605v1","updated":"2024-09-25T04:12:38Z","published":"2024-09-25T04:12:38Z","title":"Evaluating and Enhancing Large Language Models for Novelty Assessment in\n  Scholarly Publications","summary":"  Recent studies have evaluated the creativity/novelty of large language models\n(LLMs) primarily from a semantic perspective, using benchmarks from cognitive\nscience. However, accessing the novelty in scholarly publications is a largely\nunexplored area in evaluating LLMs. In this paper, we introduce a scholarly\nnovelty benchmark (SchNovel) to evaluate LLMs' ability to assess novelty in\nscholarly papers. SchNovel consists of 15000 pairs of papers across six fields\nsampled from the arXiv dataset with publication dates spanning 2 to 10 years\napart. In each pair, the more recently published paper is assumed to be more\nnovel. Additionally, we propose RAG-Novelty, which simulates the review process\ntaken by human reviewers by leveraging the retrieval of similar papers to\nassess novelty. Extensive experiments provide insights into the capabilities of\ndifferent LLMs to assess novelty and demonstrate that RAG-Novelty outperforms\nrecent baseline models.\n","authors":["Ethan Lin","Zhiyuan Peng","Yi Fang"],"pdf_url":"https://arxiv.org/pdf/2409.16605v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2409.16594v1","updated":"2024-09-25T03:39:14Z","published":"2024-09-25T03:39:14Z","title":"Generative Pre-trained Ranking Model with Over-parameterization at\n  Web-Scale (Extended Abstract)","summary":"  Learning to rank (LTR) is widely employed in web searches to prioritize\npertinent webpages from retrieved content based on input queries. However,\ntraditional LTR models encounter two principal obstacles that lead to\nsuboptimal performance: (1) the lack of well-annotated query-webpage pairs with\nranking scores covering a diverse range of search query popularities, which\nhampers their ability to address queries across the popularity spectrum, and\n(2) inadequately trained models that fail to induce generalized representations\nfor LTR, resulting in overfitting. To address these challenges, we propose a\n\\emph{\\uline{G}enerative \\uline{S}emi-\\uline{S}upervised \\uline{P}re-trained}\n(GS2P) LTR model. We conduct extensive offline experiments on both a publicly\navailable dataset and a real-world dataset collected from a large-scale search\nengine. Furthermore, we deploy GS2P in a large-scale web search engine with\nrealistic traffic, where we observe significant improvements in the real-world\napplication.\n","authors":["Yuchen Li","Haoyi Xiong","Linghe Kong","Jiang Bian","Shuaiqiang Wang","Guihai Chen","Dawei Yin"],"pdf_url":"https://arxiv.org/pdf/2409.16594v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16592v1","updated":"2024-09-25T03:37:51Z","published":"2024-09-25T03:37:51Z","title":"MambaJSCC: Adaptive Deep Joint Source-Channel Coding with Generalized\n  State Space Model","summary":"  Lightweight and efficient neural network models for deep joint source-channel\ncoding (JSCC) are crucial for semantic communications. In this paper, we\npropose a novel JSCC architecture, named MambaJSCC, that achieves\nstate-of-the-art performance with low computational and parameter overhead.\nMambaJSCC utilizes the visual state space model with channel adaptation\n(VSSM-CA) blocks as its backbone for transmitting images over wireless\nchannels, where the VSSM-CA primarily consists of the generalized state space\nmodels (GSSM) and the zero-parameter, zero-computational channel adaptation\nmethod (CSI-ReST). We design the GSSM module, leveraging reversible matrix\ntransformations to express generalized scan expanding operations, and\ntheoretically prove that two GSSM modules can effectively capture global\ninformation. We discover that GSSM inherently possesses the ability to adapt to\nchannels, a form of endogenous intelligence. Based on this, we design the\nCSI-ReST method, which injects channel state information (CSI) into the initial\nstate of GSSM to utilize its native response, and into the residual state to\nmitigate CSI forgetting, enabling effective channel adaptation without\nintroducing additional computational and parameter overhead. Experimental\nresults show that MambaJSCC not only outperforms existing JSCC methods (e.g.,\nSwinJSCC) across various scenarios but also significantly reduces parameter\nsize, computational overhead, and inference delay.\n","authors":["Tong Wu","Zhiyong Chen","Meixia Tao","Yaping Sun","Xiaodong Xu","Wenjun Zhang","Ping Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.16592v1.pdf","comment":"submitted to IEEE Journal"},{"id":"http://arxiv.org/abs/2409.08695v3","updated":"2024-09-25T03:34:45Z","published":"2024-09-13T10:27:27Z","title":"Precision Aquaculture: An Integrated Computer Vision and IoT Approach\n  for Optimized Tilapia Feeding","summary":"  Traditional fish farming practices often lead to inefficient feeding,\nresulting in environmental issues and reduced productivity. We developed an\ninnovative system combining computer vision and IoT technologies for precise\nTilapia feeding. Our solution uses real-time IoT sensors to monitor water\nquality parameters and computer vision algorithms to analyze fish size and\ncount, determining optimal feed amounts. A mobile app enables remote monitoring\nand control. We utilized YOLOv8 for keypoint detection to measure Tilapia\nweight from length, achieving \\textbf{94\\%} precision on 3,500 annotated\nimages. Pixel-based measurements were converted to centimeters using depth\nestimation for accurate feeding calculations. Our method, with data collection\nmirroring inference conditions, significantly improved results. Preliminary\nestimates suggest this approach could increase production up to 58 times\ncompared to traditional farms. Our models, code, and dataset are\nopen-source~\\footnote{The code, dataset, and models are available upon\nreasonable request.\n","authors":["Rania Hossam","Ahmed Heakl","Walid Gomaa"],"pdf_url":"https://arxiv.org/pdf/2409.08695v3.pdf","comment":"8 pages, 6 figures, 3 tables, 21th International Conference on\n  Informatics in Control, Automation, and Robotics"},{"id":"http://arxiv.org/abs/2409.16590v1","updated":"2024-09-25T03:33:47Z","published":"2024-09-25T03:33:47Z","title":"Pre-trained Graphformer-based Ranking at Web-scale Search (Extended\n  Abstract)","summary":"  Both Transformer and Graph Neural Networks (GNNs) have been employed in the\ndomain of learning to rank (LTR). However, these approaches adhere to two\ndistinct yet complementary problem formulations: ranking score regression based\non query-webpage pairs, and link prediction within query-webpage bipartite\ngraphs, respectively. While it is possible to pre-train GNNs or Transformers on\nsource datasets and subsequently fine-tune them on sparsely annotated LTR\ndatasets, the distributional shifts between the pair-based and bipartite graph\ndomains present significant challenges in integrating these heterogeneous\nmodels into a unified LTR framework at web scale. To address this, we introduce\nthe novel MPGraf model, which leverages a modular and capsule-based\npre-training strategy, aiming to cohesively integrate the regression\ncapabilities of Transformers with the link prediction strengths of GNNs. We\nconduct extensive offline and online experiments to rigorously evaluate the\nperformance of MPGraf.\n","authors":["Yuchen Li","Haoyi Xiong","Linghe Kong","Zeyi Sun","Hongyang Chen","Shuaiqiang Wang","Dawei Yin"],"pdf_url":"https://arxiv.org/pdf/2409.16590v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16586v1","updated":"2024-09-25T03:25:34Z","published":"2024-09-25T03:25:34Z","title":"AutoSTF: Decoupled Neural Architecture Search for Cost-Effective\n  Automated Spatio-Temporal Forecasting","summary":"  Spatio-temporal forecasting is a critical component of various smart city\napplications, such as transportation optimization, energy management, and\nsocio-economic analysis. Recently, several automated spatio-temporal\nforecasting methods have been proposed to automatically search the optimal\nneural network architecture for capturing complex spatio-temporal dependencies.\nHowever, the existing automated approaches suffer from expensive neural\narchitecture search overhead, which hinders their practical use and the further\nexploration of diverse spatio-temporal operators in a finer granularity. In\nthis paper, we propose AutoSTF, a decoupled automatic neural architecture\nsearch framework for cost-effective automated spatio-temporal forecasting. From\nthe efficiency perspective, we first decouple the mixed search space into\ntemporal space and spatial space and respectively devise representation\ncompression and parameter-sharing schemes to mitigate the parameter explosion.\nThe decoupled spatio-temporal search not only expedites the model optimization\nprocess but also leaves new room for more effective spatio-temporal dependency\nmodeling. From the effectiveness perspective, we propose a multi-patch transfer\nmodule to jointly capture multi-granularity temporal dependencies and extend\nthe spatial search space to enable finer-grained layer-wise spatial dependency\nsearch. Extensive experiments on eight datasets demonstrate the superiority of\nAutoSTF in terms of both accuracy and efficiency. Specifically, our proposed\nmethod achieves up to 13.48x speed-up compared to state-of-the-art automatic\nspatio-temporal forecasting methods while maintaining the best forecasting\naccuracy.\n","authors":["Tengfei Lyu","Weijia Zhang","Jinliang Deng","Hao Liu"],"pdf_url":"https://arxiv.org/pdf/2409.16586v1.pdf","comment":"16 pages, 13 figures"},{"id":"http://arxiv.org/abs/2409.15657v2","updated":"2024-09-25T03:24:39Z","published":"2024-09-24T01:40:24Z","title":"M^2PT: Multimodal Prompt Tuning for Zero-shot Instruction Learning","summary":"  Multimodal Large Language Models (MLLMs) demonstrate remarkable performance\nacross a wide range of domains, with increasing emphasis on enhancing their\nzero-shot generalization capabilities for unseen tasks across various\nmodalities. Instruction tuning has emerged as an effective strategy for\nachieving zero-shot generalization by finetuning pretrained models on diverse\nmultimodal tasks. As the scale of MLLMs continues to grow, parameter-efficient\nfinetuning becomes increasingly critical. However, most existing\nparameter-efficient approaches focus only on single modalities and often\noverlook the multimodal characteristics during finetuning. In this work, we\nintroduce a novel Multimodal Prompt Tuning (M$^2$PT) approach for efficient\ninstruction tuning of MLLMs. M$^2$PT effectively integrates visual and textual\nprompts into the vision encoder and language processor respectively during\nfinetuning, facilitating the extraction and alignment of features across\nmodalities. Empirical results on various multimodal evaluation datasets\ndemonstrate the superior performance of our approach compared to several\nstate-of-the-art baselines. A comprehensive set of ablation studies validates\nthe effectiveness of our prompt design and the efficiency of our approach.\n","authors":["Taowen Wang","Yiyang Liu","James Chenhao Liang","junhan zhao","Yiming Cui","Yuning Mao","Shaoliang Nie","Jiahao Liu","Fuli Feng","Zenglin Xu","Cheng Han","Lifu Huang","Qifan Wang","Dongfang Liu"],"pdf_url":"https://arxiv.org/pdf/2409.15657v2.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2407.12346v2","updated":"2024-09-25T03:18:14Z","published":"2024-07-17T06:42:14Z","title":"Object-Aware Query Perturbation for Cross-Modal Image-Text Retrieval","summary":"  The pre-trained vision and language (V\\&L) models have substantially improved\nthe performance of cross-modal image-text retrieval. In general, however, V\\&L\nmodels have limited retrieval performance for small objects because of the\nrough alignment between words and the small objects in the image. In contrast,\nit is known that human cognition is object-centric, and we pay more attention\nto important objects, even if they are small. To bridge this gap between the\nhuman cognition and the V\\&L model's capability, we propose a cross-modal\nimage-text retrieval framework based on ``object-aware query perturbation.''\nThe proposed method generates a key feature subspace of the detected objects\nand perturbs the corresponding queries using this subspace to improve the\nobject awareness in the image. In our proposed method, object-aware cross-modal\nimage-text retrieval is possible while keeping the rich expressive power and\nretrieval performance of existing V\\&L models without additional fine-tuning.\nComprehensive experiments on four public datasets show that our method\noutperforms conventional algorithms. Our code is publicly available at\n\\url{https://github.com/NEC-N-SOGI/query-perturbation}.\n","authors":["Naoya Sogi","Takashi Shibata","Makoto Terao"],"pdf_url":"https://arxiv.org/pdf/2407.12346v2.pdf","comment":"ECCV 2024. Code: https://github.com/NEC-N-SOGI/query-perturbation"},{"id":"http://arxiv.org/abs/2409.16578v1","updated":"2024-09-25T03:15:17Z","published":"2024-09-25T03:15:17Z","title":"FLaRe: Achieving Masterful and Adaptive Robot Policies with Large-Scale\n  Reinforcement Learning Fine-Tuning","summary":"  In recent years, the Robotics field has initiated several efforts toward\nbuilding generalist robot policies through large-scale multi-task Behavior\nCloning. However, direct deployments of these policies have led to\nunsatisfactory performance, where the policy struggles with unseen states and\ntasks. How can we break through the performance plateau of these models and\nelevate their capabilities to new heights? In this paper, we propose FLaRe, a\nlarge-scale Reinforcement Learning fine-tuning framework that integrates robust\npre-trained representations, large-scale training, and gradient stabilization\ntechniques. Our method aligns pre-trained policies towards task completion,\nachieving state-of-the-art (SoTA) performance both on previously demonstrated\nand on entirely novel tasks and embodiments. Specifically, on a set of\nlong-horizon mobile manipulation tasks, FLaRe achieves an average success rate\nof 79.5% in unseen environments, with absolute improvements of +23.6% in\nsimulation and +30.7% on real robots over prior SoTA methods. By utilizing only\nsparse rewards, our approach can enable generalizing to new capabilities beyond\nthe pretraining data with minimal human effort. Moreover, we demonstrate rapid\nadaptation to new embodiments and behaviors with less than a day of\nfine-tuning. Videos can be found on the project website at\nhttps://robot-flare.github.io/\n","authors":["Jiaheng Hu","Rose Hendrix","Ali Farhadi","Aniruddha Kembhavi","Roberto Martin-Martin","Peter Stone","Kuo-Hao Zeng","Kiana Ehsan"],"pdf_url":"https://arxiv.org/pdf/2409.16578v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14285v4","updated":"2024-09-25T03:12:27Z","published":"2024-02-22T04:55:58Z","title":"Symbolic Music Generation with Non-Differentiable Rule Guided Diffusion","summary":"  We study the problem of symbolic music generation (e.g., generating piano\nrolls), with a technical focus on non-differentiable rule guidance. Musical\nrules are often expressed in symbolic form on note characteristics, such as\nnote density or chord progression, many of which are non-differentiable which\npose a challenge when using them for guided diffusion. We propose Stochastic\nControl Guidance (SCG), a novel guidance method that only requires forward\nevaluation of rule functions that can work with pre-trained diffusion models in\na plug-and-play way, thus achieving training-free guidance for\nnon-differentiable rules for the first time. Additionally, we introduce a\nlatent diffusion architecture for symbolic music generation with high time\nresolution, which can be composed with SCG in a plug-and-play fashion. Compared\nto standard strong baselines in symbolic music generation, this framework\ndemonstrates marked advancements in music quality and rule-based\ncontrollability, outperforming current state-of-the-art generators in a variety\nof settings. For detailed demonstrations, code and model checkpoints, please\nvisit our project website: https://scg-rule-guided-music.github.io/.\n","authors":["Yujia Huang","Adishree Ghatare","Yuanzhe Liu","Ziniu Hu","Qinsheng Zhang","Chandramouli S Sastry","Siddharth Gururani","Sageev Oore","Yisong Yue"],"pdf_url":"https://arxiv.org/pdf/2402.14285v4.pdf","comment":"ICML 2024 (Oral)"},{"id":"http://arxiv.org/abs/2409.16572v1","updated":"2024-09-25T02:58:45Z","published":"2024-09-25T02:58:45Z","title":"Efficient and generalizable nested Fourier-DeepONet for\n  three-dimensional geological carbon sequestration","summary":"  Geological carbon sequestration (GCS) involves injecting CO$_2$ into\nsubsurface geological formations for permanent storage. Numerical simulations\ncould guide decisions in GCS projects by predicting CO$_2$ migration pathways\nand the pressure distribution in storage formation. However, these simulations\nare often computationally expensive due to highly coupled physics and large\nspatial-temporal simulation domains. Surrogate modeling with data-driven\nmachine learning has become a promising alternative to accelerate physics-based\nsimulations. Among these, the Fourier neural operator (FNO) has been applied to\nthree-dimensional synthetic subsurface models. Here, to further improve\nperformance, we have developed a nested Fourier-DeepONet by combining the\nexpressiveness of the FNO with the modularity of a deep operator network\n(DeepONet). This new framework is twice as efficient as a nested FNO for\ntraining and has at least 80% lower GPU memory requirement due to its\nflexibility to treat temporal coordinates separately. These performance\nimprovements are achieved without compromising prediction accuracy. In\naddition, the generalization and extrapolation ability of nested\nFourier-DeepONet beyond the training range has been thoroughly evaluated.\nNested Fourier-DeepONet outperformed the nested FNO for extrapolation in time\nwith more than 50% reduced error. It also exhibited good extrapolation accuracy\nbeyond the training range in terms of reservoir properties, number of wells,\nand injection rate.\n","authors":["Jonathan E. Lee","Min Zhu","Ziqiao Xi","Kun Wang","Yanhua O. Yuan","Lu Lu"],"pdf_url":"https://arxiv.org/pdf/2409.16572v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15898v2","updated":"2024-09-25T02:48:53Z","published":"2024-09-24T09:17:08Z","title":"FedRepOpt: Gradient Re-parameterized Optimizers in Federated Learning","summary":"  Federated Learning (FL) has emerged as a privacy-preserving method for\ntraining machine learning models in a distributed manner on edge devices.\nHowever, on-device models face inherent computational power and memory\nlimitations, potentially resulting in constrained gradient updates. As the\nmodel's size increases, the frequency of gradient updates on edge devices\ndecreases, ultimately leading to suboptimal training outcomes during any\nparticular FL round. This limits the feasibility of deploying advanced and\nlarge-scale models on edge devices, hindering the potential for performance\nenhancements. To address this issue, we propose FedRepOpt, a gradient\nre-parameterized optimizer for FL. The gradient re-parameterized method allows\ntraining a simple local model with a similar performance as a complex model by\nmodifying the optimizer's gradients according to a set of model-specific\nhyperparameters obtained from the complex models. In this work, we focus on\nVGG-style and Ghost-style models in the FL environment. Extensive experiments\ndemonstrate that models using FedRepOpt obtain a significant boost in\nperformance of 16.7% and 11.4% compared to the RepGhost-style and RepVGG-style\nnetworks, while also demonstrating a faster convergence time of 11.7% and 57.4%\ncompared to their complex structure.\n","authors":["Kin Wai Lau","Yasar Abbas Ur Rehman","Pedro Porto Buarque de Gusmão","Lai-Man Po","Lan Ma","Yuyang Xie"],"pdf_url":"https://arxiv.org/pdf/2409.15898v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00664v4","updated":"2024-09-25T02:47:56Z","published":"2024-01-01T04:35:53Z","title":"Metric Entropy-Free Sample Complexity Bounds for Sample Average\n  Approximation in Convex Stochastic Programming","summary":"  This paper studies sample average approximation (SAA) in solving convex or\nstrongly convex stochastic programming (SP) problems. Under some common\nregularity conditions, we show -- perhaps for the first time -- that SAA's\nsample complexity can be completely free from any quantification of metric\nentropy (such as the logarithm of the covering number), leading to a\nsignificantly more efficient rate with dimensionality $d$ than most existing\nresults. From the newly established complexity bounds, an important revelation\nis that SAA and the canonical stochastic mirror descent (SMD) method, two\nmainstream solution approaches to SP, entail almost identical rates of sample\nefficiency, rectifying a persistent theoretical discrepancy of SAA from SMD by\nthe order of $O(d)$. Furthermore, this paper explores non-Lipschitzian\nscenarios where SAA maintains provable efficacy but the corresponding results\nfor SMD remain mostly unexplored, indicating the potential of SAA's better\napplicability in some irregular settings.\n","authors":["Hongcheng Liu","Jindong Tong"],"pdf_url":"https://arxiv.org/pdf/2401.00664v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16554v1","updated":"2024-09-25T02:05:32Z","published":"2024-09-25T02:05:32Z","title":"EMIT- Event-Based Masked Auto Encoding for Irregular Time Series","summary":"  Irregular time series, where data points are recorded at uneven intervals,\nare prevalent in healthcare settings, such as emergency wards where vital signs\nand laboratory results are captured at varying times. This variability, which\nreflects critical fluctuations in patient health, is essential for informed\nclinical decision-making. Existing self-supervised learning research on\nirregular time series often relies on generic pretext tasks like forecasting,\nwhich may not fully utilise the signal provided by irregular time series. There\nis a significant need for specialised pretext tasks designed for the\ncharacteristics of irregular time series to enhance model performance and\nrobustness, especially in scenarios with limited data availability. This paper\nproposes a novel pretraining framework, EMIT, an event-based masking for\nirregular time series. EMIT focuses on masking-based reconstruction in the\nlatent space, selecting masking points based on the rate of change in the data.\nThis method preserves the natural variability and timing of measurements while\nenhancing the model's ability to process irregular intervals without losing\nessential information. Extensive experiments on the MIMIC-III and PhysioNet\nChallenge datasets demonstrate the superior performance of our event-based\nmasking strategy. The code has been released at\nhttps://github.com/hrishi-ds/EMIT .\n","authors":["Hrishikesh Patel","Ruihong Qiu","Adam Irwin","Shazia Sadiq","Sen Wang"],"pdf_url":"https://arxiv.org/pdf/2409.16554v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16546v1","updated":"2024-09-25T01:39:02Z","published":"2024-09-25T01:39:02Z","title":"AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization","summary":"  Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision.\n","authors":["Yifan Tan","Haoze Wang","Chao Yan","Yangdong Deng"],"pdf_url":"https://arxiv.org/pdf/2409.16546v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16541v1","updated":"2024-09-25T01:30:16Z","published":"2024-09-25T01:30:16Z","title":"Monge-Kantorovich Fitting With Sobolev Budgets","summary":"  We consider the problem of finding the ``best'' approximation of an\n$n$-dimensional probability measure $\\rho$ using a measure $\\nu$ whose support\nis parametrized by $f : \\mathbb{R}^m \\to \\mathbb{R}^n$ where $m < n$. We\nquantify the performance of the approximation with the Monge-Kantorovich\n$p$-cost (also called the Wasserstein $p$-cost) $\\mathbb{W}_p^p(\\rho, \\nu)$,\nand constrain the complexity of the approximation by bounding the $W^{k,q}$\nSobolev norm of $f$, which acts as a ``budget.'' We may then reformulate the\nproblem as minimizing a functional $\\mathscr{J}_p(f)$ under a constraint on the\nSobolev budget.\n  We treat general $k \\geq 1$ for the Sobolev differentiability order (though\n$q, m$ are chosen to restrict $W^{k,q}$ to the supercritical regime $k q > m$\nto guarantee existence of optimizers). The problem is closely related to (but\ndistinct from) principal curves with length constraints when $m=1, k = 1$ and\nsmoothing splines when $k > 1$. New aspects and challenges arise from the\nhigher order differentiability condition.\n  We study the gradient of $\\mathscr{J}_p$, which is given by a vector field\nalong $f$ we call the barycenter field. We use it to construct improvements to\na given $f$, which gives a nontrivial (almost) strict monotonicty relation\nbetween the functional $\\mathscr{J}_p$ and the Sobolev budget. We also provide\na natural discretization scheme and establish its consistency. We use this\nscheme to model a generative learning task; in particular, we demonstrate that\nadding a constraint like ours as a soft penalty yields substantial improvement\nin training a GAN to produce images of handwritten digits, with performance\ncompetitive with weight-decay.\n","authors":["Forest Kobayashi","Jonathan Hayase","Young-Heon Kim"],"pdf_url":"https://arxiv.org/pdf/2409.16541v1.pdf","comment":"68 pages, 23 figures, 50 pages without figures"},{"id":"http://arxiv.org/abs/2409.16538v1","updated":"2024-09-25T01:22:10Z","published":"2024-09-25T01:22:10Z","title":"Source-Free Domain Adaptation for YOLO Object Detection","summary":"  Source-free domain adaptation (SFDA) is a challenging problem in object\ndetection, where a pre-trained source model is adapted to a new target domain\nwithout using any source domain data for privacy and efficiency reasons. Most\nstate-of-the-art SFDA methods for object detection have been proposed for\nFaster-RCNN, a detector that is known to have high computational complexity.\nThis paper focuses on domain adaptation techniques for real-world vision\nsystems, particularly for the YOLO family of single-shot detectors known for\ntheir fast baselines and practical applications. Our proposed SFDA method -\nSource-Free YOLO (SF-YOLO) - relies on a teacher-student framework in which the\nstudent receives images with a learned, target domain-specific augmentation,\nallowing the model to be trained with only unlabeled target data and without\nrequiring feature alignment. A challenge with self-training using a\nmean-teacher architecture in the absence of labels is the rapid decline of\naccuracy due to noisy or drifting pseudo-labels. To address this issue, a\nteacher-to-student communication mechanism is introduced to help stabilize the\ntraining and reduce the reliance on annotated target data for model selection.\nDespite its simplicity, our approach is competitive with state-of-the-art\ndetectors on several challenging benchmark datasets, even sometimes\noutperforming methods that use source data for adaptation.\n","authors":["Simon Varailhon","Masih Aminbeidokhti","Marco Pedersoli","Eric Granger"],"pdf_url":"https://arxiv.org/pdf/2409.16538v1.pdf","comment":"ECCV 2024: European Conference on Computer Vision - Workshop on\n  Out-of-Distribution Generalization in Computer Vision Foundation Models,\n  Milan Italy"},{"id":"http://arxiv.org/abs/2409.16537v1","updated":"2024-09-25T01:09:45Z","published":"2024-09-25T01:09:45Z","title":"A QoE-Aware Split Inference Accelerating Algorithm for NOMA-based Edge\n  Intelligence","summary":"  Even the AI has been widely used and significantly changed our life,\ndeploying the large AI models on resource limited edge devices directly is not\nappropriate. Thus, the model split inference is proposed to improve the\nperformance of edge intelligence, in which the AI model is divided into\ndifferent sub models and the resource-intensive sub model is offloaded to edge\nserver wirelessly for reducing resource requirements and inference latency.\nHowever, the previous works mainly concentrate on improving and optimizing the\nsystem QoS, ignore the effect of QoE which is another critical item for the\nusers except for QoS. Even the QoE has been widely learned in EC, considering\nthe differences between task offloading in EC and split inference in EI, and\nthe specific issues in QoE which are still not addressed in EC and EI, these\nalgorithms cannot work effectively in edge split inference scenarios. Thus, an\neffective resource allocation algorithm is proposed in this paper, for\naccelerating split inference in EI and achieving the tradeoff between inference\ndelay, QoE, and resource consumption, abbreviated as ERA. Specifically, the ERA\ntakes the resource consumption, QoE, and inference latency into account to find\nthe optimal model split strategy and resource allocation strategy. Since the\nminimum inference delay and resource consumption, and maximum QoE cannot be\nsatisfied simultaneously, the gradient descent based algorithm is adopted to\nfind the optimal tradeoff between them. Moreover, the loop iteration GD\napproach is developed to reduce the complexity of the GD algorithm caused by\nparameter discretization. Additionally, the properties of the proposed\nalgorithms are investigated, including convergence, complexity, and\napproximation error. The experimental results demonstrate that the performance\nof ERA is much better than that of the previous studies.\n","authors":["Xin Yuan","Ning Li","Quan Chen","Wenchao Xu","Zhaoxin Zhang","Song Guo"],"pdf_url":"https://arxiv.org/pdf/2409.16537v1.pdf","comment":"16pages, 19figures. arXiv admin note: substantial text overlap with\n  arXiv:2312.15850"},{"id":"http://arxiv.org/abs/2409.09755v2","updated":"2024-09-25T00:43:03Z","published":"2024-09-15T14:57:38Z","title":"Analysis of Centrifugal Clutches in Two-Speed Automatic Transmissions\n  with Deep Learning-Based Engagement Prediction","summary":"  This paper presents a comprehensive numerical analysis of centrifugal clutch\nsystems integrated with a two-speed automatic transmission, a key component in\nautomotive torque transfer. Centrifugal clutches enable torque transmission\nbased on rotational speed without external controls. The study systematically\nexamines various clutch configurations effects on transmission dynamics,\nfocusing on torque transfer, upshifting, and downshifting behaviors under\ndifferent conditions. A Deep Neural Network (DNN) model predicts clutch\nengagement using parameters such as spring preload and shoe mass, offering an\nefficient alternative to complex simulations. The integration of deep learning\nand numerical modeling provides critical insights for optimizing clutch\ndesigns, enhancing transmission performance and efficiency.\n","authors":["Bo-Yi Lin","Kai Chun Lin"],"pdf_url":"https://arxiv.org/pdf/2409.09755v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.05207v5","updated":"2024-09-25T00:35:55Z","published":"2022-11-09T21:33:40Z","title":"Improving Clinician Performance in Classification of EEG Patterns on the\n  Ictal-Interictal-Injury Continuum using Interpretable Machine Learning","summary":"  In intensive care units (ICUs), critically ill patients are monitored with\nelectroencephalograms (EEGs) to prevent serious brain injury. The number of\npatients who can be monitored is constrained by the availability of trained\nphysicians to read EEGs, and EEG interpretation can be subjective and prone to\ninter-observer variability. Automated deep learning systems for EEG could\nreduce human bias and accelerate the diagnostic process. However, black box\ndeep learning models are untrustworthy, difficult to troubleshoot, and lack\naccountability in real-world applications, leading to a lack of trust and\nadoption by clinicians. To address these challenges, we propose a novel\ninterpretable deep learning model that not only predicts the presence of\nharmful brainwave patterns but also provides high-quality case-based\nexplanations of its decisions. Our model performs better than the corresponding\nblack box model, despite being constrained to be interpretable. The learned 2D\nembedded space provides the first global overview of the structure of\nictal-interictal-injury continuum brainwave patterns. The ability to understand\nhow our model arrived at its decisions will not only help clinicians to\ndiagnose and treat harmful brain activities more accurately but also increase\ntheir trust and adoption of machine learning models in clinical practice; this\ncould be an integral component of the ICU neurologists' standard workflow.\n","authors":["Alina Jade Barnett","Zhicheng Guo","Jin Jing","Wendong Ge","Peter W. Kaplan","Wan Yee Kong","Ioannis Karakis","Aline Herlopian","Lakshman Arcot Jayagopal","Olga Taraschenko","Olga Selioutski","Gamaleldin Osman","Daniel Goldenholz","Cynthia Rudin","M. Brandon Westover"],"pdf_url":"https://arxiv.org/pdf/2211.05207v5.pdf","comment":"24 pages including appendices, 9 figures, published at NEJM AI"},{"id":"http://arxiv.org/abs/2403.07937v2","updated":"2024-09-25T00:28:55Z","published":"2024-03-08T08:10:29Z","title":"Speech Robust Bench: A Robustness Benchmark For Speech Recognition","summary":"  As Automatic Speech Recognition (ASR) models become ever more pervasive, it\nis important to ensure that they make reliable predictions under corruptions\npresent in the physical and digital world. We propose Speech Robust Bench\n(SRB), a comprehensive benchmark for evaluating the robustness of ASR models to\ndiverse corruptions. SRB is composed of 114 input perturbations which simulate\nan heterogeneous range of corruptions that ASR models may encounter when\ndeployed in the wild. We use SRB to evaluate the robustness of several\nstate-of-the-art ASR models and observe that model size and certain modeling\nchoices such as the use of discrete representations, or self-training appear to\nbe conducive to robustness. We extend this analysis to measure the robustness\nof ASR models on data from various demographic subgroups, namely English and\nSpanish speakers, and males and females. Our results revealed noticeable\ndisparities in the model's robustness across subgroups. We believe that SRB\nwill significantly facilitate future research towards robust ASR models, by\nmaking it easier to conduct comprehensive and comparable robustness\nevaluations.\n","authors":["Muhammad A. Shah","David Solans Noguero","Mikko A. Heikkila","Bhiksha Raj","Nicolas Kourtellis"],"pdf_url":"https://arxiv.org/pdf/2403.07937v2.pdf","comment":"submitted to NeurIPS datasets and benchmark track 2025"},{"id":"http://arxiv.org/abs/2405.16226v3","updated":"2024-09-25T00:09:58Z","published":"2024-05-25T13:34:16Z","title":"Detecting Adversarial Data via Perturbation Forgery","summary":"  As a defense strategy against adversarial attacks, adversarial detection aims\nto identify and filter out adversarial data from the data flow based on\ndiscrepancies in distribution and noise patterns between natural and\nadversarial data. Although previous detection methods achieve high performance\nin detecting gradient-based adversarial attacks, new attacks based on\ngenerative models with imbalanced and anisotropic noise patterns evade\ndetection. Even worse, existing techniques either necessitate access to attack\ndata before deploying a defense or incur a significant time cost for inference,\nrendering them impractical for defending against newly emerging attacks that\nare unseen by defenders. In this paper, we explore the proximity relationship\nbetween adversarial noise distributions and demonstrate the existence of an\nopen covering for them. By learning to distinguish this open covering from the\ndistribution of natural data, we can develop a detector with strong\ngeneralization capabilities against all types of adversarial attacks. Based on\nthis insight, we heuristically propose Perturbation Forgery, which includes\nnoise distribution perturbation, sparse mask generation, and pseudo-adversarial\ndata production, to train an adversarial detector capable of detecting unseen\ngradient-based, generative-model-based, and physical adversarial attacks, while\nremaining agnostic to any specific models. Comprehensive experiments conducted\non multiple general and facial datasets, with a wide spectrum of attacks,\nvalidate the strong generalization of our method.\n","authors":["Qian Wang","Chen Li","Yuchen Luo","Hefei Ling","Ping Li","Jiazhong Chen","Shijuan Huang","Ning Yu"],"pdf_url":"https://arxiv.org/pdf/2405.16226v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09481v4","updated":"2024-09-25T00:07:50Z","published":"2023-12-15T01:38:26Z","title":"Continual Adversarial Defense","summary":"  In response to the rapidly evolving nature of adversarial attacks against\nvisual classifiers on a monthly basis, numerous defenses have been proposed to\ngeneralize against as many known attacks as possible. However, designing a\ndefense method that generalizes to all types of attacks is not realistic\nbecause the environment in which defense systems operate is dynamic and\ncomprises various unique attacks that emerge as time goes on. A well-matched\napproach to the dynamic environment lies in a defense system that continuously\ncollects adversarial data online to quickly improve itself. Therefore, we put\nforward a practical defense deployment against a challenging threat model and\npropose, for the first time, the Continual Adversarial Defense (CAD) framework\nthat adapts to attack sequences under four principles: (1) continual adaptation\nto new attacks without catastrophic forgetting, (2) few-shot adaptation, (3)\nmemory-efficient adaptation, and (4) high accuracy on both clean and\nadversarial data. We explore and integrate cutting-edge continual learning,\nfew-shot learning, and ensemble learning techniques to qualify the principles.\nExtensive experiments validate the effectiveness of our approach against\nmultiple stages of modern adversarial attacks and demonstrate significant\nimprovements over numerous baseline methods. In particular, CAD is capable of\nquickly adapting with minimal budget and a low cost of defense failure while\nmaintaining good performance against previous attacks. Our research sheds light\non a brand-new paradigm for continual defense adaptation against dynamic and\nevolving attacks.\n","authors":["Qian Wang","Yaoyao Liu","Hefei Ling","Yingwei Li","Qihao Liu","Ping Li","Jiazhong Chen","Alan Yuille","Ning Yu"],"pdf_url":"https://arxiv.org/pdf/2312.09481v4.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2409.17104v1","updated":"2024-09-25T17:16:53Z","published":"2024-09-25T17:16:53Z","title":"Language-oriented Semantic Communication for Image Transmission with\n  Fine-Tuned Diffusion Model","summary":"  Ubiquitous image transmission in emerging applications brings huge overheads\nto limited wireless resources. Since that text has the characteristic of\nconveying a large amount of information with very little data, the transmission\nof the descriptive text of an image can reduce the amount of transmitted data.\nIn this context, this paper develops a novel semantic communication framework\nbased on a text-2-image generative model (Gen-SC). In particular, a transmitter\nconverts the input image to textual modality data. Then the text is transmitted\nthrough a noisy channel to the receiver. The receiver then uses the received\ntext to generate images. Additionally, to improve the robustness of text\ntransmission over noisy channels, we designed a transformer-based text\ntransmission codec model. Moreover, we obtained a personalized knowledge base\nby fine-tuning the diffusion model to meet the requirements of task-oriented\ntransmission scenarios. Simulation results show that the proposed framework can\nachieve high perceptual quality with reducing the transmitted data volume by up\nto 99% and is robust to wireless channel noise in terms of portrait image\ntransmission.\n","authors":["Xinfeng Wei","Haonan Tong","Nuocheng Yang","Changchuan Yin"],"pdf_url":"https://arxiv.org/pdf/2409.17104v1.pdf","comment":"6 pages, 9 figures, accepted by Wireless Communications and Signal\n  Processing (WCSP) 2024"},{"id":"http://arxiv.org/abs/2409.16937v1","updated":"2024-09-25T13:51:19Z","published":"2024-09-25T13:51:19Z","title":"Semi-Supervised Cognitive State Classification from Speech with\n  Multi-View Pseudo-Labeling","summary":"  The lack of labeled data is a common challenge in speech classification\ntasks, particularly those requiring extensive subjective assessment, such as\ncognitive state classification. In this work, we propose a Semi-Supervised\nLearning (SSL) framework, introducing a novel multi-view pseudo-labeling method\nthat leverages both acoustic and linguistic characteristics to select the most\nconfident data for training the classification model. Acoustically, unlabeled\ndata are compared to labeled data using the Frechet audio distance, calculated\nfrom embeddings generated by multiple audio encoders. Linguistically, large\nlanguage models are prompted to revise automatic speech recognition\ntranscriptions and predict labels based on our proposed task-specific\nknowledge. High-confidence data are identified when pseudo-labels from both\nsources align, while mismatches are treated as low-confidence data. A bimodal\nclassifier is then trained to iteratively label the low-confidence data until a\npredefined criterion is met. We evaluate our SSL framework on emotion\nrecognition and dementia detection tasks. Experimental results demonstrate that\nour method achieves competitive performance compared to fully supervised\nlearning using only 30% of the labeled data and significantly outperforms two\nselected baselines.\n","authors":["Yuanchao Li","Zixing Zhang","Jing Han","Peter Bell","Catherine Lai"],"pdf_url":"https://arxiv.org/pdf/2409.16937v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.05445v2","updated":"2024-09-25T11:02:53Z","published":"2024-08-10T05:33:05Z","title":"Navigating Weight Prediction with Diet Diary","summary":"  Current research in food analysis primarily concentrates on tasks such as\nfood recognition, recipe retrieval and nutrition estimation from a single\nimage. Nevertheless, there is a significant gap in exploring the impact of food\nintake on physiological indicators (e.g., weight) over time. This paper\naddresses this gap by introducing the DietDiary dataset, which encompasses\ndaily dietary diaries and corresponding weight measurements of real users.\nFurthermore, we propose a novel task of weight prediction with a dietary diary\nthat aims to leverage historical food intake and weight to predict future\nweights. To tackle this task, we propose a model-agnostic time series\nforecasting framework. Specifically, we introduce a Unified Meal Representation\nLearning (UMRL) module to extract representations for each meal. Additionally,\nwe design a diet-aware loss function to associate food intake with weight\nvariations. By conducting experiments on the DietDiary dataset with two\nstate-of-the-art time series forecasting models, NLinear and iTransformer, we\ndemonstrate that our proposed framework achieves superior performance compared\nto the original models. We make our dataset, code, and models publicly\navailable at: https://yxg1005.github.io/weight-prediction/.\n","authors":["Yinxuan Gui","Bin Zhu","Jingjing Chen","Chong-Wah Ngo","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2408.05445v2.pdf","comment":"ACM MM'24 oral"},{"id":"http://arxiv.org/abs/2403.00781v3","updated":"2024-09-25T06:31:09Z","published":"2024-02-18T06:07:17Z","title":"ChatDiet: Empowering Personalized Nutrition-Oriented Food Recommender\n  Chatbots through an LLM-Augmented Framework","summary":"  The profound impact of food on health necessitates advanced\nnutrition-oriented food recommendation services. Conventional methods often\nlack the crucial elements of personalization, explainability, and\ninteractivity. While Large Language Models (LLMs) bring interpretability and\nexplainability, their standalone use falls short of achieving true\npersonalization. In this paper, we introduce ChatDiet, a novel LLM-powered\nframework designed specifically for personalized nutrition-oriented food\nrecommendation chatbots. ChatDiet integrates personal and population models,\ncomplemented by an orchestrator, to seamlessly retrieve and process pertinent\ninformation. The personal model leverages causal discovery and inference\ntechniques to assess personalized nutritional effects for a specific user,\nwhereas the population model provides generalized information on food\nnutritional content. The orchestrator retrieves, synergizes and delivers the\noutput of both models to the LLM, providing tailored food recommendations\ndesigned to support targeted health outcomes. The result is a dynamic delivery\nof personalized and explainable food recommendations, tailored to individual\nuser preferences. Our evaluation of ChatDiet includes a compelling case study,\nwhere we establish a causal personal model to estimate individual nutrition\neffects. Our assessments, including a food recommendation test showcasing a\n92\\% effectiveness rate, coupled with illustrative dialogue examples,\nunderscore ChatDiet's strengths in explainability, personalization, and\ninteractivity.\n","authors":["Zhongqi Yang","Elahe Khatibi","Nitish Nagesh","Mahyar Abbasian","Iman Azimi","Ramesh Jain","Amir M. Rahmani"],"pdf_url":"https://arxiv.org/pdf/2403.00781v3.pdf","comment":"Published on Smart Health"},{"id":"http://arxiv.org/abs/2404.15637v2","updated":"2024-09-25T01:17:48Z","published":"2024-04-24T04:18:31Z","title":"HybridVC: Efficient Voice Style Conversion with Text and Audio Prompts","summary":"  We introduce HybridVC, a voice conversion (VC) framework built upon a\npre-trained conditional variational autoencoder (CVAE) that combines the\nstrengths of a latent model with contrastive learning. HybridVC supports text\nand audio prompts, enabling more flexible voice style conversion. HybridVC\nmodels a latent distribution conditioned on speaker embeddings acquired by a\npretrained speaker encoder and optimises style text embeddings to align with\nthe speaker style information through contrastive learning in parallel.\nTherefore, HybridVC can be efficiently trained under limited computational\nresources. Our experiments demonstrate HybridVC's superior training efficiency\nand its capability for advanced multi-modal voice style conversion. This\nunderscores its potential for widespread applications such as user-defined\npersonalised voice in various social media platforms. A comprehensive ablation\nstudy further validates the effectiveness of our method.\n","authors":["Xinlei Niu","Jing Zhang","Charles Patrick Martin"],"pdf_url":"https://arxiv.org/pdf/2404.15637v2.pdf","comment":"Proceedings of Interspeech"}]},"2024-09-24T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2409.09201v2","updated":"2024-09-24T23:39:49Z","published":"2024-09-13T21:28:54Z","title":"Contextual Evaluation of Large Language Models for Classifying Tropical\n  and Infectious Diseases","summary":"  While large language models (LLMs) have shown promise for medical question\nanswering, there is limited work focused on tropical and infectious\ndisease-specific exploration. We build on an opensource tropical and infectious\ndiseases (TRINDs) dataset, expanding it to include demographic and semantic\nclinical and consumer augmentations yielding 11000+ prompts. We evaluate LLM\nperformance on these, comparing generalist and medical LLMs, as well as LLM\noutcomes to human experts. We demonstrate through systematic experimentation,\nthe benefit of contextual information such as demographics, location, gender,\nrisk factors for optimal LLM response. Finally we develop a prototype of\nTRINDs-LM, a research tool that provides a playground to navigate how context\nimpacts LLM outputs for health.\n","authors":["Mercy Asiedu","Nenad Tomasev","Chintan Ghate","Tiya Tiyasirichokchai","Awa Dieng","Oluwatosin Akande","Geoffrey Siwo","Steve Adudans","Sylvanus Aitkins","Odianosen Ehiakhamen","Eric Ndombi","Katherine Heller"],"pdf_url":"https://arxiv.org/pdf/2409.09201v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14028v3","updated":"2024-09-24T23:23:50Z","published":"2024-08-26T05:38:27Z","title":"SurGen: Text-Guided Diffusion Model for Surgical Video Generation","summary":"  Diffusion-based video generation models have made significant strides,\nproducing outputs with improved visual fidelity, temporal coherence, and user\ncontrol. These advancements hold great promise for improving surgical education\nby enabling more realistic, diverse, and interactive simulation environments.\nIn this study, we introduce SurGen, a text-guided diffusion model tailored for\nsurgical video synthesis. SurGen produces videos with the highest resolution\nand longest duration among existing surgical video generation models. We\nvalidate the visual and temporal quality of the outputs using standard image\nand video generation metrics. Additionally, we assess their alignment to the\ncorresponding text prompts through a deep learning classifier trained on\nsurgical data. Our results demonstrate the potential of diffusion models to\nserve as valuable educational tools for surgical trainees.\n","authors":["Joseph Cho","Samuel Schmidgall","Cyril Zakka","Mrudang Mathur","Dhamanpreet Kaur","Rohan Shad","William Hiesinger"],"pdf_url":"https://arxiv.org/pdf/2408.14028v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.11800v2","updated":"2024-09-24T22:50:15Z","published":"2024-08-21T17:43:11Z","title":"WeQA: A Benchmark for Retrieval Augmented Generation in Wind Energy\n  Domain","summary":"  In the rapidly evolving landscape of Natural Language Processing (NLP) and\ntext generation, the emergence of Retrieval Augmented Generation (RAG) presents\na promising avenue for improving the quality and reliability of generated text\nby leveraging information retrieved from user specified database. Benchmarking\nis essential to evaluate and compare the performance of the different RAG\nconfigurations in terms of retriever and generator, providing insights into\ntheir effectiveness, scalability, and suitability for the specific domain and\napplications. In this paper, we present a comprehensive framework to generate a\ndomain relevant RAG benchmark. Our framework is based on automatic\nquestion-answer generation with Human (domain experts)-AI Large Language Model\n(LLM) teaming. As a case study, we demonstrate the framework by introducing\nWeQA, a first-of-its-kind benchmark on the wind energy domain which comprises\nof multiple scientific documents/reports related to environmental impact of\nwind energy projects. Our framework systematically evaluates RAG performance\nusing diverse metrics and multiple question types with varying complexity\nlevel. We also demonstrate the performance of different models on our\nbenchmark.\n","authors":["Rounak Meyur","Hung Phan","Sridevi Wagle","Jan Strube","Mahantesh Halappanavar","Sameera Horawalavithana","Anurag Acharya","Sai Munikoti"],"pdf_url":"https://arxiv.org/pdf/2408.11800v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16494v1","updated":"2024-09-24T22:36:58Z","published":"2024-09-24T22:36:58Z","title":"A Unified Hallucination Mitigation Framework for Large Vision-Language\n  Models","summary":"  Hallucination is a common problem for Large Vision-Language Models (LVLMs)\nwith long generations which is difficult to eradicate. The generation with\nhallucinations is partially inconsistent with the image content. To mitigate\nhallucination, current studies either focus on the process of model inference\nor the results of model generation, but the solutions they design sometimes do\nnot deal appropriately with various types of queries and the hallucinations of\nthe generations about these queries. To accurately deal with various\nhallucinations, we present a unified framework, Dentist, for hallucination\nmitigation. The core step is to first classify the queries, then perform\ndifferent processes of hallucination mitigation based on the classification\nresult, just like a dentist first observes the teeth and then makes a plan. In\na simple deployment, Dentist can classify queries as perception or reasoning\nand easily mitigate potential hallucinations in answers which has been\ndemonstrated in our experiments. On MMbench, we achieve a 13.44%/10.2%/15.8%\nimprovement in accuracy on Image Quality, a Coarse Perception visual question\nanswering (VQA) task, over the baseline InstructBLIP/LLaVA/VisualGLM.\n","authors":["Yue Chang","Liqiang Jing","Xiaopeng Zhang","Yue Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.16494v1.pdf","comment":"Accepted by TMLR"},{"id":"http://arxiv.org/abs/2409.16490v1","updated":"2024-09-24T22:31:39Z","published":"2024-09-24T22:31:39Z","title":"Exploring Knowledge Tracing in Tutor-Student Dialogues","summary":"  Recent advances in large language models (LLMs) have led to the development\nof artificial intelligence (AI)-powered tutoring chatbots, showing promise in\nproviding broad access to high-quality personalized education. Existing works\nhave primarily studied how to make LLMs follow tutoring principles but not how\nto model student behavior in dialogues. However, analyzing student dialogue\nturns can serve as a formative assessment, since open-ended student discourse\nmay indicate their knowledge levels and reveal specific misconceptions. In this\nwork, we present a first attempt at performing knowledge tracing (KT) in\ntutor-student dialogues. We propose LLM prompting methods to identify the\nknowledge components/skills involved in each dialogue turn and diagnose whether\nthe student responds correctly to the tutor, and verify the LLM's effectiveness\nvia an expert human evaluation. We then apply a range of KT methods on the\nresulting labeled data to track student knowledge levels over an entire\ndialogue. We conduct experiments on two tutoring dialogue datasets, and show\nthat a novel yet simple LLM-based method, LLMKT, significantly outperforms\nexisting KT methods in predicting student response correctness in dialogues. We\nperform extensive qualitative analyses to highlight the challenges in dialogue\nKT and outline multiple avenues for future work.\n","authors":["Alexander Scarlatos","Andrew Lan"],"pdf_url":"https://arxiv.org/pdf/2409.16490v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18895v2","updated":"2024-09-24T22:23:07Z","published":"2024-06-27T05:17:04Z","title":"Can we teach language models to gloss endangered languages?","summary":"  Interlinear glossed text (IGT) is a popular format in language documentation\nprojects, where each morpheme is labeled with a descriptive annotation.\nAutomating the creation of interlinear glossed text can be desirable to reduce\nannotator effort and maintain consistency across annotated corpora. Prior\nresearch has explored a number of statistical and neural methods for\nautomatically producing IGT.\n  As large language models (LLMs) have showed promising results across\nmultilingual tasks, even for rare, endangered languages, it is natural to\nwonder whether they can be utilized for the task of generating IGT. We explore\nwhether LLMs can be effective at the task of interlinear glossing with\nin-context learning, without any traditional training. We propose new\napproaches for selecting examples to provide in-context, observing that\ntargeted selection can significantly improve performance. We find that\nLLM-based methods beat standard transformer baselines, despite requiring no\ntraining at all. These approaches still underperform state-of-the-art\nsupervised systems for the task, but are highly practical for researchers\noutside of the NLP community, requiring minimal effort to use.\n","authors":["Michael Ginn","Mans Hulden","Alexis Palmer"],"pdf_url":"https://arxiv.org/pdf/2406.18895v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16469v1","updated":"2024-09-24T21:42:25Z","published":"2024-09-24T21:42:25Z","title":"Spelling Correction through Rewriting of Non-Autoregressive ASR Lattices","summary":"  For end-to-end Automatic Speech Recognition (ASR) models, recognizing\npersonal or rare phrases can be hard. A promising way to improve accuracy is\nthrough spelling correction (or rewriting) of the ASR lattice, where\npotentially misrecognized phrases are replaced with acoustically similar and\ncontextually relevant alternatives. However, rewriting is challenging for ASR\nmodels trained with connectionist temporal classification (CTC) due to noisy\nhypotheses produced by a non-autoregressive, context-independent beam search.\n  We present a finite-state transducer (FST) technique for rewriting wordpiece\nlattices generated by Transformer-based CTC models. Our algorithm performs\ngrapheme-to-phoneme (G2P) conversion directly from wordpieces into phonemes,\navoiding explicit word representations and exploiting the richness of the CTC\nlattice. Our approach requires no retraining or modification of the ASR model.\nWe achieved up to a 15.2% relative reduction in sentence error rate (SER) on a\ntest set with contextually relevant entities.\n","authors":["Leonid Velikovich","Christopher Li","Diamantino Caseiro","Shankar Kumar","Pat Rondon","Kandarp Joshi","Xavier Velez"],"pdf_url":"https://arxiv.org/pdf/2409.16469v1.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2409.16461v1","updated":"2024-09-24T21:24:07Z","published":"2024-09-24T21:24:07Z","title":"Strategies for Improving NL-to-FOL Translation with LLMs: Data\n  Generation, Incremental Fine-Tuning, and Verification","summary":"  Logical reasoning is a fundamental task in natural language processing that\npresents significant challenges to Large Language Models (LLMs). The inherent\ncharacteristics of logical reasoning makes it well-suited for symbolic\nrepresentations such as first-order logic (FOL). Research in symbolic logical\nreasoning explored FOL generation using state-of-the-art LLMs (i.e., GPT-4) to\nproduce FOL translations of natural language (NL) statements, but errors in\ntranslation are usually not the focus. We address this by categorizing the\ntranslation errors in FOL statements generated by LLMs. To make progress\ntowards improving the quality of FOL translations for smaller language models\nsuch as LLaMA-2 13B and Mistral 7B, we create ProofFOL, a high-quality\nFOL-annotated subset of ProofWriter dataset using GPT-4o. The models fine-tuned\non this silver standard data achieve a significant gain in performance when\ncompared to larger language models such as LLaMA-2 70B. In addition to\nimproving the model using large data, we also tackle the issue of data scarcity\nand introduce an incremental framework encompassing of data augmentation and\nverification steps. In the augmentation process, a single pair of (premises,\nconclusion) is split into multiple new instances based on the predicates and\nFOLs. This data is used for fine-tuning, and the inference on this model\ngenerates FOLs with fewer errors over the model trained on the original data.\nOur investigation on the translation errors leads to generation of a\nperturbation dataset, which is used to train a verifier that corrects potential\nsyntactic and semantic FOL translation errors. We demonstrate an efficient\nmethod for making the most of a limited existing human-annotated dataset. Our\nresults show state-of-the-art performance for ProofWriter and ProntoQA datasets\nusing ProofFOL on LLaMA-2 and Mistral models.\n","authors":["Ramya Keerthy Thatikonda","Jiuzhou Han","Wray Buntine","Ehsan Shareghi"],"pdf_url":"https://arxiv.org/pdf/2409.16461v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16452v1","updated":"2024-09-24T20:44:30Z","published":"2024-09-24T20:44:30Z","title":"FMDLlama: Financial Misinformation Detection based on Large Language\n  Models","summary":"  The emergence of social media has made the spread of misinformation easier.\nIn the financial domain, the accuracy of information is crucial for various\naspects of financial market, which has made financial misinformation detection\n(FMD) an urgent problem that needs to be addressed. Large language models\n(LLMs) have demonstrated outstanding performance in various fields. However,\ncurrent studies mostly rely on traditional methods and have not explored the\napplication of LLMs in the field of FMD. The main reason is the lack of FMD\ninstruction tuning datasets and evaluation benchmarks. In this paper, we\npropose FMDLlama, the first open-sourced instruction-following LLMs for FMD\ntask based on fine-tuning Llama3.1 with instruction data, the first multi-task\nFMD instruction dataset (FMDID) to support LLM instruction tuning, and a\ncomprehensive FMD evaluation benchmark (FMD-B) with classification and\nexplanation generation tasks to test the FMD ability of LLMs. We compare our\nmodels with a variety of LLMs on FMD-B, where our model outperforms all other\nopen-sourced LLMs as well as ChatGPT.\n","authors":["Zhiwei Liu","Xin Zhang","Kailai Yang","Qianqian Xie","Jimin Huang","Sophia Ananiadou"],"pdf_url":"https://arxiv.org/pdf/2409.16452v1.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2312.12141v4","updated":"2024-09-24T20:36:03Z","published":"2023-12-19T13:23:18Z","title":"Neuron-Level Knowledge Attribution in Large Language Models","summary":"  Identifying important neurons for final predictions is essential for\nunderstanding the mechanisms of large language models. Due to computational\nconstraints, current attribution techniques struggle to operate at neuron\nlevel. In this paper, we propose a static method for pinpointing significant\nneurons. Compared to seven other methods, our approach demonstrates superior\nperformance across three metrics. Additionally, since most static methods\ntypically only identify \"value neurons\" directly contributing to the final\nprediction, we propose a method for identifying \"query neurons\" which activate\nthese \"value neurons\". Finally, we apply our methods to analyze six types of\nknowledge across both attention and feed-forward network (FFN) layers. Our\nmethod and analysis are helpful for understanding the mechanisms of knowledge\nstorage and set the stage for future research in knowledge editing. The code is\navailable on https://github.com/zepingyu0512/neuron-attribution.\n","authors":["Zeping Yu","Sophia Ananiadou"],"pdf_url":"https://arxiv.org/pdf/2312.12141v4.pdf","comment":"Accepted by EMNLP 2024 main. This paper aims to identify the\n  important neurons in large language models"},{"id":"http://arxiv.org/abs/2402.02872v3","updated":"2024-09-24T20:27:53Z","published":"2024-02-05T10:39:32Z","title":"How do Large Language Models Learn In-Context? Query and Key Matrices of\n  In-Context Heads are Two Towers for Metric Learning","summary":"  We investigate the mechanism of in-context learning (ICL) on sentence\nclassification tasks with semantically-unrelated labels (\"foo\"/\"bar\"). We find\nintervening in only 1\\% heads (named \"in-context heads\") significantly affects\nICL accuracy from 87.6\\% to 24.4\\%. To understand this phenomenon, we analyze\nthe value-output vectors in these heads and discover that the vectors at each\nlabel position contain substantial information about the corresponding labels.\nFurthermore, we observe that the prediction shift from \"foo\" to \"bar\" is due to\nthe respective reduction and increase in these heads' attention scores at \"foo\"\nand \"bar\" positions. Therefore, we propose a hypothesis for ICL: in in-context\nheads, the value-output matrices extract label features, while the query-key\nmatrices compute the similarity between the features at the last position and\nthose at each label position. The query and key matrices can be considered as\ntwo towers that learn the similarity metric between the last position's\nfeatures and each demonstration at label positions. Using this hypothesis, we\nexplain the majority label bias and recency bias in ICL and propose two methods\nto reduce these biases by 22\\% and 17\\%, respectively.\n","authors":["Zeping Yu","Sophia Ananiadou"],"pdf_url":"https://arxiv.org/pdf/2402.02872v3.pdf","comment":"Accepted by EMNLP 2024 main. Mechanistic interpretability for\n  in-contexting in large language models"},{"id":"http://arxiv.org/abs/2409.16430v1","updated":"2024-09-24T19:50:38Z","published":"2024-09-24T19:50:38Z","title":"A Comprehensive Survey of Bias in LLMs: Current Landscape and Future\n  Directions","summary":"  Large Language Models(LLMs) have revolutionized various applications in\nnatural language processing (NLP) by providing unprecedented text generation,\ntranslation, and comprehension capabilities. However, their widespread\ndeployment has brought to light significant concerns regarding biases embedded\nwithin these models. This paper presents a comprehensive survey of biases in\nLLMs, aiming to provide an extensive review of the types, sources, impacts, and\nmitigation strategies related to these biases. We systematically categorize\nbiases into several dimensions. Our survey synthesizes current research\nfindings and discusses the implications of biases in real-world applications.\nAdditionally, we critically assess existing bias mitigation techniques and\npropose future research directions to enhance fairness and equity in LLMs. This\nsurvey serves as a foundational resource for researchers, practitioners, and\npolicymakers concerned with addressing and understanding biases in LLMs.\n","authors":["Rajesh Ranjan","Shailja Gupta","Surya Narayan Singh"],"pdf_url":"https://arxiv.org/pdf/2409.16430v1.pdf","comment":"2 Tables, 1 Figure"},{"id":"http://arxiv.org/abs/2409.16399v1","updated":"2024-09-24T18:58:23Z","published":"2024-09-24T18:58:23Z","title":"Revisiting Acoustic Features for Robust ASR","summary":"  Automatic Speech Recognition (ASR) systems must be robust to the myriad types\nof noises present in real-world environments including environmental noise,\nroom impulse response, special effects as well as attacks by malicious actors\n(adversarial attacks). Recent works seek to improve accuracy and robustness by\ndeveloping novel Deep Neural Networks (DNNs) and curating diverse training\ndatasets for them, while using relatively simple acoustic features. While this\napproach improves robustness to the types of noise present in the training\ndata, it confers limited robustness against unseen noises and negligible\nrobustness to adversarial attacks. In this paper, we revisit the approach of\nearlier works that developed acoustic features inspired by biological auditory\nperception that could be used to perform accurate and robust ASR. In contrast,\nSpecifically, we evaluate the ASR accuracy and robustness of several\nbiologically inspired acoustic features. In addition to several features from\nprior works, such as gammatone filterbank features (GammSpec), we also propose\ntwo new acoustic features called frequency masked spectrogram (FreqMask) and\ndifference of gammatones spectrogram (DoGSpec) to simulate the\nneuro-psychological phenomena of frequency masking and lateral suppression.\nExperiments on diverse models and datasets show that (1) DoGSpec achieves\nsignificantly better robustness than the highly popular log mel spectrogram\n(LogMelSpec) with minimal accuracy degradation, and (2) GammSpec achieves\nbetter accuracy and robustness to non-adversarial noises from the Speech Robust\nBench benchmark, but it is outperformed by DoGSpec against adversarial attacks.\n","authors":["Muhammad A. Shah","Bhiksha Raj"],"pdf_url":"https://arxiv.org/pdf/2409.16399v1.pdf","comment":"submitted to ICASSP 2025"},{"id":"http://arxiv.org/abs/2409.16383v1","updated":"2024-09-24T18:35:09Z","published":"2024-09-24T18:35:09Z","title":"RISCORE: Enhancing In-Context Riddle Solving in Language Models through\n  Context-Reconstructed Example Augmentation","summary":"  Riddle-solving requires advanced reasoning skills, pushing LLMs to engage in\nabstract thinking and creative problem-solving, often revealing limitations in\ntheir cognitive abilities. In this paper, we examine the riddle-solving\ncapabilities of LLMs using a multiple-choice format, exploring how different\nprompting techniques impact performance on riddles that demand diverse\nreasoning skills. To enhance results, we introduce RISCORE (RIddle Solving with\nCOntext REcontruciton) a novel fully automated prompting method that generates\nand utilizes contextually reconstructed sentence-based puzzles in conjunction\nwith the original examples to create few-shot exemplars. Our experiments\ndemonstrate that RISCORE significantly improves the performance of language\nmodels in both vertical and lateral thinking tasks, surpassing traditional\nexemplar selection strategies across a variety of few-shot settings.\n","authors":["Ioannis Panagiotopoulos","Giorgos Filandrianos","Maria Lymperaiou","Giorgos Stamou"],"pdf_url":"https://arxiv.org/pdf/2409.16383v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16371v1","updated":"2024-09-24T18:05:10Z","published":"2024-09-24T18:05:10Z","title":"Do the Right Thing, Just Debias! Multi-Category Bias Mitigation Using\n  LLMs","summary":"  This paper tackles the challenge of building robust and generalizable bias\nmitigation models for language. Recognizing the limitations of existing\ndatasets, we introduce ANUBIS, a novel dataset with 1507 carefully curated\nsentence pairs encompassing nine social bias categories. We evaluate\nstate-of-the-art models like T5, utilizing Supervised Fine-Tuning (SFT),\nReinforcement Learning (PPO, DPO), and In-Context Learning (ICL) for effective\nbias mitigation. Our analysis focuses on multi-class social bias reduction,\ncross-dataset generalizability, and environmental impact of the trained models.\nANUBIS and our findings offer valuable resources for building more equitable AI\nsystems and contribute to the development of responsible and unbiased\ntechnologies with broad societal impact.\n","authors":["Amartya Roy","Danush Khanna","Devanshu Mahapatra"," Vasanthakumar","Avirup Das","Kripabandhu Ghosh"],"pdf_url":"https://arxiv.org/pdf/2409.16371v1.pdf","comment":"17 pages, 5 Figures"},{"id":"http://arxiv.org/abs/2409.13335v2","updated":"2024-09-24T17:40:26Z","published":"2024-09-20T08:56:09Z","title":"Beyond the binary: Limitations and possibilities of gender-related\n  speech technology research","summary":"  This paper presents a review of 107 research papers relating to speech and\nsex or gender in ISCA Interspeech publications between 2013 and 2023. We note\nthe scarcity of work on this topic and find that terminology, particularly the\nword gender, is used in ways that are underspecified and often out of step with\nthe prevailing view in social sciences that gender is socially constructed and\nis a spectrum as opposed to a binary category. We draw attention to the\npotential problems that this can cause for already marginalised groups, and\nsuggest some questions for researchers to ask themselves when undertaking work\non speech and gender.\n","authors":["Ariadna Sanchez","Alice Ross","Nina Markl"],"pdf_url":"https://arxiv.org/pdf/2409.13335v2.pdf","comment":"Accepted at Spoken Language Technology (SLT) Workshop 2024"},{"id":"http://arxiv.org/abs/2409.07638v2","updated":"2024-09-24T17:34:07Z","published":"2024-09-11T21:48:33Z","title":"Can We Count on LLMs? The Fixed-Effect Fallacy and Claims of GPT-4\n  Capabilities","summary":"  In this paper we explore evaluation of LLM capabilities. We present\nmeasurements of GPT-4 performance on several deterministic tasks; each task\ninvolves a basic calculation and takes as input parameter some element drawn\nfrom a large well-defined population (e.g., count elements in a list, multiply\ntwo k-digit numbers, etc). We examine several conditions per-task and perform\nenough trials so that statistically significant differences can be detected.\nThis allows us to investigate the sensitivity of task-accuracy both to query\nphrasing and input parameter population. We find that seemingly trivial\nmodifications in the task-prompt or input population can yield differences far\nlarger than can be explained by sampling effects. For example, performance on a\nsimple list-counting task varies with query-phrasing and list-length, but also\nwith list composition (i.e., the thing-to-be-counted) and object frequency\n(e.g., success when an element accounts for $\\approx$ 50\\% of a list is\ndifferent from when it accounts for $\\approx$ 70\\% etc).\n  We conclude that efforts to quantify LLM capabilities easily succumb to the\nlanguage-as-fixed-effect fallacy, where experimental observations are\nimproperly generalized beyond what the data supports. A consequence appears to\nbe that intuitions that have been formed based on interactions with humans form\na very unreliable guide as to which input modifications should ``make no\ndifference'' to LLM performance.\n","authors":["Thomas Ball","Shuo Chen","Cormac Herley"],"pdf_url":"https://arxiv.org/pdf/2409.07638v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16341v1","updated":"2024-09-24T17:20:02Z","published":"2024-09-24T17:20:02Z","title":"Quality Matters: Evaluating Synthetic Data for Tool-Using LLMs","summary":"  Training large language models (LLMs) for external tool usage is a rapidly\nexpanding field, with recent research focusing on generating synthetic data to\naddress the shortage of available data. However, the absence of systematic data\nquality checks poses complications for properly training and testing models. To\nthat end, we propose two approaches for assessing the reliability of data for\ntraining LLMs to use external tools. The first approach uses intuitive,\nhuman-defined correctness criteria. The second approach uses a model-driven\nassessment with in-context evaluation. We conduct a thorough evaluation of data\nquality on two popular benchmarks, followed by an extrinsic evaluation that\nshowcases the impact of data quality on model performance. Our results\ndemonstrate that models trained on high-quality data outperform those trained\non unvalidated data, even when trained with a smaller quantity of data. These\nfindings empirically support the significance of assessing and ensuring the\nreliability of training data for tool-using LLMs.\n","authors":["Shadi Iskander","Nachshon Cohen","Zohar Karnin","Ori Shapira","Sofia Tolmach"],"pdf_url":"https://arxiv.org/pdf/2409.16341v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16243v1","updated":"2024-09-24T17:07:45Z","published":"2024-09-24T17:07:45Z","title":"A fast and sound tagging method for discontinuous named-entity\n  recognition","summary":"  We introduce a novel tagging scheme for discontinuous named entity\nrecognition based on an explicit description of the inner structure of\ndiscontinuous mentions. We rely on a weighted finite state automaton for both\nmarginal and maximum a posteriori inference. As such, our method is sound in\nthe sense that (1) well-formedness of predicted tag sequences is ensured via\nthe automaton structure and (2) there is an unambiguous mapping between\nwell-formed sequences of tags and (discontinuous) mentions. We evaluate our\napproach on three English datasets in the biomedical domain, and report\ncomparable results to state-of-the-art while having a way simpler and faster\nmodel.\n","authors":["Caio Corro"],"pdf_url":"https://arxiv.org/pdf/2409.16243v1.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2409.15272v2","updated":"2024-09-24T16:51:45Z","published":"2024-09-23T17:59:05Z","title":"OmniBench: Towards The Future of Universal Omni-Language Models","summary":"  Recent advancements in multimodal large language models (MLLMs) have aimed to\nintegrate and interpret data across diverse modalities. However, the capacity\nof these models to concurrently process and reason about multiple modalities\nremains inadequately explored, partly due to the lack of comprehensive\nmodality-wise benchmarks. We introduce OmniBench, a novel benchmark designed to\nrigorously evaluate models' ability to recognize, interpret, and reason across\nvisual, acoustic, and textual inputs simultaneously. We define models capable\nof such tri-modal processing as omni-language models (OLMs). OmniBench is\ndistinguished by high-quality human annotations, ensuring that accurate\nresponses require integrated understanding and reasoning across all three\nmodalities. Our main findings reveal that: i) most OLMs exhibit critical\nlimitations in instruction-following and reasoning capabilities within\ntri-modal contexts; and ii) most baselines models perform poorly (below 50\\%\naccuracy) even when provided with alternative textual representations of images\nor/and audio. These results suggest that the ability to construct a consistent\ncontext from text, image, and audio is often overlooked in existing MLLM\ntraining paradigms. We advocate for future research to focus on developing more\nrobust tri-modal integration techniques and training strategies to enhance OLM\nperformance across diverse modalities. The codes and live leaderboard could be\nfound at https://m-a-p.ai/OmniBench.\n","authors":["Yizhi Li","Ge Zhang","Yinghao Ma","Ruibin Yuan","Kang Zhu","Hangyu Guo","Yiming Liang","Jiaheng Liu","Jian Yang","Siwei Wu","Xingwei Qu","Jinjie Shi","Xinyue Zhang","Zhenzhu Yang","Xiangzhou Wang","Zhaoxiang Zhang","Zachary Liu","Emmanouil Benetos","Wenhao Huang","Chenghua Lin"],"pdf_url":"https://arxiv.org/pdf/2409.15272v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16235v1","updated":"2024-09-24T16:51:36Z","published":"2024-09-24T16:51:36Z","title":"EuroLLM: Multilingual Language Models for Europe","summary":"  The quality of open-weight LLMs has seen significant improvement, yet they\nremain predominantly focused on English. In this paper, we introduce the\nEuroLLM project, aimed at developing a suite of open-weight multilingual LLMs\ncapable of understanding and generating text in all official European Union\nlanguages, as well as several additional relevant languages. We outline the\nprogress made to date, detailing our data collection and filtering process, the\ndevelopment of scaling laws, the creation of our multilingual tokenizer, and\nthe data mix and modeling configurations. Additionally, we release our initial\nmodels: EuroLLM-1.7B and EuroLLM-1.7B-Instruct and report their performance on\nmultilingual general benchmarks and machine translation.\n","authors":["Pedro Henrique Martins","Patrick Fernandes","João Alves","Nuno M. Guerreiro","Ricardo Rei","Duarte M. Alves","José Pombal","Amin Farajian","Manuel Faysse","Mateusz Klimaszewski","Pierre Colombo","Barry Haddow","José G. C. de Souza","Alexandra Birch","André F. T. Martins"],"pdf_url":"https://arxiv.org/pdf/2409.16235v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.01721v2","updated":"2024-09-24T16:40:39Z","published":"2024-06-03T18:27:44Z","title":"DuQuant: Distributing Outliers via Dual Transformation Makes Stronger\n  Quantized LLMs","summary":"  Quantization of large language models (LLMs) faces significant challenges,\nparticularly due to the presence of outlier activations that impede efficient\nlow-bit representation. Traditional approaches predominantly address\n$\\textit{Normal Outliers}$, which are activations across all tokens with\nrelatively large magnitudes. However, these methods struggle with smoothing\n$\\textit{Massive Outliers}$ that display significantly larger values, which\nleads to significant performance degradation in low-bit quantization. In this\npaper, we introduce DuQuant, a novel approach that utilizes rotation and\npermutation transformations to more effectively mitigate both massive and\nnormal outliers. First, DuQuant starts by constructing rotation matrices, using\nspecific outlier dimensions as prior knowledge, to redistribute outliers to\nadjacent channels by block-wise rotation. Second, We further employ a zigzag\npermutation to balance the distribution of outliers across blocks, thereby\nreducing block-wise variance. A subsequent rotation further smooths the\nactivation landscape, enhancing model performance. DuQuant simplifies the\nquantization process and excels in managing outliers, outperforming the\nstate-of-the-art baselines across various sizes and types of LLMs on multiple\ntasks, even with 4-bit weight-activation quantization. Our code is available at\nhttps://github.com/Hsu1023/DuQuant.\n","authors":["Haokun Lin","Haobo Xu","Yichen Wu","Jingzhi Cui","Yingtao Zhang","Linzhan Mou","Linqi Song","Zhenan Sun","Ying Wei"],"pdf_url":"https://arxiv.org/pdf/2406.01721v2.pdf","comment":"26 pages, 13 figures, Website at https://duquant.github.io"},{"id":"http://arxiv.org/abs/2409.16220v1","updated":"2024-09-24T16:31:33Z","published":"2024-09-24T16:31:33Z","title":"Towards Enhancing Linked Data Retrieval in Conversational UIs using\n  Large Language Models","summary":"  Despite the recent broad adoption of Large Language Models (LLMs) across\nvarious domains, their potential for enriching information systems in\nextracting and exploring Linked Data (LD) and Resource Description Framework\n(RDF) triplestores has not been extensively explored. This paper examines the\nintegration of LLMs within existing systems, emphasising the enhancement of\nconversational user interfaces (UIs) and their capabilities for data extraction\nby producing more accurate SPARQL queries without the requirement for model\nretraining. Typically, conversational UI models necessitate retraining with the\nintroduction of new datasets or updates, limiting their functionality as\ngeneral-purpose extraction tools. Our approach addresses this limitation by\nincorporating LLMs into the conversational UI workflow, significantly enhancing\ntheir ability to comprehend and process user queries effectively. By leveraging\nthe advanced natural language understanding capabilities of LLMs, our method\nimproves RDF entity extraction within web systems employing conventional\nchatbots. This integration facilitates a more nuanced and context-aware\ninteraction model, critical for handling the complex query patterns often\nencountered in RDF datasets and Linked Open Data (LOD) endpoints. The\nevaluation of this methodology shows a marked enhancement in system\nexpressivity and the accuracy of responses to user queries, indicating a\npromising direction for future research in this area. This investigation not\nonly underscores the versatility of LLMs in enhancing existing information\nsystems but also sets the stage for further explorations into their potential\napplications within more specialised domains of web information systems.\n","authors":["Omar Mussa","Omer Rana","Benoît Goossens","Pablo Orozco-Terwengel","Charith Perera"],"pdf_url":"https://arxiv.org/pdf/2409.16220v1.pdf","comment":"This paper has been accepted at the 25th International Web\n  Information Systems Engineering Conference (WISE 2024)"},{"id":"http://arxiv.org/abs/2406.07424v3","updated":"2024-09-24T15:43:28Z","published":"2024-06-11T16:26:18Z","title":"MINERS: Multilingual Language Models as Semantic Retrievers","summary":"  Words have been represented in a high-dimensional vector space that encodes\ntheir semantic similarities, enabling downstream applications such as\nretrieving synonyms, antonyms, and relevant contexts. However, despite recent\nadvances in multilingual language models (LMs), the effectiveness of these\nmodels' representations in semantic retrieval contexts has not been\ncomprehensively explored. To fill this gap, this paper introduces the MINERS, a\nbenchmark designed to evaluate the ability of multilingual LMs in semantic\nretrieval tasks, including bitext mining and classification via\nretrieval-augmented contexts. We create a comprehensive framework to assess the\nrobustness of LMs in retrieving samples across over 200 diverse languages,\nincluding extremely low-resource languages in challenging cross-lingual and\ncode-switching settings. Our results demonstrate that by solely retrieving\nsemantically similar embeddings yields performance competitive with\nstate-of-the-art approaches, without requiring any fine-tuning.\n","authors":["Genta Indra Winata","Ruochen Zhang","David Ifeoluwa Adelani"],"pdf_url":"https://arxiv.org/pdf/2406.07424v3.pdf","comment":"Accepted by EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2409.16191v1","updated":"2024-09-24T15:38:11Z","published":"2024-09-24T15:38:11Z","title":"HelloBench: Evaluating Long Text Generation Capabilities of Large\n  Language Models","summary":"  In recent years, Large Language Models (LLMs) have demonstrated remarkable\ncapabilities in various tasks (e.g., long-context understanding), and many\nbenchmarks have been proposed. However, we observe that long text generation\ncapabilities are not well investigated. Therefore, we introduce the\nHierarchical Long Text Generation Benchmark (HelloBench), a comprehensive,\nin-the-wild, and open-ended benchmark to evaluate LLMs' performance in\ngenerating long text. Based on Bloom's Taxonomy, HelloBench categorizes long\ntext generation tasks into five subtasks: open-ended QA, summarization, chat,\ntext completion, and heuristic text generation. Besides, we propose\nHierarchical Long Text Evaluation (HelloEval), a human-aligned evaluation\nmethod that significantly reduces the time and effort required for human\nevaluation while maintaining a high correlation with human evaluation. We have\nconducted extensive experiments across around 30 mainstream LLMs and observed\nthat the current LLMs lack long text generation capabilities. Specifically,\nfirst, regardless of whether the instructions include explicit or implicit\nlength constraints, we observe that most LLMs cannot generate text that is\nlonger than 4000 words. Second, we observe that while some LLMs can generate\nlonger text, many issues exist (e.g., severe repetition and quality\ndegradation). Third, to demonstrate the effectiveness of HelloEval, we compare\nHelloEval with traditional metrics (e.g., ROUGE, BLEU, etc.) and LLM-as-a-Judge\nmethods, which show that HelloEval has the highest correlation with human\nevaluation. We release our code in https://github.com/Quehry/HelloBench.\n","authors":["Haoran Que","Feiyu Duan","Liqun He","Yutao Mou","Wangchunshu Zhou","Jiaheng Liu","Wenge Rong","Zekun Moore Wang","Jian Yang","Ge Zhang","Junran Peng","Zhaoxiang Zhang","Songyang Zhang","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2409.16191v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16167v1","updated":"2024-09-24T15:08:41Z","published":"2024-09-24T15:08:41Z","title":"Merging LoRAs like Playing LEGO: Pushing the Modularity of LoRA to\n  Extremes Through Rank-Wise Clustering","summary":"  Low-Rank Adaptation (LoRA) has emerged as a popular technique for fine-tuning\nlarge language models (LLMs) to various domains due to its modular design and\nwidespread availability on platforms like Huggingface. This modularity has\nsparked interest in combining multiple LoRAs to enhance LLM capabilities.\nHowever, existing methods for LoRA composition primarily focus on task-specific\nadaptations that require additional training, and current model merging\ntechniques often fail to fully leverage LoRA's modular nature, leading to\nparameter interference and performance degradation. In this paper, we\ninvestigate the feasibility of disassembling and reassembling multiple LoRAs at\na finer granularity, analogous to assembling LEGO blocks. We introduce the\nconcept of Minimal Semantic Units (MSUs), where the parameters corresponding to\neach rank in LoRA function as independent units. These MSUs demonstrate\npermutation invariance and concatenation-summation equivalence properties,\nenabling flexible combinations to create new LoRAs. Building on these insights,\nwe propose the LoRA-LEGO framework. This framework conducts rank-wise parameter\nclustering by grouping MSUs from different LoRAs into $k$ clusters. The\ncentroid of each cluster serves as a representative MSU, enabling the assembly\nof a merged LoRA with an adjusted rank of $k$. Additionally, we apply a dual\nreweighting strategy to optimize the scale of the merged LoRA. Experiments\nacross various benchmarks demonstrate that our method outperforms existing\napproaches in LoRA merging.\n","authors":["Ziyu Zhao","Tao Shen","Didi Zhu","Zexi Li","Jing Su","Xuwu Wang","Kun Kuang","Fei Wu"],"pdf_url":"https://arxiv.org/pdf/2409.16167v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06800v3","updated":"2024-09-24T15:04:49Z","published":"2024-07-09T12:14:48Z","title":"Learn and Don't Forget: Adding a New Language to ASR Foundation Models","summary":"  Foundation ASR models often support many languages, e.g. 100 languages in\nWhisper. However, there has been limited work on integrating an additional,\ntypically low-resource, language, while maintaining performance on the original\nlanguage set. Fine-tuning, while simple, may degrade the accuracy of the\noriginal set. We compare three approaches that exploit adaptation parameters:\nsoft language code tuning, train only the language code; soft prompt tuning,\ntrain prepended tokens; and LoRA where a small set of additional parameters are\noptimised. Elastic Weight Consolidation (EWC) offers an alternative compromise\nwith the potential to maintain performance in specific target languages.\nResults show that direct fine-tuning yields the best performance for the new\nlanguage but degrades existing language capabilities. EWC can address this\nissue for specific languages. If only adaptation parameters are used, the\nlanguage capabilities are maintained but at the cost of performance in the new\nlanguage.\n","authors":["Mengjie Qian","Siyuan Tang","Rao Ma","Kate M. Knill","Mark J. F. Gales"],"pdf_url":"https://arxiv.org/pdf/2407.06800v3.pdf","comment":"Proceedings of Interspeech"},{"id":"http://arxiv.org/abs/2409.16146v1","updated":"2024-09-24T14:52:14Z","published":"2024-09-24T14:52:14Z","title":"Controlling Risk of Retrieval-augmented Generation: A Counterfactual\n  Prompting Framework","summary":"  Retrieval-augmented generation (RAG) has emerged as a popular solution to\nmitigate the hallucination issues of large language models. However, existing\nstudies on RAG seldom address the issue of predictive uncertainty, i.e., how\nlikely it is that a RAG model's prediction is incorrect, resulting in\nuncontrollable risks in real-world applications. In this work, we emphasize the\nimportance of risk control, ensuring that RAG models proactively refuse to\nanswer questions with low confidence. Our research identifies two critical\nlatent factors affecting RAG's confidence in its predictions: the quality of\nthe retrieved results and the manner in which these results are utilized. To\nguide RAG models in assessing their own confidence based on these two latent\nfactors, we develop a counterfactual prompting framework that induces the\nmodels to alter these factors and analyzes the effect on their answers. We also\nintroduce a benchmarking procedure to collect answers with the option to\nabstain, facilitating a series of experiments. For evaluation, we introduce\nseveral risk-related metrics and the experimental results demonstrate the\neffectiveness of our approach.\n","authors":["Lu Chen","Ruqing Zhang","Jiafeng Guo","Yixing Fan","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2409.16146v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.00578v2","updated":"2024-09-24T14:50:52Z","published":"2023-10-01T05:37:55Z","title":"Nine-year-old children outperformed ChatGPT in emotion: Evidence from\n  Chinese writing","summary":"  ChatGPT has been demonstrated to possess significant capabilities in\ngenerating intricate, human-like text, and recent studies have established that\nits performance in theory of mind tasks is comparable to that of a\nnine-year-old child. However, it remains uncertain whether ChatGPT surpasses\nnine-year-old children in Chinese writing proficiency. To explore this, our\nstudy juxtaposed the Chinese writing performance of ChatGPT and nine-year-old\nchildren on both narrative and scientific topics, aiming to uncover the\nrelative strengths and weaknesses of ChatGPT in writing.\n  The collected data were analyzed across five linguistic dimensions: fluency,\naccuracy, complexity, cohesion, and emotion. Each dimension underwent\nassessment through precise indices. The findings revealed that nine-year-old\nchildren excelled beyond ChatGPT in terms of fluency and cohesion within their\nwriting. In contrast, ChatGPT manifested a superior performance in accuracy\ncompared to the children. Concerning complexity, children exhibited superior\nskills in science-themed writing, while ChatGPT prevailed in nature-themed\nwriting. Significantly, this research is pioneering in revealing that\nnine-year-old children convey stronger emotions than ChatGPT in their Chinese\ncompositions.\n","authors":["Siyi Cao","Yizhong Xu","Tongquan Zhou","Siruo Zhou"],"pdf_url":"https://arxiv.org/pdf/2310.00578v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16136v1","updated":"2024-09-24T14:43:14Z","published":"2024-09-24T14:43:14Z","title":"HA-FGOVD: Highlighting Fine-grained Attributes via Explicit Linear\n  Composition for Open-Vocabulary Object Detection","summary":"  Open-vocabulary object detection (OVD) models are considered to be Large\nMulti-modal Models (LMM), due to their extensive training data and a large\nnumber of parameters. Mainstream OVD models prioritize object coarse-grained\ncategory rather than focus on their fine-grained attributes, e.g., colors or\nmaterials, thus failed to identify objects specified with certain attributes.\nHowever, OVD models are pretrained on large-scale image-text pairs with rich\nattribute words, whose latent feature space can represent the global text\nfeature as a linear composition of fine-grained attribute tokens without\nhighlighting them. Therefore, we propose in this paper a universal and explicit\napproach for frozen mainstream OVD models that boosts their attribute-level\ndetection capabilities by highlighting fine-grained attributes in explicit\nlinear space. Firstly, a LLM is leveraged to highlight attribute words within\nthe input text as a zero-shot prompted task. Secondly, by strategically\nadjusting the token masks, the text encoders of OVD models extract both global\ntext and attribute-specific features, which are then explicitly composited as\ntwo vectors in linear space to form the new attribute-highlighted feature for\ndetection tasks, where corresponding scalars are hand-crafted or learned to\nreweight both two vectors. Notably, these scalars can be seamlessly transferred\namong different OVD models, which proves that such an explicit linear\ncomposition is universal. Empirical evaluation on the FG-OVD dataset\ndemonstrates that our proposed method uniformly improves fine-grained\nattribute-level OVD of various mainstream models and achieves new\nstate-of-the-art performance.\n","authors":["Yuqi Ma","Mengyin Liu","Chao Zhu","Xu-Cheng Yin"],"pdf_url":"https://arxiv.org/pdf/2409.16136v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2310.15961v2","updated":"2024-09-24T14:40:57Z","published":"2023-10-24T16:03:57Z","title":"Mixture of Tokens: Continuous MoE through Cross-Example Aggregation","summary":"  Mixture of Experts (MoE) models based on Transformer architecture are pushing\nthe boundaries of language and vision tasks. The allure of these models lies in\ntheir ability to substantially increase the parameter count without a\ncorresponding increase in FLOPs. Most widely adopted MoE models are\ndiscontinuous with respect to their parameters - often referred to as sparse.\nAt the same time, existing continuous MoE designs either lag behind their\nsparse counterparts or are incompatible with autoregressive decoding. Motivated\nby the observation that the adaptation of fully continuous methods has been an\noverarching trend in deep learning, we develop Mixture of Tokens (MoT), a\nsimple, continuous architecture that is capable of scaling the number of\nparameters similarly to sparse MoE models. Unlike conventional methods, MoT\nassigns mixtures of tokens from different examples to each expert. This\narchitecture is fully compatible with autoregressive training and generation.\nOur best models not only achieve a 3x increase in training speed over dense\nTransformer models in language pretraining but also match the performance of\nstate-of-the-art MoE architectures. Additionally, a close connection between\nMoT and MoE is demonstrated through a novel technique we call transition\ntuning.\n","authors":["Szymon Antoniak","Michał Krutul","Maciej Pióro","Jakub Krajewski","Jan Ludziejewski","Kamil Ciebiera","Krystian Król","Tomasz Odrzygóźdź","Marek Cygan","Sebastian Jaszczur"],"pdf_url":"https://arxiv.org/pdf/2310.15961v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16133v1","updated":"2024-09-24T14:40:44Z","published":"2024-09-24T14:40:44Z","title":"Implicit assessment of language learning during practice as accurate as\n  explicit testing","summary":"  Assessment of proficiency of the learner is an essential part of Intelligent\nTutoring Systems (ITS). We use Item Response Theory (IRT) in computer-aided\nlanguage learning for assessment of student ability in two contexts: in test\nsessions, and in exercises during practice sessions. Exhaustive testing across\na wide range of skills can provide a detailed picture of proficiency, but may\nbe undesirable for a number of reasons. Therefore, we first aim to replace\nexhaustive tests with efficient but accurate adaptive tests. We use learner\ndata collected from exhaustive tests under imperfect conditions, to train an\nIRT model to guide adaptive tests. Simulations and experiments with real\nlearner data confirm that this approach is efficient and accurate. Second, we\nexplore whether we can accurately estimate learner ability directly from the\ncontext of practice with exercises, without testing. We transform learner data\ncollected from exercise sessions into a form that can be used for IRT modeling.\nThis is done by linking the exercises to {\\em linguistic constructs}; the\nconstructs are then treated as \"items\" within IRT. We present results from\nlarge-scale studies with thousands of learners. Using teacher assessments of\nstudent ability as \"ground truth,\" we compare the estimates obtained from tests\nvs. those from exercises. The experiments confirm that the IRT models can\nproduce accurate ability estimation based on exercises.\n","authors":["Jue Hou","Anisia Katinskaia","Anh-Duc Vu","Roman Yangarber"],"pdf_url":"https://arxiv.org/pdf/2409.16133v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16120v1","updated":"2024-09-24T14:30:21Z","published":"2024-09-24T14:30:21Z","title":"MOSS: Enabling Code-Driven Evolution and Context Management for AI\n  Agents","summary":"  Developing AI agents powered by large language models (LLMs) faces\nsignificant challenges in achieving true Turing completeness and adaptive,\ncode-driven evolution. Current approaches often generate code independently of\nits runtime context, relying heavily on the LLM's memory, which results in\ninefficiencies and limits adaptability. Manual protocol development in sandbox\nenvironments further constrains the agent's autonomous adaptability. Crucially,\nachieving consistency in code and context across multi-turn interactions and\nensuring isolation of local variables within each interaction remains an\nunsolved problem.\n  We introduce MOSS (llM-oriented Operating System Simulation), a novel\nframework that addresses these challenges by integrating code generation with a\ndynamic context management system. MOSS ensures consistency and adaptability by\nusing a mechanism that maintains the Python context across interactions,\nincluding isolation of local variables and preservation of runtime integrity.\nAt its core, the framework employs an Inversion of Control (IoC) container in\nconjunction with decorators to enforce the least knowledge principle, allowing\nagents to focus on abstract interfaces rather than concrete implementations.\nThis facilitates seamless integration of new tools and libraries, enables\nruntime instance replacement, and reduces prompt complexity, providing a \"what\nyou see is what you get\" environment for the agent.\n  Through a series of case studies, we show how this framework can enhance the\nefficiency and capabilities of agent development and highlight its advantages\nin moving towards Turing-complete agents capable of evolving through code.\n","authors":["Ming Zhu","Yi Zhou"],"pdf_url":"https://arxiv.org/pdf/2409.16120v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16221v2","updated":"2024-09-24T14:25:58Z","published":"2024-07-23T06:56:54Z","title":"Do LLMs Know When to NOT Answer? Investigating Abstention Abilities of\n  Large Language Models","summary":"  Abstention Ability (AA) is a critical aspect of Large Language Model (LLM)\nreliability, referring to an LLM's capability to withhold responses when\nuncertain or lacking a definitive answer, without compromising performance.\nAlthough previous studies have attempted to improve AA, they lack a\nstandardised evaluation method and remain unsuitable for black-box models where\ntoken prediction probabilities are inaccessible. This makes comparative\nanalysis challenging, especially for state-of-the-art closed-source commercial\nLLMs. This paper bridges this gap by introducing a black-box evaluation\napproach and a new dataset, Abstain-QA, crafted to rigorously assess AA across\nvaried question types (answerable and unanswerable), domains (well-represented\nand under-represented), and task types (fact centric and reasoning). We also\npropose a new confusion matrix, the ''Answerable-Unanswerable Confusion\nMatrix'' (AUCM) which serves as the basis for evaluating AA, by offering a\nstructured and precise approach for assessment. Finally, we explore the impact\nof three prompting strategies-Strict Prompting, Verbal Confidence Thresholding,\nand Chain-of-Thought (CoT)-on improving AA. Our results indicate that even\npowerful models like GPT-4, Mixtral 8x22b encounter difficulties with\nabstention; however, strategic approaches such as Strict prompting and CoT can\nenhance this capability.\n","authors":["Nishanth Madhusudhan","Sathwik Tejaswi Madhusudhan","Vikas Yadav","Masoud Hashemi"],"pdf_url":"https://arxiv.org/pdf/2407.16221v2.pdf","comment":"8 pages (excluding limitations, references and appendix) and 5\n  figures"},{"id":"http://arxiv.org/abs/2406.01238v3","updated":"2024-09-24T13:53:59Z","published":"2024-06-03T11:56:07Z","title":"EffiQA: Efficient Question-Answering with Strategic Multi-Model\n  Collaboration on Knowledge Graphs","summary":"  While large language models (LLMs) have shown remarkable capabilities in\nnatural language processing, they struggle with complex, multi-step reasoning\ntasks involving knowledge graphs (KGs). Existing approaches that integrate LLMs\nand KGs either underutilize the reasoning abilities of LLMs or suffer from\nprohibitive computational costs due to tight coupling. To address these\nlimitations, we propose a novel collaborative framework named EffiQA that can\nstrike a balance between performance and efficiency via an iterative paradigm.\nEffiQA consists of three stages: global planning, efficient KG exploration, and\nself-reflection. Specifically, EffiQA leverages the commonsense capability of\nLLMs to explore potential reasoning pathways through global planning. Then, it\noffloads semantic pruning to a small plug-in model for efficient KG\nexploration. Finally, the exploration results are fed to LLMs for\nself-reflection to further improve the global planning and efficient KG\nexploration. Empirical evidence on multiple KBQA benchmarks shows EffiQA's\neffectiveness, achieving an optimal balance between reasoning accuracy and\ncomputational costs. We hope the proposed new framework will pave the way for\nefficient, knowledge-intensive querying by redefining the integration of LLMs\nand KGs, fostering future research on knowledge-based question answering.\n","authors":["Zixuan Dong","Baoyun Peng","Yufei Wang","Jia Fu","Xiaodong Wang","Yongxue Shan","Xin Zhou"],"pdf_url":"https://arxiv.org/pdf/2406.01238v3.pdf","comment":"10 pages, 4 figures, 3 tables"},{"id":"http://arxiv.org/abs/2409.16096v1","updated":"2024-09-24T13:50:32Z","published":"2024-09-24T13:50:32Z","title":"Exploring Hint Generation Approaches in Open-Domain Question Answering","summary":"  Automatic Question Answering (QA) systems rely on contextual information to\nprovide accurate answers. Commonly, contexts are prepared through either\nretrieval-based or generation-based methods. The former involves retrieving\nrelevant documents from a corpus like Wikipedia, whereas the latter uses\ngenerative models such as Large Language Models (LLMs) to generate the context.\nIn this paper, we introduce a novel context preparation approach called HINTQA,\nwhich employs Automatic Hint Generation (HG) techniques. Unlike traditional\nmethods, HINTQA prompts LLMs to produce hints about potential answers for the\nquestion rather than generating relevant context. We evaluate our approach\nacross three QA datasets including TriviaQA, NaturalQuestions, and Web\nQuestions, examining how the number and order of hints impact performance. Our\nfindings show that the HINTQA surpasses both retrieval-based and\ngeneration-based approaches. We demonstrate that hints enhance the accuracy of\nanswers more than retrieved and generated contexts.\n","authors":["Jamshid Mozafari","Abdelrahman Abdallah","Bhawna Piryani","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2409.16096v1.pdf","comment":"Accepted at EMNLP 2024"},{"id":"http://arxiv.org/abs/2403.15491v2","updated":"2024-09-24T13:25:01Z","published":"2024-03-21T15:41:02Z","title":"Open Conversational LLMs do not know most Spanish words","summary":"  The growing interest in Large Language Models (LLMs) and in particular in\nconversational models with which users can interact has led to the development\nof a large number of open-source chat LLMs. These models are evaluated on a\nwide range of benchmarks to assess their capabilities in answering questions or\nsolving problems on almost any possible topic or to test their ability to\nreason or interpret texts. Instead, the evaluation of the knowledge that these\nmodels have of the languages has received much less attention. For example, the\nwords that they can recognize and use in different languages. In this paper, we\nevaluate the knowledge that open-source chat LLMs have of Spanish words by\ntesting a sample of words in a reference dictionary. The results show that\nopen-source chat LLMs produce incorrect meanings for an important fraction of\nthe words and are not able to use most of the words correctly to write\nsentences with context. These results show how Spanish is left behind in the\nopen-source LLM race and highlight the need to push for linguistic fairness in\nconversational LLMs ensuring that they provide similar performance across\nlanguages.\n","authors":["Javier Conde","Miguel González","Nina Melero","Raquel Ferrando","Gonzalo Martínez","Elena Merino-Gómez","José Alberto Hernández","Pedro Reviriego"],"pdf_url":"https://arxiv.org/pdf/2403.15491v2.pdf","comment":"Procesamiento del Lenguaje Natural, 73, 95-108"},{"id":"http://arxiv.org/abs/2409.15188v2","updated":"2024-09-24T13:03:24Z","published":"2024-09-23T16:39:12Z","title":"PALLM: Evaluating and Enhancing PALLiative Care Conversations with Large\n  Language Models","summary":"  Effective patient-provider communication is crucial in clinical care,\ndirectly impacting patient outcomes and quality of life. Traditional evaluation\nmethods, such as human ratings, patient feedback, and provider\nself-assessments, are often limited by high costs and scalability issues.\nAlthough existing natural language processing (NLP) techniques show promise,\nthey struggle with the nuances of clinical communication and require sensitive\nclinical data for training, reducing their effectiveness in real-world\napplications. Emerging large language models (LLMs) offer a new approach to\nassessing complex communication metrics, with the potential to advance the\nfield through integration into passive sensing and just-in-time intervention\nsystems. This study explores LLMs as evaluators of palliative care\ncommunication quality, leveraging their linguistic, in-context learning, and\nreasoning capabilities. Specifically, using simulated scripts crafted and\nlabeled by healthcare professionals, we test proprietary models (e.g., GPT-4)\nand fine-tune open-source LLMs (e.g., LLaMA2) with a synthetic dataset\ngenerated by GPT-4 to evaluate clinical conversations, to identify key metrics\nsuch as `understanding' and `empathy'. Our findings demonstrated LLMs' superior\nperformance in evaluating clinical communication, providing actionable feedback\nwith reasoning, and demonstrating the feasibility and practical viability of\ndeveloping in-house LLMs. This research highlights LLMs' potential to enhance\npatient-provider interactions and lays the groundwork for downstream steps in\ndeveloping LLM-empowered clinical health systems.\n","authors":["Zhiyuan Wang","Fangxu Yuan","Virginia LeBaron","Tabor Flickinger","Laura E. Barnes"],"pdf_url":"https://arxiv.org/pdf/2409.15188v2.pdf","comment":"Accepted by ACM Transactions on Computing for Healthcare, Special\n  Issue on Large Language Models, Conversational Systems, and Generative AI in\n  Health, pending minor revisions"},{"id":"http://arxiv.org/abs/2409.15097v2","updated":"2024-09-24T12:56:13Z","published":"2024-09-23T15:11:07Z","title":"Efficiently Dispatching Flash Attention For Partially Filled Attention\n  Masks","summary":"  Transformers are widely used across various applications, many of which yield\nsparse or partially filled attention matrices. Examples include attention masks\ndesigned to reduce the quadratic complexity of attention, sequence packing\ntechniques, and recent innovations like tree masking for fast validation in\nMEDUSA. Despite the inherent sparsity in these matrices, the state-of-the-art\nalgorithm Flash Attention still processes them with quadratic complexity as\nthough they were dense. In this paper, we introduce Binary Block Masking, a\nhighly efficient modification that enhances Flash Attention by making it\nmask-aware. We further propose two optimizations: one tailored for masks with\ncontiguous non-zero patterns and another for extremely sparse masks. Our\nexperiments on attention masks derived from real-world scenarios demonstrate up\nto a 9x runtime improvement. The implementation will be publicly released to\nfoster further research and application.\n","authors":["Agniv Sharma","Jonas Geiping"],"pdf_url":"https://arxiv.org/pdf/2409.15097v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16025v1","updated":"2024-09-24T12:24:34Z","published":"2024-09-24T12:24:34Z","title":"Unlocking Markets: A Multilingual Benchmark to Cross-Market Question\n  Answering","summary":"  Users post numerous product-related questions on e-commerce platforms,\naffecting their purchase decisions. Product-related question answering (PQA)\nentails utilizing product-related resources to provide precise responses to\nusers. We propose a novel task of Multilingual Cross-market Product-based\nQuestion Answering (MCPQA) and define the task as providing answers to\nproduct-related questions in a main marketplace by utilizing information from\nanother resource-rich auxiliary marketplace in a multilingual context. We\nintroduce a large-scale dataset comprising over 7 million questions from 17\nmarketplaces across 11 languages. We then perform automatic translation on the\nElectronics category of our dataset, naming it as McMarket. We focus on two\nsubtasks: review-based answer generation and product-related question ranking.\nFor each subtask, we label a subset of McMarket using an LLM and further\nevaluate the quality of the annotations via human assessment. We then conduct\nexperiments to benchmark our dataset, using models ranging from traditional\nlexical models to LLMs in both single-market and cross-market scenarios across\nMcMarket and the corresponding LLM subset. Results show that incorporating\ncross-market information significantly enhances performance in both tasks.\n","authors":["Yifei Yuan","Yang Deng","Anders Søgaard","Mohammad Aliannejadi"],"pdf_url":"https://arxiv.org/pdf/2409.16025v1.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2409.16022v1","updated":"2024-09-24T12:23:15Z","published":"2024-09-24T12:23:15Z","title":"AI Can Be Cognitively Biased: An Exploratory Study on Threshold Priming\n  in LLM-Based Batch Relevance Assessment","summary":"  Cognitive biases are systematic deviations in thinking that lead to\nirrational judgments and problematic decision-making, extensively studied\nacross various fields. Recently, large language models (LLMs) have shown\nadvanced understanding capabilities but may inherit human biases from their\ntraining data. While social biases in LLMs have been well-studied, cognitive\nbiases have received less attention, with existing research focusing on\nspecific scenarios. The broader impact of cognitive biases on LLMs in various\ndecision-making contexts remains underexplored. We investigated whether LLMs\nare influenced by the threshold priming effect in relevance judgments, a core\ntask and widely-discussed research topic in the Information Retrieval (IR)\ncoummunity. The priming effect occurs when exposure to certain stimuli\nunconsciously affects subsequent behavior and decisions. Our experiment\nemployed 10 topics from the TREC 2019 Deep Learning passage track collection,\nand tested AI judgments under different document relevance scores, batch\nlengths, and LLM models, including GPT-3.5, GPT-4, LLaMa2-13B and LLaMa2-70B.\nResults showed that LLMs tend to give lower scores to later documents if\nearlier ones have high relevance, and vice versa, regardless of the combination\nand model used. Our finding demonstrates that LLM%u2019s judgments, similar to\nhuman judgments, are also influenced by threshold priming biases, and suggests\nthat researchers and system engineers should take into account potential\nhuman-like cognitive biases in designing, evaluating, and auditing LLMs in IR\ntasks and beyond.\n","authors":["Nuo Chen","Jiqun Liu","Xiaoyu Dong","Qijiong Liu","Tetsuya Sakai","Xiao-Ming Wu"],"pdf_url":"https://arxiv.org/pdf/2409.16022v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16005v1","updated":"2024-09-24T12:06:31Z","published":"2024-09-24T12:06:31Z","title":"Bridging Speech and Text: Enhancing ASR with Pinyin-to-Character\n  Pre-training in LLMs","summary":"  The integration of large language models (LLMs) with pre-trained speech\nmodels has opened up new avenues in automatic speech recognition (ASR). While\nLLMs excel in multimodal understanding tasks, effectively leveraging their\ncapabilities for ASR remains a significant challenge. This paper presents a\nnovel training approach to enhance LLM performance in ASR tasks. We propose\npre-training LLMs on Pinyin embedding sequences, which represent pronunciation\nfeatures, to generate corresponding Chinese characters. This step enables the\nLLM to adapt to generating text from pronunciation features before encountering\nreal speech data. Furthermore, we fine-tune the LoRA parameters to enhance the\nLLM's understanding of speech modality information. In AISHELL-1 corpus, our\napproach yields a 9.5% relative improvement in ASR tasks compared to the\nbaseline without Pinyi-to-Character pre-training. Additionally, incorporating\nauxiliary text data for Pinyi-to-Character pre-training further boosts\nperformance, achieving a 19.0% relative improvement.\n","authors":["Yang Yuhang","Peng Yizhou","Eng Siong Chng","Xionghu Zhong"],"pdf_url":"https://arxiv.org/pdf/2409.16005v1.pdf","comment":"Accepted by ISCSLP2024-Special session-Speech Processing in LLM Era"},{"id":"http://arxiv.org/abs/2406.02481v4","updated":"2024-09-24T12:00:29Z","published":"2024-06-04T16:49:06Z","title":"Large Language Models as Carriers of Hidden Messages","summary":"  Simple fine-tuning can embed hidden text into large language models (LLMs),\nwhich is revealed only when triggered by a specific query. Applications include\nLLM fingerprinting, where a unique identifier is embedded to verify licensing\ncompliance, and steganography, where the LLM carries hidden messages disclosed\nthrough a trigger query.\n  Our work demonstrates that embedding hidden text via fine-tuning, although\nseemingly secure due to the vast number of potential triggers, is vulnerable to\nextraction through analysis of the LLM's output decoding process. We introduce\nan extraction attack called Unconditional Token Forcing (UTF), which\niteratively feeds tokens from the LLM's vocabulary to reveal sequences with\nhigh token probabilities, indicating hidden text candidates. We also present\nUnconditional Token Forcing Confusion (UTFC), a defense paradigm that makes\nhidden text resistant to all known extraction attacks without degrading the\ngeneral performance of LLMs compared to standard fine-tuning. UTFC has both\nbenign (improving LLM fingerprinting) and malign applications (using LLMs to\ncreate covert communication channels).\n","authors":["Jakub Hoscilowicz","Pawel Popiolek","Jan Rudkowski","Jedrzej Bieniasz","Artur Janicki"],"pdf_url":"https://arxiv.org/pdf/2406.02481v4.pdf","comment":"Work in progress. Code is available at\n  https://github.com/j-hoscilowic/zurek-stegano"},{"id":"http://arxiv.org/abs/2403.03888v3","updated":"2024-09-24T11:39:42Z","published":"2024-03-06T17:48:06Z","title":"FaaF: Facts as a Function for the evaluation of generated text","summary":"  The demand for accurate and efficient verification of information in texts\ngenerated by large language models (LMs) is at an all-time high, but remains\nunresolved. Recent efforts have focused on extracting and verifying atomic\nfacts from these texts via prompting LM evaluators. However, we demonstrate\nthat this method of prompting is unreliable when faced with incomplete or\ninaccurate reference information. We introduce Facts as a Function (FaaF), a\nnew approach to the fact verification task that leverages the function-calling\ncapabilities of LMs. FaaF significantly enhances the ability of LMs to identify\nunsupported facts in texts, while also improving efficiency and significantly\nlowering costs compared to prompt-based methods. Additionally, we propose a\nframework for evaluating factual recall in Retrieval Augmented Generation (RAG)\nsystems, which we employ to compare prompt-based and FaaF methods using various\nLMs under challenging conditions.\n","authors":["Vasileios Katranidis","Gabor Barany"],"pdf_url":"https://arxiv.org/pdf/2403.03888v3.pdf","comment":"14 pages, 3 figures"},{"id":"http://arxiv.org/abs/2311.14517v3","updated":"2024-09-24T11:22:04Z","published":"2023-11-24T14:45:53Z","title":"tinyCLAP: Distilling Constrastive Language-Audio Pretrained Models","summary":"  Contrastive Language-Audio Pretraining (CLAP) became of crucial importance in\nthe field of audio and speech processing. Its employment ranges from sound\nevent detection to text-to-audio generation. However, one of the main\nlimitations is the considerable amount of data required in the training process\nand the overall computational complexity during inference. This paper\ninvestigates how we can reduce the complexity of contrastive language-audio\npre-trained models, yielding an efficient model that we call tinyCLAP. We\nderive an unimodal distillation loss from first principles and explore how the\ndimensionality of the shared, multimodal latent space can be reduced via\npruning. TinyCLAP uses only 6% of the original Microsoft CLAP parameters with a\nminimal reduction (less than 5%) in zero-shot classification performance across\nthe three sound event detection datasets on which it was tested\n","authors":["Francesco Paissan","Elisabetta Farella"],"pdf_url":"https://arxiv.org/pdf/2311.14517v3.pdf","comment":"Proceedings of Interspeech. Please use the citation available at\n  https://www.isca-archive.org/interspeech_2024/paissan24_interspeech.html"},{"id":"http://arxiv.org/abs/2409.15979v1","updated":"2024-09-24T11:21:43Z","published":"2024-09-24T11:21:43Z","title":"Finetuning LLMs for Comparative Assessment Tasks","summary":"  Automated assessment in natural language generation is a challenging task.\nInstruction-tuned large language models (LLMs) have shown promise in\nreference-free evaluation, particularly through comparative assessment.\nHowever, the quadratic computational complexity of pairwise comparisons limits\nits scalability. To address this, efficient comparative assessment has been\nexplored by applying comparative strategies on zero-shot LLM probabilities. We\npropose a framework for finetuning LLMs for comparative assessment to align the\nmodel's output with the target distribution of comparative probabilities. By\ntraining on soft probabilities, our approach improves state-of-the-art\nperformance while maintaining high performance with an efficient subset of\ncomparisons.\n","authors":["Vatsal Raina","Adian Liusie","Mark Gales"],"pdf_url":"https://arxiv.org/pdf/2409.15979v1.pdf","comment":"8 pages, 5 figures, 6 tables"},{"id":"http://arxiv.org/abs/2409.15977v1","updated":"2024-09-24T11:18:09Z","published":"2024-09-24T11:18:09Z","title":"StyleSinger 2: Zero-Shot Singing Voice Synthesis with Style Transfer and\n  Multi-Level Style Control","summary":"  Zero-shot singing voice synthesis (SVS) with style transfer and style control\naims to generate high-quality singing voices with unseen timbres and styles\n(including singing method, emotion, rhythm, technique, and pronunciation) from\naudio and text prompts. However, the multifaceted nature of singing styles\nposes a significant challenge for effective modeling, transfer, and control.\nFurthermore, current SVS models often fail to generate singing voices rich in\nstylistic nuances for unseen singers. To address these challenges, we introduce\nStyleSinger 2, the first zero-shot SVS model for style transfer across\ncross-lingual speech and singing styles, along with multi-level style control.\nSpecifically, StyleSinger 2 proposes three primary modules: 1) the clustering\nstyle encoder employs a clustering vector quantization model to stably condense\nstyle information into a compact latent space; 2) the Style and Duration\nLanguage Model (S\\&D-LM) concurrently predicts style information and phoneme\nduration, which benefits both; 3) the style adaptive decoder uses a novel\nmel-style adaptive normalization method to generate singing voices with\nenhanced details. Experimental results show that StyleSinger 2 outperforms all\nbaseline models in synthesis quality, singer similarity, and style\ncontrollability across various tasks, including zero-shot style transfer,\nmulti-level style control, cross-lingual style transfer, and speech-to-singing\nstyle transfer. Singing voice samples can be accessed at\nhttps://stylesinger2.github.io/.\n","authors":["Yu Zhang","Ziyue Jiang","Ruiqi Li","Changhao Pan","Jinzheng He","Rongjie Huang","Chuxin Wang","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2409.15977v1.pdf","comment":"Accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2409.15949v1","updated":"2024-09-24T10:24:53Z","published":"2024-09-24T10:24:53Z","title":"Beats of Bias: Analyzing Lyrics with Topic Modeling and Gender Bias\n  Measurements","summary":"  This paper uses topic modeling and bias measurement techniques to analyze and\ndetermine gender bias in English song lyrics. We utilize BERTopic to cluster\n537,553 English songs into distinct topics and chart their development over\ntime. Our analysis shows the thematic shift in song lyrics over the years, from\nthemes of romance to the increasing sexualization of women in songs. We observe\nlarge amounts of profanity and misogynistic lyrics on various topics,\nespecially in the overall biggest cluster. Furthermore, to analyze gender bias\nacross topics and genres, we employ the Single Category Word Embedding\nAssociation Test (SC-WEAT) to compute bias scores for the word embeddings\ntrained on the most popular topics as well as for each genre. We find that\nwords related to intelligence and strength tend to show a male bias across\ngenres, as opposed to appearance and weakness words, which are more\nfemale-biased; however, a closer look also reveals differences in biases across\ntopics.\n","authors":["Danqing Chen","Adithi Satish","Rasul Khanbayov","Carolin M. Schuster","Georg Groh"],"pdf_url":"https://arxiv.org/pdf/2409.15949v1.pdf","comment":"Accepted and presented at the 17th International Conference on Social\n  Computing, Behavioral-Cultural Modeling, & Prediction and Behavior\n  Representation in Modeling and Simulation (see\n  https://sbp-brims.org/2024/papers/working-papers/Chen_SBP-BRiMS2024_Final_31.pdf\n  )"},{"id":"http://arxiv.org/abs/2406.11176v2","updated":"2024-09-24T10:01:31Z","published":"2024-06-17T03:29:13Z","title":"Watch Every Step! LLM Agent Learning via Iterative Step-Level Process\n  Refinement","summary":"  Large language model agents have exhibited exceptional performance across a\nrange of complex interactive tasks. Recent approaches have utilized tuning with\nexpert trajectories to enhance agent performance, yet they primarily\nconcentrate on outcome rewards, which may lead to errors or suboptimal actions\ndue to the absence of process supervision signals. In this paper, we introduce\nthe Iterative step-level Process Refinement (IPR) framework, which provides\ndetailed step-by-step guidance to enhance agent training. Specifically, we\nadopt the Monte Carlo method to estimate step-level rewards. During each\niteration, the agent explores along the expert trajectory and generates new\nactions. These actions are then evaluated against the corresponding step of\nexpert trajectory using step-level rewards. Such comparison helps identify\ndiscrepancies, yielding contrastive action pairs that serve as training data\nfor the agent. Our experiments on three complex agent tasks demonstrate that\nour framework outperforms a variety of strong baselines. Moreover, our\nanalytical findings highlight the effectiveness of IPR in augmenting action\nefficiency and its applicability to diverse models.\n","authors":["Weimin Xiong","Yifan Song","Xiutian Zhao","Wenhao Wu","Xun Wang","Ke Wang","Cheng Li","Wei Peng","Sujian Li"],"pdf_url":"https://arxiv.org/pdf/2406.11176v2.pdf","comment":"Accepted to EMNLP 2024 (Main Conference)"},{"id":"http://arxiv.org/abs/2409.15934v1","updated":"2024-09-24T09:57:43Z","published":"2024-09-24T09:57:43Z","title":"Automated test generation to evaluate tool-augmented LLMs as\n  conversational AI agents","summary":"  Tool-augmented LLMs are a promising approach to create AI agents that can\nhave realistic conversations, follow procedures, and call appropriate\nfunctions. However, evaluating them is challenging due to the diversity of\npossible conversations, and existing datasets focus only on single interactions\nand function-calling. We present a test generation pipeline to evaluate LLMs as\nconversational AI agents. Our framework uses LLMs to generate diverse tests\ngrounded on user-defined procedures. For that, we use intermediate graphs to\nlimit the LLM test generator's tendency to hallucinate content that is not\ngrounded on input procedures, and enforces high coverage of the possible\nconversations. Additionally, we put forward ALMITA, a manually curated dataset\nfor evaluating AI agents in customer support, and use it to evaluate existing\nLLMs. Our results show that while tool-augmented LLMs perform well in single\ninteractions, they often struggle to handle complete conversations. While our\nfocus is on customer support, our method is general and capable of AI agents\nfor different domains.\n","authors":["Samuel Arcadinho","David Aparicio","Mariana Almeida"],"pdf_url":"https://arxiv.org/pdf/2409.15934v1.pdf","comment":"14 pages, 5 figures, Submitted to GenBench@EMNLP2024"},{"id":"http://arxiv.org/abs/2409.15933v1","updated":"2024-09-24T09:57:25Z","published":"2024-09-24T09:57:25Z","title":"SLIMER-IT: Zero-Shot NER on Italian Language","summary":"  Traditional approaches to Named Entity Recognition (NER) frame the task into\na BIO sequence labeling problem. Although these systems often excel in the\ndownstream task at hand, they require extensive annotated data and struggle to\ngeneralize to out-of-distribution input domains and unseen entity types. On the\ncontrary, Large Language Models (LLMs) have demonstrated strong zero-shot\ncapabilities. While several works address Zero-Shot NER in English, little has\nbeen done in other languages. In this paper, we define an evaluation framework\nfor Zero-Shot NER, applying it to the Italian language. Furthermore, we\nintroduce SLIMER-IT, the Italian version of SLIMER, an instruction-tuning\napproach for zero-shot NER leveraging prompts enriched with definition and\nguidelines. Comparisons with other state-of-the-art models, demonstrate the\nsuperiority of SLIMER-IT on never-seen-before entity tags.\n","authors":["Andrew Zamai","Leonardo Rigutini","Marco Maggini","Andrea Zugarini"],"pdf_url":"https://arxiv.org/pdf/2409.15933v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09920v2","updated":"2024-09-24T09:48:36Z","published":"2024-06-14T11:02:21Z","title":"Knowledge Editing in Language Models via Adapted Direct Preference\n  Optimization","summary":"  Large Language Models (LLMs) can become outdated over time as they may lack\nupdated world knowledge, leading to factual knowledge errors and gaps.\nKnowledge Editing (KE) aims to overcome this challenge using weight updates\nthat do not require expensive retraining. We propose treating KE as an LLM\nalignment problem. Toward this goal, we introduce Knowledge Direct Preference\nOptimization (KDPO), a variation of the Direct Preference Optimization (DPO)\nthat is more effective for knowledge modifications. Our method is based on an\nonline approach that continually updates the knowledge stored in the model. We\nuse the current knowledge as a negative sample and the new knowledge we want to\nintroduce as a positive sample in a process called DPO. We also use\nteacher-forcing for negative sample generation and optimize using the positive\nsample, which helps maintain localized changes. We tested our KE method on\nvarious datasets and models, comparing it to several cutting-edge methods, with\n100 and 500 sequential edits. Additionally, we conducted an ablation study\ncomparing our method to the standard DPO approach. Our experimental results\nshow that our modified DPO method allows for more refined KE, achieving similar\nor better performance compared to previous methods.\n","authors":["Amit Rozner","Barak Battash","Lior Wolf","Ofir Lindenbaum"],"pdf_url":"https://arxiv.org/pdf/2406.09920v2.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2409.15924v1","updated":"2024-09-24T09:46:27Z","published":"2024-09-24T09:46:27Z","title":"Multilingual Transfer and Domain Adaptation for Low-Resource Languages\n  of Spain","summary":"  This article introduces the submission status of the Translation into\nLow-Resource Languages of Spain task at (WMT 2024) by Huawei Translation\nService Center (HW-TSC). We participated in three translation tasks: spanish to\naragonese (es-arg), spanish to aranese (es-arn), and spanish to asturian\n(es-ast). For these three translation tasks, we use training strategies such as\nmultilingual transfer, regularized dropout, forward translation and back\ntranslation, labse denoising, transduction ensemble learning and other\nstrategies to neural machine translation (NMT) model based on training deep\ntransformer-big architecture. By using these enhancement strategies, our\nsubmission achieved a competitive result in the final evaluation.\n","authors":["Yuanchang Luo","Zhanglin Wu","Daimeng Wei","Hengchao Shang","Zongyao Li","Jiaxin Guo","Zhiqiang Rao","Shaojun Li","Jinlong Yang","Yuhao Xie","Jiawei Zheng Bin Wei","Hao Yang"],"pdf_url":"https://arxiv.org/pdf/2409.15924v1.pdf","comment":"6 pages,wmt24. arXiv admin note: substantial text overlap with\n  arXiv:2409.14842; text overlap with arXiv:2409.14800"},{"id":"http://arxiv.org/abs/2409.15912v1","updated":"2024-09-24T09:28:24Z","published":"2024-09-24T09:28:24Z","title":"Explaining word embeddings with perfect fidelity: Case study in research\n  impact prediction","summary":"  Best performing approaches for scholarly document quality prediction are\nbased on embedding models, which do not allow direct explanation of classifiers\nas distinct words no longer correspond to the input features for model\ntraining. Although model-agnostic explanation methods such as Local\ninterpretable model-agnostic explanations (LIME) can be applied, these produce\nresults with questionable correspondence to the ML model. We introduce a new\nfeature importance method, Self-model Rated Entities (SMER), for logistic\nregression-based classification models trained on word embeddings. We show that\nSMER has theoretically perfect fidelity with the explained model, as its\nprediction corresponds exactly to the average of predictions for individual\nwords in the text. SMER allows us to reliably determine which words or entities\npositively contribute to predicting impactful articles. Quantitative and\nqualitative evaluation is performed through five diverse experiments conducted\non 50.000 research papers from the CORD-19 corpus. Through an AOPC curve\nanalysis, we experimentally demonstrate that SMER produces better explanations\nthan LIME for logistic regression.\n","authors":["Lucie Dvorackova","Marcin P. Joachimiak","Michal Cerny","Adriana Kubecova","Vilem Sklenak","Tomas Kliegr"],"pdf_url":"https://arxiv.org/pdf/2409.15912v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15911v1","updated":"2024-09-24T09:27:43Z","published":"2024-09-24T09:27:43Z","title":"A Modular-based Strategy for Mitigating Gradient Conflicts in\n  Simultaneous Speech Translation","summary":"  Simultaneous Speech Translation (SimulST) involves generating target language\ntext while continuously processing streaming speech input, presenting\nsignificant real-time challenges. Multi-task learning is often employed to\nenhance SimulST performance but introduces optimization conflicts between\nprimary and auxiliary tasks, potentially compromising overall efficiency. The\nexisting model-level conflict resolution methods are not well-suited for this\ntask which exacerbates inefficiencies and leads to high GPU memory consumption.\nTo address these challenges, we propose a Modular Gradient Conflict Mitigation\n(MGCM) strategy that detects conflicts at a finer-grained modular level and\nresolves them utilizing gradient projection. Experimental results demonstrate\nthat MGCM significantly improves SimulST performance, particularly under medium\nand high latency conditions, achieving a 0.68 BLEU score gain in offline tasks.\nAdditionally, MGCM reduces GPU memory consumption by over 95\\% compared to\nother conflict mitigation methods, establishing it as a robust solution for\nSimulST tasks.\n","authors":["Xiaoqian Liu","Yangfan Du","Jianjin Wang","Yuan Ge","Chen Xu","Tong Xiao","Guocheng Chen","Jingbo Zhu"],"pdf_url":"https://arxiv.org/pdf/2409.15911v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15907v1","updated":"2024-09-24T09:24:03Z","published":"2024-09-24T09:24:03Z","title":"Enhancing Text-to-SQL Capabilities of Large Language Models via Domain\n  Database Knowledge Injection","summary":"  Text-to-SQL is a subtask in semantic parsing that has seen rapid progress\nwith the evolution of Large Language Models (LLMs). However, LLMs face\nchallenges due to hallucination issues and a lack of domain-specific database\nknowledge(such as table schema and cell values). As a result, they can make\nerrors in generating table names, columns, and matching values to the correct\ncolumns in SQL statements. This paper introduces a method of knowledge\ninjection to enhance LLMs' ability to understand schema contents by\nincorporating prior knowledge. This approach improves their performance in\nText-to-SQL tasks. Experimental results show that pre-training LLMs on\ndomain-specific database knowledge and fine-tuning them on downstream\nText-to-SQL tasks significantly improves the Execution Match (EX) and Exact\nMatch (EM) metrics across various models. This effectively reduces errors in\ngenerating column names and matching values to the columns. Furthermore, the\nknowledge-injected models can be applied to many downstream Text-to-SQL tasks,\ndemonstrating the generalizability of the approach presented in this paper.\n","authors":["Xingyu Ma","Xin Tian","Lingxiang Wu","Xuepeng Wang","Xueming Tang","Jinqiao Wang"],"pdf_url":"https://arxiv.org/pdf/2409.15907v1.pdf","comment":"This paper has been accepted by ECAI 2024"},{"id":"http://arxiv.org/abs/2409.15902v1","updated":"2024-09-24T09:19:11Z","published":"2024-09-24T09:19:11Z","title":"Konstruktor: A Strong Baseline for Simple Knowledge Graph Question\n  Answering","summary":"  While being one of the most popular question types, simple questions such as\n\"Who is the author of Cinderella?\", are still not completely solved.\nSurprisingly, even the most powerful modern Large Language Models are prone to\nerrors when dealing with such questions, especially when dealing with rare\nentities. At the same time, as an answer may be one hop away from the question\nentity, one can try to develop a method that uses structured knowledge graphs\n(KGs) to answer such questions. In this paper, we introduce Konstruktor - an\nefficient and robust approach that breaks down the problem into three steps:\n(i) entity extraction and entity linking, (ii) relation prediction, and (iii)\nquerying the knowledge graph. Our approach integrates language models and\nknowledge graphs, exploiting the power of the former and the interpretability\nof the latter. We experiment with two named entity recognition and entity\nlinking methods and several relation detection techniques. We show that for\nrelation detection, the most challenging step of the workflow, a combination of\nrelation classification/generation and ranking outperforms other methods. We\nreport Konstruktor's strong results on four datasets.\n","authors":["Maria Lysyuk","Mikhail Salnikov","Pavel Braslavski","Alexander Panchenko"],"pdf_url":"https://arxiv.org/pdf/2409.15902v1.pdf","comment":"18 pages, 2 figures, 7 tables"},{"id":"http://arxiv.org/abs/2405.19846v4","updated":"2024-09-24T09:06:21Z","published":"2024-05-30T08:50:55Z","title":"Quest: Query-centric Data Synthesis Approach for Long-context Scaling of\n  Large Language Model","summary":"  Large language models, initially pre-trained with a limited context length,\ncan better handle longer texts by continuing training on a corpus with extended\ncontexts. However, obtaining effective long-context data is challenging due to\nthe scarcity and uneven distribution of long documents across different\ndomains. To address this issue, we propose a Query-centric data synthesis\nmethod, abbreviated as Quest. Quest is an interpretable method based on the\nobservation that documents retrieved by similar queries are relevant but\nlow-redundant, thus well-suited for synthesizing long-context data. The method\nis also scalable and capable of constructing large amounts of long-context\ndata. Using Quest, we synthesize a long-context dataset up to 128k context\nlength, significantly outperforming other data synthesis methods on multiple\nlong-context benchmark datasets. In addition, we further verify that the Quest\nmethod is predictable through scaling law experiments, making it a reliable\nsolution for advancing long-context models.\n","authors":["Chaochen Gao","Xing Wu","Qi Fu","Songlin Hu"],"pdf_url":"https://arxiv.org/pdf/2405.19846v4.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2307.15061v2","updated":"2024-09-24T23:49:07Z","published":"2023-07-27T17:59:56Z","title":"The RoboDepth Challenge: Methods and Advancements Towards Robust Depth\n  Estimation","summary":"  Accurate depth estimation under out-of-distribution (OoD) scenarios, such as\nadverse weather conditions, sensor failure, and noise contamination, is\ndesirable for safety-critical applications. Existing depth estimation systems,\nhowever, suffer inevitably from real-world corruptions and perturbations and\nare struggled to provide reliable depth predictions under such cases. In this\npaper, we summarize the winning solutions from the RoboDepth Challenge -- an\nacademic competition designed to facilitate and advance robust OoD depth\nestimation. This challenge was developed based on the newly established KITTI-C\nand NYUDepth2-C benchmarks. We hosted two stand-alone tracks, with an emphasis\non robust self-supervised and robust fully-supervised depth estimation,\nrespectively. Out of more than two hundred participants, nine unique and\ntop-performing solutions have appeared, with novel designs ranging from the\nfollowing aspects: spatial- and frequency-domain augmentations, masked image\nmodeling, image restoration and super-resolution, adversarial training,\ndiffusion-based noise suppression, vision-language pre-training, learned model\nensembling, and hierarchical feature enhancement. Extensive experimental\nanalyses along with insightful observations are drawn to better understand the\nrationale behind each design. We hope this challenge could lay a solid\nfoundation for future research on robust and reliable depth estimation and\nbeyond. The datasets, competition toolkit, workshop recordings, and source code\nfrom the winning teams are publicly available on the challenge website.\n","authors":["Lingdong Kong","Yaru Niu","Shaoyuan Xie","Hanjiang Hu","Lai Xing Ng","Benoit R. Cottereau","Liangjun Zhang","Hesheng Wang","Wei Tsang Ooi","Ruijie Zhu","Ziyang Song","Li Liu","Tianzhu Zhang","Jun Yu","Mohan Jing","Pengwei Li","Xiaohua Qi","Cheng Jin","Yingfeng Chen","Jie Hou","Jie Zhang","Zhen Kan","Qiang Ling","Liang Peng","Minglei Li","Di Xu","Changpeng Yang","Yuanqi Yao","Gang Wu","Jian Kuai","Xianming Liu","Junjun Jiang","Jiamian Huang","Baojun Li","Jiale Chen","Shuang Zhang","Sun Ao","Zhenyu Li","Runze Chen","Haiyong Luo","Fang Zhao","Jingze Yu"],"pdf_url":"https://arxiv.org/pdf/2307.15061v2.pdf","comment":"Technical Report; 65 pages, 34 figures, 24 tables; Code at\n  https://github.com/ldkong1205/RoboDepth"},{"id":"http://arxiv.org/abs/2409.16504v1","updated":"2024-09-24T23:26:07Z","published":"2024-09-24T23:26:07Z","title":"Low Latency Point Cloud Rendering with Learned Splatting","summary":"  Point cloud is a critical 3D representation with many emerging applications.\nBecause of the point sparsity and irregularity, high-quality rendering of point\nclouds is challenging and often requires complex computations to recover the\ncontinuous surface representation. On the other hand, to avoid visual\ndiscomfort, the motion-to-photon latency has to be very short, under 10 ms.\nExisting rendering solutions lack in either quality or speed. To tackle these\nchallenges, we present a framework that unlocks interactive, free-viewing and\nhigh-fidelity point cloud rendering. We train a generic neural network to\nestimate 3D elliptical Gaussians from arbitrary point clouds and use\ndifferentiable surface splatting to render smooth texture and surface normal\nfor arbitrary views. Our approach does not require per-scene optimization, and\nenable real-time rendering of dynamic point cloud. Experimental results\ndemonstrate the proposed solution enjoys superior visual quality and speed, as\nwell as generalizability to different scene content and robustness to\ncompression artifacts. The code is available at\nhttps://github.com/huzi96/gaussian-pcloud-render .\n","authors":["Yueyu Hu","Ran Gong","Qi Sun","Yao Wang"],"pdf_url":"https://arxiv.org/pdf/2409.16504v1.pdf","comment":"Published at CVPR 2024 Workshop on AIS: Vision, Graphics and AI for\n  Streaming (https://ai4streaming-workshop.github.io/)"},{"id":"http://arxiv.org/abs/2408.14028v3","updated":"2024-09-24T23:23:50Z","published":"2024-08-26T05:38:27Z","title":"SurGen: Text-Guided Diffusion Model for Surgical Video Generation","summary":"  Diffusion-based video generation models have made significant strides,\nproducing outputs with improved visual fidelity, temporal coherence, and user\ncontrol. These advancements hold great promise for improving surgical education\nby enabling more realistic, diverse, and interactive simulation environments.\nIn this study, we introduce SurGen, a text-guided diffusion model tailored for\nsurgical video synthesis. SurGen produces videos with the highest resolution\nand longest duration among existing surgical video generation models. We\nvalidate the visual and temporal quality of the outputs using standard image\nand video generation metrics. Additionally, we assess their alignment to the\ncorresponding text prompts through a deep learning classifier trained on\nsurgical data. Our results demonstrate the potential of diffusion models to\nserve as valuable educational tools for surgical trainees.\n","authors":["Joseph Cho","Samuel Schmidgall","Cyril Zakka","Mrudang Mathur","Dhamanpreet Kaur","Rohan Shad","William Hiesinger"],"pdf_url":"https://arxiv.org/pdf/2408.14028v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16502v1","updated":"2024-09-24T23:18:32Z","published":"2024-09-24T23:18:32Z","title":"GSplatLoc: Grounding Keypoint Descriptors into 3D Gaussian Splatting for\n  Improved Visual Localization","summary":"  Although various visual localization approaches exist, such as scene\ncoordinate and pose regression, these methods often struggle with high memory\nconsumption or extensive optimization requirements. To address these\nchallenges, we utilize recent advancements in novel view synthesis,\nparticularly 3D Gaussian Splatting (3DGS), to enhance localization. 3DGS allows\nfor the compact encoding of both 3D geometry and scene appearance with its\nspatial features. Our method leverages the dense description maps produced by\nXFeat's lightweight keypoint detection and description model. We propose\ndistilling these dense keypoint descriptors into 3DGS to improve the model's\nspatial understanding, leading to more accurate camera pose predictions through\n2D-3D correspondences. After estimating an initial pose, we refine it using a\nphotometric warping loss. Benchmarking on popular indoor and outdoor datasets\nshows that our approach surpasses state-of-the-art Neural Render Pose (NRP)\nmethods, including NeRFMatch and PNeRFLoc.\n","authors":["Gennady Sidorov","Malik Mohrat","Ksenia Lebedeva","Ruslan Rakhimov","Sergey Kolyubin"],"pdf_url":"https://arxiv.org/pdf/2409.16502v1.pdf","comment":"Project website at https://gsplatloc.github.io/"},{"id":"http://arxiv.org/abs/2409.16496v1","updated":"2024-09-24T22:59:52Z","published":"2024-09-24T22:59:52Z","title":"Real-Time Detection of Electronic Components in Waste Printed Circuit\n  Boards: A Transformer-Based Approach","summary":"  Critical Raw Materials (CRMs) such as copper, manganese, gallium, and various\nrare earths have great importance for the electronic industry. To increase the\nconcentration of individual CRMs and thus make their extraction from Waste\nPrinted Circuit Boards (WPCBs) convenient, we have proposed a practical\napproach that involves selective disassembling of the different types of\nelectronic components from WPCBs using mechatronic systems guided by artificial\nvision techniques. In this paper we evaluate the real-time accuracy of\nelectronic component detection and localization of the Real-Time DEtection\nTRansformer model architecture. Transformers have recently become very popular\nfor the extraordinary results obtained in natural language processing and\nmachine translation. Also in this case, the transformer model achieves very\ngood performances, often superior to those of the latest state of the art\nobject detection and localization models YOLOv8 and YOLOv9.\n","authors":["Muhammad Mohsin","Stefano Rovetta","Francesco Masulli","Alberto Cabri"],"pdf_url":"https://arxiv.org/pdf/2409.16496v1.pdf","comment":"International Conference on Applications in Electronics Pervading\n  Industry, Environment and Society (ApplePies2024). Proceedings are published\n  in the Springer Lecture Notes in Electrical Engineering"},{"id":"http://arxiv.org/abs/2409.16494v1","updated":"2024-09-24T22:36:58Z","published":"2024-09-24T22:36:58Z","title":"A Unified Hallucination Mitigation Framework for Large Vision-Language\n  Models","summary":"  Hallucination is a common problem for Large Vision-Language Models (LVLMs)\nwith long generations which is difficult to eradicate. The generation with\nhallucinations is partially inconsistent with the image content. To mitigate\nhallucination, current studies either focus on the process of model inference\nor the results of model generation, but the solutions they design sometimes do\nnot deal appropriately with various types of queries and the hallucinations of\nthe generations about these queries. To accurately deal with various\nhallucinations, we present a unified framework, Dentist, for hallucination\nmitigation. The core step is to first classify the queries, then perform\ndifferent processes of hallucination mitigation based on the classification\nresult, just like a dentist first observes the teeth and then makes a plan. In\na simple deployment, Dentist can classify queries as perception or reasoning\nand easily mitigate potential hallucinations in answers which has been\ndemonstrated in our experiments. On MMbench, we achieve a 13.44%/10.2%/15.8%\nimprovement in accuracy on Image Quality, a Coarse Perception visual question\nanswering (VQA) task, over the baseline InstructBLIP/LLaVA/VisualGLM.\n","authors":["Yue Chang","Liqiang Jing","Xiaopeng Zhang","Yue Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.16494v1.pdf","comment":"Accepted by TMLR"},{"id":"http://arxiv.org/abs/2409.16491v1","updated":"2024-09-24T22:31:56Z","published":"2024-09-24T22:31:56Z","title":"Proactive Schemes: A Survey of Adversarial Attacks for Social Good","summary":"  Adversarial attacks in computer vision exploit the vulnerabilities of machine\nlearning models by introducing subtle perturbations to input data, often\nleading to incorrect predictions or classifications. These attacks have evolved\nin sophistication with the advent of deep learning, presenting significant\nchallenges in critical applications, which can be harmful for society. However,\nthere is also a rich line of research from a transformative perspective that\nleverages adversarial techniques for social good. Specifically, we examine the\nrise of proactive schemes-methods that encrypt input data using additional\nsignals termed templates, to enhance the performance of deep learning models.\nBy embedding these imperceptible templates into digital media, proactive\nschemes are applied across various applications, from simple image enhancements\nto complicated deep learning frameworks to aid performance, as compared to the\npassive schemes, which don't change the input data distribution for their\nframework. The survey delves into the methodologies behind these proactive\nschemes, the encryption and learning processes, and their application to modern\ncomputer vision and natural language processing applications. Additionally, it\ndiscusses the challenges, potential vulnerabilities, and future directions for\nproactive schemes, ultimately highlighting their potential to foster the\nresponsible and secure advancement of deep learning technologies.\n","authors":["Vishal Asnani","Xi Yin","Xiaoming Liu"],"pdf_url":"https://arxiv.org/pdf/2409.16491v1.pdf","comment":"Submitted for review"},{"id":"http://arxiv.org/abs/2409.16488v1","updated":"2024-09-24T22:29:22Z","published":"2024-09-24T22:29:22Z","title":"Diffusion Models to Enhance the Resolution of Microscopy Images: A\n  Tutorial","summary":"  Diffusion models have emerged as a prominent technique in generative modeling\nwith neural networks, making their mark in tasks like text-to-image translation\nand super-resolution. In this tutorial, we provide a comprehensive guide to\nbuild denoising diffusion probabilistic models (DDPMs) from scratch, with a\nspecific focus on transforming low-resolution microscopy images into their\ncorresponding high-resolution versions. We provide the theoretical background,\nmathematical derivations, and a detailed Python code implementation using\nPyTorch, along with techniques to enhance model performance.\n","authors":["Harshith Bachimanchi","Giovanni Volpe"],"pdf_url":"https://arxiv.org/pdf/2409.16488v1.pdf","comment":"45 pages, 8 figures"},{"id":"http://arxiv.org/abs/2407.17571v2","updated":"2024-09-24T22:24:23Z","published":"2024-07-24T18:04:17Z","title":"Diffusion Models For Multi-Modal Generative Modeling","summary":"  Diffusion-based generative modeling has been achieving state-of-the-art\nresults on various generation tasks. Most diffusion models, however, are\nlimited to a single-generation modeling. Can we generalize diffusion models\nwith the ability of multi-modal generative training for more generalizable\nmodeling? In this paper, we propose a principled way to define a diffusion\nmodel by constructing a unified multi-modal diffusion model in a common\ndiffusion space. We define the forward diffusion process to be driven by an\ninformation aggregation from multiple types of task-data, e.g., images for a\ngeneration task and labels for a classification task. In the reverse process,\nwe enforce information sharing by parameterizing a shared backbone denoising\nnetwork with additional modality-specific decoder heads. Such a structure can\nsimultaneously learn to generate different types of multi-modal data with a\nmulti-task loss, which is derived from a new multi-modal variational lower\nbound that generalizes the standard diffusion model. We propose several\nmultimodal generation settings to verify our framework, including image\ntransition, masked-image training, joint image-label and joint\nimage-representation generative modeling. Extensive experimental results on\nImageNet indicate the effectiveness of our framework for various multi-modal\ngenerative modeling, which we believe is an important research direction worthy\nof more future explorations.\n","authors":["Changyou Chen","Han Ding","Bunyamin Sisman","Yi Xu","Ouye Xie","Benjamin Z. Yao","Son Dinh Tran","Belinda Zeng"],"pdf_url":"https://arxiv.org/pdf/2407.17571v2.pdf","comment":"Published as a conference paper at ICLR 2024"},{"id":"http://arxiv.org/abs/2409.16470v1","updated":"2024-09-24T21:44:26Z","published":"2024-09-24T21:44:26Z","title":"Frequency-based View Selection in Gaussian Splatting Reconstruction","summary":"  Three-dimensional reconstruction is a fundamental problem in robotics\nperception. We examine the problem of active view selection to perform 3D\nGaussian Splatting reconstructions with as few input images as possible.\nAlthough 3D Gaussian Splatting has made significant progress in image rendering\nand 3D reconstruction, the quality of the reconstruction is strongly impacted\nby the selection of 2D images and the estimation of camera poses through\nStructure-from-Motion (SfM) algorithms. Current methods to select views that\nrely on uncertainties from occlusions, depth ambiguities, or neural network\npredictions directly are insufficient to handle the issue and struggle to\ngeneralize to new scenes. By ranking the potential views in the frequency\ndomain, we are able to effectively estimate the potential information gain of\nnew viewpoints without ground truth data. By overcoming current constraints on\nmodel architecture and efficacy, our method achieves state-of-the-art results\nin view selection, demonstrating its potential for efficient image-based 3D\nreconstruction.\n","authors":["Monica M. Q. Li","Pierre-Yves Lajoie","Giovanni Beltrame"],"pdf_url":"https://arxiv.org/pdf/2409.16470v1.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2409.16465v1","updated":"2024-09-24T21:33:14Z","published":"2024-09-24T21:33:14Z","title":"Initialization of Monocular Visual Navigation for Autonomous Agents\n  Using Modified Structure from Small Motion","summary":"  We propose a standalone monocular visual Simultaneous Localization and\nMapping (vSLAM) initialization pipeline for autonomous robots in space. Our\nmethod, a state-of-the-art factor graph optimization pipeline, enhances\nclassical Structure from Small Motion (SfSM) to robustly initialize a monocular\nagent in weak-perspective projection scenes. Furthermore, it overcomes visual\nestimation challenges introduced by spacecraft inspection trajectories, such\nas: center-pointing motion, which exacerbates the bas-relief ambiguity, and the\npresence of a dominant plane in the scene, which causes motion estimation\ndegeneracies in classical Structure from Motion (SfM). We validate our method\non realistic, simulated satellite inspection images exhibiting weak-perspective\nprojection, and we demonstrate its effectiveness and improved performance\ncompared to other monocular initialization procedures.\n","authors":["Juan-Diego Florez","Mehregan Dor","Panagiotis Tsiotras"],"pdf_url":"https://arxiv.org/pdf/2409.16465v1.pdf","comment":"6 pages, 1 page for references, 6 figures, 1 table, IEEEtran format\n  This work has been submitted to the IEEE for possible publication. Copyright\n  may be transferred without notice, after which this version may no longer be\n  accessible"},{"id":"http://arxiv.org/abs/2406.02584v3","updated":"2024-09-24T20:50:21Z","published":"2024-05-30T20:48:10Z","title":"A Scoping Review of Earth Observation and Machine Learning for Causal\n  Inference: Implications for the Geography of Poverty","summary":"  Earth observation (EO) data such as satellite imagery can have far-reaching\nimpacts on our understanding of the geography of poverty, especially when\ncoupled with machine learning (ML) and computer vision. Early research in\ncomputer vision used predictive models to estimate living conditions,\nespecially in contexts where data availability on poverty was scarce. Recent\nwork has progressed beyond using EO data to predict such outcomes -- now also\nusing it to conduct causal inference. However, how such EO-ML models are used\nfor causality remains incompletely mapped. To address this gap, we conduct a\nscoping review where we first document the growth of interest in using\nsatellite images and other sources of EO data in causal analysis. We then trace\nthe methodological relationship between spatial statistics and ML methods\nbefore discussing five ways in which EO data has been used in scientific\nworkflows -- (1) outcome imputation for downstream causal analysis, (2) EO\nimage deconfounding, (3) EO-based treatment effect heterogeneity, (4) EO-based\ntransportability analysis, and (5) image-informed causal discovery. We\nconsolidate these observations by providing a detailed workflow for how\nresearchers can incorporate EO data in causal analysis going forward -- from\ndata requirements to choice of computer vision model and evaluation metrics.\nWhile our discussion focuses on health and living conditions outcomes, our\nworkflow applies to other measures of sustainable development where EO data are\ninformative.\n","authors":["Kazuki Sakamoto","Connor T. Jerzak","Adel Daoud"],"pdf_url":"https://arxiv.org/pdf/2406.02584v3.pdf","comment":"To appear as: Sakamoto, Kazuki, Connor T. Jerzak, and Adel Daoud. \"A\n  Scoping Review of Earth Observation and Machine Learning for Causal\n  Inference: Implications for the Geography of Poverty.\" In Geography of\n  Poverty, edited by Ola Hall and Ibrahim Wahab. Edward Elgar Publishing\n  (Cheltenham, UK), 2025"},{"id":"http://arxiv.org/abs/2409.16446v1","updated":"2024-09-24T20:26:16Z","published":"2024-09-24T20:26:16Z","title":"Underground Mapping and Localization Based on Ground-Penetrating Radar","summary":"  3D object reconstruction based on deep neural networks has gained increasing\nattention in recent years. However, 3D reconstruction of underground objects to\ngenerate point cloud maps remains a challenge. Ground Penetrating Radar (GPR)\nis one of the most powerful and extensively used tools for detecting and\nlocating underground objects such as plant root systems and pipelines, with its\ncost-effectiveness and continuously evolving technology. This paper introduces\na parabolic signal detection network based on deep convolutional neural\nnetworks, utilizing B-scan images from GPR sensors. The detected keypoints can\naid in accurately fitting parabolic curves used to interpret the original GPR\nB-scan images as cross-sections of the object model. Additionally, a multi-task\npoint cloud network was designed to perform both point cloud segmentation and\ncompletion simultaneously, filling in sparse point cloud maps. For unknown\nlocations, GPR A-scan data can be used to match corresponding A-scan data in\nthe constructed map, pinpointing the position to verify the accuracy of the map\nconstruction by the model. Experimental results demonstrate the effectiveness\nof our method.\n","authors":["Jinchang Zhang","Guoyu Lu"],"pdf_url":"https://arxiv.org/pdf/2409.16446v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2409.16478v1","updated":"2024-09-24T21:54:22Z","published":"2024-09-24T21:54:22Z","title":"Algorithmic Drift: A Simulation Framework to Study the Effects of\n  Recommender Systems on User Preferences","summary":"  Digital platforms such as social media and e-commerce websites adopt\nRecommender Systems to provide value to the user. However, the social\nconsequences deriving from their adoption are still unclear. Many scholars\nargue that recommenders may lead to detrimental effects, such as\nbias-amplification deriving from the feedback loop between algorithmic\nsuggestions and users' choices. Nonetheless, the extent to which recommenders\ninfluence changes in users leaning remains uncertain. In this context, it is\nimportant to provide a controlled environment for evaluating the recommendation\nalgorithm before deployment. To address this, we propose a stochastic\nsimulation framework that mimics user-recommender system interactions in a\nlong-term scenario. In particular, we simulate the user choices by formalizing\na user model, which comprises behavioral aspects, such as the user resistance\ntowards the recommendation algorithm and their inertia in relying on the\nreceived suggestions. Additionally, we introduce two novel metrics for\nquantifying the algorithm's impact on user preferences, specifically in terms\nof drift over time. We conduct an extensive evaluation on multiple synthetic\ndatasets, aiming at testing the robustness of our framework when considering\ndifferent scenarios and hyper-parameters setting. The experimental results\nprove that the proposed methodology is effective in detecting and quantifying\nthe drift over the users preferences by means of the simulation. All the code\nand data used to perform the experiments are publicly available.\n","authors":["Erica Coppolillo","Simone Mungari","Ettore Ritacco","Francesco Fabbri","Marco Minici","Francesco Bonchi","Giuseppe Manco"],"pdf_url":"https://arxiv.org/pdf/2409.16478v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16408v1","updated":"2024-09-24T19:17:15Z","published":"2024-09-24T19:17:15Z","title":"Modern Hopfield Networks meet Encoded Neural Representations --\n  Addressing Practical Considerations","summary":"  Content-addressable memories such as Modern Hopfield Networks (MHN) have been\nstudied as mathematical models of auto-association and storage/retrieval in the\nhuman declarative memory, yet their practical use for large-scale content\nstorage faces challenges. Chief among them is the occurrence of meta-stable\nstates, particularly when handling large amounts of high dimensional content.\nThis paper introduces Hopfield Encoding Networks (HEN), a framework that\nintegrates encoded neural representations into MHNs to improve pattern\nseparability and reduce meta-stable states. We show that HEN can also be used\nfor retrieval in the context of hetero association of images with natural\nlanguage queries, thus removing the limitation of requiring access to partial\ncontent in the same domain. Experimental results demonstrate substantial\nreduction in meta-stable states and increased storage capacity while still\nenabling perfect recall of a significantly larger number of inputs advancing\nthe practical utility of associative memory networks for real-world tasks.\n","authors":["Satyananda Kashyap","Niharika S. D'Souza","Luyao Shi","Ken C. L. Wong","Hongzhi Wang","Tanveer Syeda-Mahmood"],"pdf_url":"https://arxiv.org/pdf/2409.16408v1.pdf","comment":"17 pages, 8 figures, workshop submission to Neurips"},{"id":"http://arxiv.org/abs/2409.13711v2","updated":"2024-09-24T18:38:02Z","published":"2024-09-06T18:44:25Z","title":"WebQuest: A Benchmark for Multimodal QA on Web Page Sequences","summary":"  The rise of powerful multimodal LLMs has enhanced the viability of building\nweb agents which can, with increasing levels of autonomy, assist users to\nretrieve information and complete tasks on various human-computer interfaces.\nIt is hence necessary to build challenging benchmarks that span a wide-variety\nof use cases reflecting real-world usage. In this work, we present WebQuest, a\nmulti-page question-answering dataset that requires reasoning across multiple\nrelated web pages. In contrast to existing UI benchmarks that focus on\nmulti-step web navigation and task completion, our dataset evaluates\ninformation extraction, multimodal retrieval and composition of information\nfrom many web pages. WebQuest includes three question categories: single-screen\nQA, multi-screen QA, and QA based on navigation traces. We evaluate leading\nproprietary multimodal models like GPT-4V, Gemini Flash, Claude 3, and open\nsource models like InstructBLIP, PaliGemma on our dataset, revealing a\nsignificant gap between single-screen and multi-screen reasoning. Finally, we\ninvestigate inference time techniques like Chain-of-Thought prompting to\nimprove model capabilities on multi-screen reasoning.\n","authors":["Maria Wang","Srinivas Sunkara","Gilles Baechler","Jason Lin","Yun Zhu","Fedir Zubach","Lei Shu","Jindong Chen"],"pdf_url":"https://arxiv.org/pdf/2409.13711v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00859v2","updated":"2024-09-24T16:54:35Z","published":"2024-08-01T18:17:25Z","title":"GLoCIM: Global-view Long Chain Interest Modeling for news recommendation","summary":"  Accurately recommending candidate news articles to users has always been the\ncore challenge of news recommendation system. News recommendations often\nrequire modeling of user interest to match candidate news. Recent efforts have\nprimarily focused on extracting local subgraph information in a global click\ngraph constructed by the clicked news sequence of all users. Howerer, the\ncomputational complexity of extracting global click graph information has\nhindered the ability to utilize far-reaching linkage which is hidden between\ntwo distant nodes in global click graph collaboratively among similar users. To\novercome the problem above, we propose a Global-view Long Chain Interests\nModeling for news recommendation (GLoCIM), which combines neighbor interest\nwith long chain interest distilled from a global click graph, leveraging the\ncollaboration among similar users to enhance news recommendation. We therefore\ndesign a long chain selection algorithm and long chain interest encoder to\nobtain global-view long chain interest from the global click graph. We design a\ngated network to integrate long chain interest with neighbor interest to\nachieve the collaborative interest among similar users. Subsequently we\naggregate it with local news category-enhanced representation to generate final\nuser representation. Then candidate news representation can be formed to match\nuser representation to achieve news recommendation. Experimental results on\nreal-world datasets validate the effectiveness of our method to improve the\nperformance of news recommendation.\n","authors":["Zhen Yang","Wenhui Wang","Tao Qi","Peng Zhang","Tianyun Zhang","Ru Zhang","Jianyi Liu","Yongfeng Huang"],"pdf_url":"https://arxiv.org/pdf/2408.00859v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16220v1","updated":"2024-09-24T16:31:33Z","published":"2024-09-24T16:31:33Z","title":"Towards Enhancing Linked Data Retrieval in Conversational UIs using\n  Large Language Models","summary":"  Despite the recent broad adoption of Large Language Models (LLMs) across\nvarious domains, their potential for enriching information systems in\nextracting and exploring Linked Data (LD) and Resource Description Framework\n(RDF) triplestores has not been extensively explored. This paper examines the\nintegration of LLMs within existing systems, emphasising the enhancement of\nconversational user interfaces (UIs) and their capabilities for data extraction\nby producing more accurate SPARQL queries without the requirement for model\nretraining. Typically, conversational UI models necessitate retraining with the\nintroduction of new datasets or updates, limiting their functionality as\ngeneral-purpose extraction tools. Our approach addresses this limitation by\nincorporating LLMs into the conversational UI workflow, significantly enhancing\ntheir ability to comprehend and process user queries effectively. By leveraging\nthe advanced natural language understanding capabilities of LLMs, our method\nimproves RDF entity extraction within web systems employing conventional\nchatbots. This integration facilitates a more nuanced and context-aware\ninteraction model, critical for handling the complex query patterns often\nencountered in RDF datasets and Linked Open Data (LOD) endpoints. The\nevaluation of this methodology shows a marked enhancement in system\nexpressivity and the accuracy of responses to user queries, indicating a\npromising direction for future research in this area. This investigation not\nonly underscores the versatility of LLMs in enhancing existing information\nsystems but also sets the stage for further explorations into their potential\napplications within more specialised domains of web information systems.\n","authors":["Omar Mussa","Omer Rana","Benoît Goossens","Pablo Orozco-Terwengel","Charith Perera"],"pdf_url":"https://arxiv.org/pdf/2409.16220v1.pdf","comment":"This paper has been accepted at the 25th International Web\n  Information Systems Engineering Conference (WISE 2024)"},{"id":"http://arxiv.org/abs/2409.16182v1","updated":"2024-09-24T15:26:38Z","published":"2024-09-24T15:26:38Z","title":"TiM4Rec: An Efficient Sequential Recommendation Model Based on\n  Time-Aware Structured State Space Duality Model","summary":"  Sequential recommendation represents a pivotal branch of recommendation\nsystems, centered around dynamically analyzing the sequential dependencies\nbetween user preferences and their interactive behaviors. Despite the\nTransformer architecture-based models achieving commendable performance within\nthis domain, their quadratic computational complexity relative to the sequence\ndimension impedes efficient modeling. In response, the innovative Mamba\narchitecture, characterized by linear computational complexity, has emerged.\nMamba4Rec further pioneers the application of Mamba in sequential\nrecommendation. Nonetheless, Mamba 1's hardware-aware algorithm struggles to\nefficiently leverage modern matrix computational units, which lead to the\nproposal of the improved State Space Duality (SSD), also known as Mamba 2.\nWhile the SSD4Rec successfully adapts the SSD architecture for sequential\nrecommendation, showing promising results in high-dimensional contexts, it\nsuffers significant performance drops in low-dimensional scenarios crucial for\npure ID sequential recommendation tasks. Addressing this challenge, we propose\na novel sequential recommendation backbone model, TiM4Rec, which ameliorates\nthe low-dimensional performance loss of the SSD architecture while preserving\nits computational efficiency. Drawing inspiration from TiSASRec, we develop a\ntime-aware enhancement method tailored for the linear computation demands of\nthe SSD architecture, thereby enhancing its adaptability and achieving\nstate-of-the-art (SOTA) performance in both low and high-dimensional modeling.\nThe code for our model is publicly accessible at\nhttps://github.com/AlwaysFHao/TiM4Rec.\n","authors":["Hao Fan","Mengyi Zhu","Yanrong Hu","Hailin Feng","Zhijie He","Hongjiu Liu","Qingyang Liu"],"pdf_url":"https://arxiv.org/pdf/2409.16182v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16143v1","updated":"2024-09-24T14:50:21Z","published":"2024-09-24T14:50:21Z","title":"Seeing Faces in Things: A Model and Dataset for Pareidolia","summary":"  The human visual system is well-tuned to detect faces of all shapes and\nsizes. While this brings obvious survival advantages, such as a better chance\nof spotting unknown predators in the bush, it also leads to spurious face\ndetections. ``Face pareidolia'' describes the perception of face-like structure\namong otherwise random stimuli: seeing faces in coffee stains or clouds in the\nsky. In this paper, we study face pareidolia from a computer vision\nperspective. We present an image dataset of ``Faces in Things'', consisting of\nfive thousand web images with human-annotated pareidolic faces. Using this\ndataset, we examine the extent to which a state-of-the-art human face detector\nexhibits pareidolia, and find a significant behavioral gap between humans and\nmachines. We find that the evolutionary need for humans to detect animal faces,\nas well as human faces, may explain some of this gap. Finally, we propose a\nsimple statistical model of pareidolia in images. Through studies on human\nsubjects and our pareidolic face detectors we confirm a key prediction of our\nmodel regarding what image conditions are most likely to induce pareidolia.\nDataset and Website: https://aka.ms/faces-in-things\n","authors":["Mark Hamilton","Simon Stent","Vasha DuTell","Anne Harrington","Jennifer Corbett","Ruth Rosenholtz","William T. Freeman"],"pdf_url":"https://arxiv.org/pdf/2409.16143v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16096v1","updated":"2024-09-24T13:50:32Z","published":"2024-09-24T13:50:32Z","title":"Exploring Hint Generation Approaches in Open-Domain Question Answering","summary":"  Automatic Question Answering (QA) systems rely on contextual information to\nprovide accurate answers. Commonly, contexts are prepared through either\nretrieval-based or generation-based methods. The former involves retrieving\nrelevant documents from a corpus like Wikipedia, whereas the latter uses\ngenerative models such as Large Language Models (LLMs) to generate the context.\nIn this paper, we introduce a novel context preparation approach called HINTQA,\nwhich employs Automatic Hint Generation (HG) techniques. Unlike traditional\nmethods, HINTQA prompts LLMs to produce hints about potential answers for the\nquestion rather than generating relevant context. We evaluate our approach\nacross three QA datasets including TriviaQA, NaturalQuestions, and Web\nQuestions, examining how the number and order of hints impact performance. Our\nfindings show that the HINTQA surpasses both retrieval-based and\ngeneration-based approaches. We demonstrate that hints enhance the accuracy of\nanswers more than retrieved and generated contexts.\n","authors":["Jamshid Mozafari","Abdelrahman Abdallah","Bhawna Piryani","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2409.16096v1.pdf","comment":"Accepted at EMNLP 2024"},{"id":"http://arxiv.org/abs/2408.09847v3","updated":"2024-09-24T13:11:43Z","published":"2024-08-19T09:50:20Z","title":"Fashion Image-to-Image Translation for Complementary Item Retrieval","summary":"  The increasing demand for online fashion retail has boosted research in\nfashion compatibility modeling and item retrieval, focusing on matching user\nqueries (textual descriptions or reference images) with compatible fashion\nitems. A key challenge is top-bottom retrieval, where precise compatibility\nmodeling is essential. Traditional methods, often based on Bayesian\nPersonalized Ranking (BPR), have shown limited performance. Recent efforts have\nexplored using generative models in compatibility modeling and item retrieval,\nwhere generated images serve as additional inputs. However, these approaches\noften overlook the quality of generated images, which could be crucial for\nmodel performance. Additionally, generative models typically require large\ndatasets, posing challenges when such data is scarce.\n  To address these issues, we introduce the Generative Compatibility Model\n(GeCo), a two-stage approach that improves fashion image retrieval through\npaired image-to-image translation. First, the Complementary Item Generation\nModel (CIGM), built on Conditional Generative Adversarial Networks (GANs),\ngenerates target item images (e.g., bottoms) from seed items (e.g., tops),\noffering conditioning signals for retrieval. These generated samples are then\nintegrated into GeCo, enhancing compatibility modeling and retrieval accuracy.\nEvaluations on three datasets show that GeCo outperforms state-of-the-art\nbaselines. Key contributions include: (i) the GeCo model utilizing paired\nimage-to-image translation within the Composed Image Retrieval framework, (ii)\ncomprehensive evaluations on benchmark datasets, and (iii) the release of a new\nFashion Taobao dataset designed for top-bottom retrieval, promoting further\nresearch.\n","authors":["Matteo Attimonelli","Claudio Pomo","Dietmar Jannach","Tommaso Di Noia"],"pdf_url":"https://arxiv.org/pdf/2408.09847v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09031v2","updated":"2024-09-24T12:30:20Z","published":"2024-03-14T01:46:56Z","title":"Projected Gradient Descent for Spectral Compressed Sensing via Symmetric\n  Hankel Factorization","summary":"  Current spectral compressed sensing methods via Hankel matrix completion\nemploy symmetric factorization to demonstrate the low-rank property of the\nHankel matrix. However, previous non-convex gradient methods only utilize\nasymmetric factorization to achieve spectral compressed sensing. In this paper,\nwe propose a novel nonconvex projected gradient descent method for spectral\ncompressed sensing via symmetric factorization named Symmetric Hankel Projected\nGradient Descent (SHGD), which updates only one matrix and avoids a balancing\nregularization term. SHGD reduces about half of the computation and storage\ncosts compared to the prior gradient method based on asymmetric factorization.\n{Besides, the symmetric factorization employed in our work is completely novel\nto the prior low-rank factorization model, introducing a new factorization\nambiguity under complex orthogonal transformation}. Novel distance metrics are\ndesigned for our factorization method and a linear convergence guarantee to the\ndesired signal is established with $O(r^2\\log(n))$ observations. Numerical\nsimulations demonstrate the superior performance of the proposed SHGD method in\nphase transitions and computation efficiency compared to state-of-the-art\nmethods.\n","authors":["Jinsheng Li","Wei Cui","Xu Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.09031v2.pdf","comment":"accepted in IEEE Transactions on Signal Processing"},{"id":"http://arxiv.org/abs/2307.02147v3","updated":"2024-09-24T10:42:43Z","published":"2023-07-05T09:42:51Z","title":"Recommendation Unlearning via Influence Function","summary":"  Recommendation unlearning is an emerging task to serve users for erasing\nunusable data (e.g., some historical behaviors) from a well-trained recommender\nmodel. Existing methods process unlearning requests by fully or partially\nretraining the model after removing the unusable data. However, these methods\nare impractical due to the high computation cost of full retraining and the\nhighly possible performance damage of partial training. In this light, a\ndesired recommendation unlearning method should obtain a similar model as full\nretraining in a more efficient manner, i.e., achieving complete, efficient and\nharmless unlearning.\n  In this work, we propose a new Influence Function-based Recommendation\nUnlearning (IFRU) framework, which efficiently updates the model without\nretraining by estimating the influence of the unusable data on the model via\nthe influence function. In the light that recent recommender models use\nhistorical data for both the constructions of the optimization loss and the\ncomputational graph (e.g., neighborhood aggregation), IFRU jointly estimates\nthe direct influence of unusable data on optimization loss and the spillover\ninfluence on the computational graph to pursue complete unlearning.\nFurthermore, we propose an importance-based pruning algorithm to reduce the\ncost of the influence function. IFRU is harmless and applicable to mainstream\ndifferentiable models. Extensive experiments demonstrate that IFRU achieves\nmore than 250 times acceleration compared to retraining-based methods with\nrecommendation performance comparable to full retraining. Codes are avaiable at\nhttps://github.com/baiyimeng/IFRU.\n","authors":["Yang Zhang","Zhiyu Hu","Yimeng Bai","Jiancan Wu","Qifan Wang","Fuli Feng"],"pdf_url":"https://arxiv.org/pdf/2307.02147v3.pdf","comment":"Accepted by ACM TORS"},{"id":"http://arxiv.org/abs/2409.15933v1","updated":"2024-09-24T09:57:25Z","published":"2024-09-24T09:57:25Z","title":"SLIMER-IT: Zero-Shot NER on Italian Language","summary":"  Traditional approaches to Named Entity Recognition (NER) frame the task into\na BIO sequence labeling problem. Although these systems often excel in the\ndownstream task at hand, they require extensive annotated data and struggle to\ngeneralize to out-of-distribution input domains and unseen entity types. On the\ncontrary, Large Language Models (LLMs) have demonstrated strong zero-shot\ncapabilities. While several works address Zero-Shot NER in English, little has\nbeen done in other languages. In this paper, we define an evaluation framework\nfor Zero-Shot NER, applying it to the Italian language. Furthermore, we\nintroduce SLIMER-IT, the Italian version of SLIMER, an instruction-tuning\napproach for zero-shot NER leveraging prompts enriched with definition and\nguidelines. Comparisons with other state-of-the-art models, demonstrate the\nsuperiority of SLIMER-IT on never-seen-before entity tags.\n","authors":["Andrew Zamai","Leonardo Rigutini","Marco Maggini","Andrea Zugarini"],"pdf_url":"https://arxiv.org/pdf/2409.15933v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15857v1","updated":"2024-09-24T08:29:10Z","published":"2024-09-24T08:29:10Z","title":"Ducho meets Elliot: Large-scale Benchmarks for Multimodal Recommendation","summary":"  In specific domains like fashion, music, and movie recommendation, the\nmulti-faceted features characterizing products and services may influence each\ncustomer on online selling platforms differently, paving the way to novel\nmultimodal recommendation models that can learn from such multimodal content.\nAccording to the literature, the common multimodal recommendation pipeline\ninvolves (i) extracting multimodal features, (ii) refining their high-level\nrepresentations to suit the recommendation task, (iii) optionally fusing all\nmultimodal features, and (iv) predicting the user-item score. While great\neffort has been put into designing optimal solutions for (ii-iv), to the best\nof our knowledge, very little attention has been devoted to exploring\nprocedures for (i). In this respect, the existing literature outlines the large\navailability of multimodal datasets and the ever-growing number of large models\naccounting for multimodal-aware tasks, but (at the same time) an unjustified\nadoption of limited standardized solutions. This motivates us to explore more\nextensive techniques for the (i) stage of the pipeline. To this end, this paper\nsettles as the first attempt to offer a large-scale benchmarking for multimodal\nrecommender systems, with a specific focus on multimodal extractors.\nSpecifically, we take advantage of two popular and recent frameworks for\nmultimodal feature extraction and reproducibility in recommendation, Ducho and\nElliot, to offer a unified and ready-to-use experimental environment able to\nrun extensive benchmarking analyses leveraging novel multimodal feature\nextractors. Results, largely validated under different hyper-parameter settings\nfor the chosen extractors, provide important insights on how to train and tune\nthe next generation of multimodal recommendation algorithms.\n","authors":["Matteo Attimonelli","Danilo Danese","Angela Di Fazio","Daniele Malitesta","Claudio Pomo","Tommaso Di Noia"],"pdf_url":"https://arxiv.org/pdf/2409.15857v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15828v1","updated":"2024-09-24T07:47:04Z","published":"2024-09-24T07:47:04Z","title":"Mitigating Digital Discrimination in Dating Apps -- The Dutch Breeze\n  case","summary":"  In September 2023, the Netherlands Institute for Human Rights, the Dutch\nnon-discrimination authority, decided that Breeze, a Dutch dating app, was\njustified in suspecting that their algorithm discriminated against non-white.\nConsequently, the Institute decided that Breeze must prevent this\ndiscrimination based on ethnicity. This paper explores two questions. (i) Is\nthe discrimination based on ethnicity in Breeze's matching algorithm illegal?\n(ii) How can dating apps mitigate or stop discrimination in their matching\nalgorithms? We illustrate the legal and technical difficulties dating apps face\nin tackling discrimination and illustrate promising solutions. We analyse the\nBreeze decision in-depth, combining insights from computer science and law. We\ndiscuss the implications of this judgment for scholarship and practice in the\nfield of fair and non-discriminatory machine learning.\n","authors":["Tim de Jonge","Frederik Zuiderveen Borgesius"],"pdf_url":"https://arxiv.org/pdf/2409.15828v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15763v1","updated":"2024-09-24T05:39:53Z","published":"2024-09-24T05:39:53Z","title":"IRSC: A Zero-shot Evaluation Benchmark for Information Retrieval through\n  Semantic Comprehension in Retrieval-Augmented Generation Scenarios","summary":"  In Retrieval-Augmented Generation (RAG) tasks using Large Language Models\n(LLMs), the quality of retrieved information is critical to the final output.\nThis paper introduces the IRSC benchmark for evaluating the performance of\nembedding models in multilingual RAG tasks. The benchmark encompasses five\nretrieval tasks: query retrieval, title retrieval, part-of-paragraph retrieval,\nkeyword retrieval, and summary retrieval. Our research addresses the current\nlack of comprehensive testing and effective comparison methods for embedding\nmodels in RAG scenarios. We introduced new metrics: the Similarity of Semantic\nComprehension Index (SSCI) and the Retrieval Capability Contest Index (RCCI),\nand evaluated models such as Snowflake-Arctic, BGE, GTE, and M3E. Our\ncontributions include: 1) the IRSC benchmark, 2) the SSCI and RCCI metrics, and\n3) insights into the cross-lingual limitations of embedding models. The IRSC\nbenchmark aims to enhance the understanding and development of accurate\nretrieval systems in RAG tasks. All code and datasets are available at:\nhttps://github.com/Jasaxion/IRSC\\_Benchmark\n","authors":["Hai Lin","Shaoxiong Zhan","Junyou Su","Haitao Zheng","Hui Wang"],"pdf_url":"https://arxiv.org/pdf/2409.15763v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15724v1","updated":"2024-09-24T04:17:21Z","published":"2024-09-24T04:17:21Z","title":"LLM-Cure: LLM-based Competitor User Review Analysis for Feature\n  Enhancement","summary":"  The exponential growth of the mobile app market underscores the importance of\nconstant innovation and rapid response to user demands. As user satisfaction is\nparamount to the success of a mobile application (app), developers typically\nrely on user reviews, which represent user feedback that includes ratings and\ncomments to identify areas for improvement. However, the sheer volume of user\nreviews poses challenges in manual analysis, necessitating automated\napproaches. Existing automated approaches either analyze only the target apps\nreviews, neglecting the comparison of similar features to competitors or fail\nto provide suggestions for feature enhancement. To address these gaps, we\npropose a Large Language Model (LLM)-based Competitive User Review Analysis for\nFeature Enhancement) (LLM-Cure), an approach powered by LLMs to automatically\ngenerate suggestion s for mobile app feature improvements. More specifically,\nLLM-Cure identifies and categorizes features within reviews by applying LLMs.\nWhen provided with a complaint in a user review, LLM-Cure curates highly rated\n(4 and 5 stars) reviews in competing apps related to the complaint and proposes\npotential improvements tailored to the target application. We evaluate LLM-Cure\non 1,056,739 reviews of 70 popular Android apps. Our evaluation demonstrates\nthat LLM-Cure significantly outperforms the state-of-the-art approaches in\nassigning features to reviews by up to 13% in F1-score, up to 16% in recall and\nup to 11% in precision. Additionally, LLM-Cure demonstrates its capability to\nprovide suggestions for resolving user complaints. We verify the suggestions\nusing the release notes that reflect the changes of features in the target\nmobile app. LLM-Cure achieves a promising average of 73% of the implementation\nof the provided suggestions.\n","authors":["Maram Assi","Safwat Hassan","Ying Zou"],"pdf_url":"https://arxiv.org/pdf/2409.15724v1.pdf","comment":"25 pages"},{"id":"http://arxiv.org/abs/2409.15700v1","updated":"2024-09-24T03:30:19Z","published":"2024-09-24T03:30:19Z","title":"Making Text Embedders Few-Shot Learners","summary":"  Large language models (LLMs) with decoder-only architectures demonstrate\nremarkable in-context learning (ICL) capabilities. This feature enables them to\neffectively handle both familiar and novel tasks by utilizing examples provided\nwithin their input context. Recognizing the potential of this capability, we\npropose leveraging the ICL feature in LLMs to enhance the process of text\nembedding generation. To this end, we introduce a novel model bge-en-icl, which\nemploys few-shot examples to produce high-quality text embeddings. Our approach\nintegrates task-related examples directly into the query side, resulting in\nsignificant improvements across various tasks. Additionally, we have\ninvestigated how to effectively utilize LLMs as embedding models, including\nvarious attention mechanisms, pooling methods, etc. Our findings suggest that\nretaining the original framework often yields the best results, underscoring\nthat simplicity is best. Experimental results on the MTEB and AIR-Bench\nbenchmarks demonstrate that our approach sets new state-of-the-art (SOTA)\nperformance. Our model, code and dataset are freely available at\nhttps://github.com/FlagOpen/FlagEmbedding .\n","authors":["Chaofan Li","MingHao Qin","Shitao Xiao","Jianlyu Chen","Kun Luo","Yingxia Shao","Defu Lian","Zheng Liu"],"pdf_url":"https://arxiv.org/pdf/2409.15700v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15690v1","updated":"2024-09-24T03:06:25Z","published":"2024-09-24T03:06:25Z","title":"A Survey of Stance Detection on Social Media: New Directions and\n  Perspectives","summary":"  In modern digital environments, users frequently express opinions on\ncontentious topics, providing a wealth of information on prevailing attitudes.\nThe systematic analysis of these opinions offers valuable insights for\ndecision-making in various sectors, including marketing and politics. As a\nresult, stance detection has emerged as a crucial subfield within affective\ncomputing, enabling the automatic detection of user stances in social media\nconversations and providing a nuanced understanding of public sentiment on\ncomplex issues. Recent years have seen a surge of research interest in\ndeveloping effective stance detection methods, with contributions from multiple\ncommunities, including natural language processing, web science, and social\ncomputing. This paper provides a comprehensive survey of stance detection\ntechniques on social media, covering task definitions, datasets, approaches,\nand future works. We review traditional stance detection models, as well as\nstate-of-the-art methods based on large language models, and discuss their\nstrengths and limitations. Our survey highlights the importance of stance\ndetection in understanding public opinion and sentiment, and identifies gaps in\ncurrent research. We conclude by outlining potential future directions for\nstance detection on social media, including the need for more robust and\ngeneralizable models, and the importance of addressing emerging challenges such\nas multi-modal stance detection and stance detection in low-resource languages.\n","authors":["Bowen Zhang","Genan Dai","Fuqiang Niu","Nan Yin","Xiaomao Fan","Hu Huang"],"pdf_url":"https://arxiv.org/pdf/2409.15690v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.07597v5","updated":"2024-09-24T03:01:25Z","published":"2023-09-14T10:57:50Z","title":"C-Pack: Packed Resources For General Chinese Embeddings","summary":"  We introduce C-Pack, a package of resources that significantly advance the\nfield of general Chinese embeddings. C-Pack includes three critical resources.\n1) C-MTEB is a comprehensive benchmark for Chinese text embeddings covering 6\ntasks and 35 datasets. 2) C-MTP is a massive text embedding dataset curated\nfrom labeled and unlabeled Chinese corpora for training embedding models. 3)\nC-TEM is a family of embedding models covering multiple sizes. Our models\noutperform all prior Chinese text embeddings on C-MTEB by up to +10% upon the\ntime of the release. We also integrate and optimize the entire suite of\ntraining methods for C-TEM. Along with our resources on general Chinese\nembedding, we release our data and models for English text embeddings. The\nEnglish models achieve state-of-the-art performance on MTEB benchmark;\nmeanwhile, our released English data is 2 times larger than the Chinese data.\nAll these resources are made publicly available at\nhttps://github.com/FlagOpen/FlagEmbedding.\n","authors":["Shitao Xiao","Zheng Liu","Peitian Zhang","Niklas Muennighoff","Defu Lian","Jian-Yun Nie"],"pdf_url":"https://arxiv.org/pdf/2309.07597v5.pdf","comment":"SIGIR 2024"},{"id":"http://arxiv.org/abs/2409.06793v2","updated":"2024-09-24T02:09:10Z","published":"2024-09-10T18:02:51Z","title":"Adversarial Attacks to Multi-Modal Models","summary":"  Multi-modal models have gained significant attention due to their powerful\ncapabilities. These models effectively align embeddings across diverse data\nmodalities, showcasing superior performance in downstream tasks compared to\ntheir unimodal counterparts. Recent study showed that the attacker can\nmanipulate an image or audio file by altering it in such a way that its\nembedding matches that of an attacker-chosen targeted input, thereby deceiving\ndownstream models. However, this method often underperforms due to inherent\ndisparities in data from different modalities. In this paper, we introduce\nCrossFire, an innovative approach to attack multi-modal models. CrossFire\nbegins by transforming the targeted input chosen by the attacker into a format\nthat matches the modality of the original image or audio file. We then\nformulate our attack as an optimization problem, aiming to minimize the angular\ndeviation between the embeddings of the transformed input and the modified\nimage or audio file. Solving this problem determines the perturbations to be\nadded to the original media. Our extensive experiments on six real-world\nbenchmark datasets reveal that CrossFire can significantly manipulate\ndownstream tasks, surpassing existing attacks. Additionally, we evaluate six\ndefensive strategies against CrossFire, finding that current defenses are\ninsufficient to counteract our CrossFire.\n","authors":["Zhihao Dou","Xin Hu","Haibo Yang","Zhuqing Liu","Minghong Fang"],"pdf_url":"https://arxiv.org/pdf/2409.06793v2.pdf","comment":"To appear in the ACM Workshop on Large AI Systems and Models with\n  Privacy and Safety Analysis 2024 (LAMPS '24)"},{"id":"http://arxiv.org/abs/2409.15626v1","updated":"2024-09-24T00:09:41Z","published":"2024-09-24T00:09:41Z","title":"Qualitative Insights Tool (QualIT): LLM Enhanced Topic Modeling","summary":"  Topic modeling is a widely used technique for uncovering thematic structures\nfrom large text corpora. However, most topic modeling approaches e.g. Latent\nDirichlet Allocation (LDA) struggle to capture nuanced semantics and contextual\nunderstanding required to accurately model complex narratives. Recent\nadvancements in this area include methods like BERTopic, which have\ndemonstrated significantly improved topic coherence and thus established a new\nstandard for benchmarking. In this paper, we present a novel approach, the\nQualitative Insights Tool (QualIT) that integrates large language models (LLMs)\nwith existing clustering-based topic modeling approaches. Our method leverages\nthe deep contextual understanding and powerful language generation capabilities\nof LLMs to enrich the topic modeling process using clustering. We evaluate our\napproach on a large corpus of news articles and demonstrate substantial\nimprovements in topic coherence and topic diversity compared to baseline topic\nmodeling techniques. On the 20 ground-truth topics, our method shows 70% topic\ncoherence (vs 65% & 57% benchmarks) and 95.5% topic diversity (vs 85% & 72%\nbenchmarks). Our findings suggest that the integration of LLMs can unlock new\nopportunities for topic modeling of dynamic and complex text data, as is common\nin talent management research contexts.\n","authors":["Satya Kapoor","Alex Gil","Sreyoshi Bhaduri","Anshul Mittal","Rutu Mulkar"],"pdf_url":"https://arxiv.org/pdf/2409.15626v1.pdf","comment":"6 pages, 4 tables, 1 figure"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2408.14028v3","updated":"2024-09-24T23:23:50Z","published":"2024-08-26T05:38:27Z","title":"SurGen: Text-Guided Diffusion Model for Surgical Video Generation","summary":"  Diffusion-based video generation models have made significant strides,\nproducing outputs with improved visual fidelity, temporal coherence, and user\ncontrol. These advancements hold great promise for improving surgical education\nby enabling more realistic, diverse, and interactive simulation environments.\nIn this study, we introduce SurGen, a text-guided diffusion model tailored for\nsurgical video synthesis. SurGen produces videos with the highest resolution\nand longest duration among existing surgical video generation models. We\nvalidate the visual and temporal quality of the outputs using standard image\nand video generation metrics. Additionally, we assess their alignment to the\ncorresponding text prompts through a deep learning classifier trained on\nsurgical data. Our results demonstrate the potential of diffusion models to\nserve as valuable educational tools for surgical trainees.\n","authors":["Joseph Cho","Samuel Schmidgall","Cyril Zakka","Mrudang Mathur","Dhamanpreet Kaur","Rohan Shad","William Hiesinger"],"pdf_url":"https://arxiv.org/pdf/2408.14028v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16502v1","updated":"2024-09-24T23:18:32Z","published":"2024-09-24T23:18:32Z","title":"GSplatLoc: Grounding Keypoint Descriptors into 3D Gaussian Splatting for\n  Improved Visual Localization","summary":"  Although various visual localization approaches exist, such as scene\ncoordinate and pose regression, these methods often struggle with high memory\nconsumption or extensive optimization requirements. To address these\nchallenges, we utilize recent advancements in novel view synthesis,\nparticularly 3D Gaussian Splatting (3DGS), to enhance localization. 3DGS allows\nfor the compact encoding of both 3D geometry and scene appearance with its\nspatial features. Our method leverages the dense description maps produced by\nXFeat's lightweight keypoint detection and description model. We propose\ndistilling these dense keypoint descriptors into 3DGS to improve the model's\nspatial understanding, leading to more accurate camera pose predictions through\n2D-3D correspondences. After estimating an initial pose, we refine it using a\nphotometric warping loss. Benchmarking on popular indoor and outdoor datasets\nshows that our approach surpasses state-of-the-art Neural Render Pose (NRP)\nmethods, including NeRFMatch and PNeRFLoc.\n","authors":["Gennady Sidorov","Malik Mohrat","Ksenia Lebedeva","Ruslan Rakhimov","Sergey Kolyubin"],"pdf_url":"https://arxiv.org/pdf/2409.16502v1.pdf","comment":"Project website at https://gsplatloc.github.io/"},{"id":"http://arxiv.org/abs/2409.08419v2","updated":"2024-09-24T23:16:02Z","published":"2024-09-12T22:45:10Z","title":"Introducing CausalBench: A Flexible Benchmark Framework for Causal\n  Analysis and Machine Learning","summary":"  While witnessing the exceptional success of machine learning (ML)\ntechnologies in many applications, users are starting to notice a critical\nshortcoming of ML: correlation is a poor substitute for causation. The\nconventional way to discover causal relationships is to use randomized\ncontrolled experiments (RCT); in many situations, however, these are\nimpractical or sometimes unethical. Causal learning from observational data\noffers a promising alternative. While being relatively recent, causal learning\naims to go far beyond conventional machine learning, yet several major\nchallenges remain. Unfortunately, advances are hampered due to the lack of\nunified benchmark datasets, algorithms, metrics, and evaluation service\ninterfaces for causal learning. In this paper, we introduce {\\em CausalBench},\na transparent, fair, and easy-to-use evaluation platform, aiming to (a) enable\nthe advancement of research in causal learning by facilitating scientific\ncollaboration in novel algorithms, datasets, and metrics and (b) promote\nscientific objectivity, reproducibility, fairness, and awareness of bias in\ncausal learning research. CausalBench provides services for benchmarking data,\nalgorithms, models, and metrics, impacting the needs of a broad of scientific\nand engineering disciplines.\n","authors":["Ahmet Kapkiç","Pratanu Mandal","Shu Wan","Paras Sheth","Abhinav Gorantla","Yoonhyuk Choi","Huan Liu","K. Selçuk Candan"],"pdf_url":"https://arxiv.org/pdf/2409.08419v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16499v1","updated":"2024-09-24T23:11:47Z","published":"2024-09-24T23:11:47Z","title":"Learning Linear Dynamics from Bilinear Observations","summary":"  We consider the problem of learning a realization of a partially observed\ndynamical system with linear state transitions and bilinear observations. Under\nvery mild assumptions on the process and measurement noises, we provide a\nfinite time analysis for learning the unknown dynamics matrices (up to a\nsimilarity transform). Our analysis involves a regression problem with\nheavy-tailed and dependent data. Moreover, each row of our design matrix\ncontains a Kronecker product of current input with a history of inputs, making\nit difficult to guarantee persistence of excitation. We overcome these\nchallenges, first providing a data-dependent high probability error bound for\narbitrary but fixed inputs. Then, we derive a data-independent error bound for\ninputs chosen according to a simple random design. Our main results provide an\nupper bound on the statistical error rates and sample complexity of learning\nthe unknown dynamics matrices from a single finite trajectory of bilinear\nobservations.\n","authors":["Yahya Sattar","Yassir Jedra","Sarah Dean"],"pdf_url":"https://arxiv.org/pdf/2409.16499v1.pdf","comment":"35 pages, 3 figures"},{"id":"http://arxiv.org/abs/2409.16495v1","updated":"2024-09-24T22:49:27Z","published":"2024-09-24T22:49:27Z","title":"Flight: A FaaS-Based Framework for Complex and Hierarchical Federated\n  Learning","summary":"  Federated Learning (FL) is a decentralized machine learning paradigm where\nmodels are trained on distributed devices and are aggregated at a central\nserver. Existing FL frameworks assume simple two-tier network topologies where\nend devices are directly connected to the aggregation server. While this is a\npractical mental model, it does not exploit the inherent topology of real-world\ndistributed systems like the Internet-of-Things. We present Flight, a novel FL\nframework that supports complex hierarchical multi-tier topologies,\nasynchronous aggregation, and decouples the control plane from the data plane.\nWe compare the performance of Flight against Flower, a state-of-the-art FL\nframework. Our results show that Flight scales beyond Flower, supporting up to\n2048 simultaneous devices, and reduces FL makespan across several models.\nFinally, we show that Flight's hierarchical FL model can reduce communication\noverheads by more than 60%.\n","authors":["Nathaniel Hudson","Valerie Hayot-Sasson","Yadu Babuji","Matt Baughman","J. Gregory Pauloski","Ryan Chard","Ian Foster","Kyle Chard"],"pdf_url":"https://arxiv.org/pdf/2409.16495v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16490v1","updated":"2024-09-24T22:31:39Z","published":"2024-09-24T22:31:39Z","title":"Exploring Knowledge Tracing in Tutor-Student Dialogues","summary":"  Recent advances in large language models (LLMs) have led to the development\nof artificial intelligence (AI)-powered tutoring chatbots, showing promise in\nproviding broad access to high-quality personalized education. Existing works\nhave primarily studied how to make LLMs follow tutoring principles but not how\nto model student behavior in dialogues. However, analyzing student dialogue\nturns can serve as a formative assessment, since open-ended student discourse\nmay indicate their knowledge levels and reveal specific misconceptions. In this\nwork, we present a first attempt at performing knowledge tracing (KT) in\ntutor-student dialogues. We propose LLM prompting methods to identify the\nknowledge components/skills involved in each dialogue turn and diagnose whether\nthe student responds correctly to the tutor, and verify the LLM's effectiveness\nvia an expert human evaluation. We then apply a range of KT methods on the\nresulting labeled data to track student knowledge levels over an entire\ndialogue. We conduct experiments on two tutoring dialogue datasets, and show\nthat a novel yet simple LLM-based method, LLMKT, significantly outperforms\nexisting KT methods in predicting student response correctness in dialogues. We\nperform extensive qualitative analyses to highlight the challenges in dialogue\nKT and outline multiple avenues for future work.\n","authors":["Alexander Scarlatos","Andrew Lan"],"pdf_url":"https://arxiv.org/pdf/2409.16490v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2409.16404v1","updated":"2024-09-24T19:06:18Z","published":"2024-09-24T19:06:18Z","title":"FastTalker: Jointly Generating Speech and Conversational Gestures from\n  Text","summary":"  Generating 3D human gestures and speech from a text script is critical for\ncreating realistic talking avatars. One solution is to leverage separate\npipelines for text-to-speech (TTS) and speech-to-gesture (STG), but this\napproach suffers from poor alignment of speech and gestures and slow inference\ntimes. In this paper, we introduce FastTalker, an efficient and effective\nframework that simultaneously generates high-quality speech audio and 3D human\ngestures at high inference speeds. Our key insight is reusing the intermediate\nfeatures from speech synthesis for gesture generation, as these features\ncontain more precise rhythmic information than features re-extracted from\ngenerated speech. Specifically, 1) we propose an end-to-end framework that\nconcurrently generates speech waveforms and full-body gestures, using\nintermediate speech features such as pitch, onset, energy, and duration\ndirectly for gesture decoding; 2) we redesign the causal network architecture\nto eliminate dependencies on future inputs for real applications; 3) we employ\nReinforcement Learning-based Neural Architecture Search (NAS) to enhance both\nperformance and inference speed by optimizing our network architecture.\nExperimental results on the BEAT2 dataset demonstrate that FastTalker achieves\nstate-of-the-art performance in both speech synthesis and gesture generation,\nprocessing speech and gestures in 0.17 seconds per second on an NVIDIA 3090.\n","authors":["Zixin Guo","Jian Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.16404v1.pdf","comment":"European Conference on Computer Vision Workshop"},{"id":"http://arxiv.org/abs/2409.16136v1","updated":"2024-09-24T14:43:14Z","published":"2024-09-24T14:43:14Z","title":"HA-FGOVD: Highlighting Fine-grained Attributes via Explicit Linear\n  Composition for Open-Vocabulary Object Detection","summary":"  Open-vocabulary object detection (OVD) models are considered to be Large\nMulti-modal Models (LMM), due to their extensive training data and a large\nnumber of parameters. Mainstream OVD models prioritize object coarse-grained\ncategory rather than focus on their fine-grained attributes, e.g., colors or\nmaterials, thus failed to identify objects specified with certain attributes.\nHowever, OVD models are pretrained on large-scale image-text pairs with rich\nattribute words, whose latent feature space can represent the global text\nfeature as a linear composition of fine-grained attribute tokens without\nhighlighting them. Therefore, we propose in this paper a universal and explicit\napproach for frozen mainstream OVD models that boosts their attribute-level\ndetection capabilities by highlighting fine-grained attributes in explicit\nlinear space. Firstly, a LLM is leveraged to highlight attribute words within\nthe input text as a zero-shot prompted task. Secondly, by strategically\nadjusting the token masks, the text encoders of OVD models extract both global\ntext and attribute-specific features, which are then explicitly composited as\ntwo vectors in linear space to form the new attribute-highlighted feature for\ndetection tasks, where corresponding scalars are hand-crafted or learned to\nreweight both two vectors. Notably, these scalars can be seamlessly transferred\namong different OVD models, which proves that such an explicit linear\ncomposition is universal. Empirical evaluation on the FG-OVD dataset\ndemonstrates that our proposed method uniformly improves fine-grained\nattribute-level OVD of various mainstream models and achieves new\nstate-of-the-art performance.\n","authors":["Yuqi Ma","Mengyin Liu","Chao Zhu","Xu-Cheng Yin"],"pdf_url":"https://arxiv.org/pdf/2409.16136v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2409.15813v1","updated":"2024-09-24T07:19:30Z","published":"2024-09-24T07:19:30Z","title":"Layer-wise Model Merging for Unsupervised Domain Adaptation in\n  Segmentation Tasks","summary":"  Merging parameters of multiple models has resurfaced as an effective strategy\nto enhance task performance and robustness, but prior work is limited by the\nhigh costs of ensemble creation and inference. In this paper, we leverage the\nabundance of freely accessible trained models to introduce a cost-free approach\nto model merging. It focuses on a layer-wise integration of merged models,\naiming to maintain the distinctiveness of the task-specific final layers while\nunifying the initial layers, which are primarily associated with feature\nextraction. This approach ensures parameter consistency across all layers,\nessential for boosting performance. Moreover, it facilitates seamless\nintegration of knowledge, enabling effective merging of models from different\ndatasets and tasks. Specifically, we investigate its applicability in\nUnsupervised Domain Adaptation (UDA), an unexplored area for model merging, for\nSemantic and Panoptic Segmentation. Experimental results demonstrate\nsubstantial UDA improvements without additional costs for merging\nsame-architecture models from distinct datasets ($\\uparrow 2.6\\%$ mIoU) and\ndifferent-architecture models with a shared backbone ($\\uparrow 6.8\\%$ mIoU).\nFurthermore, merging Semantic and Panoptic Segmentation models increases mPQ by\n$\\uparrow 7\\%$. These findings are validated across a wide variety of UDA\nstrategies, architectures, and datasets.\n","authors":["Roberto Alcover-Couso","Juan C. SanMiguel","Marcos Escudero-Viñolo","Jose M Martínez"],"pdf_url":"https://arxiv.org/pdf/2409.15813v1.pdf","comment":null}]},"2024-09-23T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2409.15576v1","updated":"2024-09-23T22:23:08Z","published":"2024-09-23T22:23:08Z","title":"Optimizing News Text Classification with Bi-LSTM and Attention Mechanism\n  for Efficient Data Processing","summary":"  The development of Internet technology has led to a rapid increase in news\ninformation. Filtering out valuable content from complex information has become\nan urgentproblem that needs to be solved. In view of the shortcomings of\ntraditional manual classification methods that are time-consuming and\ninefficient, this paper proposes an automaticclassification scheme for news\ntexts based on deep learning. This solution achieves efficient classification\nand management of news texts by introducing advanced machine learning\nalgorithms, especially an optimization model that combines Bi-directional Long\nShort-Term Memory Network (Bi-LSTM) and Attention Mechanism. Experimental\nresults show that this solution can not only significantly improve the accuracy\nand timeliness of classification, but also significantly reduce the need for\nmanual intervention. It has important practical significance for improving the\ninformation processing capabilities of the news industry and accelerating the\nspeed of information flow. Through comparative analysis of multiple common\nmodels, the effectiveness and advancement of the proposed method are proved,\nlaying a solid foundation for future news text classification research.\n","authors":["Bingyao Liu","Jiajing Chen","Rui Wang","Junming Huang","Yuanshuai Luo","Jianjun Wei"],"pdf_url":"https://arxiv.org/pdf/2409.15576v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15568v1","updated":"2024-09-23T21:50:35Z","published":"2024-09-23T21:50:35Z","title":"Cross-Domain Latent Factors Sharing via Implicit Matrix Factorization","summary":"  Data sparsity has been one of the long-standing problems for recommender\nsystems. One of the solutions to mitigate this issue is to exploit knowledge\navailable in other source domains. However, many cross-domain recommender\nsystems introduce a complex architecture that makes them less scalable in\npractice. On the other hand, matrix factorization methods are still considered\nto be strong baselines for single-domain recommendations. In this paper, we\nintroduce the CDIMF, a model that extends the standard implicit matrix\nfactorization with ALS to cross-domain scenarios. We apply the Alternating\nDirection Method of Multipliers to learn shared latent factors for overlapped\nusers while factorizing the interaction matrix. In a dual-domain setting,\nexperiments on industrial datasets demonstrate a competing performance of CDIMF\nfor both cold-start and warm-start. The proposed model can outperform most\nother recent cross-domain and single-domain models. We also provide the code to\nreproduce experiments on GitHub.\n","authors":["Abdulaziz Samra","Evgeney Frolov","Alexey Vasilev","Alexander Grigorievskiy","Anton Vakhrushev"],"pdf_url":"https://arxiv.org/pdf/2409.15568v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15558v1","updated":"2024-09-23T21:29:03Z","published":"2024-09-23T21:29:03Z","title":"Stalactite: Toolbox for Fast Prototyping of Vertical Federated Learning\n  Systems","summary":"  Machine learning (ML) models trained on datasets owned by different\norganizations and physically located in remote databases offer benefits in many\nreal-world use cases. State regulations or business requirements often prevent\ndata transfer to a central location, making it difficult to utilize standard\nmachine learning algorithms. Federated Learning (FL) is a technique that\nenables models to learn from distributed datasets without revealing the\noriginal data. Vertical Federated learning (VFL) is a type of FL where data\nsamples are divided by features across several data owners. For instance, in a\nrecommendation task, a user can interact with various sets of items, and the\nlogs of these interactions are stored by different organizations. In this demo\npaper, we present \\emph{Stalactite} - an open-source framework for VFL that\nprovides the necessary functionality for building prototypes of VFL systems. It\nhas several advantages over the existing frameworks. In particular, it allows\nresearchers to focus on the algorithmic side rather than engineering and to\neasily deploy learning in a distributed environment. It implements several VFL\nalgorithms and has a built-in homomorphic encryption layer. We demonstrate its\nuse on a real-world recommendation datasets.\n","authors":["Anastasiia Zakharova","Dmitriy Alexandrov","Maria Khodorchenko","Nikolay Butakov","Alexey Vasilev","Maxim Savchenko","Alexander Grigorievskiy"],"pdf_url":"https://arxiv.org/pdf/2409.15558v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15260v1","updated":"2024-09-23T17:56:08Z","published":"2024-09-23T17:56:08Z","title":"Generative AI Is Not Ready for Clinical Use in Patient Education for\n  Lower Back Pain Patients, Even With Retrieval-Augmented Generation","summary":"  Low back pain (LBP) is a leading cause of disability globally. Following the\nonset of LBP and subsequent treatment, adequate patient education is crucial\nfor improving functionality and long-term outcomes. Despite advancements in\npatient education strategies, significant gaps persist in delivering\npersonalized, evidence-based information to patients with LBP. Recent\nadvancements in large language models (LLMs) and generative artificial\nintelligence (GenAI) have demonstrated the potential to enhance patient\neducation. However, their application and efficacy in delivering educational\ncontent to patients with LBP remain underexplored and warrant further\ninvestigation. In this study, we introduce a novel approach utilizing LLMs with\nRetrieval-Augmented Generation (RAG) and few-shot learning to generate tailored\neducational materials for patients with LBP. Physical therapists manually\nevaluated our model responses for redundancy, accuracy, and completeness using\na Likert scale. In addition, the readability of the generated education\nmaterials is assessed using the Flesch Reading Ease score. The findings\ndemonstrate that RAG-based LLMs outperform traditional LLMs, providing more\naccurate, complete, and readable patient education materials with less\nredundancy. Having said that, our analysis reveals that the generated materials\nare not yet ready for use in clinical practice. This study underscores the\npotential of AI-driven models utilizing RAG to improve patient education for\nLBP; however, significant challenges remain in ensuring the clinical relevance\nand granularity of content generated by these models.\n","authors":["Yi-Fei Zhao","Allyn Bove","David Thompson","James Hill","Yi Xu","Yufan Ren","Andrea Hassman","Leming Zhou","Yanshan Wang"],"pdf_url":"https://arxiv.org/pdf/2409.15260v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04548v2","updated":"2024-09-23T17:38:54Z","published":"2024-06-06T23:09:54Z","title":"GNNAnatomy: Systematic Generation and Evaluation of Multi-Level\n  Explanations for Graph Neural Networks","summary":"  Graph Neural Networks (GNNs) excel in machine learning tasks involving\ngraphs, such as node classification, graph classification, and link prediction.\nHowever, explaining their decision-making process is challenging due to the\ncomplex transformations GNNs perform by aggregating relational information from\ngraph topology. Existing methods for explaining GNNs face key limitations: (1)\nlack of flexibility in generating explanations at varying levels, (2)\ndifficulty in identifying unique substructures relevant to class\ndifferentiation, and (3) little support to ensure the trustworthiness of\nexplanations. To address these challenges, we introduce GNNAnatomy, a visual\nanalytics system designed to generate and evaluate multi-level GNN explanations\nfor graph classification tasks. GNNAnatomy uses graphlets, primitive graph\nsubstructures, to identify the most critical substructures in a graph class by\nanalyzing the correlation between GNN predictions and graphlet frequencies.\nThese correlations are presented interactively for user-selected group of\ngraphs through our visual analytics system. To further validate top-ranked\ngraphlets, we measure the change in classification confidence after removing\neach graphlet from the original graph. We demonstrate the effectiveness of\nGNNAnatomy through case studies on synthetic and real-world graph datasets from\nsociology and biology domains. Additionally, we compare GNNAnatomy with\nstate-of-the-art explainable GNN methods to showcase its utility and\nversatility.\n","authors":["Hsiao-Ying Lu","Yiran Li","Ujwal Pratap Krishna Kaluvakolanu Thyagarajan","Kwan-Liu Ma"],"pdf_url":"https://arxiv.org/pdf/2406.04548v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15163v1","updated":"2024-09-23T16:16:08Z","published":"2024-09-23T16:16:08Z","title":"Lessons Learned on Information Retrieval in Electronic Health Records: A\n  Comparison of Embedding Models and Pooling Strategies","summary":"  Objective: Applying large language models (LLMs) to the clinical domain is\nchallenging due to the context-heavy nature of processing medical records.\nRetrieval-augmented generation (RAG) offers a solution by facilitating\nreasoning over large text sources. However, there are many parameters to\noptimize in just the retrieval system alone. This paper presents an ablation\nstudy exploring how different embedding models and pooling methods affect\ninformation retrieval for the clinical domain.\n  Methods: Evaluating on three retrieval tasks on two electronic health record\n(EHR) data sources, we compared seven models, including medical- and\ngeneral-domain models, specialized encoder embedding models, and off-the-shelf\ndecoder LLMs. We also examine the choice of embedding pooling strategy for each\nmodel, independently on the query and the text to retrieve.\n  Results: We found that the choice of embedding model significantly impacts\nretrieval performance, with BGE, a comparatively small general-domain model,\nconsistently outperforming all others, including medical-specific models.\nHowever, our findings also revealed substantial variability across datasets and\nquery text phrasings. We also determined the best pooling methods for each of\nthese models to guide future design of retrieval systems.\n  Discussion: The choice of embedding model, pooling strategy, and query\nformulation can significantly impact retrieval performance and the performance\nof these models on other public benchmarks does not necessarily transfer to new\ndomains. Further studies such as this one are vital for guiding\nempirically-grounded development of retrieval frameworks, such as in the\ncontext of RAG, for the clinical domain.\n","authors":["Skatje Myers","Timothy A. Miller","Yanjun Gao","Matthew M. Churpek","Anoop Mayampurath","Dmitriy Dligach","Majid Afshar"],"pdf_url":"https://arxiv.org/pdf/2409.15163v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15133v1","updated":"2024-09-23T15:38:12Z","published":"2024-09-23T15:38:12Z","title":"Don't Use LLMs to Make Relevance Judgments","summary":"  Making the relevance judgments for a TREC-style test collection can be\ncomplex and expensive. A typical TREC track usually involves a team of six\ncontractors working for 2-4 weeks. Those contractors need to be trained and\nmonitored. Software has to be written to support recording relevance judgments\ncorrectly and efficiently. The recent advent of large language models that\nproduce astoundingly human-like flowing text output in response to a natural\nlanguage prompt has inspired IR researchers to wonder how those models might be\nused in the relevance judgment collection process. At the ACM SIGIR 2024\nconference, a workshop ``LLM4Eval'' provided a venue for this work, and\nfeatured a data challenge activity where participants reproduced TREC deep\nlearning track judgments, as was done by Thomas et al (arXiv:2408.08896,\narXiv:2309.10621). I was asked to give a keynote at the workshop, and this\npaper presents that keynote in article form. The bottom-line-up-front message\nis, don't use LLMs to create relevance judgments for TREC-style evaluations.\n","authors":["Ian Soboroff"],"pdf_url":"https://arxiv.org/pdf/2409.15133v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15060v1","updated":"2024-09-23T14:35:06Z","published":"2024-09-23T14:35:06Z","title":"EMERS: Energy Meter for Recommender Systems","summary":"  Due to recent advancements in machine learning, recommender systems use\nincreasingly more energy for training, evaluation, and deployment. However, the\nrecommender systems community often does not report the energy consumption of\ntheir experiments. In today's research landscape, no tools exist to easily\nmeasure the energy consumption of recommender systems experiments. To bridge\nthis gap, we introduce EMERS, the first software library that simplifies\nmeasuring, monitoring, recording, and sharing the energy consumption of\nrecommender systems experiments. EMERS measures energy consumption with smart\npower plugs and offers a user interface to monitor and compare the energy\nconsumption of recommender systems experiments. Thereby, EMERS improves\nsustainability awareness and simplifies self-reporting energy consumption for\nrecommender systems practitioners and researchers.\n","authors":["Lukas Wegmeth","Tobias Vente","Alan Said","Joeran Beel"],"pdf_url":"https://arxiv.org/pdf/2409.15060v1.pdf","comment":"Accepted at the RecSoGood 2024 Workshop co-located with the 18th ACM\n  Conference on Recommender Systems"},{"id":"http://arxiv.org/abs/2409.15004v1","updated":"2024-09-23T13:28:06Z","published":"2024-09-23T13:28:06Z","title":"ViBERTgrid BiLSTM-CRF: Multimodal Key Information Extraction from\n  Unstructured Financial Documents","summary":"  Multimodal key information extraction (KIE) models have been studied\nextensively on semi-structured documents. However, their investigation on\nunstructured documents is an emerging research topic. The paper presents an\napproach to adapt a multimodal transformer (i.e., ViBERTgrid previously\nexplored on semi-structured documents) for unstructured financial documents, by\nincorporating a BiLSTM-CRF layer. The proposed ViBERTgrid BiLSTM-CRF model\ndemonstrates a significant improvement in performance (up to 2 percentage\npoints) on named entity recognition from unstructured documents in financial\ndomain, while maintaining its KIE performance on semi-structured documents. As\nan additional contribution, we publicly released token-level annotations for\nthe SROIE dataset in order to pave the way for its use in multimodal sequence\nlabeling models.\n","authors":["Furkan Pala","Mehmet Yasin Akpınar","Onur Deniz","Gülşen Eryiğit"],"pdf_url":"https://arxiv.org/pdf/2409.15004v1.pdf","comment":"Accepted in MIDAS (The 8th Workshop on MIning DAta for financial\n  applicationS) workshop of ECML PKDD 2023 conference"},{"id":"http://arxiv.org/abs/2409.14945v1","updated":"2024-09-23T12:02:23Z","published":"2024-09-23T12:02:23Z","title":"Adaptive Learning on User Segmentation: Universal to Specific\n  Representation via Bipartite Neural Interaction","summary":"  Recently, models for user representation learning have been widely applied in\nclick-through-rate (CTR) and conversion-rate (CVR) prediction. Usually, the\nmodel learns a universal user representation as the input for subsequent\nscenario-specific models. However, in numerous industrial applications (e.g.,\nrecommendation and marketing), the business always operates such applications\nas various online activities among different user segmentation. These\nsegmentation are always created by domain experts. Due to the difference in\nuser distribution (i.e., user segmentation) and business objectives in\nsubsequent tasks, learning solely on universal representation may lead to\ndetrimental effects on both model performance and robustness. In this paper, we\npropose a novel learning framework that can first learn general universal user\nrepresentation through information bottleneck. Then, merge and learn a\nsegmentation-specific or a task-specific representation through neural\ninteraction. We design the interactive learning process by leveraging a\nbipartite graph architecture to model the representation learning and merging\nbetween contextual clusters and each user segmentation. Our proposed method is\nevaluated in two open-source benchmarks, two offline business datasets, and\ndeployed on two online marketing applications to predict users' CVR. The\nresults demonstrate that our method can achieve superior performance and\nsurpass the baseline methods.\n","authors":["Xiaoyu Tan","Yongxin Deng","Chao Qu","Siqiao Xue","Xiaoming Shi","James Zhang","Xihe Qiu"],"pdf_url":"https://arxiv.org/pdf/2409.14945v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14872v1","updated":"2024-09-23T10:10:24Z","published":"2024-09-23T10:10:24Z","title":"FedSlate:A Federated Deep Reinforcement Learning Recommender System","summary":"  Reinforcement learning methods have been used to optimize long-term user\nengagement in recommendation systems. However, existing reinforcement\nlearning-based recommendation systems do not fully exploit the relevance of\nindividual user behavior across different platforms. One potential solution is\nto aggregate data from various platforms in a centralized location and use the\naggregated data for training. However, this approach raises economic and legal\nconcerns, including increased communication costs and potential threats to user\nprivacy. To address these challenges, we propose \\textbf{FedSlate}, a federated\nreinforcement learning recommendation algorithm that effectively utilizes\ninformation that is prohibited from being shared at a legal level. We employ\nthe SlateQ algorithm to assist FedSlate in learning users' long-term behavior\nand evaluating the value of recommended content. We extend the existing\napplication scope of recommendation systems from single-user single-platform to\nsingle-user multi-platform and address cross-platform learning challenges by\nintroducing federated learning. We use RecSim to construct a simulation\nenvironment for evaluating FedSlate and compare its performance with\nstate-of-the-art benchmark recommendation models. Experimental results\ndemonstrate the superior effects of FedSlate over baseline methods in various\nenvironmental settings, and FedSlate facilitates the learning of recommendation\nstrategies in scenarios where baseline methods are completely inapplicable.\nCode is available at \\textit{https://github.com/TianYaDY/FedSlate}.\n","authors":["Yongxin Deng","Xiaoyu Tan","Xihe Qiu","Yaochu Jin"],"pdf_url":"https://arxiv.org/pdf/2409.14872v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14810v1","updated":"2024-09-23T08:39:07Z","published":"2024-09-23T08:39:07Z","title":"Pre-trained Language Model and Knowledge Distillation for Lightweight\n  Sequential Recommendation","summary":"  Sequential recommendation models user interests based on historical behaviors\nto provide personalized recommendation. Previous sequential recommendation\nalgorithms primarily employ neural networks to extract features of user\ninterests, achieving good performance. However, due to the recommendation\nsystem datasets sparsity, these algorithms often employ small-scale network\nframeworks, resulting in weaker generalization capability. Recently, a series\nof sequential recommendation algorithms based on large pre-trained language\nmodels have been proposed. Nonetheless, given the real-time demands of\nrecommendation systems, the challenge remains in applying pre-trained language\nmodels for rapid recommendations in real scenarios. To address this, we propose\na sequential recommendation algorithm based on a pre-trained language model and\nknowledge distillation. The key of proposed algorithm is to transfer\npre-trained knowledge across domains and achieve lightweight inference by\nknowledge distillation. The algorithm operates in two stages: in the first\nstage, we fine-tune the pre-trained language model on the recommendation\ndataset to transfer the pre-trained knowledge to the recommendation task; in\nthe second stage, we distill the trained language model to transfer the learned\nknowledge to a lightweight model. Extensive experiments on multiple public\nrecommendation datasets show that the proposed algorithm enhances\nrecommendation accuracy and provide timely recommendation services.\n","authors":["Li Li","Mingyue Cheng","Zhiding Liu","Hao Zhang","Qi Liu","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2409.14810v1.pdf","comment":"in Chinese language"},{"id":"http://arxiv.org/abs/2409.14689v1","updated":"2024-09-23T03:23:20Z","published":"2024-09-23T03:23:20Z","title":"EDGE-Rec: Efficient and Data-Guided Edge Diffusion For Recommender\n  Systems Graphs","summary":"  Most recommender systems research focuses on binary historical user-item\ninteraction encodings to predict future interactions. User features, item\nfeatures, and interaction strengths remain largely under-utilized in this space\nor only indirectly utilized, despite proving largely effective in large-scale\nproduction recommendation systems. We propose a new attention mechanism,\nloosely based on the principles of collaborative filtering, called Row-Column\nSeparable Attention RCSA to take advantage of real-valued interaction weights\nas well as user and item features directly. Building on this mechanism, we\nadditionally propose a novel Graph Diffusion Transformer GDiT architecture\nwhich is trained to iteratively denoise the weighted interaction matrix of the\nuser-item interaction graph directly. The weighted interaction matrix is built\nfrom the bipartite structure of the user-item interaction graph and\ncorresponding edge weights derived from user-item rating interactions. Inspired\nby the recent progress in text-conditioned image generation, our method\ndirectly produces user-item rating predictions on the same scale as the\noriginal ratings by conditioning the denoising process on user and item\nfeatures with a principled approach.\n","authors":["Utkarsh Priyam","Hemit Shah","Edoardo Botta"],"pdf_url":"https://arxiv.org/pdf/2409.14689v1.pdf","comment":"6 pages, 13 figures"},{"id":"http://arxiv.org/abs/2409.14683v1","updated":"2024-09-23T03:12:43Z","published":"2024-09-23T03:12:43Z","title":"Reducing the Footprint of Multi-Vector Retrieval with Minimal\n  Performance Impact via Token Pooling","summary":"  Over the last few years, multi-vector retrieval methods, spearheaded by\nColBERT, have become an increasingly popular approach to Neural IR. By storing\nrepresentations at the token level rather than at the document level, these\nmethods have demonstrated very strong retrieval performance, especially in\nout-of-domain settings. However, the storage and memory requirements necessary\nto store the large number of associated vectors remain an important drawback,\nhindering practical adoption. In this paper, we introduce a simple\nclustering-based token pooling approach to aggressively reduce the number of\nvectors that need to be stored. This method can reduce the space & memory\nfootprint of ColBERT indexes by 50% with virtually no retrieval performance\ndegradation. This method also allows for further reductions, reducing the\nvector count by 66%-to-75% , with degradation remaining below 5% on a vast\nmajority of datasets. Importantly, this approach requires no architectural\nchange nor query-time processing, and can be used as a simple drop-in during\nindexation with any ColBERT-like model.\n","authors":["Benjamin Clavié","Antoine Chaffin","Griffin Adams"],"pdf_url":"https://arxiv.org/pdf/2409.14683v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14682v1","updated":"2024-09-23T03:12:33Z","published":"2024-09-23T03:12:33Z","title":"Robust Training Objectives Improve Embedding-based Retrieval in\n  Industrial Recommendation Systems","summary":"  Improving recommendation systems (RS) can greatly enhance the user experience\nacross many domains, such as social media. Many RS utilize embedding-based\nretrieval (EBR) approaches to retrieve candidates for recommendation. In an EBR\nsystem, the embedding quality is key. According to recent literature,\nself-supervised multitask learning (SSMTL) has showed strong performance on\nacademic benchmarks in embedding learning and resulted in an overall\nimprovement in multiple downstream tasks, demonstrating a larger resilience to\nthe adverse conditions between each downstream task and thereby increased\nrobustness and task generalization ability through the training objective.\nHowever, whether or not the success of SSMTL in academia as a robust training\nobjectives translates to large-scale (i.e., over hundreds of million users and\ninteractions in-between) industrial RS still requires verification. Simply\nadopting academic setups in industrial RS might entail two issues. Firstly,\nmany self-supervised objectives require data augmentations (e.g., embedding\nmasking/corruption) over a large portion of users and items, which is\nprohibitively expensive in industrial RS. Furthermore, some self-supervised\nobjectives might not align with the recommendation task, which might lead to\nredundant computational overheads or negative transfer. In light of these two\nchallenges, we evaluate using a robust training objective, specifically SSMTL,\nthrough a large-scale friend recommendation system on a social media platform\nin the tech sector, identifying whether this increase in robustness can work at\nscale in enhancing retrieval in the production setting. Through online A/B\ntesting with SSMTL-based EBR, we observe statistically significant increases in\nkey metrics in the friend recommendations, with up to 5.45% improvements in new\nfriends made and 1.91% improvements in new friends made with cold-start users.\n","authors":["Matthew Kolodner","Mingxuan Ju","Zihao Fan","Tong Zhao","Elham Ghazizadeh","Yan Wu","Neil Shah","Yozen Liu"],"pdf_url":"https://arxiv.org/pdf/2409.14682v1.pdf","comment":"RobustRecSys workshop @ RecSys 2024"}],"Multimedia":[{"id":"http://arxiv.org/abs/2409.15551v1","updated":"2024-09-23T21:07:06Z","published":"2024-09-23T21:07:06Z","title":"Revise, Reason, and Recognize: LLM-Based Emotion Recognition via\n  Emotion-Specific Prompts and ASR Error Correction","summary":"  Annotating and recognizing speech emotion using prompt engineering has\nrecently emerged with the advancement of Large Language Models (LLMs), yet its\nefficacy and reliability remain questionable. In this paper, we conduct a\nsystematic study on this topic, beginning with the proposal of novel prompts\nthat incorporate emotion-specific knowledge from acoustics, linguistics, and\npsychology. Subsequently, we examine the effectiveness of LLM-based prompting\non Automatic Speech Recognition (ASR) transcription, contrasting it with\nground-truth transcription. Furthermore, we propose a Revise-Reason-Recognize\nprompting pipeline for robust LLM-based emotion recognition from spoken\nlanguage with ASR errors. Additionally, experiments on context-aware learning,\nin-context learning, and instruction tuning are performed to examine the\nusefulness of LLM training schemes in this direction. Finally, we investigate\nthe sensitivity of LLMs to minor prompt variations. Experimental results\ndemonstrate the efficacy of the emotion-specific prompts, ASR error correction,\nand LLM training schemes for LLM-based emotion recognition. Our study aims to\nrefine the use of LLMs in emotion recognition and related domains.\n","authors":["Yuanchao Li","Yuan Gong","Chao-Han Huck Yang","Peter Bell","Catherine Lai"],"pdf_url":"https://arxiv.org/pdf/2409.15551v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15545v1","updated":"2024-09-23T20:59:15Z","published":"2024-09-23T20:59:15Z","title":"Rethinking Emotion Bias in Music via Frechet Audio Distance","summary":"  The subjective nature of music emotion introduces inherent bias in both\nrecognition and generation, especially when relying on a single audio encoder,\nemotion classifier, or evaluation metric. In this work, we conduct a study on\nMusic Emotion Recognition (MER) and Emotional Music Generation (EMG), employing\ndiverse audio encoders alongside the Frechet Audio Distance (FAD), a\nreference-free evaluation metric. Our study begins with a benchmark evaluation\nof MER, highlighting the limitations associated with using a single audio\nencoder and the disparities observed across different measurements. We then\npropose assessing MER performance using FAD from multiple encoders to provide a\nmore objective measure of music emotion. Furthermore, we introduce an enhanced\nEMG approach designed to improve both the variation and prominence of generated\nmusic emotion, thus enhancing realism. Additionally, we investigate the realism\ndisparities between the emotions conveyed in real and synthetic music,\ncomparing our EMG model against two baseline models. Experimental results\nunderscore the emotion bias problem in both MER and EMG and demonstrate the\npotential of using FAD and diverse audio encoders to evaluate music emotion\nobjectively.\n","authors":["Yuanchao Li","Azalea Gui","Dimitra Emmanouilidou","Hannes Gamper"],"pdf_url":"https://arxiv.org/pdf/2409.15545v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15157v1","updated":"2024-09-23T16:04:50Z","published":"2024-09-23T16:04:50Z","title":"LoVA: Long-form Video-to-Audio Generation","summary":"  Video-to-audio (V2A) generation is important for video editing and\npost-processing, enabling the creation of semantics-aligned audio for silent\nvideo. However, most existing methods focus on generating short-form audio for\nshort video segment (less than 10 seconds), while giving little attention to\nthe scenario of long-form video inputs. For current UNet-based diffusion V2A\nmodels, an inevitable problem when handling long-form audio generation is the\ninconsistencies within the final concatenated audio. In this paper, we first\nhighlight the importance of long-form V2A problem. Besides, we propose LoVA, a\nnovel model for Long-form Video-to-Audio generation. Based on the Diffusion\nTransformer (DiT) architecture, LoVA proves to be more effective at generating\nlong-form audio compared to existing autoregressive models and UNet-based\ndiffusion models. Extensive objective and subjective experiments demonstrate\nthat LoVA achieves comparable performance on 10-second V2A benchmark and\noutperforms all other baselines on a benchmark with long-form video input.\n","authors":["Xin Cheng","Xihua Wang","Yihan Wu","Yuyue Wang","Ruihua Song"],"pdf_url":"https://arxiv.org/pdf/2409.15157v1.pdf","comment":"Submitted to ICASSP 2025"},{"id":"http://arxiv.org/abs/2409.14925v1","updated":"2024-09-23T11:20:44Z","published":"2024-09-23T11:20:44Z","title":"DanceCamAnimator: Keyframe-Based Controllable 3D Dance Camera Synthesis","summary":"  Synthesizing camera movements from music and dance is highly challenging due\nto the contradicting requirements and complexities of dance cinematography.\nUnlike human movements, which are always continuous, dance camera movements\ninvolve both continuous sequences of variable lengths and sudden drastic\nchanges to simulate the switching of multiple cameras. However, in previous\nworks, every camera frame is equally treated and this causes jittering and\nunavoidable smoothing in post-processing. To solve these problems, we propose\nto integrate animator dance cinematography knowledge by formulating this task\nas a three-stage process: keyframe detection, keyframe synthesis, and tween\nfunction prediction. Following this formulation, we design a novel end-to-end\ndance camera synthesis framework \\textbf{DanceCamAnimator}, which imitates\nhuman animation procedures and shows powerful keyframe-based controllability\nwith variable lengths. Extensive experiments on the DCM dataset demonstrate\nthat our method surpasses previous baselines quantitatively and qualitatively.\nCode will be available at\n\\url{https://github.com/Carmenw1203/DanceCamAnimator-Official}.\n","authors":["Zixuan Wang","Jiayi Li","Xiaoyu Qin","Shikun Sun","Songtao Zhou","Jia Jia","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2409.14925v1.pdf","comment":"Accepted by ACM Multimedia 2024"},{"id":"http://arxiv.org/abs/2409.14829v1","updated":"2024-09-23T08:59:55Z","published":"2024-09-23T08:59:55Z","title":"RoWSFormer: A Robust Watermarking Framework with Swin Transformer for\n  Enhanced Geometric Attack Resilience","summary":"  In recent years, digital watermarking techniques based on deep learning have\nbeen widely studied. To achieve both imperceptibility and robustness of image\nwatermarks, most current methods employ convolutional neural networks to build\nrobust watermarking frameworks. However, despite the success of CNN-based\nwatermarking models, they struggle to achieve robustness against geometric\nattacks due to the limitations of convolutional neural networks in capturing\nglobal and long-range relationships. To address this limitation, we propose a\nrobust watermarking framework based on the Swin Transformer, named RoWSFormer.\nSpecifically, we design the Locally-Channel Enhanced Swin Transformer Block as\nthe core of both the encoder and decoder. This block utilizes the\nself-attention mechanism to capture global and long-range information, thereby\nsignificantly improving adaptation to geometric distortions. Additionally, we\nconstruct the Frequency-Enhanced Transformer Block to extract frequency domain\ninformation, which further strengthens the robustness of the watermarking\nframework. Experimental results demonstrate that our RoWSFormer surpasses\nexisting state-of-the-art watermarking methods. For most non-geometric attacks,\nRoWSFormer improves the PSNR by 3 dB while maintaining the same extraction\naccuracy. In the case of geometric attacks (such as rotation, scaling, and\naffine transformations), RoWSFormer achieves over a 6 dB improvement in PSNR,\nwith extraction accuracy exceeding 97\\%.\n","authors":["Weitong Chen","Yuheng Li"],"pdf_url":"https://arxiv.org/pdf/2409.14829v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14827v1","updated":"2024-09-23T08:59:22Z","published":"2024-09-23T08:59:22Z","title":"AIM 2024 Challenge on Video Saliency Prediction: Methods and Results","summary":"  This paper reviews the Challenge on Video Saliency Prediction at AIM 2024.\nThe goal of the participants was to develop a method for predicting accurate\nsaliency maps for the provided set of video sequences. Saliency maps are widely\nexploited in various applications, including video compression, quality\nassessment, visual perception studies, the advertising industry, etc. For this\ncompetition, a previously unused large-scale audio-visual mouse saliency\n(AViMoS) dataset of 1500 videos with more than 70 observers per video was\ncollected using crowdsourced mouse tracking. The dataset collection methodology\nhas been validated using conventional eye-tracking data and has shown high\nconsistency. Over 30 teams registered in the challenge, and there are 7 teams\nthat submitted the results in the final phase. The final phase solutions were\ntested and ranked by commonly used quality metrics on a private test subset.\nThe results of this evaluation and the descriptions of the solutions are\npresented in this report. All data, including the private test subset, is made\npublicly available on the challenge homepage -\nhttps://challenges.videoprocessing.ai/challenges/video-saliency-prediction.html.\n","authors":["Andrey Moskalenko","Alexey Bryncev","Dmitry Vatolin","Radu Timofte","Gen Zhan","Li Yang","Yunlong Tang","Yiting Liao","Jiongzhi Lin","Baitao Huang","Morteza Moradi","Mohammad Moradi","Francesco Rundo","Concetto Spampinato","Ali Borji","Simone Palazzo","Yuxin Zhu","Yinan Sun","Huiyu Duan","Yuqin Cao","Ziheng Jia","Qiang Hu","Xiongkuo Min","Guangtao Zhai","Hao Fang","Runmin Cong","Xiankai Lu","Xiaofei Zhou","Wei Zhang","Chunyu Zhao","Wentao Mu","Tao Deng","Hamed R. Tavakoli"],"pdf_url":"https://arxiv.org/pdf/2409.14827v1.pdf","comment":"ECCVW 2024"},{"id":"http://arxiv.org/abs/2311.05920v3","updated":"2024-09-23T05:50:41Z","published":"2023-11-10T08:09:42Z","title":"Feeding the Crave: How People with Eating Disorders Get Trapped in the\n  Perpetual Cycle of Digital Food Content","summary":"  Recent studies have examined how digital food content impacts viewers'\ndietary health. A few have found that individuals with eating disorders are\nparticularly sensitive to digital food content, such as eating and cooking\nvideos, which contribute to disordered eating behaviors. However, there is a\nlack of comprehensive studies that investigate how these individuals interact\nwith various digital food content. To fill this gap, we conducted two rounds of\nstudies (N=23 and 22, respectively) with individuals with eating disorders to\nunderstand their motivations and practices of consuming digital food content.\nOur study reveals that participants anticipate positive effects from food media\nto overcome their condition, but in practice, it often exacerbates their\ndisorder. We also discovered that many participants experienced a cycle of\nquitting and returning to digital food content consumption. Based on these\nfindings, we articulate design implications for digital food content and\nmultimedia platforms to support vulnerable individuals.\n","authors":["Ryuhaerang Choi","Subin Park","Sujin Han","Sung-Ju Lee"],"pdf_url":"https://arxiv.org/pdf/2311.05920v3.pdf","comment":"25 pages, 4 figures"},{"id":"http://arxiv.org/abs/2408.16132v2","updated":"2024-09-23T05:08:20Z","published":"2024-08-28T20:48:04Z","title":"SVDD 2024: The Inaugural Singing Voice Deepfake Detection Challenge","summary":"  With the advancements in singing voice generation and the growing presence of\nAI singers on media platforms, the inaugural Singing Voice Deepfake Detection\n(SVDD) Challenge aims to advance research in identifying AI-generated singing\nvoices from authentic singers. This challenge features two tracks: a controlled\nsetting track (CtrSVDD) and an in-the-wild scenario track (WildSVDD). The\nCtrSVDD track utilizes publicly available singing vocal data to generate\ndeepfakes using state-of-the-art singing voice synthesis and conversion\nsystems. Meanwhile, the WildSVDD track expands upon the existing SingFake\ndataset, which includes data sourced from popular user-generated content\nwebsites. For the CtrSVDD track, we received submissions from 47 teams, with 37\nsurpassing our baselines and the top team achieving a 1.65% equal error rate.\nFor the WildSVDD track, we benchmarked the baselines. This paper reviews these\nresults, discusses key findings, and outlines future directions for SVDD\nresearch.\n","authors":["You Zhang","Yongyi Zang","Jiatong Shi","Ryuichi Yamamoto","Tomoki Toda","Zhiyao Duan"],"pdf_url":"https://arxiv.org/pdf/2408.16132v2.pdf","comment":"6 pages, Accepted by 2024 IEEE Spoken Language Technology Workshop\n  (SLT 2024)"},{"id":"http://arxiv.org/abs/2409.14708v1","updated":"2024-09-23T05:01:43Z","published":"2024-09-23T05:01:43Z","title":"A Multimedia Framework for Continuum Robots: Systematic, Computational,\n  and Control Perspectives","summary":"  Continuum robots, which often rely on interdisciplinary and multimedia\ncollaborations, have been increasingly recognized for their potential to\nrevolutionize the field of human-robot interaction (HRI) in varied applications\ndue to their adaptive, responsive, and flexible characteristics. Despite their\npromises, the lack of an integrated framework poses significant challenges for\nboth users and developers, resulting in inefficiency and complexity during\npreliminary developments. Thus, this paper introduces a unified framework for\nbionic robotics that addresses these challenges by integrating system\narchitecture, dynamics computation, and control strategy. The proposed method\nallows for efficient modeling and quick preview of the results in both digital\nand physical environments, which can enhance the quality of robot developments.\n","authors":["Po-Yu Hsieh","June-Hao Hou"],"pdf_url":"https://arxiv.org/pdf/2409.14708v1.pdf","comment":"7 pages, 8 figures"},{"id":"http://arxiv.org/abs/2409.14703v1","updated":"2024-09-23T04:49:08Z","published":"2024-09-23T04:49:08Z","title":"MemeCLIP: Leveraging CLIP Representations for Multimodal Meme\n  Classification","summary":"  The complexity of text-embedded images presents a formidable challenge in\nmachine learning given the need for multimodal understanding of the multiple\naspects of expression conveyed in them. While previous research in multimodal\nanalysis has primarily focused on singular aspects such as hate speech and its\nsubclasses, our study expands the focus to encompass multiple aspects of\nlinguistics: hate, target, stance, and humor detection. We introduce a novel\ndataset PrideMM comprising text-embedded images associated with the LGBTQ+\nPride movement, thereby addressing a serious gap in existing resources. We\nconduct extensive experimentation on PrideMM by using unimodal and multimodal\nbaseline methods to establish benchmarks for each task. Additionally, we\npropose a novel framework MemeCLIP for efficient downstream learning while\npreserving the knowledge of the pre-trained CLIP model. The results of our\nexperiments show that MemeCLIP achieves superior performance compared to\npreviously proposed frameworks on two real-world datasets. We further compare\nthe performance of MemeCLIP and zero-shot GPT-4 on the hate classification\ntask. Finally, we discuss the shortcomings of our model by qualitatively\nanalyzing misclassified samples. Our code and dataset are publicly available\nat: https://github.com/SiddhantBikram/MemeCLIP.\n","authors":["Siddhant Bikram Shah","Shuvam Shiwakoti","Maheep Chaudhary","Haohan Wang"],"pdf_url":"https://arxiv.org/pdf/2409.14703v1.pdf","comment":"Accepted to EMNLP 2024 (Main)"}]},"2024-09-22T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2409.14609v1","updated":"2024-09-22T22:09:35Z","published":"2024-09-22T22:09:35Z","title":"Nirjas: An open source framework for extracting metadata from the source\n  code","summary":"  Metadata and comments are critical elements of any software development\nprocess. In this paper, we explain how metadata and comments in source code can\nplay an essential role in comprehending software. We introduce a Python-based\nopen-source framework, Nirjas, which helps in extracting this metadata in a\nstructured manner. Various syntaxes, types, and widely accepted conventions\nexist for adding comments in source files of different programming languages.\nEdge cases can create noise in extraction, for which we use Regex to accurately\nretrieve metadata. Non-Regex methods can give results but often miss accuracy\nand noise separation. Nirjas also separates different types of comments, source\ncode, and provides details about those comments, such as line number, file\nname, language used, total SLOC, etc. Nirjas is a standalone Python\nframework/library and can be easily installed via source or pip (the Python\npackage installer). Nirjas was initially created as part of a Google Summer of\nCode project and is currently developed and maintained under the FOSSology\norganization.\n","authors":["Ayush Bhardwaj"," Sahil","Kaushlendra Pratap","Gaurav Mishra"],"pdf_url":"https://arxiv.org/pdf/2409.14609v1.pdf","comment":"2022 12th International Conference on Cloud Computing, Data Science &\n  Engineering (Confluence)"},{"id":"http://arxiv.org/abs/2409.14516v1","updated":"2024-09-22T16:20:00Z","published":"2024-09-22T16:20:00Z","title":"Beyond Words: Evaluating Large Language Models in Transportation\n  Planning","summary":"  The resurgence and rapid advancement of Generative Artificial Intelligence\n(GenAI) in 2023 has catalyzed transformative shifts across numerous industry\nsectors, including urban transportation and logistics. This study investigates\nthe evaluation of Large Language Models (LLMs), specifically GPT-4 and\nPhi-3-mini, to enhance transportation planning. The study assesses the\nperformance and spatial comprehension of these models through a\ntransportation-informed evaluation framework that includes general geospatial\nskills, general transportation domain skills, and real-world transportation\nproblem-solving. Utilizing a mixed-methods approach, the research encompasses\nan evaluation of the LLMs' general Geographic Information System (GIS) skills,\ngeneral transportation domain knowledge as well as abilities to support human\ndecision-making in the real-world transportation planning scenarios of\ncongestion pricing. Results indicate that GPT-4 demonstrates superior accuracy\nand reliability across various GIS and transportation-specific tasks compared\nto Phi-3-mini, highlighting its potential as a robust tool for transportation\nplanners. Nonetheless, Phi-3-mini exhibits competence in specific analytical\nscenarios, suggesting its utility in resource-constrained environments. The\nfindings underscore the transformative potential of GenAI technologies in urban\ntransportation planning. Future work could explore the application of newer\nLLMs and the impact of Retrieval-Augmented Generation (RAG) techniques, on a\nbroader set of real-world transportation planning and operations challenges, to\ndeepen the integration of advanced AI models in transportation management\npractices.\n","authors":["Shaowei Ying","Zhenlong Li","Manzhu Yu"],"pdf_url":"https://arxiv.org/pdf/2409.14516v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.02377v2","updated":"2024-09-22T14:50:52Z","published":"2024-06-04T14:55:14Z","title":"XRec: Large Language Models for Explainable Recommendation","summary":"  Recommender systems help users navigate information overload by providing\npersonalized recommendations aligned with their preferences. Collaborative\nFiltering (CF) is a widely adopted approach, but while advanced techniques like\ngraph neural networks (GNNs) and self-supervised learning (SSL) have enhanced\nCF models for better user representations, they often lack the ability to\nprovide explanations for the recommended items. Explainable recommendations aim\nto address this gap by offering transparency and insights into the\nrecommendation decision-making process, enhancing users' understanding. This\nwork leverages the language capabilities of Large Language Models (LLMs) to\npush the boundaries of explainable recommender systems. We introduce a\nmodel-agnostic framework called XRec, which enables LLMs to provide\ncomprehensive explanations for user behaviors in recommender systems. By\nintegrating collaborative signals and designing a lightweight collaborative\nadaptor, the framework empowers LLMs to understand complex patterns in\nuser-item interactions and gain a deeper understanding of user preferences. Our\nextensive experiments demonstrate the effectiveness of XRec, showcasing its\nability to generate comprehensive and meaningful explanations that outperform\nbaseline approaches in explainable recommender systems. We open-source our\nmodel implementation at https://github.com/HKUDS/XRec.\n","authors":["Qiyao Ma","Xubin Ren","Chao Huang"],"pdf_url":"https://arxiv.org/pdf/2406.02377v2.pdf","comment":"Accepted to EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2407.06716v2","updated":"2024-09-22T13:08:11Z","published":"2024-07-09T09:43:42Z","title":"Analyzing the Effectiveness of Listwise Reranking with Positional\n  Invariance on Temporal Generalizability","summary":"  This working note outlines our participation in the retrieval task at CLEF\n2024. We highlight the considerable gap between studying retrieval performance\non static knowledge documents and understanding performance in real-world\nenvironments. Therefore, Addressing these discrepancies and measuring the\ntemporal persistence of IR systems is crucial. By investigating the LongEval\nbenchmark, specifically designed for such dynamic environments, our findings\ndemonstrate the effectiveness of a listwise reranking approach, which\nproficiently handles inaccuracies induced by temporal distribution shifts.\nAmong listwise rerankers, our findings show that ListT5, which effectively\nmitigates the positional bias problem by adopting the Fusion-in-Decoder\narchitecture, is especially effective, and more so, as temporal drift\nincreases, on the test-long subset.\n","authors":["Soyoung Yoon","Jongyoon Kim","Seung-won Hwang"],"pdf_url":"https://arxiv.org/pdf/2407.06716v2.pdf","comment":"Accepted at CLEF 2024 LongEval track"}],"Multimedia":[{"id":"http://arxiv.org/abs/2409.14340v1","updated":"2024-09-22T06:57:33Z","published":"2024-09-22T06:57:33Z","title":"Self-Supervised Audio-Visual Soundscape Stylization","summary":"  Speech sounds convey a great deal of information about the scenes, resulting\nin a variety of effects ranging from reverberation to additional ambient\nsounds. In this paper, we manipulate input speech to sound as though it was\nrecorded within a different scene, given an audio-visual conditional example\nrecorded from that scene. Our model learns through self-supervision, taking\nadvantage of the fact that natural video contains recurring sound events and\ntextures. We extract an audio clip from a video and apply speech enhancement.\nWe then train a latent diffusion model to recover the original speech, using\nanother audio-visual clip taken from elsewhere in the video as a conditional\nhint. Through this process, the model learns to transfer the conditional\nexample's sound properties to the input speech. We show that our model can be\nsuccessfully trained using unlabeled, in-the-wild videos, and that an\nadditional visual signal can improve its sound prediction abilities. Please see\nour project webpage for video results:\nhttps://tinglok.netlify.app/files/avsoundscape/\n","authors":["Tingle Li","Renhao Wang","Po-Yao Huang","Andrew Owens","Gopala Anumanchipalli"],"pdf_url":"https://arxiv.org/pdf/2409.14340v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2409.14319v1","updated":"2024-09-22T05:13:11Z","published":"2024-09-22T05:13:11Z","title":"Scene-Text Grounding for Text-Based Video Question Answering","summary":"  Existing efforts in text-based video question answering (TextVideoQA) are\ncriticized for their opaque decisionmaking and heavy reliance on scene-text\nrecognition. In this paper, we propose to study Grounded TextVideoQA by forcing\nmodels to answer questions and spatio-temporally localize the relevant\nscene-text regions, thus decoupling QA from scenetext recognition and promoting\nresearch towards interpretable QA. The task has three-fold significance. First,\nit encourages scene-text evidence versus other short-cuts for answer\npredictions. Second, it directly accepts scene-text regions as visual answers,\nthus circumventing the problem of ineffective answer evaluation by stringent\nstring matching. Third, it isolates the challenges inherited in VideoQA and\nscene-text recognition. This enables the diagnosis of the root causes for\nfailure predictions, e.g., wrong QA or wrong scene-text recognition? To achieve\nGrounded TextVideoQA, we propose the T2S-QA model that highlights a\ndisentangled temporal-to-spatial contrastive learning strategy for\nweakly-supervised scene-text grounding and grounded TextVideoQA. To facilitate\nevaluation, we construct a new dataset ViTXT-GQA which features 52K scene-text\nbounding boxes within 2.2K temporal segments related to 2K questions and 729\nvideos. With ViTXT-GQA, we perform extensive experiments and demonstrate the\nsevere limitations of existing techniques in Grounded TextVideoQA. While T2S-QA\nachieves superior results, the large performance gap with human leaves ample\nspace for improvement. Our further analysis of oracle scene-text inputs posits\nthat the major challenge is scene-text recognition. To advance the research of\nGrounded TextVideoQA, our dataset and code are at\n\\url{https://github.com/zhousheng97/ViTXT-GQA.git}\n","authors":["Sheng Zhou","Junbin Xiao","Xun Yang","Peipei Song","Dan Guo","Angela Yao","Meng Wang","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2409.14319v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.05100v2","updated":"2024-09-22T03:53:13Z","published":"2023-11-09T02:24:51Z","title":"Self-similarity Prior Distillation for Unsupervised Remote Physiological\n  Measurement","summary":"  Remote photoplethysmography (rPPG) is a noninvasive technique that aims to\ncapture subtle variations in facial pixels caused by changes in blood volume\nresulting from cardiac activities. Most existing unsupervised methods for rPPG\ntasks focus on the contrastive learning between samples while neglecting the\ninherent self-similar prior in physiological signals. In this paper, we propose\na Self-Similarity Prior Distillation (SSPD) framework for unsupervised rPPG\nestimation, which capitalizes on the intrinsic self-similarity of cardiac\nactivities. Specifically, we first introduce a physical-prior embedded\naugmentation technique to mitigate the effect of various types of noise. Then,\nwe tailor a self-similarity-aware network to extract more reliable self-similar\nphysiological features. Finally, we develop a hierarchical self-distillation\nparadigm to assist the network in disentangling self-similar physiological\npatterns from facial videos. Comprehensive experiments demonstrate that the\nunsupervised SSPD framework achieves comparable or even superior performance\ncompared to the state-of-the-art supervised methods. Meanwhile, SSPD maintains\nthe lowest inference time and computation cost among end-to-end models.\n","authors":["Xinyu Zhang","Weiyu Sun","Hao Lu","Ying Chen","Yun Ge","Xiaolin Huang","Jie Yuan","Yingcong Chen"],"pdf_url":"https://arxiv.org/pdf/2311.05100v2.pdf","comment":null}]},"2024-09-21T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2409.14217v1","updated":"2024-09-21T18:39:53Z","published":"2024-09-21T18:39:53Z","title":"Revisiting BPR: A Replicability Study of a Common Recommender System\n  Baseline","summary":"  Bayesian Personalized Ranking (BPR), a collaborative filtering approach based\non matrix factorization, frequently serves as a benchmark for recommender\nsystems research. However, numerous studies often overlook the nuances of BPR\nimplementation, claiming that it performs worse than newly proposed methods\nacross various tasks. In this paper, we thoroughly examine the features of the\nBPR model, indicating their impact on its performance, and investigate\nopen-source BPR implementations. Our analysis reveals inconsistencies between\nthese implementations and the original BPR paper, leading to a significant\ndecrease in performance of up to 50% for specific implementations. Furthermore,\nthrough extensive experiments on real-world datasets under modern evaluation\nsettings, we demonstrate that with proper tuning of its hyperparameters, the\nBPR model can achieve performance levels close to state-of-the-art methods on\nthe top-n recommendation tasks and even outperform them on specific datasets.\nSpecifically, on the Million Song Dataset, the BPR model with hyperparameters\ntuning statistically significantly outperforms Mult-VAE by 10% in NDCG@100 with\nbinary relevance function.\n","authors":["Aleksandr Milogradskii","Oleg Lashinin","Alexander P","Marina Ananyeva","Sergey Kolesnikov"],"pdf_url":"https://arxiv.org/pdf/2409.14217v1.pdf","comment":"This paper is accepted at the Reproducibility track of the ACM RecSys\n  '24 conference"},{"id":"http://arxiv.org/abs/2409.14192v1","updated":"2024-09-21T16:46:15Z","published":"2024-09-21T16:46:15Z","title":"Knowledge in Triples for LLMs: Enhancing Table QA Accuracy with Semantic\n  Extraction","summary":"  Integrating structured knowledge from tabular formats poses significant\nchallenges within natural language processing (NLP), mainly when dealing with\ncomplex, semi-structured tables like those found in the FeTaQA dataset. These\ntables require advanced methods to interpret and generate meaningful responses\naccurately. Traditional approaches, such as SQL and SPARQL, often fail to fully\ncapture the semantics of such data, especially in the presence of irregular\ntable structures like web tables. This paper addresses these challenges by\nproposing a novel approach that extracts triples straightforward from tabular\ndata and integrates it with a retrieval-augmented generation (RAG) model to\nenhance the accuracy, coherence, and contextual richness of responses generated\nby a fine-tuned GPT-3.5-turbo-0125 model. Our approach significantly\noutperforms existing baselines on the FeTaQA dataset, particularly excelling in\nSacre-BLEU and ROUGE metrics. It effectively generates contextually accurate\nand detailed long-form answers from tables, showcasing its strength in complex\ndata interpretation.\n","authors":["Hossein Sholehrasa","Sanaz Saki Norouzi","Pascal Hitzler","Majid Jaberi-Douraki"],"pdf_url":"https://arxiv.org/pdf/2409.14192v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14078v1","updated":"2024-09-21T09:13:50Z","published":"2024-09-21T09:13:50Z","title":"Data Generation via Latent Factor Simulation for Fairness-aware\n  Re-ranking","summary":"  Synthetic data is a useful resource for algorithmic research. It allows for\nthe evaluation of systems under a range of conditions that might be difficult\nto achieve in real world settings. In recommender systems, the use of synthetic\ndata is somewhat limited; some work has concentrated on building user-item\ninteraction data at large scale. We believe that fairness-aware recommendation\nresearch can benefit from simulated data as it allows the study of protected\ngroups and their interactions without depending on sensitive data that needs\nprivacy protection. In this paper, we propose a novel type of data for\nfairness-aware recommendation: synthetic recommender system outputs that can be\nused to study re-ranking algorithms.\n","authors":["Elena Stefancova","Cassidy All","Joshua Paup","Martin Homola","Nicholas Mattei","Robin Burke"],"pdf_url":"https://arxiv.org/pdf/2409.14078v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10081v3","updated":"2024-09-21T08:27:16Z","published":"2024-03-15T07:45:37Z","title":"DRAGIN: Dynamic Retrieval Augmented Generation based on the Information\n  Needs of Large Language Models","summary":"  Dynamic retrieval augmented generation (RAG) paradigm actively decides when\nand what to retrieve during the text generation process of Large Language\nModels (LLMs). There are two key elements of this paradigm: identifying the\noptimal moment to activate the retrieval module (deciding when to retrieve) and\ncrafting the appropriate query once retrieval is triggered (determining what to\nretrieve). However, current dynamic RAG methods fall short in both aspects.\nFirstly, the strategies for deciding when to retrieve often rely on static\nrules. Moreover, the strategies for deciding what to retrieve typically limit\nthemselves to the LLM's most recent sentence or the last few tokens, while the\nLLM's real-time information needs may span across the entire context. To\novercome these limitations, we introduce a new framework, DRAGIN, i.e., Dynamic\nRetrieval Augmented Generation based on the real-time Information Needs of\nLLMs. Our framework is specifically designed to make decisions on when and what\nto retrieve based on the LLM's real-time information needs during the text\ngeneration process. We evaluate DRAGIN along with existing methods\ncomprehensively over 4 knowledge-intensive generation datasets. Experimental\nresults show that DRAGIN achieves superior performance on all tasks,\ndemonstrating the effectiveness of our method. We have open-sourced all the\ncode, data, and models in GitHub: https://github.com/oneal2000/DRAGIN/tree/main\n","authors":["Weihang Su","Yichen Tang","Qingyao Ai","Zhijing Wu","Yiqun Liu"],"pdf_url":"https://arxiv.org/pdf/2403.10081v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14038v1","updated":"2024-09-21T06:49:34Z","published":"2024-09-21T06:49:34Z","title":"OAEI-LLM: A Benchmark Dataset for Understanding Large Language Model\n  Hallucinations in Ontology Matching","summary":"  Hallucinations of large language models (LLMs) commonly occur in\ndomain-specific downstream tasks, with no exception in ontology matching (OM).\nThe prevalence of using LLMs for OM raises the need for benchmarks to better\nunderstand LLM hallucinations. The OAEI-LLM dataset is an extended version of\nthe Ontology Alignment Evaluation Initiative (OAEI) datasets that evaluate\nLLM-specific hallucinations in OM tasks. We outline the methodology used in\ndataset construction and schema extension, and provide examples of potential\nuse cases.\n","authors":["Zhangcheng Qiang","Kerry Taylor","Weiqing Wang","Jing Jiang"],"pdf_url":"https://arxiv.org/pdf/2409.14038v1.pdf","comment":"4 pages, 1 figure"},{"id":"http://arxiv.org/abs/2409.14034v1","updated":"2024-09-21T06:32:28Z","published":"2024-09-21T06:32:28Z","title":"Cost-Effective Community-Hierarchy-Based Mutual Voting Approach for\n  Influence Maximization in Complex Networks","summary":"  Various types of promising techniques have come into being for influence\nmaximization whose aim is to identify influential nodes in complex networks. In\nessence, real-world applications usually have high requirements on the balance\nbetween time complexity and accuracy of influential nodes identification. To\naddress the challenges of imperfect node influence measurement and inefficient\nseed nodes selection mechanism in such class of foregoing techniques, this\narticle proposes a novel approach called Cost-Effective\nCommunity-Hierarchy-Based Mutual Voting for influence maximization in complex\nnetworks. First, we develop a method for measuring the importance of different\nnodes in networks based on an original concept of Dual-Scale\nCommunity-Hierarchy Information that synthesizes both hierarchy structural\ninformation and community structural information of nodes. The community\nstructural information contained in the nodes is measured by a new notion of\nHierarchical-Community Entropy. Second, we develop a method named\nCost-Effective Mutual-Influence-based Voting for seed nodes selection.\nHereinto, a low-computational-cost mutual voting mechanism and an updating\nstrategy called Lazy Score Updating Strategy are newly constructed for\noptimizing the selecting of seed nodes. Third, we develop a balance index to\nevaluate the performance of different methods in striking the tradeoff between\ntime complexity and the accuracy of influential nodes identification. Finally,\nwe demonstrate the approach performance over ten public datasets. The extensive\nexperiments show that the proposed approach outperforms 16 state-of-the-art\ntechniques on the balance between time complexity and accuracy of influential\nnodes identification. Compared with the method with the second highest value of\nthe balance index, our approach can be improved by at most 9.29%.\n","authors":["Yi Liu","Xiaoan Tang","Witold Pedrycz","Qiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.14034v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2409.14087v1","updated":"2024-09-21T09:53:11Z","published":"2024-09-21T09:53:11Z","title":"BRep Boundary and Junction Detection for CAD Reverse Engineering","summary":"  In machining process, 3D reverse engineering of the mechanical system is an\nintegral, highly important, and yet time consuming step to obtain parametric\nCAD models from 3D scans. Therefore, deep learning-based Scan-to-CAD modeling\ncan offer designers enormous editability to quickly modify CAD model, being\nable to parse all its structural compositions and design steps. In this paper,\nwe propose a supervised boundary representation (BRep) detection network\nBRepDetNet from 3D scans of CC3D and ABC dataset. We have carefully annotated\nthe 50K and 45K scans of both the datasets with appropriate topological\nrelations (e.g., next, mate, previous) between the geometrical primitives\n(i.e., boundaries, junctions, loops, faces) of their BRep data structures. The\nproposed solution decomposes the Scan-to-CAD problem in Scan-to-BRep ensuring\nthe right step towards feature-based modeling, and therefore, leveraging other\nexisting BRep-to-CAD modeling methods. Our proposed Scan-to-BRep neural network\nlearns to detect BRep boundaries and junctions by minimizing focal-loss and\nnon-maximal suppression (NMS) during training time. Experimental results show\nthat our BRepDetNet with NMS-Loss achieves impressive results.\n","authors":["Sk Aziz Ali","Mohammad Sadil Khan","Didier Stricker"],"pdf_url":"https://arxiv.org/pdf/2409.14087v1.pdf","comment":"6 pages, 5 figures"},{"id":"http://arxiv.org/abs/2409.13982v1","updated":"2024-09-21T02:17:35Z","published":"2024-09-21T02:17:35Z","title":"CUS3D :CLIP-based Unsupervised 3D Segmentation via Object-level Denoise","summary":"  To ease the difficulty of acquiring annotation labels in 3D data, a common\nmethod is using unsupervised and open-vocabulary semantic segmentation, which\nleverage 2D CLIP semantic knowledge. In this paper, unlike previous research\nthat ignores the ``noise'' raised during feature projection from 2D to 3D, we\npropose a novel distillation learning framework named CUS3D. In our approach,\nan object-level denosing projection module is designed to screen out the\n``noise'' and ensure more accurate 3D feature. Based on the obtained features,\na multimodal distillation learning module is designed to align the 3D feature\nwith CLIP semantic feature space with object-centered constrains to achieve\nadvanced unsupervised semantic segmentation. We conduct comprehensive\nexperiments in both unsupervised and open-vocabulary segmentation, and the\nresults consistently showcase the superiority of our model in achieving\nadvanced unsupervised segmentation results and its effectiveness in\nopen-vocabulary segmentation.\n","authors":["Fuyang Yu","Runze Tian","Zhen Wang","Xiaochuan Wang","Xiaohui Liang"],"pdf_url":"https://arxiv.org/pdf/2409.13982v1.pdf","comment":"6 pages,3 figures"}]},"2024-09-20T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2409.13888v1","updated":"2024-09-20T20:39:23Z","published":"2024-09-20T20:39:23Z","title":"Causal Feature Selection Method for Contextual Multi-Armed Bandits in\n  Recommender System","summary":"  Features (a.k.a. context) are critical for contextual multi-armed bandits\n(MAB) performance. In practice of large scale online system, it is important to\nselect and implement important features for the model: missing important\nfeatures can led to sub-optimal reward outcome, and including irrelevant\nfeatures can cause overfitting, poor model interpretability, and implementation\ncost. However, feature selection methods for conventional machine learning\nmodels fail short for contextual MAB use cases, as conventional methods select\nfeatures correlated with the outcome variable, but not necessarily causing\nheterogeneuous treatment effect among arms which are truely important for\ncontextual MAB. In this paper, we introduce model-free feature selection\nmethods designed for contexutal MAB problem, based on heterogeneous causal\neffect contributed by the feature to the reward distribution. Empirical\nevaluation is conducted based on synthetic data as well as real data from an\nonline experiment for optimizing content cover image in a recommender system.\nThe results show this feature selection method effectively selects the\nimportant features that lead to higher contextual MAB reward than unimportant\nfeatures. Compared with model embedded method, this model-free method has\nadvantage of fast computation speed, ease of implementation, and prune of model\nmis-specification issues.\n","authors":["Zhenyu Zhao","Yexi Jiang"],"pdf_url":"https://arxiv.org/pdf/2409.13888v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.13847v1","updated":"2024-09-20T18:42:04Z","published":"2024-09-20T18:42:04Z","title":"Segment Discovery: Enhancing E-commerce Targeting","summary":"  Modern e-commerce services frequently target customers with incentives or\ninterventions to engage them in their products such as games, shopping, video\nstreaming, etc. This customer engagement increases acquisition of more\ncustomers and retention of existing ones, leading to more business for the\ncompany while improving customer experience. Often, customers are either\nrandomly targeted or targeted based on the propensity of desirable behavior.\nHowever, such policies can be suboptimal as they do not target the set of\ncustomers who would benefit the most from the intervention and they may also\nnot take account of any constraints. In this paper, we propose a policy\nframework based on uplift modeling and constrained optimization that identifies\ncustomers to target for a use-case specific intervention so as to maximize the\nvalue to the business, while taking account of any given constraints. We\ndemonstrate improvement over state-of-the-art targeting approaches using two\nlarge-scale experimental studies and a production implementation.\n","authors":["Qiqi Li","Roopali Singh","Charin Polpanumas","Tanner Fiez","Namita Kumar","Shreya Chakrabarti"],"pdf_url":"https://arxiv.org/pdf/2409.13847v1.pdf","comment":"Accepted at the CONSEQUENCES'24 workshop, co-located with ACM\n  RecSys'24"},{"id":"http://arxiv.org/abs/2305.16326v3","updated":"2024-09-20T18:17:38Z","published":"2023-05-10T13:40:06Z","title":"Large language models in biomedical natural language processing:\n  benchmarks, baselines, and recommendations","summary":"  Biomedical literature is growing rapidly, making it challenging to curate and\nextract knowledge manually. Biomedical natural language processing (BioNLP)\ntechniques that can automatically extract information from biomedical\nliterature help alleviate this burden. Recently, large Language Models (LLMs),\nsuch as GPT-3 and GPT-4, have gained significant attention for their impressive\nperformance. However, their effectiveness in BioNLP tasks and impact on method\ndevelopment and downstream users remain understudied. This pilot study (1)\nestablishes the baseline performance of GPT-3 and GPT-4 at both zero-shot and\none-shot settings in eight BioNLP datasets across four applications: named\nentity recognition, relation extraction, multi-label document classification,\nand semantic similarity and reasoning, (2) examines the errors produced by the\nLLMs and categorized the errors into three types: missingness, inconsistencies,\nand unwanted artificial content, and (3) provides suggestions for using LLMs in\nBioNLP applications. We make the datasets, baselines, and results publicly\navailable to the community via\nhttps://github.com/qingyu-qc/gpt_bionlp_benchmark.\n","authors":["Qingyu Chen","Jingcheng Du","Yan Hu","Vipina Kuttichi Keloth","Xueqing Peng","Kalpana Raja","Rui Zhang","Zhiyong Lu","Hua Xu"],"pdf_url":"https://arxiv.org/pdf/2305.16326v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.16089v3","updated":"2024-09-20T17:53:41Z","published":"2023-07-29T22:40:59Z","title":"Train Once, Use Flexibly: A Modular Framework for Multi-Aspect Neural\n  News Recommendation","summary":"  Recent neural news recommenders (NNRs) extend content-based recommendation\n(1) by aligning additional aspects (e.g., topic, sentiment) between candidate\nnews and user history or (2) by diversifying recommendations w.r.t. these\naspects. This customization is achieved by ``hardcoding`` additional\nconstraints into the NNR's architecture and/or training objectives: any change\nin the desired recommendation behavior thus requires retraining the model with\na modified objective. This impedes widespread adoption of multi-aspect news\nrecommenders. In this work, we introduce MANNeR, a modular framework for\nmulti-aspect neural news recommendation that supports on-the-fly customization\nover individual aspects at inference time. With metric-based learning as its\nbackbone, MANNeR learns aspect-specialized news encoders and then flexibly and\nlinearly combines the resulting aspect-specific similarity scores into\ndifferent ranking functions, alleviating the need for ranking function-specific\nretraining of the model. Extensive experimental results show that MANNeR\nconsistently outperforms state-of-the-art NNRs on both standard content-based\nrecommendation and single- and multi-aspect customization. Lastly, we validate\nthat MANNeR's aspect-customization module is robust to language and domain\ntransfer.\n","authors":["Andreea Iana","Goran Glavaš","Heiko Paulheim"],"pdf_url":"https://arxiv.org/pdf/2307.16089v3.pdf","comment":"Accepted at the 2024 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP 2024)"},{"id":"http://arxiv.org/abs/2409.13628v1","updated":"2024-09-20T16:36:46Z","published":"2024-09-20T16:36:46Z","title":"Beauty Beyond Words: Explainable Beauty Product Recommendations Using\n  Ingredient-Based Product Attributes","summary":"  Accurate attribute extraction is critical for beauty product recommendations\nand building trust with customers. This remains an open problem, as existing\nsolutions are often unreliable and incomplete. We present a system to extract\nbeauty-specific attributes using end-to-end supervised learning based on beauty\nproduct ingredients. A key insight to our system is a novel energy-based\nimplicit model architecture. We show that this implicit model architecture\noffers significant benefits in terms of accuracy, explainability, robustness,\nand flexibility. Furthermore, our implicit model can be easily fine-tuned to\nincorporate additional attributes as they become available, making it more\nuseful in real-world applications. We validate our model on a major e-commerce\nskincare product catalog dataset and demonstrate its effectiveness. Finally, we\nshowcase how ingredient-based attribute extraction contributes to enhancing the\nexplainability of beauty recommendations.\n","authors":["Siliang Liu","Rahul Suresh","Amin Banitalebi-Dehkordi"],"pdf_url":"https://arxiv.org/pdf/2409.13628v1.pdf","comment":"18th ACM Conference on Recommender Systems, Workshop on Strategic and\n  Utility-aware REcommendation"},{"id":"http://arxiv.org/abs/2409.13621v1","updated":"2024-09-20T16:32:54Z","published":"2024-09-20T16:32:54Z","title":"Advancing Event Causality Identification via Heuristic Semantic\n  Dependency Inquiry Network","summary":"  Event Causality Identification (ECI) focuses on extracting causal relations\nbetween events in texts. Existing methods for ECI primarily rely on causal\nfeatures and external knowledge. However, these approaches fall short in two\ndimensions: (1) causal features between events in a text often lack explicit\nclues, and (2) external knowledge may introduce bias, while specific problems\nrequire tailored analyses. To address these issues, we propose SemDI - a simple\nand effective Semantic Dependency Inquiry Network for ECI. SemDI captures\nsemantic dependencies within the context using a unified encoder. Then, it\nutilizes a Cloze Analyzer to generate a fill-in token based on comprehensive\ncontext understanding. Finally, this fill-in token is used to inquire about the\ncausal relation between two events. Extensive experiments demonstrate the\neffectiveness of SemDI, surpassing state-of-the-art methods on three widely\nused benchmarks. Code is available at https://github.com/hrlics/SemDI.\n","authors":["Haoran Li","Qiang Gao","Hongmei Wu","Li Huang"],"pdf_url":"https://arxiv.org/pdf/2409.13621v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.13545v1","updated":"2024-09-20T14:39:42Z","published":"2024-09-20T14:39:42Z","title":"Data Augmentation for Sequential Recommendation: A Survey","summary":"  As an essential branch of recommender systems, sequential recommendation (SR)\nhas received much attention due to its well-consistency with real-world\nsituations. However, the widespread data sparsity issue limits the SR model's\nperformance. Therefore, researchers have proposed many data augmentation (DA)\nmethods to mitigate this phenomenon and have achieved impressive progress. In\nthis survey, we provide a comprehensive review of DA methods for SR. We start\nby introducing the research background and motivation. Then, we categorize\nexisting methodologies regarding their augmentation principles, objects, and\npurposes. Next, we present a comparative discussion of their advantages and\ndisadvantages, followed by the exhibition and analysis of representative\nexperimental results. Finally, we outline directions for future research and\nsummarize this survey. We also maintain a repository with a paper list at\n\\url{https://github.com/KingGugu/DA-CL-4Rec}.\n","authors":["Yizhou Dang","Enneng Yang","Yuting Liu","Guibing Guo","Linying Jiang","Jianzhe Zhao","Xingwei Wang"],"pdf_url":"https://arxiv.org/pdf/2409.13545v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.13483v1","updated":"2024-09-20T13:15:53Z","published":"2024-09-20T13:15:53Z","title":"A Multimodal Dense Retrieval Approach for Speech-Based Open-Domain\n  Question Answering","summary":"  Speech-based open-domain question answering (QA over a large corpus of text\npassages with spoken questions) has emerged as an important task due to the\nincreasing number of users interacting with QA systems via speech interfaces.\nPassage retrieval is a key task in speech-based open-domain QA. So far,\nprevious works adopted pipelines consisting of an automatic speech recognition\n(ASR) model that transcribes the spoken question before feeding it to a dense\ntext retriever. Such pipelines have several limitations. The need for an ASR\nmodel limits the applicability to low-resource languages and specialized\ndomains with no annotated speech data. Furthermore, the ASR model propagates\nits errors to the retriever. In this work, we try to alleviate these\nlimitations by proposing an ASR-free, end-to-end trained multimodal dense\nretriever that can work directly on spoken questions. Our experimental results\nshowed that, on shorter questions, our retriever is a promising alternative to\nthe \\textit{ASR and Retriever} pipeline, achieving better retrieval performance\nin cases where ASR would have mistranscribed important words in the question or\nhave produced a transcription with a high word error rate.\n","authors":["Georgios Sidiropoulos","Evangelos Kanoulas"],"pdf_url":"https://arxiv.org/pdf/2409.13483v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.13425v1","updated":"2024-09-20T11:46:37Z","published":"2024-09-20T11:46:37Z","title":"Procedure Model for Building Knowledge Graphs for Industry Applications","summary":"  Enterprise knowledge graphs combine business data and organizational\nknowledge by means of a semantic network of concepts, properties, individuals\nand relationships. The graph-based integration of previously unconnected\ninformation with domain knowledge provides new insights and enables intelligent\nbusiness applications. However, knowledge graph construction is a large\ninvestment which requires a joint effort of domain and technical experts. This\npaper presents a practical step-by-step procedure model for building an RDF\nknowledge graph that interconnects heterogeneous data and expert knowledge for\nan industry use case. The self-contained process adapts the \"Cross Industry\nStandard Process for Data Mining\" and uses competency questions throughout the\nentire development cycle. The procedure model starts with business and data\nunderstanding, describes tasks for ontology modeling and the graph setup, and\nends with process steps for evaluation and deployment.\n","authors":["Sascha Meckler"],"pdf_url":"https://arxiv.org/pdf/2409.13425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.13385v1","updated":"2024-09-20T10:36:49Z","published":"2024-09-20T10:36:49Z","title":"Contextual Compression in Retrieval-Augmented Generation for Large\n  Language Models: A Survey","summary":"  Large Language Models (LLMs) showcase remarkable abilities, yet they struggle\nwith limitations such as hallucinations, outdated knowledge, opacity, and\ninexplicable reasoning. To address these challenges, Retrieval-Augmented\nGeneration (RAG) has proven to be a viable solution, leveraging external\ndatabases to improve the consistency and coherence of generated content,\nespecially valuable for complex, knowledge-rich tasks, and facilitates\ncontinuous improvement by leveraging domain-specific insights. By combining the\nintrinsic knowledge of LLMs with the vast, dynamic repositories of external\ndatabases, RAG achieves a synergistic effect. However, RAG is not without its\nlimitations, including a limited context window, irrelevant information, and\nthe high processing overhead for extensive contextual data. In this\ncomprehensive work, we explore the evolution of Contextual Compression\nparadigms, providing an in-depth examination of the field. Finally, we outline\nthe current challenges and suggest potential research and development\ndirections, paving the way for future advancements in this area.\n","authors":["Sourav Verma"],"pdf_url":"https://arxiv.org/pdf/2409.13385v1.pdf","comment":"Ongoing Work"},{"id":"http://arxiv.org/abs/2409.13376v1","updated":"2024-09-20T10:24:39Z","published":"2024-09-20T10:24:39Z","title":"More Clustering Quality Metrics for ABCDE","summary":"  ABCDE is a technique for evaluating clusterings of very large populations of\nitems. Given two clusterings, namely a Baseline clustering and an Experiment\nclustering, ABCDE can characterize their differences with impact and quality\nmetrics, and thus help to determine which clustering to prefer. We previously\ndescribed the basic quality metrics of ABCDE, namely the GoodSplitRate,\nBadSplitRate, GoodMergeRate, BadMergeRate and DeltaPrecision, and how to\nestimate them on the basis of human judgements. This paper extends that\ntreatment with more quality metrics. It describes a technique that aims to\ncharacterize the DeltaRecall of the clustering change. It introduces a new\nmetric, called IQ, to characterize the degree to which the clustering diff\ntranslates into an improvement in the quality. Ideally, a large diff would\nimprove the quality by a large amount. Finally, this paper mentions ways to\ncharacterize the absolute Precision and Recall of a single clustering with\nABCDE.\n","authors":["Stephan van Staden"],"pdf_url":"https://arxiv.org/pdf/2409.13376v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11301v2","updated":"2024-09-20T08:48:30Z","published":"2024-09-17T15:57:33Z","title":"TISIS : Trajectory Indexing for SImilarity Search","summary":"  Social media platforms enable users to share diverse types of information,\nincluding geolocation data that captures their movement patterns. Such\ngeolocation data can be leveraged to reconstruct the trajectory of a user's\nvisited Points of Interest (POIs). A key requirement in numerous applications\nis the ability to measure the similarity between such trajectories, as this\nfacilitates the retrieval of trajectories that are similar to a given reference\ntrajectory. This is the main focus of our work. Existing methods predominantly\nrely on applying a similarity function to each candidate trajectory to identify\nthose that are sufficiently similar. However, this approach becomes\ncomputationally expensive when dealing with large-scale datasets. To mitigate\nthis challenge, we propose TISIS, an efficient method that uses trajectory\nindexing to quickly find similar trajectories that share common POIs in the\nsame order. Furthermore, to account for scenarios where POIs in trajectories\nmay not exactly match but are contextually similar, we introduce TISIS*, a\nvariant of TISIS that incorporates POI embeddings. This extension allows for\nmore comprehensive retrieval of similar trajectories by considering semantic\nsimilarities between POIs, beyond mere exact matches. Extensive experimental\nevaluations demonstrate that the proposed approach significantly outperforms a\nbaseline method based on the well-known Longest Common SubSequence (LCSS)\nalgorithm, yielding substantial performance improvements across various\nreal-world datasets.\n","authors":["Sara Jarrad","Hubert Naacke","Stephane Gancarski"],"pdf_url":"https://arxiv.org/pdf/2409.11301v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.09603v5","updated":"2024-09-20T08:06:34Z","published":"2023-04-19T12:17:46Z","title":"Visualising Personal Data Flows: Insights from a Case Study of\n  Booking.com","summary":"  Commercial organisations are holding and processing an ever-increasing amount\nof personal data. Policies and laws are continually changing to require these\ncompanies to be more transparent regarding the collection, storage, processing\nand sharing of this data. This paper reports our work of taking Booking.com as\na case study to visualise personal data flows extracted from their privacy\npolicy. By showcasing how the company shares its consumers' personal data, we\nraise questions and extend discussions on the challenges and limitations of\nusing privacy policies to inform online users about the true scale and the\nlandscape of personal data flows. This case study can inform us about future\nresearch on more data flow-oriented privacy policy analysis and on the\nconstruction of a more comprehensive ontology on personal data flows in\ncomplicated business ecosystems.\n","authors":["Haiyue Yuan","Matthew Boakes","Xiao Ma","Dongmei Cao","Shujun Li"],"pdf_url":"https://arxiv.org/pdf/2304.09603v5.pdf","comment":"This is the full edition of a paper published in Intelligent\n  Information Systems: CAiSE Forum 2023, Zaragoza, Spain, June 12-16, 2023,\n  Proceedings, Lecture Notes in Business Information Processing (LNBIP), Volume\n  477, pp. 52-60, 2023, Springer Nature,\n  https://link.springer.com/book/10.1007/978-3-031-34674-3_7"},{"id":"http://arxiv.org/abs/2409.12912v2","updated":"2024-09-20T07:36:54Z","published":"2024-09-19T17:07:31Z","title":"The Relevance of Item-Co-Exposure For Exposure Bias Mitigation","summary":"  Through exposing items to users, implicit feedback recommender systems\ninfluence the logged interactions, and, ultimately, their own recommendations.\nThis effect is called exposure bias and it can lead to issues such as filter\nbubbles and echo chambers. Previous research employed the multinomial logit\nmodel (MNL) with exposure information to reduce exposure bias on synthetic\ndata.\n  This extended abstract summarizes our previous study in which we investigated\nwhether (i) these findings hold for human-generated choices, (ii) other\ndiscrete choice models mitigate bias better, and (iii) an item's estimated\nrelevance can depend on the relevances of the other items that were presented\nwith it. We collected a data set of biased and unbiased choices in a controlled\nonline user study and measured the effects of overexposure and competition.\n  We found that (i) the discrete choice models effectively mitigated exposure\nbias on human-generated choice data, (ii) there were no significant differences\nin robustness among the different discrete choice models, and (iii) only\nmultivariate discrete choice models were robust to competition between items.\nWe conclude that discrete choice models mitigate exposure bias effectively\nbecause they consider item-co-exposure. Moreover, exposing items alongside more\nor less popular items can bias future recommendations significantly and item\nexposure must be tracked for overcoming exposure bias. We consider our work\nvital for understanding what exposure bias it, how it forms, and how it can be\nmitigated.\n","authors":["Thorsten Krause","Alina Deriyeva","Jan Heinrich Beinke","Gerrit York Bartels","Oliver Thomas"],"pdf_url":"https://arxiv.org/pdf/2409.12912v2.pdf","comment":"Accepted at the CONSEQUENCES '24 workshop, co-located with ACM RecSys\n  '24"},{"id":"http://arxiv.org/abs/2409.08479v2","updated":"2024-09-20T04:52:16Z","published":"2024-09-13T02:08:47Z","title":"Exploring Information Retrieval Landscapes: An Investigation of a Novel\n  Evaluation Techniques and Comparative Document Splitting Methods","summary":"  The performance of Retrieval-Augmented Generation (RAG) systems in\ninformation retrieval is significantly influenced by the characteristics of the\ndocuments being processed. In this study, the structured nature of textbooks,\nthe conciseness of articles, and the narrative complexity of novels are shown\nto require distinct retrieval strategies. A comparative evaluation of multiple\ndocument-splitting methods reveals that the Recursive Character Splitter\noutperforms the Token-based Splitter in preserving contextual integrity. A\nnovel evaluation technique is introduced, utilizing an open-source model to\ngenerate a comprehensive dataset of question-and-answer pairs, simulating\nrealistic retrieval scenarios to enhance testing efficiency and metric\nreliability. The evaluation employs weighted scoring metrics, including\nSequenceMatcher, BLEU, METEOR, and BERT Score, to assess the system's accuracy\nand relevance. This approach establishes a refined standard for evaluating the\nprecision of RAG systems, with future research focusing on optimizing chunk and\noverlap sizes to improve retrieval accuracy and efficiency.\n","authors":["Esmaeil Narimissa","David Raithel"],"pdf_url":"https://arxiv.org/pdf/2409.08479v2.pdf","comment":"This article is 16 pages long and includes detailed comparisons of\n  RAG systems and document splitting techniques"},{"id":"http://arxiv.org/abs/2409.13210v1","updated":"2024-09-20T04:37:36Z","published":"2024-09-20T04:37:36Z","title":"A Unified Causal Framework for Auditing Recommender Systems for Ethical\n  Concerns","summary":"  As recommender systems become widely deployed in different domains, they\nincreasingly influence their users' beliefs and preferences. Auditing\nrecommender systems is crucial as it not only ensures the continuous\nimprovement of recommendation algorithms but also safeguards against potential\nissues like biases and ethical concerns. In this paper, we view recommender\nsystem auditing from a causal lens and provide a general recipe for defining\nauditing metrics. Under this general causal auditing framework, we categorize\nexisting auditing metrics and identify gaps in them -- notably, the lack of\nmetrics for auditing user agency while accounting for the multi-step dynamics\nof the recommendation process. We leverage our framework and propose two\nclasses of such metrics:future- and past-reacheability and stability, that\nmeasure the ability of a user to influence their own and other users'\nrecommendations, respectively. We provide both a gradient-based and a black-box\napproach for computing these metrics, allowing the auditor to compute them\nunder different levels of access to the recommender system. In our experiments,\nwe demonstrate the efficacy of methods for computing the proposed metrics and\ninspect the design of recommender systems through these proposed metrics.\n","authors":["Vibhhu Sharma","Shantanu Gupta","Nil-Jana Akpinar","Zachary C. Lipton","Liu Leqi"],"pdf_url":"https://arxiv.org/pdf/2409.13210v1.pdf","comment":"28 pages"},{"id":"http://arxiv.org/abs/2409.13175v1","updated":"2024-09-20T03:02:42Z","published":"2024-09-20T03:02:42Z","title":"RPAF: A Reinforcement Prediction-Allocation Framework for Cache\n  Allocation in Large-Scale Recommender Systems","summary":"  Modern recommender systems are built upon computation-intensive\ninfrastructure, and it is challenging to perform real-time computation for each\nrequest, especially in peak periods, due to the limited computational\nresources. Recommending by user-wise result caches is widely used when the\nsystem cannot afford a real-time recommendation. However, it is challenging to\nallocate real-time and cached recommendations to maximize the users' overall\nengagement. This paper shows two key challenges to cache allocation, i.e., the\nvalue-strategy dependency and the streaming allocation. Then, we propose a\nreinforcement prediction-allocation framework (RPAF) to address these issues.\nRPAF is a reinforcement-learning-based two-stage framework containing\nprediction and allocation stages. The prediction stage estimates the values of\nthe cache choices considering the value-strategy dependency, and the allocation\nstage determines the cache choices for each individual request while satisfying\nthe global budget constraint. We show that the challenge of training RPAF\nincludes globality and the strictness of budget constraints, and a relaxed\nlocal allocator (RLA) is proposed to address this issue. Moreover, a PoolRank\nalgorithm is used in the allocation stage to deal with the streaming allocation\nproblem. Experiments show that RPAF significantly improves users' engagement\nunder computational budget constraints.\n","authors":["Shuo Su","Xiaoshuang Chen","Yao Wang","Yulin Wu","Ziqiang Zhang","Kaiqiao Zhan","Ben Wang","Kun Gai"],"pdf_url":"https://arxiv.org/pdf/2409.13175v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2409.13689v1","updated":"2024-09-20T17:59:01Z","published":"2024-09-20T17:59:01Z","title":"Temporally Aligned Audio for Video with Autoregression","summary":"  We introduce V-AURA, the first autoregressive model to achieve high temporal\nalignment and relevance in video-to-audio generation. V-AURA uses a\nhigh-framerate visual feature extractor and a cross-modal audio-visual feature\nfusion strategy to capture fine-grained visual motion events and ensure precise\ntemporal alignment. Additionally, we propose VisualSound, a benchmark dataset\nwith high audio-visual relevance. VisualSound is based on VGGSound, a video\ndataset consisting of in-the-wild samples extracted from YouTube. During the\ncuration, we remove samples where auditory events are not aligned with the\nvisual ones. V-AURA outperforms current state-of-the-art models in temporal\nalignment and semantic relevance while maintaining comparable audio quality.\nCode, samples, VisualSound and models are available at\nhttps://v-aura.notion.site\n","authors":["Ilpo Viertola","Vladimir Iashin","Esa Rahtu"],"pdf_url":"https://arxiv.org/pdf/2409.13689v1.pdf","comment":"Submitted to ICASSP 2025. Project page https://v-aura.notion.site"},{"id":"http://arxiv.org/abs/2309.15889v2","updated":"2024-09-20T11:48:51Z","published":"2023-09-27T16:30:59Z","title":"High Perceptual Quality Wireless Image Delivery with Denoising Diffusion\n  Models","summary":"  We consider the image transmission problem over a noisy wireless channel via\ndeep learning-based joint source-channel coding (DeepJSCC) along with a\ndenoising diffusion probabilistic model (DDPM) at the receiver. Specifically,\nwe are interested in the perception-distortion trade-off in the practical\nfinite block length regime, in which separate source and channel coding can be\nhighly suboptimal. We introduce a novel scheme, where the conventional DeepJSCC\nencoder targets transmitting a lower resolution version of the image, which\nlater can be refined thanks to the generative model available at the receiver.\nIn particular, we utilize the range-null space decomposition of the target\nimage; DeepJSCC transmits the range-space of the image, while DDPM\nprogressively refines its null space contents. Through extensive experiments,\nwe demonstrate significant improvements in distortion and perceptual quality of\nreconstructed images compared to standard DeepJSCC and the state-of-the-art\ngenerative learning-based method.\n","authors":["Selim F. Yilmaz","Xueyan Niu","Bo Bai","Wei Han","Lei Deng","Deniz Gunduz"],"pdf_url":"https://arxiv.org/pdf/2309.15889v2.pdf","comment":"6 pages, 5 figures. Published at INFOCOM 2024 Workshops"},{"id":"http://arxiv.org/abs/2309.04084v2","updated":"2024-09-20T09:22:47Z","published":"2023-09-08T02:50:54Z","title":"Towards Efficient SDRTV-to-HDRTV by Learning from Image Formation","summary":"  Modern displays can render video content with high dynamic range (HDR) and\nwide color gamut (WCG). However, most resources are still in standard dynamic\nrange (SDR). Therefore, transforming existing SDR content into the HDRTV\nstandard holds significant value. This paper defines and analyzes the\nSDRTV-to-HDRTV task by modeling the formation of SDRTV/HDRTV content. Our\nfindings reveal that a naive endto-end supervised training approach suffers\nfrom severe gamut transition errors. To address this, we propose a new\nthree-step solution called HDRTVNet++, which includes adaptive global color\nmapping, local enhancement, and highlight refinement. The adaptive global color\nmapping step utilizes global statistics for image-adaptive color adjustments. A\nlocal enhancement network further enhances details, and the two sub-networks\nare combined as a generator to achieve highlight consistency through GANbased\njoint training. Designed for ultra-high-definition TV content, our method is\nboth effective and lightweight for processing 4K resolution images. We also\nconstructed a dataset using HDR videos in the HDR10 standard, named HDRTV1K,\ncontaining 1235 training and 117 testing images, all in 4K resolution.\nAdditionally, we employ five metrics to evaluate SDRTV-to-HDRTV performance.\nOur results demonstrate state-of-the-art performance both quantitatively and\nvisually. The codes and models are available at\nhttps://github.com/xiaom233/HDRTVNet-plus.\n","authors":["Xiangyu Chen","Zheyuan Li","Zhengwen Zhang","Jimmy S. Ren","Yihao Liu","Jingwen He","Yu Qiao","Jiantao Zhou","Chao Dong"],"pdf_url":"https://arxiv.org/pdf/2309.04084v2.pdf","comment":"Extended version of HDRTVNet"},{"id":"http://arxiv.org/abs/2409.13194v1","updated":"2024-09-20T03:55:34Z","published":"2024-09-20T03:55:34Z","title":"ChemDFM-X: Towards Large Multimodal Model for Chemistry","summary":"  Rapid developments of AI tools are expected to offer unprecedented assistance\nto the research of natural science including chemistry. However, neither\nexisting unimodal task-specific specialist models nor emerging general large\nmultimodal models (LMM) can cover the wide range of chemical data modality and\ntask categories. To address the real demands of chemists, a cross-modal\nChemical General Intelligence (CGI) system, which serves as a truly practical\nand useful research assistant utilizing the great potential of LMMs, is in\ngreat need. In this work, we introduce the first Cross-modal Dialogue\nFoundation Model for Chemistry (ChemDFM-X). Diverse multimodal data are\ngenerated from an initial modality by approximate calculations and\ntask-specific model predictions. This strategy creates sufficient chemical\ntraining corpora, while significantly reducing excessive expense, resulting in\nan instruction-tuning dataset containing 7.6M data. After instruction\nfinetuning, ChemDFM-X is evaluated on extensive experiments of different\nchemical tasks with various data modalities. The results demonstrate the\ncapacity of ChemDFM-X for multimodal and inter-modal knowledge comprehension.\nChemDFM-X marks a significant milestone toward aligning all modalities in\nchemistry, a step closer to CGI.\n","authors":["Zihan Zhao","Bo Chen","Jingpiao Li","Lu Chen","Liyang Wen","Pengyu Wang","Zichen Zhu","Danyang Zhang","Ziping Wan","Yansi Li","Zhongyang Dai","Xin Chen","Kai Yu"],"pdf_url":"https://arxiv.org/pdf/2409.13194v1.pdf","comment":"19 pages, 7 figures, 11 tables"}]},"2024-09-19T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2409.15373v1","updated":"2024-09-19T18:20:54Z","published":"2024-09-19T18:20:54Z","title":"Enhancing Performance and Scalability of Large-Scale Recommendation\n  Systems with Jagged Flash Attention","summary":"  The integration of hardware accelerators has significantly advanced the\ncapabilities of modern recommendation systems, enabling the exploration of\ncomplex ranking paradigms previously deemed impractical. However, the GPU-based\ncomputational costs present substantial challenges. In this paper, we\ndemonstrate our development of an efficiency-driven approach to explore these\nparadigms, moving beyond traditional reliance on native PyTorch modules. We\naddress the specific challenges posed by ranking models' dependence on\ncategorical features, which vary in length and complicate GPU utilization. We\nintroduce Jagged Feature Interaction Kernels, a novel method designed to\nextract fine-grained insights from long categorical features through efficient\nhandling of dynamically sized tensors. We further enhance the performance of\nattention mechanisms by integrating Jagged tensors with Flash Attention. Our\nnovel Jagged Flash Attention achieves up to 9x speedup and 22x memory reduction\ncompared to dense attention. Notably, it also outperforms dense flash\nattention, with up to 3x speedup and 53% more memory efficiency. In production\nmodels, we observe 10% QPS improvement and 18% memory savings, enabling us to\nscale our recommendation systems with longer features and more complex\narchitectures.\n","authors":["Rengan Xu","Junjie Yang","Yifan Xu","Hong Li","Xing Liu","Devashish Shankar","Haoci Zhang","Meng Liu","Boyang Li","Yuxi Hu","Mingwei Tang","Zehua Zhang","Tunhou Zhang","Dai Li","Sijia Chen","Gian-Paolo Musumeci","Jiaqi Zhai","Bill Zhu","Hong Yan","Srihari Reddy"],"pdf_url":"https://arxiv.org/pdf/2409.15373v1.pdf","comment":"3 pages, 2 figures"},{"id":"http://arxiv.org/abs/2409.12959v1","updated":"2024-09-19T17:59:45Z","published":"2024-09-19T17:59:45Z","title":"MMSearch: Benchmarking the Potential of Large Models as Multi-modal\n  Search Engines","summary":"  The advent of Large Language Models (LLMs) has paved the way for AI search\nengines, e.g., SearchGPT, showcasing a new paradigm in human-internet\ninteraction. However, most current AI search engines are limited to text-only\nsettings, neglecting the multimodal user queries and the text-image interleaved\nnature of website information. Recently, Large Multimodal Models (LMMs) have\nmade impressive strides. Yet, whether they can function as AI search engines\nremains under-explored, leaving the potential of LMMs in multimodal search an\nopen question. To this end, we first design a delicate pipeline,\nMMSearch-Engine, to empower any LMMs with multimodal search capabilities. On\ntop of this, we introduce MMSearch, a comprehensive evaluation benchmark to\nassess the multimodal search performance of LMMs. The curated dataset contains\n300 manually collected instances spanning 14 subfields, which involves no\noverlap with the current LMMs' training data, ensuring the correct answer can\nonly be obtained within searching. By using MMSearch-Engine, the LMMs are\nevaluated by performing three individual tasks (requery, rerank, and\nsummarization), and one challenging end-to-end task with a complete searching\nprocess. We conduct extensive experiments on closed-source and open-source\nLMMs. Among all tested models, GPT-4o with MMSearch-Engine achieves the best\nresults, which surpasses the commercial product, Perplexity Pro, in the\nend-to-end task, demonstrating the effectiveness of our proposed pipeline. We\nfurther present error analysis to unveil current LMMs still struggle to fully\ngrasp the multimodal search tasks, and conduct ablation study to indicate the\npotential of scaling test-time computation for AI search engine. We hope\nMMSearch may provide unique insights to guide the future development of\nmultimodal AI search engine. Project Page: https://mmsearch.github.io\n","authors":["Dongzhi Jiang","Renrui Zhang","Ziyu Guo","Yanmin Wu","Jiayi Lei","Pengshuo Qiu","Pan Lu","Zehui Chen","Guanglu Song","Peng Gao","Yu Liu","Chunyuan Li","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2409.12959v1.pdf","comment":"Project Page: https://mmsearch.github.io"},{"id":"http://arxiv.org/abs/2409.12740v1","updated":"2024-09-19T13:03:07Z","published":"2024-09-19T13:03:07Z","title":"HLLM: Enhancing Sequential Recommendations via Hierarchical Large\n  Language Models for Item and User Modeling","summary":"  Large Language Models (LLMs) have achieved remarkable success in various\nfields, prompting several studies to explore their potential in recommendation\nsystems. However, these attempts have so far resulted in only modest\nimprovements over traditional recommendation models. Moreover, three critical\nquestions remain under-explored: firstly, the real value of LLMs' pre-trained\nweights, often considered to encapsulate world knowledge; secondly, the\nnecessity of fine-tuning for recommendation tasks; lastly, whether LLMs can\nexhibit the same scalability benefits in recommendation systems as they do in\nother domains. In this paper, we propose a novel Hierarchical Large Language\nModel (HLLM) architecture designed to enhance sequential recommendation\nsystems. Our approach employs a two-tier model: the first Item LLM extracts\nrich content features from the detailed text description of the item, while the\nsecond User LLM utilizes these features to predict users' future interests\nbased on their interaction history. Extensive experiments demonstrate that our\nmethod effectively leverages the pre-trained capabilities of open-source LLMs,\nand further fine-tuning leads to significant performance boosts. Additionally,\nHLLM achieves excellent scalability, with the largest configuration utilizing\n7B parameters for both item feature extraction and user interest modeling.\nMoreover, HLLM offers excellent training and serving efficiency, making it\npractical in real-world applications. Evaluations on two large-scale datasets,\nPixelRec and Amazon Reviews, show that HLLM achieves state-of-the-art results,\noutperforming traditional ID-based models by a wide margin. In online A/B\ntesting, HLLM showcases notable gains, validating its practical impact in\nreal-world recommendation scenarios. Codes are available at\nhttps://github.com/bytedance/HLLM.\n","authors":["Junyi Chen","Lu Chi","Bingyue Peng","Zehuan Yuan"],"pdf_url":"https://arxiv.org/pdf/2409.12740v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.12730v1","updated":"2024-09-19T12:55:34Z","published":"2024-09-19T12:55:34Z","title":"When SparseMoE Meets Noisy Interactions: An Ensemble View on Denoising\n  Recommendation","summary":"  Learning user preferences from implicit feedback is one of the core\nchallenges in recommendation. The difficulty lies in the potential noise within\nimplicit feedback. Therefore, various denoising recommendation methods have\nbeen proposed recently. However, most of them overly rely on the hyperparameter\nconfigurations, inevitably leading to inadequacies in model adaptability and\ngeneralization performance. In this study, we propose a novel Adaptive Ensemble\nLearning (AEL) for denoising recommendation, which employs a sparse gating\nnetwork as a brain, selecting suitable experts to synthesize appropriate\ndenoising capacities for different data samples. To address the ensemble\nlearning shortcoming of model complexity and ensure sub-recommender diversity,\nwe also proposed a novel method that stacks components to create\nsub-recommenders instead of directly constructing them. Extensive experiments\nacross various datasets demonstrate that AEL outperforms others in kinds of\npopular metrics, even in the presence of substantial and dynamic noise. Our\ncode is available at https://github.com/cpu9xx/AEL.\n","authors":["Weipu Chen","Zhuangzhuang He","Fei Liu"],"pdf_url":"https://arxiv.org/pdf/2409.12730v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.12097v2","updated":"2024-09-19T12:10:38Z","published":"2024-09-18T16:15:18Z","title":"Skill matching at scale: freelancer-project alignment for efficient\n  multilingual candidate retrieval","summary":"  Finding the perfect match between a job proposal and a set of freelancers is\nnot an easy task to perform at scale, especially in multiple languages. In this\npaper, we propose a novel neural retriever architecture that tackles this\nproblem in a multilingual setting. Our method encodes project descriptions and\nfreelancer profiles by leveraging pre-trained multilingual language models. The\nlatter are used as backbone for a custom transformer architecture that aims to\nkeep the structure of the profiles and project. This model is trained with a\ncontrastive loss on historical data. Thanks to several experiments, we show\nthat this approach effectively captures skill matching similarity and\nfacilitates efficient matching, outperforming traditional methods.\n","authors":["Warren Jouanneau","Marc Palyart","Emma Jouffroy"],"pdf_url":"https://arxiv.org/pdf/2409.12097v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.12695v1","updated":"2024-09-19T12:09:33Z","published":"2024-09-19T12:09:33Z","title":"Exploring Large Language Models for Product Attribute Value\n  Identification","summary":"  Product attribute value identification (PAVI) involves automatically\nidentifying attributes and their values from product information, enabling\nfeatures like product search, recommendation, and comparison. Existing methods\nprimarily rely on fine-tuning pre-trained language models, such as BART and T5,\nwhich require extensive task-specific training data and struggle to generalize\nto new attributes. This paper explores large language models (LLMs), such as\nLLaMA and Mistral, as data-efficient and robust alternatives for PAVI. We\npropose various strategies: comparing one-step and two-step prompt-based\napproaches in zero-shot settings and utilizing parametric and non-parametric\nknowledge through in-context learning examples. We also introduce a dense\ndemonstration retriever based on a pre-trained T5 model and perform instruction\nfine-tuning to explicitly train LLMs on task-specific instructions. Extensive\nexperiments on two product benchmarks show that our two-step approach\nsignificantly improves performance in zero-shot settings, and instruction\nfine-tuning further boosts performance when using training data, demonstrating\nthe practical benefits of using LLMs for PAVI.\n","authors":["Kassem Sabeh","Mouna Kacimi","Johann Gamper","Robert Litschko","Barbara Plank"],"pdf_url":"https://arxiv.org/pdf/2409.12695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10173v3","updated":"2024-09-19T11:21:24Z","published":"2024-09-16T11:10:29Z","title":"jina-embeddings-v3: Multilingual Embeddings With Task LoRA","summary":"  We introduce jina-embeddings-v3, a novel text embedding model with 570\nmillion parameters, achieves state-of-the-art performance on multilingual data\nand long-context retrieval tasks, supporting context lengths of up to 8192\ntokens. The model includes a set of task-specific Low-Rank Adaptation (LoRA)\nadapters to generate high-quality embeddings for query-document retrieval,\nclustering, classification, and text matching. Evaluation on the MTEB benchmark\nshows that jina-embeddings-v3 outperforms the latest proprietary embeddings\nfrom OpenAI and Cohere on English tasks, while achieving superior performance\ncompared to multilingual-e5-large-instruct across all multilingual tasks. With\na default output dimension of 1024, users can flexibly reduce the embedding\ndimensions to as low as 32 without compromising performance, enabled by\nMatryoshka Representation Learning.\n","authors":["Saba Sturua","Isabelle Mohr","Mohammad Kalim Akram","Michael Günther","Bo Wang","Markus Krimmel","Feng Wang","Georgios Mastrapas","Andreas Koukounas","Nan Wang","Han Xiao"],"pdf_url":"https://arxiv.org/pdf/2409.10173v3.pdf","comment":"20 pages, pp11-13 references, pp14-20 appendix and experiment tables"},{"id":"http://arxiv.org/abs/2409.12651v1","updated":"2024-09-19T11:00:35Z","published":"2024-09-19T11:00:35Z","title":"A Deep Dive into Fairness, Bias, Threats, and Privacy in Recommender\n  Systems: Insights and Future Research","summary":"  Recommender systems are essential for personalizing digital experiences on\ne-commerce sites, streaming services, and social media platforms. While these\nsystems are necessary for modern digital interactions, they face fairness,\nbias, threats, and privacy challenges. Bias in recommender systems can result\nin unfair treatment of specific users and item groups, and fairness concerns\ndemand that recommendations be equitable for all users and items. These systems\nare also vulnerable to various threats that compromise reliability and\nsecurity. Furthermore, privacy issues arise from the extensive use of personal\ndata, making it crucial to have robust protection mechanisms to safeguard user\ninformation. This study explores fairness, bias, threats, and privacy in\nrecommender systems. It examines how algorithmic decisions can unintentionally\nreinforce biases or marginalize specific user and item groups, emphasizing the\nneed for fair recommendation strategies. The study also looks at the range of\nthreats in the form of attacks that can undermine system integrity and\ndiscusses advanced privacy-preserving techniques. By addressing these critical\nareas, the study highlights current limitations and suggests future research\ndirections to improve recommender systems' robustness, fairness, and privacy.\nUltimately, this research aims to help develop more trustworthy and ethical\nrecommender systems that better serve diverse user populations.\n","authors":["Falguni Roy","Xiaofeng Ding","K. -K. R. Choo","Pan Zhou"],"pdf_url":"https://arxiv.org/pdf/2409.12651v1.pdf","comment":"38 pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.07623v2","updated":"2024-09-19T07:43:43Z","published":"2024-03-12T13:06:31Z","title":"Empowering Sequential Recommendation from Collaborative Signals and\n  Semantic Relatedness","summary":"  Sequential recommender systems (SRS) could capture dynamic user preferences\nby modeling historical behaviors ordered in time. Despite effectiveness,\nfocusing only on the \\textit{collaborative signals} from behaviors does not\nfully grasp user interests. It is also significant to model the\n\\textit{semantic relatedness} reflected in content features, e.g., images and\ntext. Towards that end, in this paper, we aim to enhance the SRS tasks by\neffectively unifying collaborative signals and semantic relatedness together.\nNotably, we empirically point out that it is nontrivial to achieve such a goal\ndue to semantic gap issues. Thus, we propose an end-to-end two-stream\narchitecture for sequential recommendation, named TSSR, to learn user\npreferences from ID-based and content-based sequence. Specifically, we first\npresent novel hierarchical contrasting module, including coarse user-grained\nand fine item-grained terms, to align the representations of inter-modality.\nFurthermore, we also design a two-stream architecture to learn the dependence\nof intra-modality sequence and the complex interactions of inter-modality\nsequence, which can yield more expressive capacity in understanding user\ninterests. We conduct extensive experiments on five public datasets. The\nexperimental results show that the TSSR could yield superior performance than\ncompetitive baselines. We also make our experimental codes publicly available\nat https://github.com/Mingyue-Cheng/TSSR.\n","authors":["Mingyue Cheng","Hao Zhang","Qi Liu","Fajie Yuan","Zhi Li","Zhenya Huang","Enhong Chen","Jun Zhou","Longfei Li"],"pdf_url":"https://arxiv.org/pdf/2403.07623v2.pdf","comment":"Accepted By DASFAA 2024"},{"id":"http://arxiv.org/abs/2409.12519v1","updated":"2024-09-19T07:20:10Z","published":"2024-09-19T07:20:10Z","title":"Multi-View Adaptive Contrastive Learning for Information Retrieval Based\n  Fault Localization","summary":"  Most studies focused on information retrieval-based techniques for fault\nlocalization, which built representations for bug reports and source code files\nand matched their semantic vectors through similarity measurement. However,\nsuch approaches often ignore some useful information that might help improve\nlocalization performance, such as 1) the interaction relationship between bug\nreports and source code files; 2) the similarity relationship between bug\nreports; and 3) the co-citation relationship between source code files. In this\npaper, we propose a novel approach named Multi-View Adaptive Contrastive\nLearning for Information Retrieval Fault Localization (MACL-IRFL) to learn the\nabove-mentioned relationships for software fault localization. Specifically, we\nfirst generate data augmentations from report-code interaction view,\nreport-report similarity view and code-code co-citation view separately, and\nadopt graph neural network to aggregate the information of bug reports or\nsource code files from the three views in the embedding process. Moreover, we\nperform contrastive learning across these views. Our design of contrastive\nlearning task will force the bug report representations to encode information\nshared by report-report and report-code views,and the source code file\nrepresentations shared by code-code and report-code views, thereby alleviating\nthe noise from auxiliary information. Finally, to evaluate the performance of\nour approach, we conduct extensive experiments on five open-source Java\nprojects. The results show that our model can improve over the best baseline up\nto 28.93%, 25.57% and 20.35% on Accuracy@1, MAP and MRR, respectively.\n","authors":["Chunying Zhou","Xiaoyuan Xie","Gong Chen","Peng He","Bing Li"],"pdf_url":"https://arxiv.org/pdf/2409.12519v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.12468v1","updated":"2024-09-19T05:14:55Z","published":"2024-09-19T05:14:55Z","title":"Familiarity-aware Evidence Compression for Retrieval Augmented\n  Generation","summary":"  Retrieval Augmented Generation (RAG) improves large language models (LMs) by\nincorporating non-parametric knowledge through evidence retrieval from external\nsources. However, it often struggles to filter out inconsistent and irrelevant\ninformation that can distract the LM from its tasks. While compressing the\nretrieved evidence with a compression model aims to address this issue, the\ncompressed evidence may still be unfamiliar to the target model used for\ndownstream task, potentially failing to utilize the evidence effectively. We\npropose FaviComp (Familiarity-aware Evidence Compression), a novel\ntraining-free evidence compression technique that makes retrieved evidence more\nfamiliar to the target model, while seamlessly integrating parametric knowledge\nfrom the model. Specifically, FaviComp proactively lowers the perplexity of the\ncompressed evidence with regard to the target model by combining token\nprobabilities from both the compression model and the target model to generate\ncontext that is more familiar to the target model. This approach balances the\nintegration of parametric and non-parametric knowledge, which is especially\nhelpful in complex tasks where the retrieved evidence set may not contain all\nthe necessary information. Experimental results demonstrate that FaviComp\nconsistently outperforms existing baselines in multiple open-domain QA\ndatasets, achieving high compression rates and showcasing the effective\nintegration of both parametric and non-parametric knowledge.\n","authors":["Dongwon Jung","Qin Liu","Tenghao Huang","Ben Zhou","Muhao Chen"],"pdf_url":"https://arxiv.org/pdf/2409.12468v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16933v2","updated":"2024-09-19T03:32:16Z","published":"2024-02-26T17:20:16Z","title":"Incremental Concept Formation over Visual Images Without Catastrophic\n  Forgetting","summary":"  Deep neural networks have excelled in machine learning, particularly in\nvision tasks, however, they often suffer from catastrophic forgetting when\nlearning new tasks sequentially. In this work, we introduce Cobweb4V, an\nalternative to traditional neural network approaches. Cobweb4V is a novel\nvisual classification method that builds on Cobweb, a human like learning\nsystem that is inspired by the way humans incrementally learn new concepts over\ntime. In this research, we conduct a comprehensive evaluation, showcasing\nCobweb4Vs proficiency in learning visual concepts, requiring less data to\nachieve effective learning outcomes compared to traditional methods,\nmaintaining stable performance over time, and achieving commendable asymptotic\nbehavior, without catastrophic forgetting effects. These characteristics align\nwith learning strategies in human cognition, positioning Cobweb4V as a\npromising alternative to neural network approaches.\n","authors":["Nicki Barari","Xin Lian","Christopher J. MacLellan"],"pdf_url":"https://arxiv.org/pdf/2402.16933v2.pdf","comment":"Accepted by The Eleventh Annual Conference on Advances in Cognitive\n  Systems"},{"id":"http://arxiv.org/abs/2409.11690v2","updated":"2024-09-19T03:28:21Z","published":"2024-09-18T04:10:44Z","title":"LLM-Powered Text Simulation Attack Against ID-Free Recommender Systems","summary":"  The ID-free recommendation paradigm has been proposed to address the\nlimitation that traditional recommender systems struggle to model cold-start\nusers or items with new IDs. Despite its effectiveness, this study uncovers\nthat ID-free recommender systems are vulnerable to the proposed Text Simulation\nattack (TextSimu) which aims to promote specific target items. As a novel type\nof text poisoning attack, TextSimu exploits large language models (LLM) to\nalter the textual information of target items by simulating the characteristics\nof popular items. It operates effectively in both black-box and white-box\nsettings, utilizing two key components: a unified popularity extraction module,\nwhich captures the essential characteristics of popular items, and an N-persona\nconsistency simulation strategy, which creates multiple personas to\ncollaboratively synthesize refined promotional textual descriptions for target\nitems by simulating the popular items. To withstand TextSimu-like attacks, we\nfurther explore the detection approach for identifying LLM-generated\npromotional text. Extensive experiments conducted on three datasets demonstrate\nthat TextSimu poses a more significant threat than existing poisoning attacks,\nwhile our defense method can detect malicious text of target items generated by\nTextSimu. By identifying the vulnerability, we aim to advance the development\nof more robust ID-free recommender systems.\n","authors":["Zongwei Wang","Min Gao","Junliang Yu","Xinyi Gao","Quoc Viet Hung Nguyen","Shazia Sadiq","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2409.11690v2.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2409.12380v1","updated":"2024-09-19T00:46:31Z","published":"2024-09-19T00:46:31Z","title":"Bundle Fragments into a Whole: Mining More Complete Clusters via\n  Submodular Selection of Interesting webpages for Web Topic Detection","summary":"  Organizing interesting webpages into hot topics is one of key steps to\nunderstand the trends of multimodal web data. A state-of-the-art solution is\nfirstly to organize webpages into a large volume of multi-granularity topic\ncandidates; hot topics are further identified by estimating their\ninterestingness. However, these topic candidates contain a large number of\nfragments of hot topics due to both the inefficient feature representations and\nthe unsupervised topic generation. This paper proposes a bundling-refining\napproach to mine more complete hot topics from fragments. Concretely, the\nbundling step organizes the fragment topics into coarse topics; next, the\nrefining step proposes a submodular-based method to refine coarse topics in a\nscalable approach. The propose unconventional method is simple, yet powerful by\nleveraging submodular optimization, our approach outperforms the traditional\nranking methods which involve the careful design and complex steps. Extensive\nexperiments demonstrate that the proposed approach surpasses the\nstate-of-the-art method (i.e., latent Poisson deconvolution Pang et al. (2016))\n20% accuracy and 10% one on two public data sets, respectively.\n","authors":["Junbiao Pang","Anjing Hu","Qingming Huang"],"pdf_url":"https://arxiv.org/pdf/2409.12380v1.pdf","comment":"10"}],"Multimedia":[{"id":"http://arxiv.org/abs/2404.16193v2","updated":"2024-09-19T21:19:05Z","published":"2024-04-24T20:33:25Z","title":"Improving Multi-label Recognition using Class Co-Occurrence\n  Probabilities","summary":"  Multi-label Recognition (MLR) involves the identification of multiple objects\nwithin an image. To address the additional complexity of this problem, recent\nworks have leveraged information from vision-language models (VLMs) trained on\nlarge text-images datasets for the task. These methods learn an independent\nclassifier for each object (class), overlooking correlations in their\noccurrences. Such co-occurrences can be captured from the training data as\nconditional probabilities between a pair of classes. We propose a framework to\nextend the independent classifiers by incorporating the co-occurrence\ninformation for object pairs to improve the performance of independent\nclassifiers. We use a Graph Convolutional Network (GCN) to enforce the\nconditional probabilities between classes, by refining the initial estimates\nderived from image and text sources obtained using VLMs. We validate our method\non four MLR datasets, where our approach outperforms all state-of-the-art\nmethods.\n","authors":["Samyak Rawlekar","Shubhang Bhatnagar","Vishnuvardhan Pogunulu Srinivasulu","Narendra Ahuja"],"pdf_url":"https://arxiv.org/pdf/2404.16193v2.pdf","comment":"Accepted to ICPR 2024, CVPR workshops 2024"},{"id":"http://arxiv.org/abs/2409.13049v1","updated":"2024-09-19T18:55:13Z","published":"2024-09-19T18:55:13Z","title":"DiffSSD: A Diffusion-Based Dataset For Speech Forensics","summary":"  Diffusion-based speech generators are ubiquitous. These methods can generate\nvery high quality synthetic speech and several recent incidents report their\nmalicious use. To counter such misuse, synthetic speech detectors have been\ndeveloped. Many of these detectors are trained on datasets which do not include\ndiffusion-based synthesizers. In this paper, we demonstrate that existing\ndetectors trained on one such dataset, ASVspoof2019, do not perform well in\ndetecting synthetic speech from recent diffusion-based synthesizers. We propose\nthe Diffusion-Based Synthetic Speech Dataset (DiffSSD), a dataset consisting of\nabout 200 hours of labeled speech, including synthetic speech generated by 8\ndiffusion-based open-source and 2 commercial generators. We also examine the\nperformance of existing synthetic speech detectors on DiffSSD in both\nclosed-set and open-set scenarios. The results highlight the importance of this\ndataset in detecting synthetic speech generated from recent open-source and\ncommercial speech generators.\n","authors":["Kratika Bhagtani","Amit Kumar Singh Yadav","Paolo Bestagini","Edward J. Delp"],"pdf_url":"https://arxiv.org/pdf/2409.13049v1.pdf","comment":"Submitted to IEEE International Conference on Acoustics, Speech, and\n  Signal Processing (ICASSP) 2025"},{"id":"http://arxiv.org/abs/2409.13002v1","updated":"2024-09-19T16:21:21Z","published":"2024-09-19T16:21:21Z","title":"Across-Game Engagement Modelling via Few-Shot Learning","summary":"  Domain generalisation involves learning artificial intelligence (AI) models\nthat can maintain high performance across diverse domains within a specific\ntask. In video games, for instance, such AI models can supposedly learn to\ndetect player actions across different games. Despite recent advancements in\nAI, domain generalisation for modelling the users' experience remains largely\nunexplored. While video games present unique challenges and opportunities for\nthe analysis of user experience -- due to their dynamic and rich contextual\nnature -- modelling such experiences is limited by generally small datasets. As\na result, conventional modelling methods often struggle to bridge the domain\ngap between users and games due to their reliance on large labelled training\ndata and assumptions of common distributions of user experience. In this paper,\nwe tackle this challenge by introducing a framework that decomposes the general\ndomain-agnostic modelling of user experience into several domain-specific and\ngame-dependent tasks that can be solved via few-shot learning. We test our\nframework on a variation of the publicly available GameVibe corpus, designed\nspecifically to test a model's ability to predict user engagement across\ndifferent first-person shooter games. Our findings demonstrate the superior\nperformance of few-shot learners over traditional modelling methods and thus\nshowcase the potential of few-shot learning for robust experience modelling in\nvideo games and beyond.\n","authors":["Kosmas Pinitas","Konstantinos Makantasis","Georgios N. Yannakakis"],"pdf_url":"https://arxiv.org/pdf/2409.13002v1.pdf","comment":"17 pages, accepted for publication at ECCV 2024 CV2 Workshop"},{"id":"http://arxiv.org/abs/2402.11520v2","updated":"2024-09-19T10:03:09Z","published":"2024-02-18T09:22:58Z","title":"Cross-Attention Fusion of Visual and Geometric Features for Large\n  Vocabulary Arabic Lipreading","summary":"  Lipreading involves using visual data to recognize spoken words by analyzing\nthe movements of the lips and surrounding area. It is a hot research topic with\nmany potential applications, such as human-machine interaction and enhancing\naudio speech recognition. Recent deep-learning based works aim to integrate\nvisual features extracted from the mouth region with landmark points on the lip\ncontours. However, employing a simple combination method such as concatenation\nmay not be the most effective approach to get the optimal feature vector. To\naddress this challenge, firstly, we propose a cross-attention fusion-based\napproach for large lexicon Arabic vocabulary to predict spoken words in videos.\nOur method leverages the power of cross-attention networks to efficiently\nintegrate visual and geometric features computed on the mouth region. Secondly,\nwe introduce the first large-scale Lipreading in the Wild for Arabic (LRW-AR)\ndataset containing 20,000 videos for 100-word classes, uttered by 36 speakers.\nThe experimental results obtained on LRW-AR and ArabicVisual databases showed\nthe effectiveness and robustness of the proposed approach in recognizing Arabic\nwords. Our work provides insights into the feasibility and effectiveness of\napplying lipreading techniques to the Arabic language, opening doors for\nfurther research in this field. Link to the project page:\nhttps://crns-smartvision.github.io/lrwar\n","authors":["Samar Daou","Achraf Ben-Hamadou","Ahmed Rekik","Abdelaziz Kallel"],"pdf_url":"https://arxiv.org/pdf/2402.11520v2.pdf","comment":"submitted for review"},{"id":"http://arxiv.org/abs/2409.12568v1","updated":"2024-09-19T08:41:21Z","published":"2024-09-19T08:41:21Z","title":"InfiMM-WebMath-40B: Advancing Multimodal Pre-Training for Enhanced\n  Mathematical Reasoning","summary":"  Pre-training on large-scale, high-quality datasets is crucial for enhancing\nthe reasoning capabilities of Large Language Models (LLMs), especially in\nspecialized domains such as mathematics. Despite the recognized importance, the\nMultimodal LLMs (MLLMs) field currently lacks a comprehensive open-source\npre-training dataset specifically designed for mathematical reasoning. To\naddress this gap, we introduce InfiMM-WebMath-40B, a high-quality dataset of\ninterleaved image-text documents. It comprises 24 million web pages, 85 million\nassociated image URLs, and 40 billion text tokens, all meticulously extracted\nand filtered from CommonCrawl. We provide a detailed overview of our data\ncollection and processing pipeline. To demonstrate the robustness of\nInfiMM-WebMath-40B, we conducted evaluations in both text-only and multimodal\nsettings. Our evaluations on text-only benchmarks show that, despite utilizing\nonly 40 billion tokens, our dataset significantly enhances the performance of\nour 1.3B model, delivering results comparable to DeepSeekMath-1.3B, which uses\n120 billion tokens for the same model size. Nevertheless, with the introduction\nof our multi-modal math pre-training dataset, our models set a new\nstate-of-the-art among open-source models on multi-modal math benchmarks such\nas MathVerse and We-Math. We release our data at\nhttps://huggingface.co/datasets/Infi-MM/InfiMM-WebMath-40B.\n","authors":["Xiaotian Han","Yiren Jian","Xuefeng Hu","Haogeng Liu","Yiqi Wang","Qihang Fan","Yuang Ai","Huaibo Huang","Ran He","Zhenheng Yang","Quanzeng You"],"pdf_url":"https://arxiv.org/pdf/2409.12568v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15521v2","updated":"2024-09-19T06:21:03Z","published":"2024-08-28T04:14:01Z","title":"A Simple Baseline with Single-encoder for Referring Image Segmentation","summary":"  Referring image segmentation (RIS) requires dense vision-language\ninteractions between visual pixels and textual words to segment objects based\non a given description. However, commonly adapted dual-encoders in RIS, e.g.,\nSwin transformer and BERT (uni-modal encoders) or CLIP (a multi-modal\ndual-encoder), lack dense multi-modal interactions during pre-training, leading\nto a gap with a pixel-level RIS task. To bridge this gap, existing RIS methods\noften rely on multi-modal fusion modules that interact two encoders, but this\napproach leads to high computational costs. In this paper, we present a novel\nRIS method with a single-encoder, i.e., BEiT-3, maximizing the potential of\nshared self-attention across all framework components. This enables seamless\ninteractions of two modalities from input to final prediction, producing\ngranularly aligned multi-modal features. Furthermore, we propose lightweight\nyet effective decoder modules, a Shared FPN and a Shared Mask Decoder, which\ncontribute to the high efficiency of our model. Our simple baseline with a\nsingle encoder achieves outstanding performances on the RIS benchmark datasets\nwhile maintaining computational efficiency, compared to the most recent SoTA\nmethods based on dual-encoders.\n","authors":["Seonghoon Yu","Ilchae Jung","Byeongju Han","Taeoh Kim","Yunho Kim","Dongyoon Wee","Jeany Son"],"pdf_url":"https://arxiv.org/pdf/2408.15521v2.pdf","comment":"arXiv pre-print"},{"id":"http://arxiv.org/abs/2409.12408v1","updated":"2024-09-19T02:12:26Z","published":"2024-09-19T02:12:26Z","title":"Mutual Information-based Representations Disentanglement for Unaligned\n  Multimodal Language Sequences","summary":"  The key challenge in unaligned multimodal language sequences lies in\neffectively integrating information from various modalities to obtain a refined\nmultimodal joint representation. Recently, the disentangle and fuse methods\nhave achieved the promising performance by explicitly learning\nmodality-agnostic and modality-specific representations and then fusing them\ninto a multimodal joint representation. However, these methods often\nindependently learn modality-agnostic representations for each modality and\nutilize orthogonal constraints to reduce linear correlations between\nmodality-agnostic and modality-specific representations, neglecting to\neliminate their nonlinear correlations. As a result, the obtained multimodal\njoint representation usually suffers from information redundancy, leading to\noverfitting and poor generalization of the models. In this paper, we propose a\nMutual Information-based Representations Disentanglement (MIRD) method for\nunaligned multimodal language sequences, in which a novel disentanglement\nframework is designed to jointly learn a single modality-agnostic\nrepresentation. In addition, the mutual information minimization constraint is\nemployed to ensure superior disentanglement of representations, thereby\neliminating information redundancy within the multimodal joint representation.\nFurthermore, the challenge of estimating mutual information caused by the\nlimited labeled data is mitigated by introducing unlabeled data. Meanwhile, the\nunlabeled data also help to characterize the underlying structure of multimodal\ndata, consequently further preventing overfitting and enhancing the performance\nof the models. Experimental results on several widely used benchmark datasets\nvalidate the effectiveness of our proposed approach.\n","authors":["Fan Qian","Jiqing Han","Jianchen Li","Yongjun He","Tieran Zheng","Guibin Zheng"],"pdf_url":"https://arxiv.org/pdf/2409.12408v1.pdf","comment":"31 pages, 8 figures"}]},"2024-09-18T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2409.15368v1","updated":"2024-09-18T19:36:33Z","published":"2024-09-18T19:36:33Z","title":"MedCodER: A Generative AI Assistant for Medical Coding","summary":"  Medical coding is essential for standardizing clinical data and communication\nbut is often time-consuming and prone to errors. Traditional Natural Language\nProcessing (NLP) methods struggle with automating coding due to the large label\nspace, lengthy text inputs, and the absence of supporting evidence annotations\nthat justify code selection. Recent advancements in Generative Artificial\nIntelligence (AI) offer promising solutions to these challenges. In this work,\nwe introduce MedCodER, a Generative AI framework for automatic medical coding\nthat leverages extraction, retrieval, and re-ranking techniques as core\ncomponents. MedCodER achieves a micro-F1 score of 0.60 on International\nClassification of Diseases (ICD) code prediction, significantly outperforming\nstate-of-the-art methods. Additionally, we present a new dataset containing\nmedical records annotated with disease diagnoses, ICD codes, and supporting\nevidence texts (https://doi.org/10.5281/zenodo.13308316). Ablation tests\nconfirm that MedCodER's performance depends on the integration of each of its\naforementioned components, as performance declines when these components are\nevaluated in isolation.\n","authors":["Krishanu Das Baksi","Elijah Soba","John J. Higgins","Ravi Saini","Jaden Wood","Jane Cook","Jack Scott","Nirmala Pudota","Tim Weninger","Edward Bowen","Sanmitra Bhattacharya"],"pdf_url":"https://arxiv.org/pdf/2409.15368v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15173v1","updated":"2024-09-18T18:29:15Z","published":"2024-09-18T18:29:15Z","title":"Recommendation with Generative Models","summary":"  Generative models are a class of AI models capable of creating new instances\nof data by learning and sampling from their statistical distributions. In\nrecent years, these models have gained prominence in machine learning due to\nthe development of approaches such as generative adversarial networks (GANs),\nvariational autoencoders (VAEs), and transformer-based architectures such as\nGPT. These models have applications across various domains, such as image\ngeneration, text synthesis, and music composition. In recommender systems,\ngenerative models, referred to as Gen-RecSys, improve the accuracy and\ndiversity of recommendations by generating structured outputs, text-based\ninteractions, and multimedia content. By leveraging these capabilities,\nGen-RecSys can produce more personalized, engaging, and dynamic user\nexperiences, expanding the role of AI in eCommerce, media, and beyond.\n  Our book goes beyond existing literature by offering a comprehensive\nunderstanding of generative models and their applications, with a special focus\non deep generative models (DGMs) and their classification. We introduce a\ntaxonomy that categorizes DGMs into three types: ID-driven models, large\nlanguage models (LLMs), and multimodal models. Each category addresses unique\ntechnical and architectural advancements within its respective research area.\nThis taxonomy allows researchers to easily navigate developments in Gen-RecSys\nacross domains such as conversational AI and multimodal content generation.\nAdditionally, we examine the impact and potential risks of generative models,\nemphasizing the importance of robust evaluation frameworks.\n","authors":["Yashar Deldjoo","Zhankui He","Julian McAuley","Anton Korikov","Scott Sanner","Arnau Ramisa","Rene Vidal","Maheswaran Sathiamoorthy","Atoosa Kasrizadeh","Silvia Milano","Francesco Ricci"],"pdf_url":"https://arxiv.org/pdf/2409.15173v1.pdf","comment":"This submission is a full-length book, expanding significantly on two\n  chapters previously submitted (arXiv:2409.10993v1, arXiv:2408.10946v1). It\n  includes additional chapters, context, analysis, and content, providing a\n  comprehensive presentation of the subject. We have ensured it is\n  appropriately presented as a new, distinct work. arXiv admin note:\n  substantial text overlap with arXiv:2409.10993"},{"id":"http://arxiv.org/abs/2409.12161v1","updated":"2024-09-18T17:25:31Z","published":"2024-09-18T17:25:31Z","title":"Generalized compression and compressive search of large datasets","summary":"  The Big Data explosion has necessitated the development of search algorithms\nthat scale sub-linearly in time and memory.\n  While compression algorithms and search algorithms do exist independently,\nfew algorithms offer both, and those which do are domain-specific.\n  We present panCAKES, a novel approach to compressive search, i.e., a way to\nperform $k$-NN and $\\rho$-NN search on compressed data while only decompressing\na small, relevant, portion of the data.\n  panCAKES assumes the manifold hypothesis and leverages the low-dimensional\nstructure of the data to compress and search it efficiently.\n  panCAKES is generic over any distance function for which the distance between\ntwo points is proportional to the memory cost of storing an encoding of one in\nterms of the other.\n  This property holds for many widely-used distance functions, e.g. string edit\ndistances (Levenshtein, Needleman-Wunsch, etc.) and set dissimilarity measures\n(Jaccard, Dice, etc.).\n  We benchmark panCAKES on a variety of datasets, including genomic, proteomic,\nand set data.\n  We compare compression ratios to gzip, and search performance between the\ncompressed and uncompressed versions of the same dataset.\n  panCAKES achieves compression ratios close to those of gzip, while offering\nsub-linear time performance for $k$-NN and $\\rho$-NN search.\n  We conclude that panCAKES is an efficient, general-purpose algorithm for\nexact compressive search on large datasets that obey the manifold hypothesis.\n  We provide an open-source implementation of panCAKES in the Rust programming\nlanguage.\n","authors":["Morgan E. Prior","Thomas Howard III","Emily Light","Najib Ishaq","Noah M. Daniels"],"pdf_url":"https://arxiv.org/pdf/2409.12161v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.12150v1","updated":"2024-09-18T17:15:06Z","published":"2024-09-18T17:15:06Z","title":"Decoding Style: Efficient Fine-Tuning of LLMs for Image-Guided Outfit\n  Recommendation with Preference","summary":"  Personalized outfit recommendation remains a complex challenge, demanding\nboth fashion compatibility understanding and trend awareness. This paper\npresents a novel framework that harnesses the expressive power of large\nlanguage models (LLMs) for this task, mitigating their \"black box\" and static\nnature through fine-tuning and direct feedback integration. We bridge the item\nvisual-textual gap in items descriptions by employing image captioning with a\nMultimodal Large Language Model (MLLM). This enables the LLM to extract style\nand color characteristics from human-curated fashion images, forming the basis\nfor personalized recommendations. The LLM is efficiently fine-tuned on the\nopen-source Polyvore dataset of curated fashion images, optimizing its ability\nto recommend stylish outfits. A direct preference mechanism using negative\nexamples is employed to enhance the LLM's decision-making process. This creates\na self-enhancing AI feedback loop that continuously refines recommendations in\nline with seasonal fashion trends. Our framework is evaluated on the Polyvore\ndataset, demonstrating its effectiveness in two key tasks: fill-in-the-blank,\nand complementary item retrieval. These evaluations underline the framework's\nability to generate stylish, trend-aligned outfit suggestions, continuously\nimproving through direct feedback. The evaluation results demonstrated that our\nproposed framework significantly outperforms the base LLM, creating more\ncohesive outfits. The improved performance in these tasks underscores the\nproposed framework's potential to enhance the shopping experience with accurate\nsuggestions, proving its effectiveness over the vanilla LLM based outfit\ngeneration.\n","authors":["Najmeh Forouzandehmehr","Nima Farrokhsiar","Ramin Giahi","Evren Korpeoglu","Kannan Achan"],"pdf_url":"https://arxiv.org/pdf/2409.12150v1.pdf","comment":"CIKM 2024"},{"id":"http://arxiv.org/abs/2409.15364v1","updated":"2024-09-18T16:10:47Z","published":"2024-09-18T16:10:47Z","title":"VERA: Validation and Enhancement for Retrieval Augmented systems","summary":"  Large language models (LLMs) exhibit remarkable capabilities but often\nproduce inaccurate responses, as they rely solely on their embedded knowledge.\nRetrieval-Augmented Generation (RAG) enhances LLMs by incorporating an external\ninformation retrieval system, supplying additional context along with the query\nto mitigate inaccuracies for a particular context. However, accuracy issues\nstill remain, as the model may rely on irrelevant documents or extrapolate\nincorrectly from its training knowledge. To assess and improve the performance\nof both the retrieval system and the LLM in a RAG framework, we propose\n\\textbf{VERA} (\\textbf{V}alidation and \\textbf{E}nhancement for\n\\textbf{R}etrieval \\textbf{A}ugmented systems), a system designed to: 1)\nEvaluate and enhance the retrieved context before response generation, and 2)\nEvaluate and refine the LLM-generated response to ensure precision and minimize\nerrors. VERA employs an evaluator-cum-enhancer LLM that first checks if\nexternal retrieval is necessary, evaluates the relevance and redundancy of the\nretrieved context, and refines it to eliminate non-essential information.\nPost-response generation, VERA splits the response into atomic statements,\nassesses their relevance to the query, and ensures adherence to the context.\nOur experiments demonstrate VERA's remarkable efficacy not only in improving\nthe performance of smaller open-source models, but also larger state-of-the art\nmodels. These enhancements underscore VERA's potential to produce accurate and\nrelevant responses, advancing the state-of-the-art in retrieval-augmented\nlanguage modeling. VERA's robust methodology, combining multiple evaluation and\nrefinement steps, effectively mitigates hallucinations and improves retrieval\nand response processes, making it a valuable tool for applications demanding\nhigh accuracy and reliability in information generation. .\n","authors":["Nitin Aravind Birur","Tanay Baswa","Divyanshu Kumar","Jatan Loya","Sahil Agarwal","Prashanth Harshangi"],"pdf_url":"https://arxiv.org/pdf/2409.15364v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.12043v1","updated":"2024-09-18T15:04:12Z","published":"2024-09-18T15:04:12Z","title":"Understanding the Effects of the Baidu-ULTR Logging Policy on Two-Tower\n  Models","summary":"  Despite the popularity of the two-tower model for unbiased learning to rank\n(ULTR) tasks, recent work suggests that it suffers from a major limitation that\ncould lead to its collapse in industry applications: the problem of logging\npolicy confounding. Several potential solutions have even been proposed;\nhowever, the evaluation of these methods was mostly conducted using\nsemi-synthetic simulation experiments. This paper bridges the gap between\ntheory and practice by investigating the confounding problem on the largest\nreal-world dataset, Baidu-ULTR. Our main contributions are threefold: 1) we\nshow that the conditions for the confounding problem are given on Baidu-ULTR,\n2) the confounding problem bears no significant effect on the two-tower model,\nand 3) we point to a potential mismatch between expert annotations, the golden\nstandard in ULTR, and user click behavior.\n","authors":["Morris de Haan","Philipp Hager"],"pdf_url":"https://arxiv.org/pdf/2409.12043v1.pdf","comment":"Accepted at the CONSEQUENCES '24 workshop, co-located with ACM RecSys\n  '24"},{"id":"http://arxiv.org/abs/2409.10576v2","updated":"2024-09-18T13:27:43Z","published":"2024-09-15T15:21:45Z","title":"Language Models and Retrieval Augmented Generation for Automated\n  Structured Data Extraction from Diagnostic Reports","summary":"  Purpose: To develop and evaluate an automated system for extracting\nstructured clinical information from unstructured radiology and pathology\nreports using open-weights large language models (LMs) and retrieval augmented\ngeneration (RAG), and to assess the effects of model configuration variables on\nextraction performance. Methods and Materials: The study utilized two datasets:\n7,294 radiology reports annotated for Brain Tumor Reporting and Data System\n(BT-RADS) scores and 2,154 pathology reports annotated for isocitrate\ndehydrogenase (IDH) mutation status. An automated pipeline was developed to\nbenchmark the performance of various LMs and RAG configurations. The impact of\nmodel size, quantization, prompting strategies, output formatting, and\ninference parameters was systematically evaluated. Results: The best performing\nmodels achieved over 98% accuracy in extracting BT-RADS scores from radiology\nreports and over 90% for IDH mutation status extraction from pathology reports.\nThe top model being medical fine-tuned llama3. Larger, newer, and domain\nfine-tuned models consistently outperformed older and smaller models. Model\nquantization had minimal impact on performance. Few-shot prompting\nsignificantly improved accuracy. RAG improved performance for complex pathology\nreports but not for shorter radiology reports. Conclusions: Open LMs\ndemonstrate significant potential for automated extraction of structured\nclinical data from unstructured clinical reports with local privacy-preserving\napplication. Careful model selection, prompt engineering, and semi-automated\noptimization using annotated data are critical for optimal performance. These\napproaches could be reliable enough for practical use in research workflows,\nhighlighting the potential for human-machine collaboration in healthcare data\nextraction.\n","authors":["Mohamed Sobhi Jabal","Pranav Warman","Jikai Zhang","Kartikeye Gupta","Ayush Jain","Maciej Mazurowski","Walter Wiggins","Kirti Magudia","Evan Calabrese"],"pdf_url":"https://arxiv.org/pdf/2409.10576v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11905v1","updated":"2024-09-18T12:05:30Z","published":"2024-09-18T12:05:30Z","title":"AlignBot: Aligning VLM-powered Customized Task Planning with User\n  Reminders Through Fine-Tuning for Household Robots","summary":"  This paper presents AlignBot, a novel framework designed to optimize\nVLM-powered customized task planning for household robots by effectively\naligning with user reminders. In domestic settings, aligning task planning with\nuser reminders poses significant challenges due to the limited quantity,\ndiversity, and multimodal nature of the reminders. To address these challenges,\nAlignBot employs a fine-tuned LLaVA-7B model, functioning as an adapter for\nGPT-4o. This adapter model internalizes diverse forms of user reminders-such as\npersonalized preferences, corrective guidance, and contextual assistance-into\nstructured instruction-formatted cues that prompt GPT-4o in generating\ncustomized task plans. Additionally, AlignBot integrates a dynamic retrieval\nmechanism that selects task-relevant historical successes as prompts for\nGPT-4o, further enhancing task planning accuracy. To validate the effectiveness\nof AlignBot, experiments are conducted in real-world household environments,\nwhich are constructed within the laboratory to replicate typical household\nsettings. A multimodal dataset with over 1,500 entries derived from volunteer\nreminders is used for training and evaluation. The results demonstrate that\nAlignBot significantly improves customized task planning, outperforming\nexisting LLM- and VLM-powered planners by interpreting and aligning with user\nreminders, achieving 86.8% success rate compared to the vanilla GPT-4o baseline\nat 21.6%, reflecting a 65% improvement and over four times greater\neffectiveness. Supplementary materials are available at:\nhttps://yding25.com/AlignBot/\n","authors":[" Zhaxizhuoma","Pengan Chen","Ziniu Wu","Jiawei Sun","Dong Wang","Peng Zhou","Nieqing Cao","Yan Ding","Bin Zhao","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2409.11905v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11860v1","updated":"2024-09-18T10:30:50Z","published":"2024-09-18T10:30:50Z","title":"Retrieve, Annotate, Evaluate, Repeat: Leveraging Multimodal LLMs for\n  Large-Scale Product Retrieval Evaluation","summary":"  Evaluating production-level retrieval systems at scale is a crucial yet\nchallenging task due to the limited availability of a large pool of\nwell-trained human annotators. Large Language Models (LLMs) have the potential\nto address this scaling issue and offer a viable alternative to humans for the\nbulk of annotation tasks. In this paper, we propose a framework for assessing\nthe product search engines in a large-scale e-commerce setting, leveraging\nMultimodal LLMs for (i) generating tailored annotation guidelines for\nindividual queries, and (ii) conducting the subsequent annotation task. Our\nmethod, validated through deployment on a large e-commerce platform,\ndemonstrates comparable quality to human annotations, significantly reduces\ntime and cost, facilitates rapid problem discovery, and provides an effective\nsolution for production-level quality control at scale.\n","authors":["Kasra Hosseini","Thomas Kober","Josip Krapac","Roland Vollgraf","Weiwei Cheng","Ana Peleteiro Ramallo"],"pdf_url":"https://arxiv.org/pdf/2409.11860v1.pdf","comment":"13 pages, 5 figures, 4 Tables"},{"id":"http://arxiv.org/abs/2212.08841v3","updated":"2024-09-18T09:09:07Z","published":"2022-12-17T10:43:25Z","title":"AugTriever: Unsupervised Dense Retrieval by Scalable Data Augmentation","summary":"  Dense retrievers have made significant strides in text retrieval and\nopen-domain question answering. However, most of these achievements have relied\nheavily on extensive human-annotated supervision. In this study, we aim to\ndevelop unsupervised methods for improving dense retrieval models. We propose\ntwo approaches that enable annotation-free and scalable training by creating\npseudo querydocument pairs: query extraction and transferred query generation.\nThe query extraction method involves selecting salient spans from the original\ndocument to generate pseudo queries. On the other hand, the transferred query\ngeneration method utilizes generation models trained for other NLP tasks, such\nas summarization, to produce pseudo queries. Through extensive experimentation,\nwe demonstrate that models trained using these augmentation methods can achieve\ncomparable, if not better, performance than multiple strong dense baselines.\nMoreover, combining these strategies leads to further improvements, resulting\nin superior performance of unsupervised dense retrieval, unsupervised domain\nadaptation and supervised finetuning, benchmarked on both BEIR and ODQA\ndatasets. Code and datasets are publicly available at\nhttps://github.com/salesforce/AugTriever.\n","authors":["Rui Meng","Ye Liu","Semih Yavuz","Divyansh Agarwal","Lifu Tu","Ning Yu","Jianguo Zhang","Meghana Bhat","Yingbo Zhou"],"pdf_url":"https://arxiv.org/pdf/2212.08841v3.pdf","comment":"DCAI24, October 25, 2024, Boise, ID"},{"id":"http://arxiv.org/abs/2409.11798v1","updated":"2024-09-18T08:30:20Z","published":"2024-09-18T08:30:20Z","title":"The Factuality of Large Language Models in the Legal Domain","summary":"  This paper investigates the factuality of large language models (LLMs) as\nknowledge bases in the legal domain, in a realistic usage scenario: we allow\nfor acceptable variations in the answer, and let the model abstain from\nanswering when uncertain. First, we design a dataset of diverse factual\nquestions about case law and legislation. We then use the dataset to evaluate\nseveral LLMs under different evaluation methods, including exact, alias, and\nfuzzy matching. Our results show that the performance improves significantly\nunder the alias and fuzzy matching methods. Further, we explore the impact of\nabstaining and in-context examples, finding that both strategies enhance\nprecision. Finally, we demonstrate that additional pre-training on legal\ndocuments, as seen with SaulLM, further improves factual precision from 63% to\n81%.\n","authors":["Rajaa El Hamdani","Thomas Bonald","Fragkiskos Malliaros","Nils Holzenberger","Fabian Suchanek"],"pdf_url":"https://arxiv.org/pdf/2409.11798v1.pdf","comment":"CIKM 2024, short paper"},{"id":"http://arxiv.org/abs/2409.11728v1","updated":"2024-09-18T06:33:11Z","published":"2024-09-18T06:33:11Z","title":"Active Reconfigurable Intelligent Surface Empowered Synthetic Aperture\n  Radar Imaging","summary":"  Synthetic Aperture Radar (SAR) utilizes the movement of the radar antenna\nover a specific area of interest to achieve higher spatial resolution imaging.\nIn this paper, we aim to investigate the realization of SAR imaging for a\nstationary radar system with the assistance of active reconfigurable\nintelligent surface (ARIS) mounted on an unmanned aerial vehicle (UAV). As the\nUAV moves along the stationary trajectory, the ARIS can not only build a\nhigh-quality virtual line-of-sight (LoS) propagation path, but its mobility can\nalso effectively create a much larger virtual aperture, which can be utilized\nto realize a SAR system. In this paper, we first present a range-Doppler (RD)\nimaging algorithm to obtain imaging results for the proposed ARIS-empowered SAR\nsystem. Then, to further improve the SAR imaging performance, we attempt to\noptimize the reflection coefficients of ARIS to maximize the signal-to-noise\nratio (SNR) at the stationary radar receiver under the constraints of ARIS\nmaximum power and amplification factor. An effective algorithm based on\nfractional programming (FP) and majorization minimization (MM) methods is\ndeveloped to solve the resulting non-convex problem. Simulation results\nvalidate the effectiveness of ARIS-assisted SAR imaging and our proposed RD\nimaging and ARIS optimization algorithms.\n","authors":["Yifan Sun","Rang Liu","Zhiping Lu","Honghao Luo","Ming Li","Qian Liu"],"pdf_url":"https://arxiv.org/pdf/2409.11728v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11699v1","updated":"2024-09-18T04:43:41Z","published":"2024-09-18T04:43:41Z","title":"FLARE: Fusing Language Models and Collaborative Architectures for\n  Recommender Enhancement","summary":"  Hybrid recommender systems, combining item IDs and textual descriptions,\noffer potential for improved accuracy. However, previous work has largely\nfocused on smaller datasets and model architectures. This paper introduces\nFlare (Fusing Language models and collaborative Architectures for Recommender\nEnhancement), a novel hybrid recommender that integrates a language model (mT5)\nwith a collaborative filtering model (Bert4Rec) using a Perceiver network. This\narchitecture allows Flare to effectively combine collaborative and content\ninformation for enhanced recommendations.\n  We conduct a two-stage evaluation, first assessing Flare's performance\nagainst established baselines on smaller datasets, where it demonstrates\ncompetitive accuracy. Subsequently, we evaluate Flare on a larger, more\nrealistic dataset with a significantly larger item vocabulary, introducing new\nbaselines for this setting. Finally, we showcase Flare's inherent ability to\nsupport critiquing, enabling users to provide feedback and refine\nrecommendations. We further leverage critiquing as an evaluation method to\nassess the model's language understanding and its transferability to the\nrecommendation task.\n","authors":["Liam Hebert","Marialena Kyriakidi","Hubert Pham","Krishna Sayana","James Pine","Sukhdeep Sodhi","Ambarish Jash"],"pdf_url":"https://arxiv.org/pdf/2409.11699v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11695v1","updated":"2024-09-18T04:31:22Z","published":"2024-09-18T04:31:22Z","title":"Basket-Enhanced Heterogenous Hypergraph for Price-Sensitive Next Basket\n  Recommendation","summary":"  Next Basket Recommendation (NBR) is a new type of recommender system that\npredicts combinations of items users are likely to purchase together. Existing\nNBR models often overlook a crucial factor, which is price, and do not fully\ncapture item-basket-user interactions. To address these limitations, we propose\na novel method called Basket-augmented Dynamic Heterogeneous Hypergraph (BDHH).\nBDHH utilizes a heterogeneous multi-relational graph to capture the intricate\nrelationships among item features, with price as a critical factor. Moreover,\nour approach includes a basket-guided dynamic augmentation network that could\ndynamically enhances item-basket-user interactions. Experiments on real-world\ndatasets demonstrate that BDHH significantly improves recommendation accuracy,\nproviding a more comprehensive understanding of user behavior.\n","authors":["Yuening Zhou","Yulin Wang","Qian Cui","Xinyu Guan","Francisco Cisternas"],"pdf_url":"https://arxiv.org/pdf/2409.11695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10992v2","updated":"2024-09-18T04:08:44Z","published":"2024-09-17T08:51:02Z","title":"A Best-of-Both Approach to Improve Match Predictions and Reciprocal\n  Recommendations for Job Search","summary":"  Matching users with mutual preferences is a critical aspect of services\ndriven by reciprocal recommendations, such as job search. To produce\nrecommendations in such scenarios, one can predict match probabilities and\nconstruct rankings based on these predictions. However, this direct match\nprediction approach often underperforms due to the extreme sparsity of match\nlabels. Therefore, most existing methods predict preferences separately for\neach direction (e.g., job seeker to employer and employer to job seeker) and\nthen aggregate the predictions to generate overall matching scores and produce\nrecommendations. However, this typical approach often leads to practical\nissues, such as biased error propagation between the two models. This paper\nintroduces and demonstrates a novel and practical solution to improve\nreciprocal recommendations in production by leveraging pseudo-match scores.\nSpecifically, our approach generates dense and more directly relevant\npseudo-match scores by combining the true match labels, which are accurate but\nsparse, with relatively inaccurate but dense match predictions. We then train a\nmeta-model to output the final match predictions by minimizing the prediction\nloss against the pseudo-match scores. Our method can be seen as a best-of-both\n(BoB) approach, as it combines the high-level ideas of both direct match\nprediction and the two separate models approach. It also allows for\nuser-specific weights to construct personalized pseudo-match scores, achieving\neven better matching performance through appropriate tuning of the weights.\nOffline experiments on real-world job search data demonstrate the superior\nperformance of our BoB method, particularly with personalized pseudo-match\nscores, compared to existing approaches in terms of finding potential matches.\n","authors":["Shuhei Goda","Yudai Hayashi","Yuta Saito"],"pdf_url":"https://arxiv.org/pdf/2409.10992v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11678v1","updated":"2024-09-18T03:34:31Z","published":"2024-09-18T03:34:31Z","title":"An Enhanced-State Reinforcement Learning Algorithm for Multi-Task Fusion\n  in Large-Scale Recommender Systems","summary":"  As the last key stage of Recommender Systems (RSs), Multi-Task Fusion (MTF)\nis in charge of combining multiple scores predicted by Multi-Task Learning\n(MTL) into a final score to maximize user satisfaction, which decides the\nultimate recommendation results. In recent years, to maximize long-term user\nsatisfaction within a recommendation session, Reinforcement Learning (RL) is\nwidely used for MTF in large-scale RSs. However, limited by their modeling\npattern, all the current RL-MTF methods can only utilize user features as the\nstate to generate actions for each user, but unable to make use of item\nfeatures and other valuable features, which leads to suboptimal results.\nAddressing this problem is a challenge that requires breaking through the\ncurrent modeling pattern of RL-MTF. To solve this problem, we propose a novel\nmethod called Enhanced-State RL for MTF in RSs. Unlike the existing methods\nmentioned above, our method first defines user features, item features, and\nother valuable features collectively as the enhanced state; then proposes a\nnovel actor and critic learning process to utilize the enhanced state to make\nmuch better action for each user-item pair. To the best of our knowledge, this\nnovel modeling pattern is being proposed for the first time in the field of\nRL-MTF. We conduct extensive offline and online experiments in a large-scale\nRS. The results demonstrate that our model outperforms other models\nsignificantly. Enhanced-State RL has been fully deployed in our RS more than\nhalf a year, improving +3.84% user valid consumption and +0.58% user duration\ntime compared to baseline.\n","authors":["Peng Liu","Jiawei Zhu","Cong Xu","Ming Zhao","Bin Wang"],"pdf_url":"https://arxiv.org/pdf/2409.11678v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2404.17589"},{"id":"http://arxiv.org/abs/2409.11629v1","updated":"2024-09-18T01:23:26Z","published":"2024-09-18T01:23:26Z","title":"Designing Interfaces for Multimodal Vector Search Applications","summary":"  Multimodal vector search offers a new paradigm for information retrieval by\nexposing numerous pieces of functionality which are not possible in traditional\nlexical search engines. While multimodal vector search can be treated as a drop\nin replacement for these traditional systems, the experience can be\nsignificantly enhanced by leveraging the unique capabilities of multimodal\nsearch. Central to any information retrieval system is a user who expresses an\ninformation need, traditional user interfaces with a single search bar allow\nusers to interact with lexical search systems effectively however are not\nnecessarily optimal for multimodal vector search. In this paper we explore\nnovel capabilities of multimodal vector search applications utilising CLIP\nmodels and present implementations and design patterns which better allow users\nto express their information needs and effectively interact with these systems\nin an information retrieval context.\n","authors":["Owen Pendrigh Elliott","Tom Hamer","Jesse Clark"],"pdf_url":"https://arxiv.org/pdf/2409.11629v1.pdf","comment":"12 pages, 8 figures, CIKM 2024 MMSR Workshop"}],"Multimedia":[{"id":"http://arxiv.org/abs/2303.16199v3","updated":"2024-09-18T23:54:36Z","published":"2023-03-28T17:59:12Z","title":"LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init\n  Attention","summary":"  We present LLaMA-Adapter, a lightweight adaption method to efficiently\nfine-tune LLaMA into an instruction-following model. Using 52K self-instruct\ndemonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon\nthe frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8\nA100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and\nprepend them to the word tokens at higher transformer layers. Then, a\nzero-initialized attention mechanism with zero gating is proposed, which\nadaptively injects the new instructional cues into LLaMA, while effectively\npreserves its pre-trained knowledge. With our efficient training, LLaMA-Adapter\ncan generate high-quality responses, comparable to Alpaca with fully fine-tuned\n7B parameters. Besides language commands, our approach can be simply extended\nto multi-modal instructions for learning image-conditioned LLaMA model, which\nachieves superior reasoning performance on ScienceQA and COCO Caption\nbenchmarks. Furthermore, we also evaluate the zero-initialized attention\nmechanism for fine-tuning other pre-trained models (ViT, RoBERTa) on\ntraditional vision and language tasks, demonstrating the superior\ngeneralization capacity of our approach. Code is released at\nhttps://github.com/OpenGVLab/LLaMA-Adapter.\n","authors":["Renrui Zhang","Jiaming Han","Chris Liu","Peng Gao","Aojun Zhou","Xiangfei Hu","Shilin Yan","Pan Lu","Hongsheng Li","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2303.16199v3.pdf","comment":"Accepted by ICLR 2024. Code is available at\n  https://github.com/OpenGVLab/LLaMA-Adapter"},{"id":"http://arxiv.org/abs/2409.12319v1","updated":"2024-09-18T21:17:27Z","published":"2024-09-18T21:17:27Z","title":"Large Language Models Are Strong Audio-Visual Speech Recognition\n  Learners","summary":"  Multimodal large language models (MLLMs) have recently become a focal point\nof research due to their formidable multimodal understanding capabilities. For\nexample, in the audio and speech domains, an LLM can be equipped with\n(automatic) speech recognition (ASR) abilities by just concatenating the audio\ntokens, computed with an audio encoder, and the text tokens to achieve\nstate-of-the-art results. On the contrary, tasks like visual and audio-visual\nspeech recognition (VSR/AVSR), which also exploit noise-invariant lip movement\ninformation, have received little or no attention. To bridge this gap, we\npropose Llama-AVSR, a new MLLM with strong audio-visual speech recognition\ncapabilities. It leverages pre-trained audio and video encoders to produce\nmodality-specific tokens which, together with the text tokens, are processed by\na pre-trained LLM (e.g., Llama3.1-8B) to yield the resulting response in an\nauto-regressive fashion. Llama-AVSR requires a small number of trainable\nparameters as only modality-specific projectors and LoRA modules are trained\nwhereas the multi-modal encoders and LLM are kept frozen. We evaluate our\nproposed approach on LRS3, the largest public AVSR benchmark, and we achieve\nnew state-of-the-art results for the tasks of ASR and AVSR with a WER of 0.81%\nand 0.77%, respectively. To bolster our results, we investigate the key factors\nthat underpin the effectiveness of Llama-AVSR: the choice of the pre-trained\nencoders and LLM, the efficient integration of LoRA modules, and the optimal\nperformance-efficiency trade-off obtained via modality-aware compression rates.\n","authors":["Umberto Cappellazzo","Minsu Kim","Honglie Chen","Pingchuan Ma","Stavros Petridis","Daniele Falavigna","Alessio Brutti","Maja Pantic"],"pdf_url":"https://arxiv.org/pdf/2409.12319v1.pdf","comment":"The code will be made available at this link:\n  https://github.com/umbertocappellazzo/AVSR-LLMs"},{"id":"http://arxiv.org/abs/2409.12193v1","updated":"2024-09-18T17:59:44Z","published":"2024-09-18T17:59:44Z","title":"Vista3D: Unravel the 3D Darkside of a Single Image","summary":"  We embark on the age-old quest: unveiling the hidden dimensions of objects\nfrom mere glimpses of their visible parts. To address this, we present Vista3D,\na framework that realizes swift and consistent 3D generation within a mere 5\nminutes. At the heart of Vista3D lies a two-phase approach: the coarse phase\nand the fine phase. In the coarse phase, we rapidly generate initial geometry\nwith Gaussian Splatting from a single image. In the fine phase, we extract a\nSigned Distance Function (SDF) directly from learned Gaussian Splatting,\noptimizing it with a differentiable isosurface representation. Furthermore, it\nelevates the quality of generation by using a disentangled representation with\ntwo independent implicit functions to capture both visible and obscured aspects\nof objects. Additionally, it harmonizes gradients from 2D diffusion prior with\n3D-aware diffusion priors by angular diffusion prior composition. Through\nextensive evaluation, we demonstrate that Vista3D effectively sustains a\nbalance between the consistency and diversity of the generated 3D objects.\nDemos and code will be available at https://github.com/florinshen/Vista3D.\n","authors":["Qiuhong Shen","Xingyi Yang","Michael Bi Mi","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2409.12193v1.pdf","comment":"ECCV'2024"},{"id":"http://arxiv.org/abs/2409.12140v1","updated":"2024-09-18T17:03:30Z","published":"2024-09-18T17:03:30Z","title":"MoRAG -- Multi-Fusion Retrieval Augmented Generation for Human Motion","summary":"  We introduce MoRAG, a novel multi-part fusion based retrieval-augmented\ngeneration strategy for text-based human motion generation. The method enhances\nmotion diffusion models by leveraging additional knowledge obtained through an\nimproved motion retrieval process. By effectively prompting large language\nmodels (LLMs), we address spelling errors and rephrasing issues in motion\nretrieval. Our approach utilizes a multi-part retrieval strategy to improve the\ngeneralizability of motion retrieval across the language space. We create\ndiverse samples through the spatial composition of the retrieved motions.\nFurthermore, by utilizing low-level, part-specific motion information, we can\nconstruct motion samples for unseen text descriptions. Our experiments\ndemonstrate that our framework can serve as a plug-and-play module, improving\nthe performance of motion diffusion models. Code, pretrained models and sample\nvideos will be made available at: https://motion-rag.github.io/\n","authors":["Kalakonda Sai Shashank","Shubh Maheshwari","Ravi Kiran Sarvadevabhatla"],"pdf_url":"https://arxiv.org/pdf/2409.12140v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.05508v2","updated":"2024-09-18T09:36:32Z","published":"2024-02-08T09:37:12Z","title":"Performance Evaluation of Associative Watermarking Using Statistical\n  Neurodynamics","summary":"  We theoretically evaluated the performance of our proposed associative\nwatermarking method in which the watermark is not embedded directly into the\nimage. We previously proposed a watermarking method that extends the\nzero-watermarking model by applying associative memory models. In this model,\nthe hetero-associative memory model is introduced to the mapping process\nbetween image features and watermarks, and the auto-associative memory model is\napplied to correct watermark errors. We herein show that the associative\nwatermarking model outperforms the zero-watermarking model through computer\nsimulations using actual images. In this paper, we describe how we derive the\nmacroscopic state equation for the associative watermarking model using the\nOkada theory. The theoretical results obtained by the fourth-order theory were\nin good agreement with those obtained by computer simulations. Furthermore, the\nperformance of the associative watermarking model was evaluated using the bit\nerror rate of the watermark, both theoretically and using computer simulations.\n","authors":["Ryoto Kanegae","Masaki Kawamura"],"pdf_url":"https://arxiv.org/pdf/2402.05508v2.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2409.11786v1","updated":"2024-09-18T08:10:35Z","published":"2024-09-18T08:10:35Z","title":"Efficient Low-Resolution Face Recognition via Bridge Distillation","summary":"  Face recognition in the wild is now advancing towards light-weight models,\nfast inference speed and resolution-adapted capability. In this paper, we\npropose a bridge distillation approach to turn a complex face model pretrained\non private high-resolution faces into a light-weight one for low-resolution\nface recognition. In our approach, such a cross-dataset resolution-adapted\nknowledge transfer problem is solved via two-step distillation. In the first\nstep, we conduct cross-dataset distillation to transfer the prior knowledge\nfrom private high-resolution faces to public high-resolution faces and generate\ncompact and discriminative features. In the second step, the resolution-adapted\ndistillation is conducted to further transfer the prior knowledge to synthetic\nlow-resolution faces via multi-task learning. By learning low-resolution face\nrepresentations and mimicking the adapted high-resolution knowledge, a\nlight-weight student model can be constructed with high efficiency and\npromising accuracy in recognizing low-resolution faces. Experimental results\nshow that the student model performs impressively in recognizing low-resolution\nfaces with only 0.21M parameters and 0.057MB memory. Meanwhile, its speed\nreaches up to 14,705, ~934 and 763 faces per second on GPU, CPU and mobile\nphone, respectively.\n","authors":["Shiming Ge","Shengwei Zhao","Chenyu Li","Yu Zhang","Jia Li"],"pdf_url":"https://arxiv.org/pdf/2409.11786v1.pdf","comment":"This paper is published in IEEE TIP 2020"},{"id":"http://arxiv.org/abs/2203.13883v7","updated":"2024-09-18T07:35:46Z","published":"2022-03-25T19:45:33Z","title":"Multi-modal Misinformation Detection: Approaches, Challenges and\n  Opportunities","summary":"  As social media platforms are evolving from text-based forums into\nmulti-modal environments, the nature of misinformation in social media is also\ntransforming accordingly. Taking advantage of the fact that visual modalities\nsuch as images and videos are more favorable and attractive to the users and\ntextual contents are sometimes skimmed carelessly, misinformation spreaders\nhave recently targeted contextual connections between the modalities e.g., text\nand image. Hence many researchers have developed automatic techniques for\ndetecting possible cross-modal discordance in web-based content. We analyze,\ncategorize and identify existing approaches in addition to challenges and\nshortcomings they face in order to unearth new research opportunities in the\nfield of multi-modal misinformation detection.\n","authors":["Sara Abdali","Sina shaham","Bhaskar Krishnamachari"],"pdf_url":"https://arxiv.org/pdf/2203.13883v7.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11729v1","updated":"2024-09-18T06:38:48Z","published":"2024-09-18T06:38:48Z","title":"DETECLAP: Enhancing Audio-Visual Representation Learning with Object\n  Information","summary":"  Current audio-visual representation learning can capture rough object\ncategories (e.g., ``animals'' and ``instruments''), but it lacks the ability to\nrecognize fine-grained details, such as specific categories like ``dogs'' and\n``flutes'' within animals and instruments. To address this issue, we introduce\nDETECLAP, a method to enhance audio-visual representation learning with object\ninformation. Our key idea is to introduce an audio-visual label prediction loss\nto the existing Contrastive Audio-Visual Masked AutoEncoder to enhance its\nobject awareness. To avoid costly manual annotations, we prepare object labels\nfrom both audio and visual inputs using state-of-the-art language-audio models\nand object detectors. We evaluate the method of audio-visual retrieval and\nclassification using the VGGSound and AudioSet20K datasets. Our method achieves\nimprovements in recall@10 of +1.5% and +1.2% for audio-to-visual and\nvisual-to-audio retrieval, respectively, and an improvement in accuracy of\n+0.6% for audio-visual classification.\n","authors":["Shota Nakada","Taichi Nishimura","Hokuto Munakata","Masayoshi Kondo","Tatsuya Komatsu"],"pdf_url":"https://arxiv.org/pdf/2409.11729v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2407.19493v2","updated":"2024-09-18T00:31:27Z","published":"2024-07-28T13:23:43Z","title":"Official-NV: An LLM-Generated News Video Dataset for Multimodal Fake\n  News Detection","summary":"  News media, especially video news media, have penetrated into every aspect of\ndaily life, which also brings the risk of fake news. Therefore, multimodal fake\nnews detection has recently garnered increased attention. However, the existing\ndatasets are comprised of user-uploaded videos and contain an excess amounts of\nsuperfluous data, which introduces noise into the model training process. To\naddress this issue, we construct a dataset named Official-NV, comprising\nofficially published news videos. The crawl officially published videos are\naugmented through the use of LLMs-based generation and manual verification,\nthereby expanding the dataset. Furthermore, the proposed dataset is benchmarked\nagainst several baselines to demonstrate its effectiveness in multimodal news\ndetection.\n","authors":["Yihao Wang","Lizhi Chen","Zhong Qian","Peifeng Li"],"pdf_url":"https://arxiv.org/pdf/2407.19493v2.pdf","comment":null}]}}